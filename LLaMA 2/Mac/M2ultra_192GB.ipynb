{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da7f75",
   "metadata": {},
   "source": [
    "Prepare Data & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[1m\u001b[36m70B-v2\u001b[m\u001b[m                  \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m13B-v2\u001b[m\u001b[m                  \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     \u001b[1m\u001b[36m7B-v2\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                     ggml-vocab.bin\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install Python dependencies\n",
    "# !python3 -m pip install -r requirements.txt\n",
    "\n",
    "# # llama 2\n",
    "# # convert the model to ggml FP16 format\n",
    "# !python3 convert.py --outfile models/7B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-7b/\n",
    "# !python3 convert.py --outfile models/13B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-13b/\n",
    "# !python3 convert.py --outfile models/70B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-70b/\n",
    "\n",
    "# # quantize the model to 4-bits (using q4_0 method)\n",
    "# !./quantize ./models/7B-v2/ggml-model-f16.bin ./models/7B-v2/ggml-model-q4_0.bin q4_0\n",
    "# !./quantize ./models/13B-v2/ggml-model-f16.bin ./models/13B-v2/ggml-model-q4_0.bin q4_0\n",
    "# !./quantize ./models/70B-v2/ggml-model-f16.bin ./models/70B-v2/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "embd-input-test\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/common.cpp -o common.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/console.cpp -o console.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/grammar-parser.cpp -o grammar-parser.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml-alloc.c -o ggml-alloc.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-metal.o ggml-alloc.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o ggml-alloc.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders \n",
      "c++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295714\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x125704200\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1257069a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x125706c60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x125707470\n",
      "ggml_metal_init: loaded kernel_scale                          0x125707f10\n",
      "ggml_metal_init: loaded kernel_silu                           0x125708790\n",
      "ggml_metal_init: loaded kernel_relu                           0x125706120\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14e0168a0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14e0172f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14e018b10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14e017890\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14e017af0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14e019430\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14e0197f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14e01ac70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14e01b620\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14e01bfa0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14e01c950\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14e01d430\n",
      "ggml_metal_init: loaded kernel_norm                           0x14e01e410\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14e306700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14e307620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14e307b40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14e308550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14e308fe0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x14e309b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x14e30a570\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x14e30c370\n",
      "ggml_metal_init: loaded kernel_rope                           0x14e30b920\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x14e30cbb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14e30dae0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14e30e6d0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14e30f280\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make a positive difference in the world. październik 2018\n",
      "I am not sure what the meaning of life is and I don’t think anybody else knows either. I do know that it is important for each person to decide what the meaning of their own life is, as well as to live up to their own expectations. I like to think that we are here to make a difference in this world, to contribute to something positive and inspire others along the way.\n",
      "What the meaning of my life is, however, may be different from what you consider your calling to be. That’s okay! I don’t know any two people who share the same purpose in life. We are all unique individuals with a unique mission. There is no one else just like us and there never will be (unless we are clones).\n",
      "I like to think that the meaning of life lies within each individual and that it is our job as humanity to figure out how to make ourselves happy. I know this might seem selfish but I believe it is part of what makes us so amazing. We have the ability to create anything we set our minds to doing!\n",
      "The first step in figuring out your meaning of life is setting goals for yourself and then working hard towards achieving them on a daily basis.\n",
      "This blog post provides some tips on how to do this: https://www.lifehack.org/articles/communication/how-to-find-your-purpose-in-this-crazy-world.html\n",
      "The second step is figuring out what matters most in your life and then making sure that everything you are doing aligns with those values. This will help ensure that whatever it is we’re working towards achieving will truly make us happy.\n",
      "The third step would be identifying what makes up our own unique mission as individuals which could include anything from helping others around us or simply taking care of ourselves first before worrying about anyone else!\n",
      "I am not sure what the meaning of life is and I don’t think anybody else knows either.\n",
      "But I do know that it is important for each person to decide what the meaning of their own life is, as well as live up to their expectations.\n",
      "I like to think that we are here to make a difference in this world, to contribute something positive and inspire others along the way.\n",
      "What the meaning of my life is may be different from what it means for you or\n",
      "llama_print_timings:        load time =   989.75 ms\n",
      "llama_print_timings:      sample time =   346.38 ms /   512 runs   (    0.68 ms per token,  1478.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3593.77 ms /   265 tokens (   13.56 ms per token,    73.74 tokens per second)\n",
      "llama_print_timings:        eval time =  6023.60 ms /   510 runs   (   11.81 ms per token,    84.67 tokens per second)\n",
      "llama_print_timings:       total time = 10006.52 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295725\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x142e17600\n",
      "ggml_metal_init: loaded kernel_add_row                        0x142e19c80\n",
      "ggml_metal_init: loaded kernel_mul                            0x142e19f40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x142e1a780\n",
      "ggml_metal_init: loaded kernel_scale                          0x142e1b220\n",
      "ggml_metal_init: loaded kernel_silu                           0x142e1ba80\n",
      "ggml_metal_init: loaded kernel_relu                           0x142e19400\n",
      "ggml_metal_init: loaded kernel_gelu                           0x142e1c250\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x142e1cd20\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x142e1e7f0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x142e1ea50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x142e1d760\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x142e1fef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x142e1f310\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x142e20790\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x142e210f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x142e21a50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x142e223b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100d05e80\n",
      "ggml_metal_init: loaded kernel_norm                           0x100d06f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100d07630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100d07ff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100d08ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100d095d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100d09fe0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100d0ab70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100d0b580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100d0bf90\n",
      "ggml_metal_init: loaded kernel_rope                           0x100d0c660\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100d0daa0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100d0e9c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100d0f5b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100d060e0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. everybody has their own definition of what happiness means, but you only get one shot at it so go for it.\n",
      "I was born in Toronto and grew up in a family business. My parents are entrepreneurs, so that's how I grew up, working with people every day.\n",
      "I love to eat healthy food. I don't believe in any kind of diet or anything like that. I try to be healthy as much as I can and balance it out.\n",
      "To me, fashion is about the freedom to express yourself through your style - what makes you feel confident and powerful.\n",
      "I love a challenge - so I find a lot of different ways to exercise throughout the day, whether it's walking or jumping rope with my little brother.\n",
      "I think that if you have good energy, you can do anything! You can go through any problem because positive energy helps you find positive solutions. And when you feel good, you look good! I am a big believer in karma and the fact that everything comes back to you. So, I try to treat everyone with kindness no matter what they are doing or how I feel about them at the moment.\n",
      "I'm happy when my family and friends are happy; when we spend time together, I get a lot of satisfaction from that. And I enjoy being creative - whether it be through cooking, fashion or acting! I also love being outside in nature with my dog!\n",
      "There are so many things that make me happy, but I think what makes me feel the most fulfilled is when people come up to me and tell me they've been inspired by something I said or did. It makes me feel like I can actually make a difference - even if it's just one person!\n",
      "I love my family; they are everything to me, so I am always happy when we spend time together. My friends are also very important in my life and I try to be there for them whenever possible. In addition, I have been fortunate enough to travel the world and experience different cultures - which has made me a better person and given me perspective on life!\n",
      "I believe that you can't control everything around you, but if you focus on being happy within yourself then nothing else matters. We all want to be successful and accomplish our dreams; however, it doesn't matter if we are rich or poor - what is important is who we are as people inside and\n",
      "llama_print_timings:        load time =   555.09 ms\n",
      "llama_print_timings:      sample time =   319.63 ms /   512 runs   (    0.62 ms per token,  1601.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3572.00 ms /   265 tokens (   13.48 ms per token,    74.19 tokens per second)\n",
      "llama_print_timings:        eval time =  5999.47 ms /   510 runs   (   11.76 ms per token,    85.01 tokens per second)\n",
      "llama_print_timings:       total time =  9931.98 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295736\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104726940\n",
      "ggml_metal_init: loaded kernel_add_row                        0x104728b10\n",
      "ggml_metal_init: loaded kernel_mul                            0x104728dd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1047295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10472a0a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10472a910\n",
      "ggml_metal_init: loaded kernel_relu                           0x1047282e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10472b290\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10472c240\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10472c660\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10472d840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10472cb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10472ed50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10472e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10472f680\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104730030\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1047309b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104731330\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104731e10\n",
      "ggml_metal_init: loaded kernel_norm                           0x104732e10\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104733c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104734630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104735050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104735a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104736600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104736fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104737800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x104738210\n",
      "ggml_metal_init: loaded kernel_rope                           0x104738cf0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104738f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104739ad0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10473a9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1047325f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make yourself immortal. Unterscheidung der Gleichungsarten. The Meaning Of Life Quotes Showing 1-30 of 95 “The meaning of life, as far as I can tell, is to die avoiding annoying people.” ― Ray Bradbury tags: meaning , philosophy , the-meaning-of-life . In an interview with New Scientist in 2007, Dawkins explained that he was looking for \"a phrase that embodies what evolutionary biology is really all about.\" The Meaning of Life Lyrics: I've been thinking / A lot lately / About the meaning of life / You know they say you only go around once / Is this all there is? 2019/04/17 20:53. The world is full of suffering. I’ll also briefly cover the differences between the two, and why it might be useful to look into both of them. The meaning of life is that you have the ability to choose your response to any situation - whether it's a good or bad one. I am looking for something more than just a funny quote. Life’s ultimate meaning is found in the God who made us and loves us.” ― Rick Warren tags: god , life , love . He’d been thinking about it so much, he decided to turn that thought into an existential crisis movie titled Meaning of Life. (12-03-2020) 08:42 AM. By using this website you consent to our use of cookies. I was born on a farm in 1956. The meaning of life is something that many people have contemplated for thousands of years, and still haven’t quite found the answer to. This page is about the various possible words that rhymes or sounds like meaning of life .This page … Meaning of Life is the third studio album by American rapper Kid Cudi.It was released on September 30, 2019 through N.E.R.D., Shake it Up, Mad Solar and Republic Records.The album is a concept album that explores various themes including drug addiction, depression and anxiety.. I’m not sure what the meaning of life is. The Meaning of Life, a 1983 British film by Terry Jones ; The Meaning of the Mov\n",
      "llama_print_timings:        load time =   563.93 ms\n",
      "llama_print_timings:      sample time =   316.32 ms /   512 runs   (    0.62 ms per token,  1618.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3482.98 ms /   265 tokens (   13.14 ms per token,    76.08 tokens per second)\n",
      "llama_print_timings:        eval time =  6029.59 ms /   510 runs   (   11.82 ms per token,    84.58 tokens per second)\n",
      "llama_print_timings:       total time =  9868.78 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295747\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102726940\n",
      "ggml_metal_init: loaded kernel_add_row                        0x102728b10\n",
      "ggml_metal_init: loaded kernel_mul                            0x102728dd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1027295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10272a0a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10272a910\n",
      "ggml_metal_init: loaded kernel_relu                           0x1027282e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10272b290\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10272c240\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10272c660\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10272d840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10272cb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10272ed50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10272e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10272f680\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x102730030\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1027309b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x102731330\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102731e10\n",
      "ggml_metal_init: loaded kernel_norm                           0x102732e10\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x102733c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102734630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102735050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x102735a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x102736600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x102736fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x102737800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x102738210\n",
      "ggml_metal_init: loaded kernel_rope                           0x102738cf0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x102738f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x102739ad0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10273a9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1027325f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to learn about yourself and others. That may sound like a simple statement, but it's not. It means you have to be willing to take risks, explore your passions and share your knowledge with those around you.\n",
      "The only way to do any of these things effectively is to understand who you are and how you work. You can't just go through life doing what other people tell you and assuming that you will turn out okay in the end. You have to figure out what makes you tick, so you can use it as a guide for your actions. If you don't know where you're going or why then every step along the way becomes meaningless because at some point down the road something happens and causes everything else to unravel like an old sweater that has lost its elasticity.\n",
      "The first step is figuring out who you are as a person so you can make decisions based on your beliefs instead of someone else's opinions about what they think makes sense or not (this includes parents too). The second step is finding ways to express yourself creatively while staying true to who you really are inside and out. This may sound like common sense advice but sometimes people forget this important rule: if it doesn't feel good then don't do it!\n",
      "In conclusion, the meaning of life is simple but not easy; we should all strive for happiness by being ourselves at every opportunity possible whether through art or other creative outlets such as writing poetry etcetera .\n",
      "The Meaning Of Life Is To Discover Yourself And Others\n",
      "The Meaning Of Life Is To Learn About The World Around You\n",
      "The Meaning Of Life Is To Find Your Purpose In Life\n",
      "The Meaning Of Life Is To Experience Happiness And Joy\n",
      "The Meaning Of Life Is To Give Back To Others\n",
      "The Meaning Of Life Is To Live It To Its Fullest Potential\n",
      "The Meaning Of Life Is To Understand The Value Of Time And Money\n",
      "The Meaning Of Life Is To Find Your Passion In Life\n",
      "The Meaning Of Life Is To Have Fun And Be Happy\n",
      "The Meaning of life is to discover yourself and others.\n",
      "It's a lot simpler than you think, isn't it? This doesn't mean that your entire life should be spent by yourself in your room with no one else around you at all times (though if this sounds like something enjoyable for YOU\n",
      "llama_print_timings:        load time =  2972.57 ms\n",
      "llama_print_timings:      sample time =   318.30 ms /   512 runs   (    0.62 ms per token,  1608.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3615.47 ms /   265 tokens (   13.64 ms per token,    73.30 tokens per second)\n",
      "llama_print_timings:        eval time = 18682.39 ms /   510 runs   (   36.63 ms per token,    27.30 tokens per second)\n",
      "llama_print_timings:       total time = 22655.29 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295772\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x124526940\n",
      "ggml_metal_init: loaded kernel_add_row                        0x124528b10\n",
      "ggml_metal_init: loaded kernel_mul                            0x124528dd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1245295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x12452a0a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x12452a910\n",
      "ggml_metal_init: loaded kernel_relu                           0x1245282e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12452b290\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12452c240\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12452c660\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12452d840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12452cb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12452ed50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12452e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12452f680\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x124530030\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1245309b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x124531330\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x124531e10\n",
      "ggml_metal_init: loaded kernel_norm                           0x124532e10\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x124533c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x124534630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x124535050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x124535a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x124536600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x124536fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x124537800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x124538210\n",
      "ggml_metal_init: loaded kernel_rope                           0x124538cf0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x124538f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x124539ad0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12453a9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1245325f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be in harmony with our environment. So, for me, having a healthy body and mind is just as important as a healthy planet.\n",
      "For many years I was addicted to coffee and tea, but recently I have been able to give up both completely (more on that below).\n",
      "Through my experience in the vegan community I learned about how much our bodies love plant based foods. So now I try to eat mostly whole-plant foods and limit my intake of animal products as much as possible.\n",
      "This has been a journey, which is still ongoing, but so far it has made me feel physically and mentally better than ever before.\n",
      "I’m sharing this because the vegan diet can be misunderstood by many people who are not familiar with it. I believe that the way we eat is one of our greatest opportunities to improve our health and happiness, especially when we become more conscious about how food affects us on a physical, emotional and spiritual level.\n",
      "In this post I want to highlight 5 things vegans should know about their diet.\n",
      "1) You don’t have to give up anything you love\n",
      "I used to think that being vegan meant not eating cheese and chocolate anymore, and that was a big downside for me. So my first time as a vegan I gave it up pretty quickly because of the lack of variety in my diet. But then I learned about whole-plant foods like avocados, nuts and seeds, fresh fruits and vegetables (raw or cooked) and that’s when everything changed.\n",
      "Nowadays, even though I don’t eat any animal products, I still enjoy a lot of delicious food!\n",
      "2) It is not expensive to be vegan (if you know how to shop for it)\n",
      "I often hear complaints about how expensive it is to be vegan and yes, if you are shopping at stores that don’t have many plant-based options, then the cost might be high. But let me tell you a secret: there are so many cheap ways to eat healthy on a vegan diet!\n",
      "For example, I shop for my groceries at the farmer’s market or the local organic food store where they sell fruits, vegetables and grains in large quantities. When buying food in bulk you can save a lot of money (and also\n",
      "llama_print_timings:        load time =  1701.59 ms\n",
      "llama_print_timings:      sample time =   318.19 ms /   512 runs   (    0.62 ms per token,  1609.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3610.99 ms /   265 tokens (   13.63 ms per token,    73.39 tokens per second)\n",
      "llama_print_timings:        eval time = 18649.39 ms /   510 runs   (   36.57 ms per token,    27.35 tokens per second)\n",
      "llama_print_timings:       total time = 22619.61 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295797\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x126f3d430\n",
      "ggml_metal_init: loaded kernel_add_row                        0x126f3f600\n",
      "ggml_metal_init: loaded kernel_mul                            0x126f3f8c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x126f400b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x126f40b90\n",
      "ggml_metal_init: loaded kernel_silu                           0x126f41400\n",
      "ggml_metal_init: loaded kernel_relu                           0x126f3edd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x126f41d80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x126f42d30\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x126f43150\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x126f44330\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x126f43610\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x126f45840\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x126f44c90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x126f46170\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x126f46b20\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x126f474a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x126f47e20\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x126f48900\n",
      "ggml_metal_init: loaded kernel_norm                           0x126f49900\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x126f4a750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x126f4b120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x126f4bb40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x126f4c580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x126f4d0f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x126f4daa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x126f4e2f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x126f4ed00\n",
      "ggml_metal_init: loaded kernel_rope                           0x126f4f7e0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x126f4fa40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x126f505c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x126f514a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x126f490e0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. You can’t be happy if you don’t have good health!\n",
      "I believe that a lot of people live in fear and anxiety about their health because they don’t know how to take care of themselves properly, so here are some tips from me to help you stay healthy.\n",
      "If this sounds like something you could use then keep reading!\n",
      "The Meaning Of Life: What It Means For You\n",
      "I think that the meaning of life is different for everyone.\n",
      "Some people say it’s about happiness, but I don’t agree with that because there are so many things in our lives that make us unhappy.\n",
      "For example, when you have a job and your boss doesn’t like you or someone treats you badly at work then this can cause stress which affects how happy we feel every day of the week! This causes anxiety as well because we know there will always be more bad times ahead so why bother trying?\n",
      "If you want to live a healthy life, it is important that you learn how to take care of yourself.\n",
      "This means learning about what foods are good for your body and mind – like fruits and vegetables- as well as exercising regularly in order to stay fit!\n",
      "I believe that everyone has their own definition of happiness but I think if we can find ways to make ourselves feel happy then it’ll be easier for us all when things go wrong because life isn’t perfect.\n",
      "How To Stay Healthy In Order To Live A Longer Life?\n",
      "It is important to stay healthy in order to live a longer life, so let me tell you how!\n",
      "First of all I want you know that eating well doesn’t mean being strict or boring when it comes down what we eat every day but rather making sure our diet contains lots of fruits and vegetables as well as some lean meat like chicken breast which also provides protein (the building blocks).\n",
      "Secondly, exercise is vital for keeping ourselves fit because exercising helps us burn calories so that they don’t turn into fat cells in our bodies. You should try walking at least 30 minutes each day if possible – this will help keep your heart healthy too!\n",
      "Thirdly, it’s important to get enough sleep every night because without proper rest we become stressed out which makes us feel bad about ourselves so make sure that before going to bed check if there are any\n",
      "llama_print_timings:        load time =  1638.30 ms\n",
      "llama_print_timings:      sample time =   336.66 ms /   512 runs   (    0.66 ms per token,  1520.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3632.94 ms /   265 tokens (   13.71 ms per token,    72.94 tokens per second)\n",
      "llama_print_timings:        eval time = 18653.68 ms /   510 runs   (   36.58 ms per token,    27.34 tokens per second)\n",
      "llama_print_timings:       total time = 22667.47 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295821\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x15b6374f0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x15b6396c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x15b639970\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x15b63a190\n",
      "ggml_metal_init: loaded kernel_scale                          0x15b63ac00\n",
      "ggml_metal_init: loaded kernel_silu                           0x15b63b4a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x15b638f30\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15b63bdc0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x15b63cea0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x15b63e210\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x15b63e470\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x15b63f770\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x15b63f9d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x15b63edb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x15b6401b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x15b640b30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x15b641490\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x15b641e10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x15b6427d0\n",
      "ggml_metal_init: loaded kernel_norm                           0x15b643970\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x15b644400\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x15b6449c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x15b645340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x15b645d50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x15b646780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x15b647940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x15b648320\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x15b648d30\n",
      "ggml_metal_init: loaded kernel_rope                           0x15b648f90\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x15b649a60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x15b64a630\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x15b64b520\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x15b642fb0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m what we make it.\n",
      "When you are faced with a moment of inspiration, do you have the courage to act on it?\n",
      "What does it take for you to turn your dreams into reality?\n",
      "We all think about the big picture now and then, but how often do we set goals that help us get there?\n",
      "I am here to tell you that I can help you make that happen.\n",
      "I have been a professional photographer since 1984 when my career began in the darkroom of a commercial studio in San Francisco. In those days, I had one camera and one lens, but I was blessed with amazing clients who appreciated what I did for them. At that time, there were no personal computers; most people didn't even have home telephones, let alone cell phones or digital cameras. The internet as we know it today was not a thing yet.\n",
      "But one thing has remained constant over the years - my passion and dedication to my craft. And now, with 40-plus years of experience in this field, I have the opportunity to share that passion and help you create an image that represents your very best self.\n",
      "I hope we can work together soon!\n",
      "© 2018 - 2023 Linda Schoenbachler Photography. All Rights Reserved | Web Design by Bella Media Group\n",
      "lschoenbachler@yahoo.com\n",
      "Linda Schoenbachler is a San Francisco Bay Area headshot photographer who specializes in corporate executive headshots and actor/model portfolios, as well as family portraits, wedding photography, and more. Serving clients throughout the Silicon Valley including Palo Alto, Menlo Park, Atherton, Woodside, Portola Valley, Los Altos, Sunnyvale, Mountain View, San Jose, Redwood City, Belmont, Foster City, Burlingame, Millbrae, San Carlos, Cupertino, Santa Clara, Saratoga and the greater Bay Area.\n",
      "San Francisco Headshots\n",
      "Palo Alto Headshots\n",
      "Menlo Park Headshots\n",
      "Woodside Headshots\n",
      "Atherton Headshots\n",
      "Portola Valley Headshots\n",
      "Los Altos Headshots\n",
      "Sunnyvale Headshots\n",
      "Mountain View Headshots\n",
      "San Jose Headshots\n",
      "Redwood City Headsh\n",
      "llama_print_timings:        load time =  1754.13 ms\n",
      "llama_print_timings:      sample time =   323.85 ms /   512 runs   (    0.63 ms per token,  1580.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6082.75 ms /   265 tokens (   22.95 ms per token,    43.57 tokens per second)\n",
      "llama_print_timings:        eval time =  9666.65 ms /   510 runs   (   18.95 ms per token,    52.76 tokens per second)\n",
      "llama_print_timings:       total time = 16119.65 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295839\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1196732d0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1196754c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x119675780\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x119675f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x119676a00\n",
      "ggml_metal_init: loaded kernel_silu                           0x119677270\n",
      "ggml_metal_init: loaded kernel_relu                           0x119674c70\n",
      "ggml_metal_init: loaded kernel_gelu                           0x119677bc0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x119678480\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x119679fd0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x119678d80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11967b4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11967b740\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11967ab00\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11967bf70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11967c8f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11967d250\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11967dbd0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11967e590\n",
      "ggml_metal_init: loaded kernel_norm                           0x11967f750\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1196801e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x119680dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1196817f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x119682200\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x119682d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1196838c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x119684130\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x119684550\n",
      "ggml_metal_init: loaded kernel_rope                           0x119685690\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x119685ec0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x119686dc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1196879b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11967ed70\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a happy life.\n",
      "I would like to be remembered as someone who was not afraid to do what she wanted to do in this world because she had faith that there was something bigger than her here on earth.\n",
      "My earliest childhood memory is from when I was 4 years old. It’s the memory of being happy. A couple of my favorite memories are being with friends and going to Disneyland, because it’s such a magical place.\n",
      "I am most proud of how far I have come in life. From surviving breast cancer at age 16 to graduating college at 24 years old. The fact that I’m here today makes me so happy and proud. I hope that others will see my story of resilience and find strength in it as well, because there is always a way out of the darkest places in life.\n",
      "I am grateful for having a strong support system around me, which includes family, friends, mentors, people who have been through what I’m going through, and people who are still fighting for their lives against cancer. In my opinion, we can make it through anything by using the strength of those who love us.\n",
      "I believe that if you want something enough, you will get it. You might have to work harder than anyone else does but that is okay because as long as your intentions are good then nothing bad can happen. I also believe in myself and my abilities more than anything else in the world. There’s no reason why I shouldn’t be able to accomplish whatever goals or dreams that I set out for myself.\n",
      "I think the best part of being a woman is having the opportunity to love unconditionally and experience life through many different perspectives. You are born with this ability, but it takes practice before you can fully understand what that means on an everyday basis.\n",
      "The worst part about being a woman in my opinion would be not having enough time for yourself. Women often find themselves taking care of others or putting their own needs aside because they feel like there aren’t enough hours in the day.\n",
      "I believe that life is too short to live a boring life, so I try my best every day to do something exciting and new. There are many things you can do when you want to get out of your comfort zone: traveling, trying different foods or drinks, taking up an extreme sport like sky diving or rock climbing; anything really!\n",
      "llama_print_timings:        load time =   970.31 ms\n",
      "llama_print_timings:      sample time =   324.11 ms /   512 runs   (    0.63 ms per token,  1579.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6127.58 ms /   265 tokens (   23.12 ms per token,    43.25 tokens per second)\n",
      "llama_print_timings:        eval time =  9660.38 ms /   510 runs   (   18.94 ms per token,    52.79 tokens per second)\n",
      "llama_print_timings:       total time = 16157.20 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295857\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102f27280\n",
      "ggml_metal_init: loaded kernel_add_row                        0x102f29450\n",
      "ggml_metal_init: loaded kernel_mul                            0x102f29710\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x102f29f00\n",
      "ggml_metal_init: loaded kernel_scale                          0x102f2a9e0\n",
      "ggml_metal_init: loaded kernel_silu                           0x102f2b250\n",
      "ggml_metal_init: loaded kernel_relu                           0x102f28c20\n",
      "ggml_metal_init: loaded kernel_gelu                           0x102f2bbd0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x102f2cb80\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102f2cfa0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x102f2e180\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x102f2d460\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x102f2f690\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x102f2eae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x102f2ffc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x102f30970\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x102f312f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x102f31c70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102f32750\n",
      "ggml_metal_init: loaded kernel_norm                           0x102f33750\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x102f345a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102f34f70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102f35990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x102f363d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x102f36f40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x102f378f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x102f38140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x102f38b50\n",
      "ggml_metal_init: loaded kernel_rope                           0x102f39630\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x102f39890\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x102f3a410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x102f3b2f0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x102f32f30\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to enjoy every minute of your life. It is not a cliché but it is what I live by.\n",
      "I would like to introduce you to my company, which has been providing high quality service for more than 10 years and I am very happy that we have satisfied thousands of our customers with the best in the industry.\n",
      "We have many different services that are available at reasonable prices: from a private car service to group tours, sightseeing, airport transfers, etc. We provide you with all the necessary information about Bulgaria and the best way to see it is by going around, traveling, meeting people in the small towns and villages as well as the bigger cities.\n",
      "We will make your vacation a memorable experience. Our company has one of the largest fleets in Bulgaria – we own 100 cars. We use only new, comfortable Mercedes Benz vehicles with leather seats, air conditioning, GPS navigation systems and all necessary permits to drive you around safely.\n",
      "We have highly trained drivers who will do their best to make your trip as pleasant as possible. They know Bulgaria very well and are familiar with the latest developments in our country so they can offer you any information about places of interest or historical sites that might be interesting for you during your stay here.\n",
      "I am sure that if you choose us as your partner, we will provide you with a great service at reasonable prices which will make your trip even more enjoyable!\n",
      "Dimitrov Private Tours is a company specializing in private tours in Bulgaria and its neighboring countries. We offer the best rates for our services, including: airport transfers, day trips, sightseeing tours, honeymoon packages, and much more.\n",
      "We have been providing high quality service since 2012. Our clients come from all over the world – Australia, Brazil, Canada, China, France, Germany, India, Japan, Russia, Taiwan, UK… We have satisfied thousands of customers with our service.\n",
      "Our company is located in Sofia (Bulgaria). We offer private tours to all parts of Bulgaria: Sofia, Plovdiv, Varna, Burgas and more!\n",
      "We are very proud that we have been able to satisfy hundreds of our clients with our service during last 10 years. Our team has grown from one person (Dimitrov) into a team of over 25\n",
      "llama_print_timings:        load time =   965.47 ms\n",
      "llama_print_timings:      sample time =   338.71 ms /   512 runs   (    0.66 ms per token,  1511.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6173.39 ms /   265 tokens (   23.30 ms per token,    42.93 tokens per second)\n",
      "llama_print_timings:        eval time =  9676.21 ms /   510 runs   (   18.97 ms per token,    52.71 tokens per second)\n",
      "llama_print_timings:       total time = 16229.06 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295874\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13a829a40\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13a82bc10\n",
      "ggml_metal_init: loaded kernel_mul                            0x13a82beb0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13a82c6d0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13a82d170\n",
      "ggml_metal_init: loaded kernel_silu                           0x13a82d9e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13a82b3e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13a82e360\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13a82f310\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13a82f730\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13a830910\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13a82fbf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13a831e20\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13a831270\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13a832750\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13a833100\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13a833a80\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13a834400\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13a834ee0\n",
      "ggml_metal_init: loaded kernel_norm                           0x13a835ee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13a836d30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13a837700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13a838120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13a838b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13a8396d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13a83a080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13a83a8d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13a83b2e0\n",
      "ggml_metal_init: loaded kernel_rope                           0x13a83bdc0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13a83c020\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13a83cba0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13a83da80\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13a8356c0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.70 MB, (24827.16 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (24839.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.33 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give it meaning.\n",
      "Words and music by John Denver, 1975\n",
      "#  The Duty to Die\n",
      "The idea of a duty to die has gained a lot of traction in recent years. As people live longer and health care costs grow, a large and growing segment of the population—often those who are themselves old or ill—has begun to argue that it's time for us to start thinking about whether we have an obligation to exit the stage voluntarily when life seems no longer worth living.\n",
      "The most obvious supporters of this view are terminally ill patients, whose quality of life has declined to such a degree that they would prefer not to live another day. Others who support the duty to die include philosophers and bioethicists who argue that we have an obligation to \"die well\" and to avoid imposing undue burdens on others—a position known as \"anti-selfishness.\" Still others, including some physicians, insist on a moral duty not to prolong life when doing so has become futile.\n",
      "These arguments are often accompanied by an appeal to the right of self-determination: If people have a right to choose to live or die, then they should be able to act to bring about their own deaths. The question is whether that right can be exercised unilaterally and without interference from others.\n",
      "In the United States and other countries that recognize a \"right to life,\" the legal debate has centered on the concept of _physician-assisted suicide_ (PAS), which involves physicians providing patients with information about lethal drugs, prescribing those drugs, or both. Physicians who object to PAS as ethically inappropriate have been sanctioned by their professional societies, and many countries have legalized PAS either through legislation or court decisions.\n",
      "The concept of a \"duty to die\" does not sit well with me: I don't believe that we have an obligation to die when life seems no longer worth living. But at the same time, I am troubled by arguments against PAS and euthanasia on the grounds that such interventions would represent a violation of a patient's autonomy or right to self-determination.\n",
      "As a physician whose primary focus is caring for patients nearing the end of life, I have come\n",
      "llama_print_timings:        load time =  5857.03 ms\n",
      "llama_print_timings:      sample time =   323.64 ms /   512 runs   (    0.63 ms per token,  1581.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6216.41 ms /   265 tokens (   23.46 ms per token,    42.63 tokens per second)\n",
      "llama_print_timings:        eval time = 35406.48 ms /   510 runs   (   69.42 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time = 41991.60 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295922\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11b410cf0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11b412f10\n",
      "ggml_metal_init: loaded kernel_mul                            0x11b4131c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11b413a00\n",
      "ggml_metal_init: loaded kernel_scale                          0x11b4144a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11b414ce0\n",
      "ggml_metal_init: loaded kernel_relu                           0x11b412690\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11b4154b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11b415f40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11b417a30\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11b416f40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11b416a40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11b4191a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11b4185e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11b419910\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11b41a3e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11b41ad60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11b41b6c0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11b41c080\n",
      "ggml_metal_init: loaded kernel_norm                           0x11b41d240\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11b41dcd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11b41e8b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11b41f2e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11b41fcf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11b420880\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11b4213b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11b421c20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11b422040\n",
      "ggml_metal_init: loaded kernel_rope                           0x11b423180\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11b4239b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11b4248b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11b4254a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11b41c860\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.70 MB, (24827.16 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (24839.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.33 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find happiness. That's all there is to it, and everything else is just a means to that end.\n",
      "To me, happiness is being able to do what you love, doing it well, and loving who you are while you're doing it. And for that reason, I love my job at the university library as much as I did when I was an undergraduate student interning here thirty years ago. It may not be a typical career path, but I think it's a perfect fit.\n",
      "I never imagined I would grow up to become a librarian. But then again, when I was a kid, I had no idea of what I wanted to do with my life—not that most people have much of an idea at that age. I just knew that whatever it was, I didn't want to be bored. I liked art and music and science, but nothing really clicked until I started working in the library during high school. It wasn't until then that I realized how many cool things there were to learn about, both inside and outside of books—and that I could do it for a living.\n",
      "Nowadays my job is very different than it was when I first worked here. As the head librarian at the university library, I am responsible for all aspects of the library's operation, including budgeting, personnel management, and strategic planning. But despite the increased responsibilities that have come with the position, I still find time to do what I love—to read.\n",
      "I read constantly. It helps me relax after a long day at work, and it keeps my mind sharp. Reading is also a way for me to learn about new topics and expand my knowledge base. And while I don't have as much free time nowadays, I still make sure to set aside some time each week just for myself—to read books, articles, or even just blogs that interest me.\n",
      "Of course, there are other things in life besides work and leisure activities like reading (though they're pretty darn important!). There are also family, friends, hobbies…But ultimately it all comes down to finding what makes you happy—and then pursuing it with everything you've got.\n",
      "So here's my advice: If you want to find happiness in life, start by figuring out what makes YOU happy. And once you have that figured out? Go after it like crazy\n",
      "llama_print_timings:        load time =  3951.58 ms\n",
      "llama_print_timings:      sample time =   319.71 ms /   512 runs   (    0.62 ms per token,  1601.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6260.23 ms /   265 tokens (   23.62 ms per token,    42.33 tokens per second)\n",
      "llama_print_timings:        eval time = 35334.95 ms /   510 runs   (   69.28 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time = 41957.61 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691295968\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10360c930\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10360eb20\n",
      "ggml_metal_init: loaded kernel_mul                            0x10360ede0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10360f5f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x103610050\n",
      "ggml_metal_init: loaded kernel_silu                           0x1036108d0\n",
      "ggml_metal_init: loaded kernel_relu                           0x10360e390\n",
      "ggml_metal_init: loaded kernel_gelu                           0x103611250\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x103612210\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x103613640\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x103612920\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x103614b50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x103614150\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x103615490\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x103615e10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x103616790\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x103617110\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x103617ac0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x103618420\n",
      "ggml_metal_init: loaded kernel_norm                           0x103619090\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x103619c20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10361a610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10361b050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10361ba60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10361c610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10361d130\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10361d9d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10361e390\n",
      "ggml_metal_init: loaded kernel_rope                           0x10361ed40\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10361f900\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1036207d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1036213b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x103618e00\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.70 MB, (24827.16 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (24839.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.33 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m very simple. It’s about creating a better world and helping others to do the same.\n",
      "I’m not going to be so bold as to suggest that what we create will last forever, but each contribution makes a difference. We learn from our experiences and hopefully use that knowledge to improve the future for our children. That is life: The process of learning, creating and improving by helping others. When you think about it like this, it’s not such a hard question to answer.\n",
      "Ask yourself: What would I do if I knew I couldn’t fail? Would you start that business you always dreamed of starting? Would you travel the world or would you just be happy with your family and friends? How about helping others in need, or giving back to nature?\n",
      "The meaning of life is whatever you make it.\n",
      "Life is a journey. It can take you wherever you want to go if you have enough drive and determination to get there. Life is what YOU make it. It’s not a “one size fits all” kind of thing. You can do anything you set your mind to, as long as you put in the time, effort and energy to achieve your goals.\n",
      "The meaning of life is up to the individual.\n",
      "If you want to live your best possible life, it takes work and dedication. Life isn’t always easy, but if we look at our lives as a journey, then we can see that there are many different roads we can take in order to get where we want to go. We all have the ability to create our own reality by choosing which path we take and how far down each road we travel before turning back around again.\n",
      "The meaning of life is what you make it.\n",
      "You may be wondering why I am writing this article, but if you are like me and have been searching for answers about the meaning of life for years, then perhaps I can help.\n",
      "I’ve spent a lot of time thinking about this question, and while I don’t claim to have all the answers, I do believe that there is something special about being human—something unique about our species that makes us different from other animals on Earth. We are capable of more than just surviving; we are able to create things beyond what any other creature could ever imagine!\n",
      "We have been gifted with a brain and consciousness, which allows us to think abstractly and reason logically—and these traits make it possible for us to\n",
      "llama_print_timings:        load time =  3993.36 ms\n",
      "llama_print_timings:      sample time =   323.74 ms /   512 runs   (    0.63 ms per token,  1581.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6234.94 ms /   265 tokens (   23.53 ms per token,    42.50 tokens per second)\n",
      "llama_print_timings:        eval time = 35352.99 ms /   510 runs   (   69.32 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time = 41951.97 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 70B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0bef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691296014\n",
      "llama.cpp: loading model from ./models/70B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size = 37070.96 MB\n",
      "llama_model_load_internal: mem required  = 37635.96 MB (+  160.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1507ba000\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1507bc1d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1507bc480\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1507bcca0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1507bd710\n",
      "ggml_metal_init: loaded kernel_silu                           0x1507bdfb0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1507bba40\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1507be8d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1507bf9b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1507c0d20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1507c0f80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1507c2280\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1507c24e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1507c18c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1507c2cc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1507c3640\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1507c3fa0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1507c4920\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1507c52e0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1507c6480\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1507c6f10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1507c74d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1507c7e50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1507c8860\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1507c9290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1507ca450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1507cae30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1507cb840\n",
      "ggml_metal_init: loaded kernel_rope                           0x1507cbaa0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1507cc570\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1507cd140\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1507ce030\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1507c5ac0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 37070.97 MB, (37071.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (37095.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   162.00 MB, (37257.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   237.00 MB, (37494.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   304.00 MB, (37798.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. Happiness is different for each person, so people should do what makes them happy!\n",
      "I think the meaning of life is not always about happiness but making yourself a better person and helping others who are less fortunate than you.\n",
      "The meaning of life is that there isn'm really any meaning of life. It's just to live it.\n",
      "i think the meaning of life is for us to learn and grow as people, while leaving an impact on this world, whether positive or negative.\n",
      "I believe that the purpose of our life is to spread love and kindness wherever possible. I also think that another important part of our lives is to be satisfied with what we have, and not wish for more. I believe that it is important to live a healthy lifestyle while being grateful for everything you have.\n",
      "I believe that the meaning of life is to do as much good in this world as you can so when you die you will go to heaven. So basically don't be mean and be as kind as possible and do nice things to others. It also means that you should love everybody even if they are bad people because they were created by God just like you.\n",
      "I think the meaning of life is to make a difference in the world, spread happiness to other people, and do good deeds for those who need it.\n",
      "The meaning of life to me is to be happy and help others be happy too! I think that if everyone helped each other out there would not be any wars or conflict at all.\n",
      "I believe the purpose of life is to serve God by loving one another, spreading joy wherever possible, and being a person of integrity.\n",
      "The meaning of life is to be kind, caring, and compassionate towards others. You should also try your best in school so you can get into college and have a good job!\n",
      "I think the purpose of life is to live it with happiness. That means not taking anything for granted and always being grateful for what you have. It's important to be kind, because everyone deserves respect no matter who they are or where they come from. And finally, it's essential to love unconditionally - even if someone doesn't deserve your love!\n",
      "I think that the meaning of life is to do good deeds and make a difference in this world.\n",
      "The purpose of life is to be happy by being kind, compassionate, loving others, helping\n",
      "llama_print_timings:        load time =  8931.22 ms\n",
      "llama_print_timings:      sample time =   331.31 ms /   512 runs   (    0.65 ms per token,  1545.39 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27118.64 ms /   265 tokens (  102.33 ms per token,     9.77 tokens per second)\n",
      "llama_print_timings:        eval time = 38959.35 ms /   510 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
      "llama_print_timings:       total time = 66450.50 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Currently, the grouping factor (8 for 70Bv2, 1 for everything else) has to be passed from the command line: -gqa 8\n",
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 -gqa 8 --ignore-eos -m ./models/70B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691296090\n",
      "llama.cpp: loading model from ./models/70B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size = 37070.96 MB\n",
      "llama_model_load_internal: mem required  = 37635.96 MB (+  160.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11ef26940\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11ef28b10\n",
      "ggml_metal_init: loaded kernel_mul                            0x11ef28dd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11ef295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11ef2a0a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11ef2a910\n",
      "ggml_metal_init: loaded kernel_relu                           0x11ef282e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11ef2b290\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11ef2c240\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11ef2c660\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11ef2d840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11ef2cb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11ef2ed50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11ef2e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11ef2f680\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11ef30030\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11ef309b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11ef31330\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11ef31e10\n",
      "ggml_metal_init: loaded kernel_norm                           0x11ef32e10\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11ef33c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11ef34630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11ef35050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11ef35a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11ef36600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11ef36fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11ef37800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11ef38210\n",
      "ggml_metal_init: loaded kernel_rope                           0x11ef38cf0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11ef38f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11ef39ad0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11ef3a9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11ef325f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 37070.97 MB, (37071.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (37095.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   162.00 MB, (37257.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   237.00 MB, (37494.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   304.00 MB, (37798.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live your life in such a way that you will have no regrets.\n",
      "I believe we all have choices, and our decisions shape who we become.\n",
      "I believe that one day the world will be at peace, but until then, we must work together for the common good.\n",
      "I believe in the power of love, friendship, family and community.\n",
      "I believe everyone should have equal rights no matter what race or religion they are from or where they were born.\n",
      "I believe every person has something unique about them that makes them special.\n",
      "I believe everyone should be treated fairly regardless of gender identity/expression/sexual orientation etc..\n",
      "I Believe in a God Who Loves and Accepts Me as I Am, No Matter What I Do or Don’landoesn’t Do\n",
      "I believe in a God who loves me unconditionally no matter what I do or don’t do.\n",
      "I believe that God is always with me, no matter where I am or what happens to me.\n",
      "I believe that God has given each of us gifts and talents so we can use them for the good of others.\n",
      "I believe in the power of prayer and faith to help us through difficult times.\n",
      "We all have doubts about our beliefs at one time or another, but it’s important to remember that there are many ways you can still practice your faith without being judged by others around you who may not share your views on religion or spirituality – such as reading scripture aloud daily (or listening), praying regularly throughout the day (not just once per week!), meditating often…\n",
      "I Believe in a God Who Loves and Accepts Me as I Am, No Matter What I Do or Don’t Do\n",
      "I believe in a god who loves me unconditionally. I also believe that he has given each of us gifts and talents so we can use them for the good of others. We all have doubts about our beliefs at one time or another, but it’s important to remember there are many ways you can still practice your faith without being judged by others around you who may not share your views on religion or spirituality—such as reading scripture aloud daily (or listening), praying regularly throughout the day (not just once per week!), meditating often…\n",
      "I believe in a God who loves me unconditionally. I also believe that he has given each\n",
      "llama_print_timings:        load time =  6088.61 ms\n",
      "llama_print_timings:      sample time =   337.06 ms /   512 runs   (    0.66 ms per token,  1519.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27224.31 ms /   265 tokens (  102.73 ms per token,     9.73 tokens per second)\n",
      "llama_print_timings:        eval time = 38964.23 ms /   510 runs   (   76.40 ms per token,    13.09 tokens per second)\n",
      "llama_print_timings:       total time = 66566.71 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 -gqa 8 --ignore-eos -m ./models/70B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691296163\n",
      "llama.cpp: loading model from ./models/70B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size = 37070.96 MB\n",
      "llama_model_load_internal: mem required  = 37635.96 MB (+  160.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10f626940\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10f628b10\n",
      "ggml_metal_init: loaded kernel_mul                            0x10f628dd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10f6295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10f62a0a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10f62a910\n",
      "ggml_metal_init: loaded kernel_relu                           0x10f6282e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10f62b290\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10f62c240\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10f62c660\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10f62d840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10f62cb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10f62ed50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10f62e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10f62f680\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10f630030\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10f6309b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10f631330\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10f631e10\n",
      "ggml_metal_init: loaded kernel_norm                           0x10f632e10\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10f633c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10f634630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10f635050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10f635a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10f636600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10f636fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10f637800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10f638210\n",
      "ggml_metal_init: loaded kernel_rope                           0x10f638cf0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10f638f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10f639ad0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10f63a9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10f6325f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 37070.97 MB, (37071.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (37095.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   162.00 MB, (37257.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   237.00 MB, (37494.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   304.00 MB, (37798.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. To be your best self you can possibly be and inspire others with your actions by being kind, generous and compassionate.\n",
      "I think it’s a very good question. In my opinion, meaning of life is everything we have in our lives, for example: family, friends, hobbies, relationships etc. We do what we want because of the meaning of life! The meaning of life isn’t just to be happy, but also sometimes sad or angry. I think the meaning of life is when we are satisfied with everything and everyone around us.\n",
      "I don’t really know the answer to that question, but I guess it’s a combination of what I want from life (to travel and have fun) and what my parents want for me (to get an education).\n",
      "It’s hard to say what the meaning of life is because everyone has different values. For example, some people might think that money is more important than anything else while others might value family over everything else. Some people may even believe that there is no meaning in life at all! We can only guess what our own personal definition will be based on our experiences and beliefs about how we see the world around us.\n",
      "I think there are two main reasons why someone would want to know more about their purpose: firstly they’re curious; secondly it helps them make sense out of things like death or illness which can often seem very unfair or random when compared against other areas such as family relationships where everyone seems happy most times ����\n",
      "I think life is a gift. We should be grateful for the opportunity to live, and we should use our time on Earth wisely by doing what makes us happy.\n",
      "I believe that there’s no one right way to live your life—everyone has different goals and desires, so it’s up to you how you want yours spent! Maybe you want an exciting job where you get paid well; maybe all day long at work feels like torture because of how much stress comes with every task (or lack thereof). Or perhaps your dream career doesn’t even exist yet—in which case, why not start working toward something else? The possibilities are endless when it comes down to choosing where our lives will take us next!\n",
      "The meaning of life is a question that has been asked by philosophers and theologians for centuries. It can be difficult to answer this question because there are so many\n",
      "llama_print_timings:        load time =  6046.59 ms\n",
      "llama_print_timings:      sample time =   337.30 ms /   512 runs   (    0.66 ms per token,  1517.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27391.64 ms /   265 tokens (  103.36 ms per token,     9.67 tokens per second)\n",
      "llama_print_timings:        eval time = 38857.08 ms /   510 runs   (   76.19 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:       total time = 66626.76 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 -gqa 8 --ignore-eos -m ./models/70B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce3e8",
   "metadata": {},
   "source": [
    "### 70B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5122b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691296236\n",
      "llama.cpp: loading model from ./models/70B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size = 131565.25 MB\n",
      "llama_model_load_internal: mem required  = 132130.25 MB (+  160.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102e10970\n",
      "ggml_metal_init: loaded kernel_add_row                        0x102e12b90\n",
      "ggml_metal_init: loaded kernel_mul                            0x102e12e40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x102e13680\n",
      "ggml_metal_init: loaded kernel_scale                          0x102e14120\n",
      "ggml_metal_init: loaded kernel_silu                           0x102e14990\n",
      "ggml_metal_init: loaded kernel_relu                           0x102e12310\n",
      "ggml_metal_init: loaded kernel_gelu                           0x102e15150\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x102e15bd0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102e17730\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x102e17990\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x102e16650\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x102e18e10\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x102d0d360\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x102d0e090\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x102d0e710\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x102d0f0a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x102d0fa20\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102d103a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x102d11500\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x102e19b00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102e19f00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102e1a9b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x102d11ac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x102d12b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x102d136d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x102d13930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x102d15720\n",
      "ggml_metal_init: loaded kernel_rope                           0x102d14cf0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x102e1bca0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x102e1c270\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10f424a60\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10f425be0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 21473.28 MB, offs = 115439812608, (132065.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (132089.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   162.00 MB, (132251.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   237.00 MB, (132488.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   304.00 MB, (132792.91 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "I'm a 31 year old female from the U.S. who loves to travel, explore new places and meet new people. I am currently looking for opportunities in Europe (preferably Italy). I enjoy reading, writing, working out, cooking, dancing and hanging out with friends.\n",
      "Looking for a job that is right for me.\n",
      "Sporty, outgoing, adventurous, funny, like to go out on weekends but also like to stay in some nights. I'm looking for some new experiences in life, which is why i've come to workaway!\n",
      "I'm a 23 year old student from Belgium. After finishing my bachelor degree, I decided to take a gap year and travel around the world. Since the beginning of October 2018, I have been volunteering at a hostel in the north of Thailand. Currently I am looking for a new workaway to join in december/January.\n",
      "I am an easy going person who loves to be outdoors and explore nature. I love animals, especially dogs and cats. I enjoy cooking and eating healthy foods. I have a bachelor degree in science and a post graduate diploma in teaching. I work as a teacher but am taking time off to travel and experience different cultures.\n",
      "My name is Sam, i'm 19 years old and from the UK. I love travelling, meeting new people and experiencing different cultures. I enjoy going out with my friends but also love spending time at home watching films!\n",
      "I am a 23 year old male from Melbourne Australia. My passions include rock climbing, hiking, snowboarding, camping, travel, food and music. In 2017, I quit my job as an engineer and have been travelling ever since. I am currently living in Italy and working on a farm where we make cheese from our own goat's milk.\n",
      "I live in Canada on Vancouver Island. I grew up on a 45 acre family homestead with 3 siblings, 2 parents, many animals, gardens, orchards and a small lake to swim in. I am looking for ways to learn new skills and make connections with people and communities all over the world.\n",
      "Hello! My name\n",
      "llama_print_timings:        load time = 53454.14 ms\n",
      "llama_print_timings:      sample time =   336.85 ms /   512 runs   (    0.66 ms per token,  1519.97 tokens per second)\n",
      "llama_print_timings: prompt eval time = 35105.27 ms /   265 tokens (  132.47 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:        eval time = 159693.26 ms /   510 runs   (  313.12 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:       total time = 195178.99 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 -gqa 8 --ignore-eos -m ./models/70B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86588417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691296486\n",
      "llama.cpp: loading model from ./models/70B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size = 131565.25 MB\n",
      "llama_model_load_internal: mem required  = 132130.25 MB (+  160.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100a154e0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100a176b0\n",
      "ggml_metal_init: loaded kernel_mul                            0x100a17970\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100a18160\n",
      "ggml_metal_init: loaded kernel_scale                          0x100a16cc0\n",
      "ggml_metal_init: loaded kernel_silu                           0x100a16fe0\n",
      "ggml_metal_init: loaded kernel_relu                           0x100a18ba0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100a19e20\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100a1ae70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100a1c210\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100a1b4f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100a1b750\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100a1cd90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100a1d840\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100a1e1c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100a1eb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100a1f4a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100a1fe20\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100a207e0\n",
      "ggml_metal_init: loaded kernel_norm                           0x100a219b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100a21dd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100a23190\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100a23bd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100b082b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100b091f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x136808a30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x136809900\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13680b380\n",
      "ggml_metal_init: loaded kernel_rope                           0x13680a870\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10090b850\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10090b150\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10090bad0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10090bef0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 21473.28 MB, offs = 115439812608, (132065.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (132089.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   162.00 MB, (132251.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   237.00 MB, (132488.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   304.00 MB, (132792.91 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find happiness. When I say “find happiness,” I mean that we are all born with something inside us that tells us how to make ourselves happy, and our goal in life should be to find out what it is that will make us happy.\n",
      "The meaning of life is not just about finding happiness; it’s also about sharing your happiness with others. If you have found your purpose in life then great! But if not, don’t worry because there are many ways for people who haven’t figured out their purpose yet or even those who think they know what their purpose is but still feel lost or confused about how exactly they should go about living it out day-to-day.\n",
      "So, What Does It Mean to Find Happiness?\n",
      "Happiness is not a destination; it’s a journey. To find happiness you must first be willing to let go of what was and embrace the unknown future with open arms (and maybe even an open mind). You need to stop thinking about how things should be different from reality, but instead focus on making them better than they already are by taking action every day towards your goals.\n",
      "The meaning of life is not just about finding happiness; it’s also about sharing your happiness with others! If you have found your purpose in life then great! But if not, don’t worry because there are many ways for people who haven’t figured out their purpose yet or even those who think they know what their purpose is but still feel lost or confused about how exactly they should go about living it out day-to-day.\n",
      "Happiness is a journey, and it’s one that we all have to take together. We can’t do this alone! It takes time, effort and support from others who care about you as much as you care about them. So don’t be afraid of asking for help when things get tough – because they will sometimes.\n",
      "Asking For Help When Things Get Tough\n",
      "It is important to ask for help when things get tough. You can do this by going out with friends, talking to your family and getting support from them as well. It’s also a good idea to talk about your problems in therapy sessions or group meetings at work where you know other people will understand what it feels like being alone during these difficult times because they have been through similar experiences themselves too!\n",
      "The truth is, there are many ways for people who haven’t figured out their purpose yet\n",
      "llama_print_timings:        load time = 53362.71 ms\n",
      "llama_print_timings:      sample time =   387.61 ms /   512 runs   (    0.76 ms per token,  1320.91 tokens per second)\n",
      "llama_print_timings: prompt eval time = 34195.83 ms /   265 tokens (  129.04 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time = 160773.17 ms /   510 runs   (  315.24 ms per token,     3.17 tokens per second)\n",
      "llama_print_timings:       total time = 195404.74 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 -gqa 8 --ignore-eos -m ./models/70B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e1e0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691296735\n",
      "llama.cpp: loading model from ./models/70B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: warning: assuming 70B model based on GQA == 8\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 4096\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 8\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 8\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 28672\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 70B\n",
      "llama_model_load_internal: ggml ctx size = 131565.25 MB\n",
      "llama_model_load_internal: mem required  = 132130.25 MB (+  160.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x101107ce0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x101108ee0\n",
      "ggml_metal_init: loaded kernel_mul                            0x101109190\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1011099b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10110a440\n",
      "ggml_metal_init: loaded kernel_silu                           0x10110acf0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1011086d0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10110b6a0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10110c6a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10110da80\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10110cd60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10110efc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10110f220\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10110e5a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10110fa90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x101110410\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x101110d90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x101111710\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1011127f0\n",
      "ggml_metal_init: loaded kernel_norm                           0x101113340\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x101114010\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1011149d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1011153d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x101115df0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x101116960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100d690a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100d696b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100d6b510\n",
      "ggml_metal_init: loaded kernel_rope                           0x100d6a920\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100d6ac50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100d6c340\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x101222d50\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x101223ed0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 21473.28 MB, offs = 115439812608, (132065.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (132089.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   162.00 MB, (132251.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   237.00 MB, (132488.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   304.00 MB, (132792.91 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away!\n",
      "I am passionate about working with people who know how important it is to be heard and acknowledged. Who have the courage to step out of their comfort zone, take a leap of faith, go for what they want in life and live life on their terms.\n",
      "The people I work with are committed to living fully expressed! They know that being stuck in the same old pattern and living life feeling depleted and frustrated is not how they want it to be.\n",
      "I help them learn powerful tools so they can change their current situation into one of excitement, fulfillment, happiness and success.\n",
      "My clients feel alive, vibrant, energized and are ready for the next level in their personal or professional lives. They have a plan for how to get from where they currently are, to where they want to be. They know what it takes to make it happen and then do it!\n",
      "Would you like to live fully expressed? If so, contact me today to set up your complimentary consultation. I am committed to helping you create a life of fulfillment, happiness and success. I will help you learn how to find the meaning in what you do, no matter what it is!\n",
      "When was the last time you felt fully alive? What would make that happen for you again?\n",
      "What are your dreams? How long have you been putting them off? Why?\n",
      "Are you ready to live a life of fulfillment, happiness and success?\n",
      "I am committed to helping you find the meaning in what you do, no matter what it is! Contact me today. Let’s get started. I will help you learn how to live fully expressed!\n",
      "I have had the pleasure of working with Patricia for more than 10 years. She has helped my companies and myself in so many ways, some which are immeasurable. Her advice is practical and her coaching is positive and encouraging. I always feel like she's on my side, cheering me on to be a better person.\n",
      "Patricia is an amazing coach! With warmth and compassion she has helped me find new perspectives that have enabled me to move forward and grow both personally and professionally.\n",
      "I started working with Patricia about 6 months ago, when I was at a crossroads in my life – wanting to make a change but not\n",
      "llama_print_timings:        load time = 51996.86 ms\n",
      "llama_print_timings:      sample time =   404.25 ms /   512 runs   (    0.79 ms per token,  1266.56 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33668.07 ms /   265 tokens (  127.05 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 161195.98 ms /   510 runs   (  316.07 ms per token,     3.16 tokens per second)\n",
      "llama_print_timings:       total time = 195319.24 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 -gqa 8 --ignore-eos -m ./models/70B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959fbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
