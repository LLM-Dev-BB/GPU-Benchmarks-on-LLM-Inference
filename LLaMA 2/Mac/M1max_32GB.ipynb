{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da7f75",
   "metadata": {},
   "source": [
    "Prepare Data & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m13B\u001b[m\u001b[m                     \u001b[1m\u001b[36m7B\u001b[m\u001b[m                      \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m13B-v2\u001b[m\u001b[m                  \u001b[30m\u001b[43m7B-v2\u001b[m\u001b[m                   \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m30B\u001b[m\u001b[m                     ggml-vocab.bin\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install Python dependencies\n",
    "# !python3 -m pip install -r requirements.txt\n",
    "\n",
    "# # llama 2\n",
    "# # convert the model to ggml FP16 format\n",
    "# !python3 convert.py --outfile models/7B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-7b/\n",
    "# !python3 convert.py --outfile models/13B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-13b/\n",
    "# !python3 convert.py --outfile models/70B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-70b/\n",
    "\n",
    "# # quantize the model to 4-bits (using q4_0 method)\n",
    "# !./quantize ./models/7B-v2/ggml-model-f16.bin ./models/7B-v2/ggml-model-q4_0.bin q4_0\n",
    "# !./quantize ./models/13B-v2/ggml-model-f16.bin ./models/13B-v2/ggml-model-q4_0.bin q4_0\n",
    "# !./quantize ./models/70B-v2/ggml-model-f16.bin ./models/70B-v2/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "embd-input-test\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/common.cpp -o common.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/console.cpp -o console.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/grammar-parser.cpp -o grammar-parser.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml-alloc.c -o ggml-alloc.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-metal.o ggml-alloc.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o ggml-alloc.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders \n",
      "c++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691302809\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x145e267d0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x145e288d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x145e27db0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x145e29350\n",
      "ggml_metal_init: loaded kernel_scale                          0x145e29d80\n",
      "ggml_metal_init: loaded kernel_silu                           0x145e2a5c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x145e281b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x145e2aed0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x145e2bee0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x145e2c140\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x145e2d490\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x145e2e630\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x145e2dab0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x145e2dd10\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x145e2ef40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x145e2f830\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x145e302b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x145e30ad0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x145e31560\n",
      "ggml_metal_init: loaded kernel_norm                           0x145e32660\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x145e32a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x145e33c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x145e34600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x145e35020\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x145e35b30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x145e364c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x145e36e60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x145e37820\n",
      "ggml_metal_init: loaded kernel_rope                           0x145e382b0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x145e38e90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x145e39d40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x145e3a890\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x145e31d20\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and fulfilled, which means choosing a career you are passionate about. Einzelnes und die ganze Reihe. We have been experiencing technical difficulties with our website. I'm an accountant.\n",
      "How to Make Your Partner Feel Special in 10 Simple Ways | POPSUGAR Love & Sex\n",
      "I am proud of my partner for many reasons, but here are five that make me feel so thankful and happy: 1. They are a good listener — I really value this quality because it's so rare to find someone who genuinely listens. My partner is amazing at listening. Even when they don't have much time or aren't in the mood, they will listen to me talk about my day with such interest and attention.\n",
      "I feel like my partner has a real heart for others, including me, which makes me feel loved and cared for. They are passionate — I love being around people who love what they do so much that it's contagious. My partner is super passionate about their work, which makes them happy to go into the office every day.\n",
      "I think it's amazing how excited they get about going into work, and I find it inspiring. They are always learning — I am so glad my partner loves reading and learning new things. My partner is constantly listening to podcasts or taking online courses on topics that interest them. These things make me feel happy to be around someone who wants to keep growing and developing. I have no doubt they will continue to learn and grow throughout their life. They are supportive — This doesn't just mean supporting my dreams, but also being there for me when I need it most.\n",
      "Whenever I've felt down, my partner has always been there to pick me up. Whether it be helping me clean the house or simply giving me a hug and telling me they love me, my partner is always there to support me in every way possible. I am grateful for this because I truly feel like I can't do anything without their support.\n",
      "I am so thankful that they are here with me on this journey called life! I'm proud of them for being such a good person — People have different ways of showing love and affection, but there is one thing I know: My partner does everything in their power to make sure they treat everyone with kindness.\n",
      "They may not be the most talk\n",
      "llama_print_timings:        load time =  1122.05 ms\n",
      "llama_print_timings:      sample time =  1137.61 ms /   512 runs   (    2.22 ms per token,   450.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4826.58 ms /   265 tokens (   18.21 ms per token,    54.90 tokens per second)\n",
      "llama_print_timings:        eval time = 11007.66 ms /   510 runs   (   21.58 ms per token,    46.33 tokens per second)\n",
      "llama_print_timings:       total time = 17070.27 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691302828\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100e22610\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100e24710\n",
      "ggml_metal_init: loaded kernel_mul                            0x100e23bf0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100e25190\n",
      "ggml_metal_init: loaded kernel_scale                          0x100e25bc0\n",
      "ggml_metal_init: loaded kernel_silu                           0x100e263e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x100e23ff0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100e26d10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100e27c40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100e28fb0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100e29210\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100e284c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100e2a5c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100e29a40\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100e2ad00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100e2b630\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100e2c0b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100e2c8b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100e2d390\n",
      "ggml_metal_init: loaded kernel_norm                           0x100e2e5f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100e2f250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100e2fbb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100e30570\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100e30f80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100e31a80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100e323b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100e32c00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100e335d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x100e340d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100e34c50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100e35af0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100e36680\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100e36e70\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose. everybody has a different one and you will only realise it once you have lived many years, experienced many joys and tragedies.\n",
      "Once you have found your true calling (be it helping people or being an artist) then all other things will follow.\n",
      "Life is a journey and not a destination. I hope to find mine soon but for now I am happy just knowing that my life has meaning!\n",
      "I believe in the power of love. I don't understand much of anything else, but I do know this: Love can conquer any obstacle, defeat any enemy, and endure through any hardship. It is stronger than hate or fear or death. Without it, no great thing was ever achieved. If we are all made of the stuff of God, then love is our divine inheritance, and without it, there is nothing that matters.\n",
      "I believe in a creator who made us, watches over us, loves us and provides for the needs of each human being. He is not just watching me or you but he's watching everyone. This belief has guided me through some tough times and I hope it will guide me through many more if that is his plan.\n",
      "I believe in God who is omnipotent, loving, and forgiving. All we need to do is ask Him for forgiveness when we are wrong and he will immediately forgive us without hesitation or anger.\n",
      "God is always there for you and will never abandon you. However, He does not force His love upon you; it's up to you whether you accept it or not. I hope that one day everyone can see Him as clearly as I do.\n",
      "I believe in God the Father Almighty, maker of heaven and earth, of all things visible and invisible. And in one Lord Jesus Christ, the only begotten Son of God, born of the Father before all ages. Light of light, true God of true God, begotten not created, of one essence with the Father; by whom all things were made. Who for us men and our salvation came down from heaven and was incarnate by the Holy Spirit of the Virgin Mary and became man. He was crucified for us under Pontius Pilate, suffered death and was buried. On the third day he rose again in accordance with the scriptures; he ascended into heaven and sits at the right hand of the Father.\n",
      "llama_print_timings:        load time =   837.58 ms\n",
      "llama_print_timings:      sample time =  1145.56 ms /   512 runs   (    2.24 ms per token,   446.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4827.42 ms /   265 tokens (   18.22 ms per token,    54.89 tokens per second)\n",
      "llama_print_timings:        eval time = 11050.52 ms /   510 runs   (   21.67 ms per token,    46.15 tokens per second)\n",
      "llama_print_timings:       total time = 17124.18 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691302846\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100d22530\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100d24630\n",
      "ggml_metal_init: loaded kernel_mul                            0x100d23b10\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100d250b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x100d25ae0\n",
      "ggml_metal_init: loaded kernel_silu                           0x100d26300\n",
      "ggml_metal_init: loaded kernel_relu                           0x100d23f10\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100d26c30\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100d27b60\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100d28ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100d29130\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100d283e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100d2a4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100d29960\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100d2ac20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100d2b550\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100d2bfd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100d2c7d0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100d2d2b0\n",
      "ggml_metal_init: loaded kernel_norm                           0x100d2e510\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100d2f170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100d2fad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100d30490\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100d30ea0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100d319a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100d322d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100d32b20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100d334f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x100d33ff0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100d34b70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100d35a10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100d365a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100d36d90\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. références, biographie, détail des albums studio, singles, videos et concerts des vos artistes préférés sur Last.fm\n",
      "L'interprète de ce titre est...\n",
      "- Erykah Badu\n",
      "- Prince (20th C.)\n",
      "- Dizzee Rascal\n",
      "- DJ Shadow\n",
      "http://www.last.fm/music/King+Davis\n",
      "\"Music is my life, music and lyrics. I always wanted to be a rapper but I don't know how to rap so I decided to try to write my own songs.\" - King Davis\n",
      "Raised on the streets of Detroit, Davis moved to Atlanta at age 14 and worked with producer Dallas Austin as a songwriter for acts like TLC, Boyz II Men and Mariah Carey. He released his debut album \"Hear Ye Hear Ye\" in 2005 through an independent label based in the UK called Soulspazm Records.\n",
      "King Davis is also known for his contributions to rapper Erykah Badu's album \"Worldwide Underground\", on the song \"Cell Phones\" where he is featured with Cee-Lo, Peven Everett and Badu singing background vocals. In 2010 King signed a distribution deal with Universal Music Enterprises (UMe) and his EP “Gangsta Blues” will be released through UMe in July 2010\n",
      "King Davis - Gangsta Blues (EP)\n",
      "Raised on the streets of Detroit, Davis moved to Atlanta at age 14 and worked with producer Dallas Austin as a songwriter for acts like TLC, Boyz II Men and Mariah Carey. He released his debut album \"Hear Ye Hear Ye\" in 2005 through an independent label based in the UK called Soulspazm Records. King Davis is also known for his contributions to rapper Erykah Badu's album \"Worldwide Underground\", on the song \"Cell Phones\" where he is featured with Cee-Lo, Peven Everett and Badu singing background vocals. In 2010 King signed a distribution deal with Universal Music Enterprises (UMe) and his EP “Gangsta Blues” will be released through UMe in July 2010\n",
      "Soulspazm Records\n",
      "Soulspaz\n",
      "llama_print_timings:        load time =   841.77 ms\n",
      "llama_print_timings:      sample time =   825.33 ms /   512 runs   (    1.61 ms per token,   620.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4821.38 ms /   265 tokens (   18.19 ms per token,    54.96 tokens per second)\n",
      "llama_print_timings:        eval time = 10790.83 ms /   510 runs   (   21.16 ms per token,    47.26 tokens per second)\n",
      "llama_print_timings:       total time = 16514.16 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691302863\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1457d80b0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1457da6d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1457d9ab0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1457db150\n",
      "ggml_metal_init: loaded kernel_scale                          0x1457dba80\n",
      "ggml_metal_init: loaded kernel_silu                           0x1457dc400\n",
      "ggml_metal_init: loaded kernel_relu                           0x1457d9eb0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1457dcd40\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1457ddc90\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1457de340\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1457de5a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1457df190\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1457e0660\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1457dfab0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1457e0f30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1457e1890\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1457e21d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1457e2b10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1457e3bf0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1457e4570\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1457e4970\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1457e5d20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1457e66d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1457e7080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1457e7b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1457e84a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1457e8cf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1457e9690\n",
      "ggml_metal_init: loaded kernel_rope                           0x1457ea180\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1457ead50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1457ebbb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1457ec730\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1457e3e50\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. To achieve happiness you must first acknowledge that we are all spiritual beings having a human experience. We were born into this world with specific gifts and talents for our own personal growth.\n",
      "When you find your passion, whether it's through an activity like yoga or art, being of service to others, traveling the world, working in a field where you can use your giftedness, or simply being present in the moment -- whatever it is that makes you happy and fulfilled, then you are living life. This will be different for everyone because we all have different gifts, personalities, needs, dreams, etc.\n",
      "It's important to remember that our minds are powerful and capable of creating and manifesting what we think about. So if you're unhappy and feel like your life isn't going the way you want it to, then start thinking about ways you can bring happiness into your everyday life.\n",
      "We only get one life so be sure to live it to the fullest by doing what makes you happy!\n",
      "I think we are all born with a purpose in this world and that is to find out who we are as individuals while growing spiritually, physically, emotionally, mentally, socially, etc. We must learn how to manage our thoughts so they can serve us instead of hinder us. There's always room for self-improvement!\n",
      "We don't need material possessions or wealth in order to be happy; we only need a loving heart and kindness towards others. So go out there, find your purpose, live life to the fullest, love yourself unconditionally, help those who are less fortunate than you, give back whenever possible, make time for what matters most -- family & friends -- enjoy nature's beauty by going on hikes or camping trips outside.\n",
      "Life is a gift so we must learn how to live it! :)\n",
      "I believe the meaning of life is to be happy and to love yourself unconditionally. If you can do those two things, then everything else will fall into place naturally. We were born with certain gifts and talents for our own personal growth as well as the growth of others around us so use them wisely!\n",
      "When we feel like we've reached our full potential in life and are living up to our true potential (not just settling), then that is when you can say \"I have found\n",
      "llama_print_timings:        load time =  3603.13 ms\n",
      "llama_print_timings:      sample time =  1572.13 ms /   512 runs   (    3.07 ms per token,   325.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5247.21 ms /   265 tokens (   19.80 ms per token,    50.50 tokens per second)\n",
      "llama_print_timings:        eval time = 37885.72 ms /   510 runs   (   74.29 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time = 44827.11 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691302912\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102b1f9d0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x102b21ad0\n",
      "ggml_metal_init: loaded kernel_mul                            0x102b20fb0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x102b22550\n",
      "ggml_metal_init: loaded kernel_scale                          0x102b22f80\n",
      "ggml_metal_init: loaded kernel_silu                           0x102b237a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x102b213b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x102b240d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x102b25000\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102b26370\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x102b265d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x102b25880\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x102b27980\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x102b26e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x102b280c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x102b289f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x102b29470\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x102b29c70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102b2a750\n",
      "ggml_metal_init: loaded kernel_norm                           0x102b2b9b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x102b2c610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102b2cf70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102b2d930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x102b2e340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x102b2ee40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x102b2f770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x102b2ffc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x102b30990\n",
      "ggml_metal_init: loaded kernel_rope                           0x102b31490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x102b32010\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x102b32eb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x102b33a40\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x102b34230\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it as fully as you can and to enjoy every moment.\n",
      "I think that in order to be truly happy you have to be content with who you are and what you have, but also know that there is always room for improvement and growth.\n",
      "I am a firm believer that no matter how much money or success someone has, it’s never enough, so I try to live my life by the motto “never stop learning.”\n",
      "To me, happiness means being satisfied with what you have in your life right now but always looking forward to what tomorrow might bring.\n",
      "I believe that having a positive outlook on life is important because it can help to shape your perspective and improve the quality of your experiences.\n",
      "I think that people should strive to make the best of every situation, no matter how difficult or challenging it may be.\n",
      "I believe that happiness is a choice we make every day. It’s about finding joy in the small things and appreciating what we have. It’s also about having gratitude for all the good things in our life.\n",
      "I think that being happy means different things to different people, but ultimately it comes down to feeling good and being content with your own life. It’s important to remember that happiness is not something you can find or achieve; it’s a state of mind that you have to cultivate on a daily basis.\n",
      "There are many paths to choosing happiness as an overall goal for a life well lived, but I think the most important thing is to be happy with who you are and what you have right now. If you can do that, everything else will fall into place naturally over time…and if it doesn’t? Well then maybe there was something wrong with your path in the first place ����\n",
      "I believe that happiness is a choice we make every day. It may not always be easy, but if you choose to look at life through a positive lens and focus on what makes you happy, I think it’s possible for anyone to find happiness in their lives.\n",
      "Previous article How To Start A Book Club?\n",
      "Next article What Is Happiness?\n",
      "How To Start A Book Club?\n",
      "What Is Happiness?\n",
      "How To Be Happy And Healthy?\n",
      "How To Create More Space In Your Life?\n",
      "How To Become Happy With Less Money?\n",
      "How To Make The Most Out Of Each Day?\n",
      "How To Achieve Happiness?\n",
      "\n",
      "llama_print_timings:        load time =  2720.15 ms\n",
      "llama_print_timings:      sample time =  1667.81 ms /   512 runs   (    3.26 ms per token,   306.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5325.04 ms /   265 tokens (   20.09 ms per token,    49.76 tokens per second)\n",
      "llama_print_timings:        eval time = 37920.31 ms /   510 runs   (   74.35 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time = 45058.12 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691302960\n",
      "llama.cpp: loading model from ./models/7B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x15b61f3a0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x15b6214a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x15b620980\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x15b621f20\n",
      "ggml_metal_init: loaded kernel_scale                          0x15b622950\n",
      "ggml_metal_init: loaded kernel_silu                           0x15b623190\n",
      "ggml_metal_init: loaded kernel_relu                           0x15b620d80\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15b623aa0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x15b624ab0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x15b624d10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x15b626060\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x15b627200\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x15b626680\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x15b6268e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x15b627b10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x15b628400\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x15b628e80\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x15b6296a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x15b62a130\n",
      "ggml_metal_init: loaded kernel_norm                           0x15b62b230\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x15b62b630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x15b62c830\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x15b62d1d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x15b62dbf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x15b62e700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x15b62f090\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x15b62fa30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x15b6303f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x15b630e80\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x15b631a60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x15b632910\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x15b633460\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x15b62a8f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I believe that one should not kill, but if someone tries to hurt you, then you may do what is necessary to protect yourself and your loved ones.\n",
      "I believe that love is the most beautiful feeling in the world. When a man can look at his wife and tell her with his eyes how much she means to him, when he can tell her everything, even if he feels afraid sometimes because he doesn’t want her to go away…that is true love.\n",
      "I believe there are many things that make us happy; having good health, being loved by our family, having friends who care for us and making a difference in someone else’s life.\n",
      "I believe one should respect others no matter who they are or where they come from because everyone has feelings too!\n",
      "I believe it is important to have goals, but that you shouldn’t be afraid of failure. Sometimes failing can lead to something better than what you expected.\n",
      "What do you believe in? We all have an opinion on this question and each one of us may answer differently depending on our values or beliefs. In general though there are some things most people agree with such as love, honesty, friendship and family which should be cherished above anything else!\n",
      "We live in a world where we’re constantly bombarded by so much information that it can be hard to know what to believe anymore. This is why I think it’s important for everyone to take some time out from work or school every now and then just sit back, relax, take a deep breath and ask themselves “What do you believe in?”\n",
      "It may seem like an odd question at first but if we take a step back and really think about it we can all come up with our own answers! There’s no right or wrong answer here because everyone has their own personal set of values that they hold close to heart. The key is knowing what those values are for yourself so when somebody asks you “What do you believe in?” You know exactly how your going answer them without second guessing yourself along the way\n",
      "The meaning of life is a question that many people have pondered throughout time, and it’s one that we all struggle with from time to time. While there are many different interpretations about what we should be doing here on earth or why we were put here in the first place, I think there are some things that most of us agree upon – like love for our family members and friends!\n",
      "I believe that everyone has their\n",
      "llama_print_timings:        load time =  2786.40 ms\n",
      "llama_print_timings:      sample time =  1704.62 ms /   512 runs   (    3.33 ms per token,   300.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5312.61 ms /   265 tokens (   20.05 ms per token,    49.88 tokens per second)\n",
      "llama_print_timings:        eval time = 37855.45 ms /   510 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time = 45018.23 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303008\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x126237c90\n",
      "ggml_metal_init: loaded kernel_add_row                        0x126239d90\n",
      "ggml_metal_init: loaded kernel_mul                            0x126239270\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12623a810\n",
      "ggml_metal_init: loaded kernel_scale                          0x12623b240\n",
      "ggml_metal_init: loaded kernel_silu                           0x12623ba60\n",
      "ggml_metal_init: loaded kernel_relu                           0x126239670\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12623c390\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12623d2c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12623e630\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12623e890\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12623db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12623fc40\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12623f0c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x126240380\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x126240cb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x126241730\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x126241f30\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x126242a10\n",
      "ggml_metal_init: loaded kernel_norm                           0x126243c70\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1262448d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x126245230\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x126245bf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x126246600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x126247100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x126247a30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x126248280\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x126248c50\n",
      "ggml_metal_init: loaded kernel_rope                           0x126249720\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12624a2a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12624b120\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12624bca0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12624c4b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy...\n",
      "My father once told me that a man should never forget his childhood. My father also said that it was very important to forgive oneself every day because if we don’t, then we will never really grow up. I think about my father a lot these days. Sometimes, when I am walking down the street or preparing food in the kitchen, I feel like he is nearby. I can almost hear his voice, and it makes me smile to remember all those things he once told me. It was the same for my mother; she always made sure that we knew how much she loved us.\n",
      "The first time I was taken away from home was when I was just six years old. My father had a good job at the local newspaper, and my mother was a housewife who took care of her children. We were a happy family. But then, one night, someone broke into our home and stole everything we owned. They even took the clothes off our backs! I remember how scared I felt as they searched through our things looking for anything valuable. My father tried to stop them but he was no match for these men with guns who seemed so much bigger than him. The only thing that saved us from being shot was my mother’s quick thinking; she grabbed me and ran out into the street, where we could hear gunshots coming from inside our house! I remember crying hysterically as we watched them destroy everything we had worked so hard to build up over the years.\n",
      "I believe that everyone should have a happy childhood...\n",
      "“The meaning of life is happiness” – this quote by Dalai Lama teaches us how important it is for us humans to be happy in order to live a fulfilling and successful life.\n",
      "Happiness helps you to achieve your goals, whether they are related to work or family. When you’re happy, everything seems easier; even if things get tough at times, there will always be some hope that makes it worthwhile for us all! Happiness is what gives meaning and value to our lives. It allows us not only enjoy ourselves but also makes others around us happier too – whether they know about it or not.\n",
      "I believe the meaning of life is to be happy, and I want everyone in this world to have a happy childhood...\n",
      "To be happy, you don’t need anything more than love and laughter…\n",
      "I think that the\n",
      "llama_print_timings:        load time =  2075.67 ms\n",
      "llama_print_timings:      sample time =  1521.67 ms /   512 runs   (    2.97 ms per token,   336.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8714.06 ms /   265 tokens (   32.88 ms per token,    30.41 tokens per second)\n",
      "llama_print_timings:        eval time = 18396.87 ms /   510 runs   (   36.07 ms per token,    27.72 tokens per second)\n",
      "llama_print_timings:       total time = 28754.26 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303039\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x116e971b0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x116e985d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x116e989d0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x116e993c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x116e99c90\n",
      "ggml_metal_init: loaded kernel_silu                           0x116e9a5d0\n",
      "ggml_metal_init: loaded kernel_relu                           0x116e9afd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x116e9bb90\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x116e9cad0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x116e9de40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x116e9e0a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x116e9f2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x116e9e720\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x116e9e980\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x116e9fd40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x116ea0650\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x116ea0f90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x116ea18f0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x116ea29d0\n",
      "ggml_metal_init: loaded kernel_norm                           0x116ea3360\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x116ea3760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x116ea4960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x116ea5330\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x116ea5d00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x116ea67f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x116ea7300\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x116ea7b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x116ea8540\n",
      "ggml_metal_init: loaded kernel_rope                           0x116ea8fa0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x116ea9200\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x116ea9d60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x116eaabb0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x116ea2c30\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift, whatever that may be. Mine is painting.\n",
      "I am a full-time professional artist who makes my living selling art online and in galleries across Canada. Since graduating from University with an Honours Bachelor of Fine Art degree in 2006, I have worked hard at pursuing a career as an artist. In just over a decade, I have developed a reputation for excellence, integrity and professionalism.\n",
      "The greatest compliment I could ever receive is that a piece of artwork has moved the viewer, and perhaps even inspired them to look at our world with a renewed sense of wonder.\n",
      "In my paintings, you will see scenes that are familiar from travels around North America or simply from walks in my own backyard. But these scenes have been transformed by the filter of my imagination into works that are both beautiful and evocative. I invite you to look closely at each piece to discover details which may have slipped past at first glance, details which are full of rich colour, texture, pattern and light.\n",
      "I hope my work will inspire you and give you a sense of peace and joy wherever it is in your home or office.\n",
      "2017-present: I am continuing to be represented by galleries in Toronto, Calgary, Regina, and Kelowna.\n",
      "2016: I was awarded the prestigious Signature Membership designation by the Federation of Canadian Artists. This is a very high honour reserved for only the most outstanding artists, and only 3% of all members ever achieve this level of recognition. As part of this award, my work will be shown at the upcoming Annual Signature Show at The FCA Gallery on Granville Island in Vancouver from May 19 - June 26, 2017.\n",
      "Also in 2016, I was appointed by the Federation as a juror for their annual National Art Competition. I had the privilege of reviewing over 350 submissions and selecting just under 80 pieces to be shown at this year's competition which runs from May 19 - June 26, 2017.\n",
      "2014: The Federation of Canadian Artists has chosen one of my paintings for their annual Signature Show in Vancouver. The painting was selected as Best Oil or Acrylic Pain\n",
      "llama_print_timings:        load time =  1523.16 ms\n",
      "llama_print_timings:      sample time =  1433.48 ms /   512 runs   (    2.80 ms per token,   357.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8655.23 ms /   265 tokens (   32.66 ms per token,    30.62 tokens per second)\n",
      "llama_print_timings:        eval time = 18410.96 ms /   510 runs   (   36.10 ms per token,    27.70 tokens per second)\n",
      "llama_print_timings:       total time = 28619.30 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303070\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x129e34c00\n",
      "ggml_metal_init: loaded kernel_add_row                        0x129e36d00\n",
      "ggml_metal_init: loaded kernel_mul                            0x129e361e0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x129e37780\n",
      "ggml_metal_init: loaded kernel_scale                          0x129e381b0\n",
      "ggml_metal_init: loaded kernel_silu                           0x129e389f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x129e365e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x129e39300\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x129e3a310\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x129e3a570\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x129e3b8c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x129e3ca60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x129e3bee0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x129e3c140\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x129e3d370\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x129e3dc60\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x129e3e6e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x129e3ef00\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x129e3f990\n",
      "ggml_metal_init: loaded kernel_norm                           0x129e40a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x129e40e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x129e42090\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x129e42a30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x129e43450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x129e43f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x129e448f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x129e45290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x129e45c50\n",
      "ggml_metal_init: loaded kernel_rope                           0x129e466e0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x129e472c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x129e48170\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x129e48cc0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x129e40150\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "Because, if you're not happy with it then what's the point? It could all end tomorrow or yesterday but you don't know when so you have to make the most out of everyday and get as much done as possible. You should always do something that makes you smile because that way you will enjoy life a lot more.\n",
      "The meaning of life is to be happy, I believe this because if your not then there is no point in living. It could all end tomorrow or it could start but either way we don't know so we have to make the most out of everyday and get as much done as possible. We should always do something that makes us smile because that way we will enjoy life a lot more.\n",
      "I belive the meaning of life is to be happy, I believe this because if your not then there is no point in living. It could all end tomorrow or it could start but either way we don't know so we have to make the most out of everyday and get as much done as possible. We should always do something that makes us smile because that way we will enjoy life a lot more.\n",
      "I belive the meaning of life is to be happy, I believe this because if your not then there is no point in living. It could all end tomorrow or it could start but either way we don't know so we have to make the most out of everyday and get as much done as possible. We should always do something that makes us smile because that way we will enjoy life a lot more. The meaning of life is to be happy, I believe this because if your not then there is no point in living. It could all end tomorrow or it could start but either way we don't know so we have to make the most out of everyday and get as much done as possible. We should always do something that makes us smile because that way we will enjoy life a lot more.\n",
      "I belive the meaning of life is to be happy, I believe this because if your not then there is no point in living. It could all end tomorrow or it could start but either way we don't know so we have to make the most out of everyday and get as much done as possible. We should always do something that makes us smile because that way we will enjoy life a lot more. I belive the meaning of life is to be happy, I believe this because if\n",
      "llama_print_timings:        load time =  1522.54 ms\n",
      "llama_print_timings:      sample time =  1455.43 ms /   512 runs   (    2.84 ms per token,   351.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8675.52 ms /   265 tokens (   32.74 ms per token,    30.55 tokens per second)\n",
      "llama_print_timings:        eval time = 18421.53 ms /   510 runs   (   36.12 ms per token,    27.68 tokens per second)\n",
      "llama_print_timings:       total time = 28680.74 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303100\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12ae3a560\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13ae17290\n",
      "ggml_metal_init: loaded kernel_mul                            0x13ae16b30\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13ae181b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13ae18c40\n",
      "ggml_metal_init: loaded kernel_silu                           0x13ae19480\n",
      "ggml_metal_init: loaded kernel_relu                           0x13ae178c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13ae19d90\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13ae1ace0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13ae1c070\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13ae1c2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13ae1d520\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13ae1d780\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13ae1cb40\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13ae1df60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13ae1e8a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13ae1f220\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13ae1fb60\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13ae20c40\n",
      "ggml_metal_init: loaded kernel_norm                           0x13ae21730\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13ae22350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13ae22d10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13ae23700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13ae24100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13ae24be0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13ae256f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13ae25f20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13ae268c0\n",
      "ggml_metal_init: loaded kernel_rope                           0x13ae271e0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13ae27d60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13ae28c50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13ae297e0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13ae20ea0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  8755.22 MB, offs =  16852172800, (25139.67 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (25151.84 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25553.84 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25715.84 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25907.84 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33mggml_metal_graph_compute: command buffer 5 failed with status 5\n",
      "GGML_ASSERT: ggml-metal.m:1128: false\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303119\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I love being a mom and my kids mean the world to me, but they are not my whole world. I have other passions. I am an entrepreneur, I write, I mentor young people, I work with non-profit organizations, I serve as a board member for several groups, and I still love to act (I’m currently shooting a new movie).\n",
      "My kids see me pursuing my dreams and being involved in things that mean something to me. They are happy that I am happy, but they have never once asked me to give up my career or passions for them.\n",
      "My husband is also passionate about his work, and he takes great pride in it. We are both working hard to provide a better life for our kids. It’s not just the money we earn that will make their lives richer, but the example of hard work and dedication we hope to instill in them as they grow up.\n",
      "It is my responsibility to teach my children what it means to be passionate about something – anything. And so I try to model for them how to follow your heart’s desire.\n",
      "I am not going to tell you to “follow your dream” because that often implies putting one thing above all else, and that can cause problems in relationships. What matters most is the connection we have with each other.\n",
      "Being passionate about something means different things to different people. For some, it’s a career. It could be anything from being a doctor or engineer, to working at a non-profit organization. It could be teaching children or helping animals. Or perhaps you have an idea for a product that will make life easier for others. It could even be something as simple as being passionate about spending time with your family.\n",
      "Passion is a powerful emotion and when we find what it is, we should pursue it in whatever way makes the most sense to us. And yes, our family often comes first – but that doesn’t mean you can’t follow your heart’s desire.\n",
      "Being passionate about something means different things to different people. For some, it’s a career. It could be anything from being a doctor or engineer, to working at a non-profit organization. It could be teaching children or helping animals. Or perhaps you have an idea for a product that will make life\n",
      "llama_print_timings:        load time = 21065.80 ms\n",
      "llama_print_timings:      sample time =   363.76 ms /   512 runs   (    0.71 ms per token,  1407.53 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13837.82 ms /   265 tokens (   52.22 ms per token,    19.15 tokens per second)\n",
      "llama_print_timings:        eval time = 114131.70 ms /   510 runs   (  223.79 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time = 128382.29 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303269\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m happiness. If you are happy, then you have a reason to live.\n",
      "I want to be happy again. There’s no need to worry that much about what other people will think. Just because they don’t understand me doesn’t mean I’m wrong. Even if this world never accepts me for who I am, there’s always another world waiting for me somewhere else out there.\n",
      "I thought I had found happiness in my relationship with the woman I loved. But that happiness was a lie. She betrayed me. I can’t forgive her for what she did to me. I may not be able to trust other people again, but I have learned one thing from this experience: never to put my faith in anyone but myself.\n",
      "I believe the meaning of life is happiness. If you are happy, then you have a reason to live. But if you are unhappy, then there’s no point to being alive at all. This world isn’t worth living in if it means going through pain and suffering every day just so that I can survive another one.\n",
      "So the meaning of life for me is happiness because without it what else would be left? If we didn’t have any reason whatsoever then why bother getting out of bed each morning anyway? There wouldn’t be anything worth living for!\n",
      "I believe the meaning of life is to be happy. If you are not happy, then there isn’t much point in being alive at all. I think that we should try our best to find what makes us happy and then do everything possible to pursue it. For me, this means working hard so that one day my dreams can become a reality.\n",
      "I believe the meaning of life is happiness. If you are not happy, then there isn’t much point in being alive at all. I think that we should try our best to find what makes us happy and then do everything possible to pursue it. For me, this means working hard so that one day my dreams can become a reality. It also includes spending time with loved ones who make me smile every day!\n",
      "I believe the meaning of life is happiness. If you are not happy, then there isn’t much point in being alive at all. I think that we should try our best to find what makes us happy and then do everything possible to pursue it. For me, this means working hard so that one day my dreams can become a reality.\n",
      "The\n",
      "llama_print_timings:        load time = 22156.93 ms\n",
      "llama_print_timings:      sample time =   364.49 ms /   512 runs   (    0.71 ms per token,  1404.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13140.02 ms /   265 tokens (   49.58 ms per token,    20.17 tokens per second)\n",
      "llama_print_timings:        eval time = 115213.45 ms /   510 runs   (  225.91 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:       total time = 128767.83 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e51103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691303420\n",
      "llama.cpp: loading model from ./models/13B-v2/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. I have a happy childhood and youth and this makes me feel that life is beautiful.\n",
      "I like listening to music when I am free. I like reading books, watching TV and walking in nature with my friends.\n",
      "For some people love means different things. For example, for some people love means marriage or sex. But for others love is something more than this - it's spiritual love. I think this kind of love can make me happy. It is not easy to be loved by someone and you need to deserve it, but if a man loves you - he will never leave you alone.\n",
      "I like cooking and sometimes I bake cakes or something sweet for my friends. I am good at home-making and I can do it very well. It's not difficult for me to keep the house clean. I always try to be organized and this is not a problem for me. So I think that my future husband will feel comfortable with me in his home.\n",
      "I want to find a man who will understand me, love me and respect me. I want him to be caring and attentive. I would like him to smile at me every day and shower me with kisses. I want to be loved and cared about! I don't need a king or a prince, but I want to find a man who will love me forever.\n",
      "I have never been married before and I want to meet my future husband on this site. I hope we can make each other happy and give each other all the feelings that we have lost. I dream of love, true love. I would like to fall in love with someone from another country and be loved by him!\n",
      "I think that the man should be caring, attentive, loving and kind. He must understand me and respect my feelings. The main thing is that he has a good heart and he wants to change the world for the better. I want us to have something in common so we can be happy together!\n",
      "I am a friendly and very cheerful person. I don't like to argue with people because it is not good for my mental health. But if someone offends me or makes me angry, I will show him how wrong he was!\n",
      "I believe that life is beautiful and it should be lived to the fullest. We can make our dreams come true only when we are happy. The meaning of life is different for everyone; some think about their\n",
      "llama_print_timings:        load time = 22298.99 ms\n",
      "llama_print_timings:      sample time =   364.34 ms /   512 runs   (    0.71 ms per token,  1405.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14077.38 ms /   265 tokens (   53.12 ms per token,    18.82 tokens per second)\n",
      "llama_print_timings:        eval time = 114759.81 ms /   510 runs   (  225.02 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time = 129250.55 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
