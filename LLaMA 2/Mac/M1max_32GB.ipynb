{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da7f75",
   "metadata": {},
   "source": [
    "Prepare Data & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B-v2\u001b[m\u001b[m                  ggml-vocab-llama.gguf   \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m7B-v2\u001b[m\u001b[m                   \u001b[31mtokenizer.model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b991d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the GGUF read/write example\n",
    "# !make gguf\n",
    "\n",
    "# # write a dummy GGUF model to test.gguf\n",
    "# !./gguf test.gguf w\n",
    "\n",
    "# # read the dummy GGUF model\n",
    "# !./gguf test.gguf r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install Python dependencies\n",
    "# !python3 -m pip install -r requirements.txt\n",
    "\n",
    "# # llama 2\n",
    "# # convert the model to ggml FP16 format\n",
    "# !python3 convert.py ../llama2/llama/llama-2-7b --outfile models/7B-v2/ggml-model-f16.gguf\n",
    "# !python3 convert.py ../llama2/llama/llama-2-13b --outfile models/13B-v2/ggml-model-f16.gguf\n",
    "# !python3 convert.py ../llama2/llama/llama-2-70b --outfile models/70B-v2/ggml-model-f16.gguf\n",
    "\n",
    "# # quantize the model to 4-bits (using q4_0 method)\n",
    "# !./quantize ./models/7B-v2/ggml-model-f16.gguf ./models/7B-v2/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/13B-v2/ggml-model-f16.gguf ./models/13B-v2/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/70B-v2/ggml-model-f16.gguf ./models/70B-v2/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch convert-llama2c-to-ggml embd-input-test gguf llama-bench build-info.h tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "convert-llama2c-to-ggml\n",
      "embd-input-test\n",
      "gguf\n",
      "llama-bench\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c common/common.cpp -o common.o\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c common/console.cpp -o console.o\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "cc -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml-alloc.c -o ggml-alloc.o\n",
      "\u001b[1mk_quants.c:186:11: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mvariable 'sum_x' set but not used [-Wunused-but-set-variable]\u001b[0m\n",
      "    float sum_x = 0;\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1mk_quants.c:187:11: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mvariable 'sum_x2' set but not used [-Wunused-but-set-variable]\u001b[0m\n",
      "    float sum_x2 = 0;\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1mk_quants.c:182:14: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1munused function 'make_qkx1_quants' [-Wunused-function]\u001b[0m\n",
      "static float make_qkx1_quants(int n, int nmax, const float * restrict x, uint8_t * restrict L, float * restrict the_min,\n",
      "\u001b[0;1;32m             ^\n",
      "\u001b[0m3 warnings generated.\n",
      "\u001b[1mllama.cpp:596:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1munused variable 'kB' [-Wunused-const-variable]\u001b[0m\n",
      "static const size_t kB = 1024;\n",
      "\u001b[0;1;32m                    ^\n",
      "\u001b[0m1 warning generated.\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-metal.o ggml-alloc.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o ggml-alloc.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o convert-llama2c-to-ggml  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o grammar-parser.o k_quants.o ggml-metal.o ggml-alloc.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ --shared -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/gguf/gguf.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o gguf  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o llama-bench  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit\n",
      "\u001b[1mexamples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp:591:9: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mTODO: implement file saving using gguf [-W#pragma-messages]\u001b[0m\n",
      "#pragma message(\"TODO: implement file saving using gguf\")\n",
      "\u001b[0;1;32m        ^\n",
      "\u001b[0m\u001b[1mexamples/train-text-from-scratch/train-text-from-scratch.cpp:2617:9: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mTODO: implement file saving using gguf [-W#pragma-messages]\u001b[0m\n",
      "#pragma message(\"TODO: implement file saving using gguf\")\n",
      "\u001b[0;1;32m        ^\n",
      "\u001b[0m1 warning generated.\n",
      "c++ -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "1 warning generated.\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748062\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 4096\n",
      "llama_model_load_internal: n_head       = 32\n",
      "llama_model_load_internal: n_head_kv    = 32\n",
      "llama_model_load_internal: n_layer      = 32\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 11008\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 7B\n",
      "llama_model_load_internal: model ftype  = mostly Q4_0\n",
      "llama_model_load_internal: model size   = 6.74 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3647.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13971eb40\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13961f4e0\n",
      "ggml_metal_init: loaded kernel_mul                            0x139620260\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1396207f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1396210d0\n",
      "ggml_metal_init: loaded kernel_silu                           0x139621a00\n",
      "ggml_metal_init: loaded kernel_relu                           0x139622310\n",
      "ggml_metal_init: loaded kernel_gelu                           0x139622b60\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1396235d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x139623cf0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x139625060\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x139625a50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x139626540\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x139626eb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x139627980\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1396282d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x139628c20\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x139629570\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x139629ed0\n",
      "ggml_metal_init: loaded kernel_norm                           0x139720030\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x139720bd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13962ad70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13962b3b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13962be90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13962c8a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13962d4a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13962df20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13962e9d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x13962f5a0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x1396302f0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x139630d60\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x1396317d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x139631fc0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x139632a30\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x1396334a0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x139633f10\n",
      "ggml_metal_init: loaded kernel_rope                           0x139634700\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x139635290\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x139635e70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1396369e0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x139637530\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.41 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, ( 3649.83 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3907.83 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, ( 3979.84 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live and let live. Hinweis: Diese Seite wurde zuletzt am 31. This is a list of episodes from the Nickelodeon animated television series, Hey Arnold! The following is a list of episodes from the Nickelard show Hey Arnold!.\n",
      "The series originally ran on Fox Kids until it was cancelled in , but continued to air in reruns for many years afterwards. The series was renewed by Nicktoons as part of its rebranding, and premiered on November 18,.\n",
      "After the pilot episode \"Downtown as Fruits\" was produced, The series was originally going to be aired on Fox Kids Network in with reruns airing on Nickelodeon in the s.\n",
      "When the show first premiered on Fox Kids, it received little to no promotion or fanfare whatsoever because of how different and unique Hey Arnold!\n",
      "The series has been highly praised by critics for its animation style and message-driven plots. He was also a member of the writers' group known as The Mighty Zilch , which included Mike Reiss, Tom Ruegger and Jhonen Vasquez.\n",
      "As part of that deal, Nickelodeon acquired the rights to 23 episodes already in production for Season 1, which were later aired on Fox Kids during the — season.\n",
      "The show was originally scheduled to air its 9th and final episode \"Wayne-ness\" on Saturday, July 30, The show was renewed by Nicktoons as part of its rebranding, and premiered on November 18,.\n",
      "After the pilot episode \"Downtown as Fruits\" was produced, This episode introduces Arnold's friends Gerald, Helga, Stinky, Curly, and Harold.\n",
      "Later episodes of the series also feature Mr. Wellington , a recurring minor character who is Arnold's homeroom teacher at P.\n",
      "In later seasons, he would often be depicted as being somewhat of an enigmatic figure and a potential love interest for Helga in the background.\n",
      "Hey Arnold! The series was canceled in May after five seasons due to low ratings. Hey Arnold! I believe the meaning of life is to live and let live.\n",
      "This page was last edited on 19 January , at By using this site, you agree to the Terms of\n",
      "llama_print_timings:        load time =  1350.50 ms\n",
      "llama_print_timings:      sample time =   622.44 ms /   512 runs   (    1.22 ms per token,   822.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1327.66 ms /   265 tokens (    5.01 ms per token,   199.60 tokens per second)\n",
      "llama_print_timings:        eval time = 10449.71 ms /   510 runs   (   20.49 ms per token,    48.81 tokens per second)\n",
      "llama_print_timings:       total time = 12475.53 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748076\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 4096\n",
      "llama_model_load_internal: n_head       = 32\n",
      "llama_model_load_internal: n_head_kv    = 32\n",
      "llama_model_load_internal: n_layer      = 32\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 11008\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 7B\n",
      "llama_model_load_internal: model ftype  = mostly Q4_0\n",
      "llama_model_load_internal: model size   = 6.74 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3647.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1126aaf70\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1126ad120\n",
      "ggml_metal_init: loaded kernel_mul                            0x1126ac600\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1126adbb0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1126ae5f0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1126aedf0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1126aca00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1126af740\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1126b06c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1126b0920\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1126b0d70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1126b2540\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1126b3060\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1126b39b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1126b42e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1126b4b30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1126b55c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1126b5f30\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1126b68d0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1126b1d30\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1126b7e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1126b90d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1126b9b50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1126ba770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1126bb150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1126bbbb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1126bc640\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1126bd1e0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x1126bdf30\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x1126be9d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1126bf440\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x1126bfeb0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x1126c06a0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1126c1110\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x1126c1b80\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x1126c25f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1126c2de0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1126c39f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1126c4570\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1126c50f0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1126c5c70\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.41 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, ( 3649.83 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3907.83 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, ( 3979.84 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and fulfilled. everybody wants that and everyone has his own way to get there, but for me it means being surrounded by people who are good and kind hearted, feeling safe and loved in a place where you can express yourself and feel like you belong\n",
      "I love animals, I'm not the type of person who would kill them.\n",
      "I know it doesn't sound very important but to me it is because I believe that everyone should be treated with respect and kindness no matter what your profession or status in life is.\n",
      "People often say they want to make a difference in the world, but I think everyone has the ability to do so if they only open their hearts and minds to others.\n",
      "I love to travel and explore new places, people and cultures, it helps me grow as a person and understand different perspectives on life.\n",
      "I think that what is most important in life is having faith in yourself and staying true to who you are despite all the obstacles along your journey because if you don't believe in yourself nobody else will either!\n",
      "You can ask me anytime, I would be happy to answer it :)\n",
      "Send a private message to Jasmine1702\n",
      "Find More Posts by Jasmine1702\n",
      "Re: What is the meaning of life?\n",
      "The meaning of life is to love and serve God with your whole heart.\n",
      "Hey I agree with you, that's my answer too! But we can't do it unless we know there's a God out there because if not, what's the point in loving him or serving him?\n",
      "Send a private message to jess58\n",
      "Find More Posts by jess58\n",
      "The meaning of life is to live your life so that when you die and look back on it, you have no regrets. Some people will not be happy with what they achieved in their lives, because they did not set any goals for themselves and just let life happen instead of trying to make the most out of it.\n",
      "Send a private message to Katie96\n",
      "Find More Posts by Katie96\n",
      "The meaning of life is... life!\n",
      "We live life.\n",
      "We die life.\n",
      "Meaning of life? It's all about us and living our lives. The way we want, the way we feel, how we treat others, etc..\n",
      "Life has no meaning if you are not alive. So\n",
      "llama_print_timings:        load time =   875.54 ms\n",
      "llama_print_timings:      sample time =   714.02 ms /   512 runs   (    1.39 ms per token,   717.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1327.24 ms /   265 tokens (    5.01 ms per token,   199.66 tokens per second)\n",
      "llama_print_timings:        eval time = 10530.80 ms /   510 runs   (   20.65 ms per token,    48.43 tokens per second)\n",
      "llama_print_timings:       total time = 12658.42 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748089\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 4096\n",
      "llama_model_load_internal: n_head       = 32\n",
      "llama_model_load_internal: n_head_kv    = 32\n",
      "llama_model_load_internal: n_layer      = 32\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 11008\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 7B\n",
      "llama_model_load_internal: model ftype  = mostly Q4_0\n",
      "llama_model_load_internal: model size   = 6.74 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3647.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13c713e20\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13c715330\n",
      "ggml_metal_init: loaded kernel_mul                            0x13c715730\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13c716100\n",
      "ggml_metal_init: loaded kernel_scale                          0x13c716a00\n",
      "ggml_metal_init: loaded kernel_silu                           0x13c717300\n",
      "ggml_metal_init: loaded kernel_relu                           0x13c717bb0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13c718490\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13c718e40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13c719560\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13c71a860\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13c71b250\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13c71bd90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13c71c860\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13c71d1b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13c71db00\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13c71e440\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13c71ec90\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13c71f760\n",
      "ggml_metal_init: loaded kernel_norm                           0x13c720350\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13c720eb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13c721f50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13c7229a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13c7235c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13c723f80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13c724a10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13c7254a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13c726080\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x13c726da0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x13c727810\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x13c7282d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x13c728d40\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x13c729530\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x13c729fa0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x13c72aa10\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x13c72b480\n",
      "ggml_metal_init: loaded kernel_rope                           0x13c72bc70\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13c72c880\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13c72d430\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13c72df80\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13c72eb00\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.41 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, ( 3649.83 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3907.83 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, ( 3979.84 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a good and happy life. sierpanyjacques.com: The Meaning Of Life (): Bill Watterson: Books.\n",
      "The Meaning of Life has ratings and 76 reviews. Paul said: \"The Meaning of Life,\" by Bill Watterson, is the ultimate comic strip book for a. The Meaning of Life () on IMDb: Movies, TV, Celebs, and more. I found this book in our local library when I was about 14 years old. It was a small paperback edition with black and white pictures that had been printed.\n",
      "The author presents the meaning of life through the example of his own life experiences. He describes his encounters with people who were helpful to him, such as. The Meaning Of Life: Cartoonist Bill Watterson's New Book Is A Treatise On The Human Experience. In the new book \"The Meaning of Life,\" cartoonist Bill Watterson.\n",
      "I found this book in our local library when I was about 14 years old. It was a small paperback edition with black and white pictures that had been printed on . The Meaning Of Life - Bill Watterson.\n",
      "Read Online Free Book PDF The meaning of life: cartoonist bill watterson's new book is a treatise on the human experience at www. Aug 24, · Buy The Meaning of Life by Bill Watterson online at listop.pl Paperback $ Kindle Edition $ Read with Our Free App; Audiobook.\n",
      "The meaning of life is to live a good and happy life. I believe that you have to find your own meaning in life. If you look for it, then you will find it! You can go out and try to look for happiness, or you can stay at home and wait until happiness comes looking for you.\n",
      "The Meaning of Life is a collection of comics by the author Bill Watterson. The book was released in September and collects cartoons from Watterson's comic strip Calvin and Hobbes that were published between December 2, and July 31, In the first part of the book, .\n",
      "The meaning of life is to live a good and happy life. I believe that you have to find your own meaning in life. If you look for it, then you will find it! You can go out and try to look for happiness\n",
      "llama_print_timings:        load time =   874.99 ms\n",
      "llama_print_timings:      sample time =   682.61 ms /   512 runs   (    1.33 ms per token,   750.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1326.98 ms /   265 tokens (    5.01 ms per token,   199.70 tokens per second)\n",
      "llama_print_timings:        eval time = 10469.39 ms /   510 runs   (   20.53 ms per token,    48.71 tokens per second)\n",
      "llama_print_timings:       total time = 12561.92 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-q4_0.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748103\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 4096\n",
      "llama_model_load_internal: n_head       = 32\n",
      "llama_model_load_internal: n_head_kv    = 32\n",
      "llama_model_load_internal: n_layer      = 32\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 11008\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 7B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 6.74 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 12853.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x144713e00\n",
      "ggml_metal_init: loaded kernel_add_row                        0x144715fd0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1447154b0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x144716a30\n",
      "ggml_metal_init: loaded kernel_scale                          0x144717450\n",
      "ggml_metal_init: loaded kernel_silu                           0x144717ce0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1447158b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1447185a0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1447194f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14471a850\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14471aab0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14471b3a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14471bec0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14471c810\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14471d170\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14471dc40\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14471e580\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14471edd0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14471f740\n",
      "ggml_metal_init: loaded kernel_norm                           0x144720960\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1447214d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x144721f00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x144722990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x144723590\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x144723f70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x144724a00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x144725490\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x144726050\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x144726da0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x144727840\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1447282b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x144728d20\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x144729510\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x144729f80\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x14472a9f0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x14472b460\n",
      "ggml_metal_init: loaded kernel_rope                           0x14472bc50\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x14472c830\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14472d3e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14472df60\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14472eb10\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.41 MB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.55 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, (12854.97 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13112.97 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, (13184.98 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it as fully and intensely as possible. This includes my own life, your life, the lives of others, and the life of the world around us.\n",
      "We may not be able to make a big impact on the world but we can have a huge impact on our little corner of the world. We all have a sphere of influence that is smaller than the world but bigger than ourselves.\n",
      "My sphere of influence includes my family, friends and community (including those in need) as well as anyone I meet through my work with clients or online.\n",
      "The purpose of life is to give it meaning and direction. This means having goals that help us feel a sense of fulfillment when we reach them. It also means taking steps toward achieving the goals we’ve set for ourselves so that one day they become reality.\n",
      "Life is full of challenges but there are always ways around them if you want to find them. And even better than finding a way around something is finding an easier path through it all together!\n",
      "What Is The Purpose Of Life? (Rephrased)\n",
      "The purpose of life is to live fully and intensely, giving it meaning by setting goals and taking steps toward achieving them. This means that there’s always hope even when things seem hopeless—we just need the willpower to look for the easier path.\n",
      "What Is The Purpose Of Life? (In More Detail)\n",
      "I believe that every person on this planet has a purpose in life, whether they know it or not. We all have something special about ourselves and our talents that can be used to make a positive difference in the world—it’s just up to us to figure out what those things are!\n",
      "If you don’t already know your purpose in life then ask yourself these questions: What do I want? Why am I here? How can I help others? If there is no answer then keep asking yourself until one does come up. Once it does, start thinking about how you might use this knowledge to improve yourself or the world around you.\n",
      "What Is The Purpose Of Life (In Even More Detail)\n",
      "I believe that every person on this planet has a purpose in life, whether they know it or not. We all have something special about ourselves and our talents that can be used to make a positive difference in the world—it’s just up to us to figure out what those things are! If you don’t already know your purpose then ask\n",
      "llama_print_timings:        load time =  3142.46 ms\n",
      "llama_print_timings:      sample time =  1205.18 ms /   512 runs   (    2.35 ms per token,   424.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1244.36 ms /   265 tokens (    4.70 ms per token,   212.96 tokens per second)\n",
      "llama_print_timings:        eval time = 36476.30 ms /   510 runs   (   71.52 ms per token,    13.98 tokens per second)\n",
      "llama_print_timings:       total time = 39046.78 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748146\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 4096\n",
      "llama_model_load_internal: n_head       = 32\n",
      "llama_model_load_internal: n_head_kv    = 32\n",
      "llama_model_load_internal: n_layer      = 32\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 11008\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 7B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 6.74 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 12853.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x127812300\n",
      "ggml_metal_init: loaded kernel_add_row                        0x127813810\n",
      "ggml_metal_init: loaded kernel_mul                            0x127813c10\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1278145e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x127814ee0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1278157e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1278160e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1278169c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x127817340\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x127817a70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x127818db0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1278197d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12781a310\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12781adb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12781b730\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12781c0b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12781ca10\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12781d360\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12781dcf0\n",
      "ggml_metal_init: loaded kernel_norm                           0x12781e8d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12781f490\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12781fe90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x127820f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x127821bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x127822570\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x127823040\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x127823ac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x127824650\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x1278253a0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x127825e10\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x127826880\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x1278272f0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x127827ae0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x127828550\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x127828fc0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x127829a30\n",
      "ggml_metal_init: loaded kernel_rope                           0x12782a220\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12782ae70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12782b9f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12782c570\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12782d080\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.41 MB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.55 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, (12854.97 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13112.97 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, (13184.98 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be fully present.\n",
      "I want to be fully alive, as much as possible, for as long as possible!\n",
      "I have always been fascinated with the idea that we are all just a thought away from dying…\n",
      "The most important thing is to “be” and live in full awareness of what you are experiencing.\n",
      "In this way you can learn to love anything and everything – even yourself.\n",
      "You will experience an expansion of your consciousness – you will feel connected to everyone and everything in the universe in a very profound, deep and powerful way.\n",
      "The meaning of life is to be fully present!\n",
      "I believe that we are all connected by our energy fields…\n",
      "And I know for sure that the quality of my energy field determines how healthy or unhealthy I am (mentally, physically & spiritually).\n",
      "My goal is to become more conscious of myself so that I can improve the quality of my life and thus the world around me.\n",
      "I want to become fully aware of who I really am…\n",
      "And I believe that this awareness will bring me happiness and fulfillment.\n",
      "The meaning of life is to be fully present – to experience every moment as if it were a precious gift from God!\n",
      "The Meaning Of Life: To Be Fully Present\n",
      "Do you have any comments, questions or tips for improving your own mindfulness? Let me know in the comment section below.\n",
      "Previous Post: « Why You Should Not Fear Death (and How to Face It)\n",
      "Next Post: 12 Ways To Develop A New Habit »\n",
      "I think the meaning of life is to be happy. I don’t care what you are doing, if you are not happy while doing it then there is something wrong with your life. If you want to work a dead-end job, that’s fine! As long as you are happy while doing it and you have no regrets later on in life when you realize how much time has passed by without enjoying yourself.\n",
      "I agree with you, but I think we need to be mindful of who we are spending our time around. I think the meaning of life is to find happiness in everything and everyone that surrounds us. That’s how love works.\n",
      "Hey! You wrote really nice things here ���� I am in agreement with you about being happy and being present. It sounds like a good way\n",
      "llama_print_timings:        load time =  2904.70 ms\n",
      "llama_print_timings:      sample time =  1188.56 ms /   512 runs   (    2.32 ms per token,   430.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1240.62 ms /   265 tokens (    4.68 ms per token,   213.60 tokens per second)\n",
      "llama_print_timings:        eval time = 37042.26 ms /   510 runs   (   72.63 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time = 39599.94 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748188\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 4096\n",
      "llama_model_load_internal: n_head       = 32\n",
      "llama_model_load_internal: n_head_kv    = 32\n",
      "llama_model_load_internal: n_layer      = 32\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 11008\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 7B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 6.74 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 12853.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12062af30\n",
      "ggml_metal_init: loaded kernel_add_row                        0x12062c440\n",
      "ggml_metal_init: loaded kernel_mul                            0x12062c840\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12062d210\n",
      "ggml_metal_init: loaded kernel_scale                          0x12062db10\n",
      "ggml_metal_init: loaded kernel_silu                           0x12062e410\n",
      "ggml_metal_init: loaded kernel_relu                           0x12062ed80\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12062f680\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x120630fd0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1206307e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x120631b10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x120632520\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1206330a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x120634180\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x120634af0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x120635490\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x120635e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x120636770\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1206370f0\n",
      "ggml_metal_init: loaded kernel_norm                           0x120637b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x120638710\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x120639170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x120639c00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12063a830\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12063b210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12063bc90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12063c710\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12063d2b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x12063e000\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x12063ea70\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x12063f4e0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x12063ff50\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x120640740\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1206411b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x120641c20\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x120642690\n",
      "ggml_metal_init: loaded kernel_rope                           0x120642e80\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x120643a90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x120644640\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x120645190\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x120645d10\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   73.41 MB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.55 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, (12854.97 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13112.97 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    72.02 MB, (13184.98 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose in this world. I think we all have a purpose, and it's up to us to discover what that purpose is.\n",
      "The only way out is through!\n",
      "I don't like to be around people who are negative or mean. It's not my personality type.\n",
      "I'm very passionate about life. I love the world - I love being here and experiencing everything it has to offer.\n",
      "I believe in God, but I have my own relationship with him. I don't like labels; I just call it \"God.\"\n",
      "Love is an amazing thing! It can heal all wounds and make you forget about the bad stuff.\n",
      "I think we are all here for a reason, and that purpose will be revealed to us in time.\n",
      "No matter what life throws at me, I always come out on top because my faith is stronger than any obstacle or challenge!\n",
      "When it comes down to it, no one can do anything for you except yourself. You have to want something badly enough before you will work hard enough to get it.\n",
      "I'm a big believer in karma - what goes around comes around! If I treat people with respect, they will return the favor and help me when I need it most.\n",
      "I believe that everyone is put on this earth for a reason, but we must find out what that purpose is ourselves rather than waiting for someone else to tell us.\n",
      "We all have a unique purpose in life, and it's up to each one of us to discover what our own individual calling may be. I hope you find yours soon!\n",
      "I'm not afraid to stand up for myself or others who are being mistreated - if they need help, then I will always be there for them!\n",
      "My faith has been tested many times in my life, but every time it has proven itself stronger than any challenge that comes against me.\n",
      "I am a fighter and will never give up on anything, even when the odds are stacked against me!\n",
      "There's nothing more important to me than family - they mean everything to me and I would do anything for them!\n",
      "My passion is helping people in need, whether it be through volunteering at my local church or simply being there when someone needs an ear.\n",
      "I believe that we should all strive to make the most of our lives by living each day as if it were our last - because one day\n",
      "llama_print_timings:        load time =  2905.68 ms\n",
      "llama_print_timings:      sample time =  1201.89 ms /   512 runs   (    2.35 ms per token,   425.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1245.03 ms /   265 tokens (    4.70 ms per token,   212.85 tokens per second)\n",
      "llama_print_timings:        eval time = 37144.40 ms /   510 runs   (   72.83 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time = 39728.37 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748231\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly Q4_0\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7024.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x155f21b30\n",
      "ggml_metal_init: loaded kernel_add_row                        0x155f23040\n",
      "ggml_metal_init: loaded kernel_mul                            0x155f23440\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x155f23e10\n",
      "ggml_metal_init: loaded kernel_scale                          0x155f24710\n",
      "ggml_metal_init: loaded kernel_silu                           0x155f25010\n",
      "ggml_metal_init: loaded kernel_relu                           0x155f25990\n",
      "ggml_metal_init: loaded kernel_gelu                           0x155f262c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x155f27bb0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x155f273e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x155f286e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x155f290c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x155f29c10\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x155f2a560\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x155f2aeb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x155f2b820\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x155f2c170\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x155f2cae0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x155f2d440\n",
      "ggml_metal_init: loaded kernel_norm                           0x155f2e6e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x155f2f250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x155f2fcd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x155f30750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x155f31370\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x155f31d00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x155f32780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x155f33230\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x155f33dc0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x155f34b10\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x155f35580\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x155f35ff0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x155f36a60\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x155f37250\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x155f37cc0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x155f38730\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x155f391a0\n",
      "ggml_metal_init: loaded kernel_rope                           0x155f39990\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x155f3a5a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x155f3b150\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x155f3bca0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x155f3c820\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.41 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, ( 7025.88 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7427.88 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, ( 7517.89 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live as authentically and honestly as possible; to be the best version of ourselves we can be.\n",
      "Mindfulness is the practice of living in the moment, not lost in thought or worries about the past or future but simply being present in this very moment. In this state we are more likely to make choices that are aligned with our values and beliefs, rather than those based on fear or anxiety.\n",
      "For example, if we are mindful about what we eat - we choose healthy food over foods that may harm us but also not be too rigid with ourselves which can lead to binge eating. It's finding balance between extremes so that healthy habits become sustainable lifestyle changes instead of short-term fixes for our problems or bad choices we make during times of stress when life throws unexpected curveballs at us!\n",
      "Another example would be being mindful about how much time you spend on social media - if your goal is to have a healthy relationship with technology, then it's important not only for yourself but also for the people around you who may become dependent on their phones for all kinds of reasons (such as anxiety).\n",
      "Mindfulness has many benefits including improved mental health because we are less likely to get caught up in negative thoughts or feelings when practicing mindfulness. It can help with stress relief by allowing us more time for self-care activities such as meditation and relaxation techniques which may also reduce physical symptoms like headaches due to increased blood flow throughout the body while reducing cortisol levels associated with stress hormones released during times of high anxiety.\n",
      "Mindfulness is a way of living that involves being present in your own life, without judgment or expectation. It's about accepting who you are and what you do as part of an overall approach to healthy living.\n",
      "In order to be mindful, we need to let go of past experiences and future goals so that our thoughts can flow freely through time like a river does on its way downstream--not fixed by any one thing or event but open-ended by nature's own design!\n",
      "Mindfulness is the practice of being present in each moment with awareness. It’s an active state, meaning you must pay attention to what you are doing and how it affects your body, mind, and spirit.\n",
      "In order for us humans to live well-balanced lives full of\n",
      "llama_print_timings:        load time =  1888.59 ms\n",
      "llama_print_timings:      sample time =  1729.46 ms /   512 runs   (    3.38 ms per token,   296.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2371.78 ms /   265 tokens (    8.95 ms per token,   111.73 tokens per second)\n",
      "llama_print_timings:        eval time = 19537.92 ms /   510 runs   (   38.31 ms per token,    26.10 tokens per second)\n",
      "llama_print_timings:       total time = 23793.08 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748257\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly Q4_0\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7024.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12461b640\n",
      "ggml_metal_init: loaded kernel_add_row                        0x12461d7f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x12461ccd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12461e280\n",
      "ggml_metal_init: loaded kernel_scale                          0x12461ecc0\n",
      "ggml_metal_init: loaded kernel_silu                           0x12461f4f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x12461d0d0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12461fe10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x124620d90\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x124621400\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x124621660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x124622a70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1246235d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x124624030\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1246249d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x124625470\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x124625e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x124626db0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x124627720\n",
      "ggml_metal_init: loaded kernel_norm                           0x124628170\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x124628d30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x124629790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12462a1f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12462adf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12462b7d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12462c250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12462ccd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12462d860\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x12462e5b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x12462f020\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x12462fa90\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x124630500\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x124630cf0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x124631760\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x1246321d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x124632c40\n",
      "ggml_metal_init: loaded kernel_rope                           0x124633430\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x124634070\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x124634c20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1246357a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1246362f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.41 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, ( 7025.88 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7427.88 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, ( 7517.89 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your passion, whatever it may be. Once you do that, and devote yourself to it wholeheartedly, money will come and so will success and happiness.\n",
      "I believe we are all in control of our own destiny. What you think about comes about. For this reason I try to only surround myself with happy, successful, positive people. If you want something, just go out and get it…don’t give up until you do.\n",
      "I believe you can learn a lot from your mistakes if you're willing to look closely at them. Find the lesson in every failure and use it as a springboard to greater things.\n",
      "I believe in hard work and perseverance no matter what you want to accomplish, and always giving more than 100%…even when no one is watching. It’s called integrity. I do think we all have a responsibility to others, but I also know that we must take care of ourselves first.\n",
      "I believe in living life to the fullest every day. Enjoy each moment with family and friends, because you never know what tomorrow may bring.\n",
      "I believe it is better to give than receive, even when receiving may be more gratifying.\n",
      "I believe that love is one of the most powerful forces on earth…and I believe in unconditional love. Love everyone unconditionally, including yourself, and good things will happen for you.\n",
      "I believe we should all take time out each day to give thanks for all we have been given. This includes our family, friends, possessions, health, etc. Remembering how blessed we are is a sure way to keep us humble…and that, I believe, is a good thing.\n",
      "I believe in the power of prayer and positive thinking. It can change your life for the better.\n",
      "I believe you should always try to look on the bright side of things. There’s no point in dwelling on the negative. A positive attitude will help you accomplish more than a negative one ever will. The glass is half full, not half empty.\n",
      "I believe that when we die our souls go somewhere else where they live eternally…not necessarily to heaven or hell. This is my personal belief and I know many people disagree. But it makes me feel better knowing that no matter what happens in this life, there will be a next one for all of us.\n",
      "I believe in reincarnation. There are so\n",
      "llama_print_timings:        load time =  1588.91 ms\n",
      "llama_print_timings:      sample time =   980.61 ms /   512 runs   (    1.92 ms per token,   522.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2624.60 ms /   265 tokens (    9.90 ms per token,   100.97 tokens per second)\n",
      "llama_print_timings:        eval time = 19052.09 ms /   510 runs   (   37.36 ms per token,    26.77 tokens per second)\n",
      "llama_print_timings:       total time = 22774.43 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748282\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly Q4_0\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7024.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1126871f0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1126893a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x112688880\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x112689e50\n",
      "ggml_metal_init: loaded kernel_scale                          0x11268a880\n",
      "ggml_metal_init: loaded kernel_silu                           0x11268b0a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x112688c80\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11268b980\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11268c900\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11268cfa0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11268d200\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11268e610\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11268f130\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11268fa30\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1126903d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x112690d40\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1126916b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x112692020\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x112692990\n",
      "ggml_metal_init: loaded kernel_norm                           0x1126936e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x112694280\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x112694cb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x112695710\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x112696170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x112697350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x112697dd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x112698850\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1126993e0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x11269a130\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x11269aba0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x11269b610\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x11269c080\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x11269c870\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x11269d2e0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x11269dd50\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x11269e7c0\n",
      "ggml_metal_init: loaded kernel_rope                           0x11269efb0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11269f5b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1126a00e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1126a0c70\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1126a1e20\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.41 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, ( 7025.88 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7427.88 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, ( 7517.89 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. To be content with what you have and who you are, and to not let anything get in your way of achieving that happiness.\n",
      "Sometimes it’s hard for me to find happiness because of things like my family, but I try not to let them bring me down for too long. I always look at the bright side of things. There is always a positive in every negative.\n",
      "When I am happy I feel free and nothing can stop me. I feel like anything is possible and there isn’t anything that can get in my way, because you will never go anywhere if you just sit back and watch everyone else succeed while you do nothing.\n",
      "The meaning of life to me is to be happy with who you are and what you have. To not let anyone or anything tell you who you should be. You already know who you want to be so don’t let anything stop you from being that person.\n",
      "To me, the meaning of life is to be alive. If you aren't alive then you won't get to do all those things that make you happy. Like playing video games or going on vacation. You can also go out with your friends and family and have a great time. Life is short so live it up!\n",
      "The meaning of life is to be happy every day. To make sure that no matter what happens in your life, you find the good in it. No matter how bad something might seem at first, there's always an upside to everything. I believe that when you die (which we all will) you are judged for what you did in this lifetime and if you were happy doing it, then God allows you to go wherever you want to go after death.\n",
      "The meaning of life to me is to be happy with who I am. To not let anyone tell me how to live my life or change anything about myself because they think they know better than me. It's also important for me to find joy in the small things like spending time with family, friends and pets that make up this big world we call home!\n",
      "The meaning of life is happiness. If you’re not happy then what is the point? You might as well just give up on everything and go back to sleep because there’s no reason for living if all you do is suffer through each day until finally dying from old age or some other cause.\n",
      "But how can someone be happy when they don't know what happiness even means?\n",
      "llama_print_timings:        load time =  1593.36 ms\n",
      "llama_print_timings:      sample time =  1088.27 ms /   512 runs   (    2.13 ms per token,   470.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2906.58 ms /   265 tokens (   10.97 ms per token,    91.17 tokens per second)\n",
      "llama_print_timings:        eval time = 19385.72 ms /   510 runs   (   38.01 ms per token,    26.31 tokens per second)\n",
      "llama_print_timings:       total time = 23506.14 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-q4_0.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748307\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 24826.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10091ffe0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100922190\n",
      "ggml_metal_init: loaded kernel_mul                            0x100921670\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100922c20\n",
      "ggml_metal_init: loaded kernel_scale                          0x100923660\n",
      "ggml_metal_init: loaded kernel_silu                           0x100923e90\n",
      "ggml_metal_init: loaded kernel_relu                           0x100921a70\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1009247b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100925730\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100925da0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100926000\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100927410\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100927f70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1009289d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100929370\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100929e10\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10092a7a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10092b750\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10092b9b0\n",
      "ggml_metal_init: loaded kernel_norm                           0x10092cb20\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10092d690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10092e0e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10092eb60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10092f750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100930120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100930ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100931620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1009321b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x100932f00\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x100933970\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1009343e0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x100934e50\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x100935640\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1009360b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x100936b20\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x100937590\n",
      "ggml_metal_init: loaded kernel_rope                           0x100937d80\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1009389d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100939550\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10093a060\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10093ac10\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.41 MB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  8755.22 MB, offs =  16852172800, (25139.66 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, (25141.08 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25543.08 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, (25633.09 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33mggml_metal_graph_compute: command buffer 5 failed with status 5\n",
      "GGML_ASSERT: ggml-metal.m:1109: false\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748337\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 24826.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.41 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be a good person:\n",
      "I think it's important to contribute to society and not just take from it.\n",
      "I like volunteering, being a good friend, and helping others.\n",
      "Those things bring me happiness.\n",
      "The most important thing in my life is family.\n",
      "My parents are the best! They do so much for us and I'm so grateful that they were able to immigrate from Vietnam to America.\n",
      "They have worked very hard for everything we have now.\n",
      "Without them, none of this would be possible...\n",
      "I am so thankful for all that they have done for me throughout my life.\n",
      "My brother is one of the most important people in my life as well because he has always been there for me whenever I needed someone to talk to or just hang out with.\n",
      "He's also an amazing person who helps others and gives back to society in many ways, which makes him a very special person indeed!\n",
      "My sister is another close member of our family and she plays a huge role in my life as well because she has always been there for me whenever I needed someone to talk to or just hang out with.\n",
      "She's also an amazing person who helps others and gives back to society in many ways, which makes her a very special person indeed!\n",
      "My grandparents are also a big part of my life because they have always been there for me whenever I needed someone to talk to or just hang out with.\n",
      "They're also great people who help others and give back to society in many ways, which makes them very special as well!\n",
      "This question is difficult to answer because it depends on what you consider \"meaningful.\" If you are asking about the meaning of life in terms of what makes you happy or fulfilled, then I would say that for me personally my family and friends are what make life meaningful.\n",
      "I believe that having a good relationship with your family and friends is important because it gives us a sense of belonging and security. It also helps us feel like we are part of something larger than ourselves, which can be very comforting when times are tough or stressful. Having close relationships with people who care about you can make all the difference in how you see yourself and your place in this world.\n",
      "I think that having strong friendships is also important because they provide us with support during difficult times as well as providing us with opportunities for fun activities together (like going out on weekends).\n",
      "\n",
      "llama_print_timings:        load time = 14667.23 ms\n",
      "llama_print_timings:      sample time =   364.85 ms /   512 runs   (    0.71 ms per token,  1403.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9613.42 ms /   265 tokens (   36.28 ms per token,    27.57 tokens per second)\n",
      "llama_print_timings:        eval time = 115798.02 ms /   510 runs   (  227.05 ms per token,     4.40 tokens per second)\n",
      "llama_print_timings:       total time = 125826.38 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748478\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 24826.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.41 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to serve others.\n",
      "This is why we’re here.\n",
      "I think that there are multiple layers of truth, and that they can be perceived through different lenses of understanding or belief.\n",
      "What do you mean by “truth”? Is it a factual statement about something objective, like whether the universe is expanding or not? Or is it an emotional experience, as in “that’s just how I feel?” Or something else? I believe that there are multiple levels of truth, and that they can be perceived through different lenses of understanding or belief.\n",
      "We might think of “truth” in terms of facts and figures—the kind of information we get from our senses and/or science. But this type of truth is limited because it only refers to things that are directly observable by us humans (or other living creatures). For example, when someone says something like “I saw my friend yesterday,” they’re making an objective statement about what happened in their lives—but if you weren’t there yourself then how could you possibly know whether or not it actually occurred?\n",
      "We might also think of truth as being based on emotion—the way we feel when something happens to us. But feelings aren’t always reliable indicators either: sometimes they change without warning or for no apparent reason at all! In addition, emotions can cause us to misinterpret events (such as seeing faces in clouds).\n",
      "Finally there are spiritual truths which come from within ourselves rather than from outside sources; these include things like intuition or gut reactions which tell us what’s right without any logical explanation behind them at all!\n",
      "I believe that the meaning of life is to serve others. I believe that we are here on earth for a reason, and it is our duty as human beings to help one another in any way possible.\n",
      "I believe that the purpose of life is not just about living but also about giving back. It’s about helping those around us who need it most—whether they are friends or family members or complete strangers who have found themselves in difficult situations because of things beyond their control (like natural disasters). We should all be willing to do whatever we can for others because if everyone did this then no one would ever have any problems at all!\n",
      "I believe that the meaning of life is to serve others. I believe that we are here on earth for a reason, and it is our duty as human beings\n",
      "llama_print_timings:        load time = 14083.67 ms\n",
      "llama_print_timings:      sample time =   364.38 ms /   512 runs   (    0.71 ms per token,  1405.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10044.43 ms /   265 tokens (   37.90 ms per token,    26.38 tokens per second)\n",
      "llama_print_timings:        eval time = 115056.73 ms /   510 runs   (  225.60 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:       total time = 125516.33 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e51103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1026 (c63bb1d)\n",
      "main: seed  = 1692748618\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B-v2/ggml-model-f16.gguf (version GGUF V1 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_load_internal: format       = GGUF V1 (latest)\n",
      "llama_model_load_internal: arch         = llama\n",
      "llama_model_load_internal: vocab type   = SPM\n",
      "llama_model_load_internal: n_vocab      = 32000\n",
      "llama_model_load_internal: n_ctx_train  = 4096\n",
      "llama_model_load_internal: n_ctx        = 512\n",
      "llama_model_load_internal: n_embd       = 5120\n",
      "llama_model_load_internal: n_head       = 40\n",
      "llama_model_load_internal: n_head_kv    = 40\n",
      "llama_model_load_internal: n_layer      = 40\n",
      "llama_model_load_internal: n_rot        = 128\n",
      "llama_model_load_internal: n_gqa        = 1\n",
      "llama_model_load_internal: f_norm_eps   = 1.0e-05\n",
      "llama_model_load_internal: n_ff         = 13824\n",
      "llama_model_load_internal: freq_base    = 10000.0\n",
      "llama_model_load_internal: freq_scale   = 1\n",
      "llama_model_load_internal: model type   = 13B\n",
      "llama_model_load_internal: model ftype  = mostly F16\n",
      "llama_model_load_internal: model size   = 13.02 B\n",
      "llama_model_load_internal: general.name = LLaMA\n",
      "llama_model_load_internal: BOS token = 1 '<s>'\n",
      "llama_model_load_internal: EOS token = 2 '</s>'\n",
      "llama_model_load_internal: LF token  = 13 '<0x0A>'\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 24826.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.41 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have a good time and enjoy the ride.\n",
      "I believe in love, happiness, laughter, peace, kindness, forgiveness, and compassion.\n",
      "I believe that we are all human beings here on this planet for a reason. We are all connected through our shared experience as human beings.\n",
      "I believe there is nothing more important than each other. I am grateful for the life I have been given and the people in it.\n",
      "I am blessed with many friends, family members who love me unconditionally, and so much more!\n",
      "What I Believe In: My Personal Values And Core Beliefs?\n",
      "How To Build A Business Around Your Passion?\n",
      "How To Find Purpose In Life?\n",
      "How To Be Happy In Life?\n",
      "What Are The Best Qualities Of A Leader?\n",
      "Why Is It Important To Have A Vision For Your Life?\n",
      "Life is a gift that you give to yourself every day. You can make your life as happy or miserable as you choose. If you want to be happy, then you need to find out what makes you happy and do it!\n",
      "If you are not happy with where you are at in life right now, then look at what is causing this unhappiness and change those things so that they don’t happen again. Life isn’t about waiting for something good to come along – it’s about creating your own happiness every day of your life!\n",
      "What makes me happy? I love helping people feel better about themselves by sharing information with them on how they can improve their lives through self-improvement techniques such as meditation or exercise. These things have helped me get through some tough times when nothing else seemed to work well enough for my situation at that time in my life!\n",
      "What is the meaning of life? The answer to this question has been debated for centuries, and there are many different views on it. Some believe that life has no meaning, while others think that it does have a purpose.\n",
      "There are also those who argue that life only makes sense when you take into account your own experience with death and dying; they say that without this context, we cannot truly understand what our existence means or why we exist at all. Others maintain that there is no ultimate answer to the question of meaning because our worldview varies so much from person to person due to cultural differences and individual perspectives on life itself (which may be influenced by up\n",
      "llama_print_timings:        load time = 14450.31 ms\n",
      "llama_print_timings:      sample time =   364.94 ms /   512 runs   (    0.71 ms per token,  1402.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10628.81 ms /   265 tokens (   40.11 ms per token,    24.93 tokens per second)\n",
      "llama_print_timings:        eval time = 116608.19 ms /   510 runs   (  228.64 ms per token,     4.37 tokens per second)\n",
      "llama_print_timings:       total time = 127652.95 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B-v2/ggml-model-f16.gguf  -p \"I believe the meaning of life is\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
