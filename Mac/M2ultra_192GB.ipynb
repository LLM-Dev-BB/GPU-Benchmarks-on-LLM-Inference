{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     ggml-vocab.bin\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                     \u001b[31mtokenizer.model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866619\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x116b213d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x116b22c90\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x116b24360\n",
      "ggml_metal_init: loaded kernel_scale                          0x116b24670\n",
      "ggml_metal_init: loaded kernel_silu                           0x116b24ec0\n",
      "ggml_metal_init: loaded kernel_relu                           0x116b25690\n",
      "ggml_metal_init: loaded kernel_gelu                           0x116b23340\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x116b267b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x116b26a10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x116b27ca0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x116b26ff0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x116b28740\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x116b29a80\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x116b28f90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x116b2a380\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x116b2acc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x150093570\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x150093dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x150094420\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1500954f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x150095ab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x150096530\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x150096f70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x150097940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x150098330\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x150098ea0\n",
      "ggml_metal_init: loaded kernel_rope                           0x150099f40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x15009ab10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x15009b8b0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m in discovering our purpose and living it.\n",
      "Living your purpose requires you to do three things: 1) Learn, 2) Become, and 3) Serve.\n",
      "As you go through this course, be sure to complete all 39 lessons as there are many opportunities for you to learn, become and serve along the way!\n",
      "So what is your purpose? What do you want to accomplish in life? How will you use your time, skills, talents, and interests?\n",
      "My mission is to help others discover their calling so they can have a positive impact on this world. Are you ready to get started? Let’s go!\n",
      "The best lessons are the ones we learn through experience. So let’s start with my personal story that helped me discover my purpose in life. I hope it helps shed some light for you as well.\n",
      "Do you have an idea of what your calling is? How to figure out your true purpose in life can be a challenge but it doesn’t need to be. Let’s take a look at the steps you can follow to uncover your purpose.\n",
      "What is your passion and how will you use it to make a positive impact on this world? You may not know yet, so don’t panic! Let’s start with my story of finding my passion in life. It’s an amazing one that I hope inspires you as well.\n",
      "The next step is to become the person who has what it takes to make a positive impact on this world and contribute your unique gifts, talents, interests, and skills. Let’s explore some of the ways we can do this.\n",
      "It’s now time to serve by using your passion and purpose in life to help others! What are you going to do with your gift? This is the ultimate step that helps us make an impact on this world. I hope my story will inspire you as well.\n",
      "Now let’s take a look at where you can find more information, guidance, and support to discovering your purpose in life! There are lots of great tools available online to help you with this process. Are you ready? Let’s go!\n",
      "It was really nice to learn about you and your passion for helping others. I think it is wonderful that you have such a big heart and give so much of yourself to the world around you. Thank you for all you do. You are an inspiration!\n",
      "I\n",
      "llama_print_timings:        load time =  1066.95 ms\n",
      "llama_print_timings:      sample time =   332.33 ms /   512 runs   (    0.65 ms per token)\n",
      "llama_print_timings: prompt eval time =  3185.42 ms /   265 tokens (   12.02 ms per token)\n",
      "llama_print_timings:        eval time =  7260.39 ms /   510 runs   (   14.24 ms per token)\n",
      "llama_print_timings:       total time = 10818.73 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866631\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13ef28810\n",
      "ggml_metal_init: loaded kernel_mul                            0x13ef29d00\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13ef2b250\n",
      "ggml_metal_init: loaded kernel_scale                          0x13ef2b530\n",
      "ggml_metal_init: loaded kernel_silu                           0x13ef2bd70\n",
      "ggml_metal_init: loaded kernel_relu                           0x13ef2a2a0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13ef2c6e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13ef2d650\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13ef2e990\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13ef2ebf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13ef2dea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13ef2f5b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13ef30900\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13ef2fe00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13ef31040\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13ef31ae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13ef32430\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13ef334e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13ef33e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13ef34a80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13ef35470\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13ef35e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13ef36a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13ef37250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13ef37db0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x13ef388d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x13ef394f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13ef3a070\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13ef3ad50\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it as you want. And since I have a strong belief in God, I believe that's what He wants for me too.\n",
      "I think we are on this Earth with only one purpose: to live our lives to the fullest possible extent, following our own dreams and aspirations. I know I don't need anyone's permission or approval to do it.\n",
      "I also believe that the meaning of life is to learn and grow as a person, be of help to others and make the world a better place (in the way you can). Again, no one needs to tell me what to do in order to achieve those things. They are all my own choices.\n",
      "So, I'm going to live my life, following my own dreams, doing everything I want according to what I believe is right and good for myself.\n",
      "But why should we live our lives that way? For example, if we believe in God, as I do, then we also know He created us with purpose: to glorify Him and enjoy him forever (Isaiah 43:7). That's the purpose of life, the reason for living.\n",
      "In other words, we should live our lives that way because it is what God wants from us. We have no right to ignore His will or reject it; we can only be obedient and follow Him.\n",
      "And when you think about it, that's the most freeing thing! It makes life easier to manage, doesn't it? You don't have to figure out every little detail for yourself - not even if you have a strong personality or a big imagination. Because God knows what is best for us and what He wants from us.\n",
      "So I think we should accept His will as our own. And when we do, life becomes easier; it flows naturally. There's no need to figure out everything by yourself - not even if you are an intelligent person who likes challenges! It is enough to follow God and have faith in Him.\n",
      "We can also look at it from the other angle: God wants us to live our lives according to His will, so we should do that willingly. After all, He created us for a reason; He knows what's best for us.\n",
      "And even if we don't, we have no right to disobey Him or not follow Him on purpose. It would be like saying \"no\" to God! And\n",
      "llama_print_timings:        load time =   614.42 ms\n",
      "llama_print_timings:      sample time =   334.47 ms /   512 runs   (    0.65 ms per token)\n",
      "llama_print_timings: prompt eval time =  3178.73 ms /   265 tokens (   12.00 ms per token)\n",
      "llama_print_timings:        eval time =  7258.11 ms /   510 runs   (   14.23 ms per token)\n",
      "llama_print_timings:       total time = 10811.39 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866642\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x143a26530\n",
      "ggml_metal_init: loaded kernel_mul                            0x143a27a20\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x143a28f70\n",
      "ggml_metal_init: loaded kernel_scale                          0x143a29250\n",
      "ggml_metal_init: loaded kernel_silu                           0x143a29a90\n",
      "ggml_metal_init: loaded kernel_relu                           0x143a27fc0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x143a2a400\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x143a2b370\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x143a2c6b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x143a2c910\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x143a2bbc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x143a2d2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x143a2e620\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x143a2db20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x143a2ed60\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x143a2f800\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x143a30150\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x143a31200\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x143a31b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x143a327a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x143a33190\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x143a33ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x143a34740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x143a34f70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x143a35ad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x143a365f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x143a37210\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x143a37d90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x143a38a70\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "That’s my take on what the meaning of life is, and it has changed as I have grown older. It used to be all about success, achieving goals, having a happy family. But life isn’t just about that. Life is also full of challenges. The first challenge we face in life is how do we survive? We do this by eating, drinking and sleeping. This sounds easy enough but it requires a lot of work to figure out what you are going to eat (a very difficult question), whether you should just go straight to the bar for that glass of wine or have some dinner first (I often ask myself this question) and how many hours of sleep do you need (I usually get 8.5, but I’m not a scientist).\n",
      "The second challenge we face in life is how do we find love? There are so many ways to do this nowadays that it can be confusing with too many choices: dating apps, blind dates and speed dating to name a few. Love shouldn’t be confused with sex though. You can have great sex without love or vice versa but it is hard to think of one without the other.\n",
      "The third challenge we face in life is how do we raise our children? This is the hardest thing I think you will ever do and I don’t envy anyone who does this job. There are so many choices you have to make about your child’s education, food, entertainment that it can be overwhelming. It’s easy just to give in to what they want or just let them watch telly all day but if you do that then you know you’ve failed as a parent.\n",
      "I think the meaning of life is also about giving back and not being selfish (but I am very guilty of this). It’s about making sure you leave the world in a better condition than when you found it. There are so many ways to do this: volunteering, donating money or time, voting for issues that matter to you etc. And if you have kids make sure they know how important giving back is and be an example of it yourself.\n",
      "My meaning of life story will change in the future as I get older and learn new things, but at this moment in my life I would say that’s what I think the meaning of life is.\n",
      "What do you think? What’s your meaning of life?\n",
      "llama_print_timings:        load time =   636.37 ms\n",
      "llama_print_timings:      sample time =   335.61 ms /   512 runs   (    0.66 ms per token)\n",
      "llama_print_timings: prompt eval time =  3233.20 ms /   265 tokens (   12.20 ms per token)\n",
      "llama_print_timings:        eval time =  7270.19 ms /   510 runs   (   14.26 ms per token)\n",
      "llama_print_timings:       total time = 10879.15 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866654\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x113526550\n",
      "ggml_metal_init: loaded kernel_mul                            0x113527a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x113528f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x113529270\n",
      "ggml_metal_init: loaded kernel_silu                           0x113529ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x113527fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11352a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11352b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11352c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11352c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11352bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11352d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11352e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11352db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11352ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11352f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x113530170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x113531220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x113531b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1135327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1135331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x113533bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x113534760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x113534f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x113535af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x113536610\n",
      "ggml_metal_init: loaded kernel_rope                           0x113537230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x113537db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x113538a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out what it's all about, and then be able to communicate that to other people.\n",
      "-Ralph Waldo Emerson (1803 - 1882) American essayist\n",
      "If you are not willing to encounter death daily and take risks with your life, you will never be free of the fear of dying. It is a matter of the freedom of choice. This is the nature of reality itself. Death surrounds you. It is all around you. You cannot avoid it. There is no way out.\n",
      "-Taisen Deshimaru (1914 - 1982) Japanese Zen Master\n",
      "If we do not learn how to die, we will never be able to live.\n",
      "I have died and gone to heaven.\n",
      "-Billie Holiday (1915 - 1959) American Jazz singer\n",
      "You know you're getting old when the candles cost more than the cake.\n",
      "-Bob Hope (1903 - 2003) American Comedian\n",
      "What is life, if not a succession of moments?\n",
      "-Honore de Balzac (1799 - 1850) French novelist and playwright\n",
      "-Socrates (469 BC - 399 BC) Greek Philosopher\n",
      "Life and death are questions that man has been trying to answer for thousands of years. There is no one right answer, but all of us have an opinion on what it means. I think we all have a deep need to know the meaning of life and our place in the universe. When we feel like life does not make sense, or when we feel lost, we search for answers. We want to be part of something bigger than ourselves.\n",
      "-Eleonor Roosevelt (1884 - 1962) American First Lady\n",
      "There is no such thing as a meaningless life in the world except your own. It's not possible, and if you think it is, then you don't know what life is! The moment you start to breathe, something starts to happen; you start to live. And when people begin to live, they have problems which are no different from yours. There is only one problem in the world - death, but there are so many ways of dying. That’s why I don't believe there can be a\n",
      "llama_print_timings:        load time =  3080.00 ms\n",
      "llama_print_timings:      sample time =   320.09 ms /   512 runs   (    0.63 ms per token)\n",
      "llama_print_timings: prompt eval time =  3007.81 ms /   265 tokens (   11.35 ms per token)\n",
      "llama_print_timings:        eval time = 18913.17 ms /   510 runs   (   37.08 ms per token)\n",
      "llama_print_timings:       total time = 22282.00 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866680\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x111637990\n",
      "ggml_metal_init: loaded kernel_mul                            0x111638e80\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11163a3d0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11163a6b0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11163aef0\n",
      "ggml_metal_init: loaded kernel_relu                           0x111639420\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11163b860\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11163c7d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11163db10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11163dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11163d020\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11163e730\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11163fa80\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11163ef80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1116401c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x111640c60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1116415b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x111642660\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x111642fc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x111643c00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1116445f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x111645000\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x111645ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1116463d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x111646f30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x111647a50\n",
      "ggml_metal_init: loaded kernel_rope                           0x111648670\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1116491f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x111649ed0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to firstly, be happy and make others happy. Secondly, to be a helpful person to those around you. And thirdly, to do what you love without compromising your happiness.\n",
      "As for goals in life... I'd say to be able to help other people fulfill their dreams and to reach my full potential as an artist.\n",
      "I'm not sure if that answered the question!\n",
      "This might sound silly but I really want a good set of wheels and a nice place to live, haha. Other than that... I think I'd like to have a few kids one day.\n",
      "I've always wanted to travel, so it would be awesome to be able to see more of the world!\n",
      "It takes courage to be happy.\n",
      "\"Being happy is not just happiness at your fingertips; it takes courage and effort.\" - Unknown\n",
      "What do you think about when you lay in bed? Or what is something that has stuck with you from a movie/book/etc.?\n",
      "I guess I'm quite the romantic, so if there are any pretty things around me (a window or even just a nice view), I'll gaze at it for ages. If not... I either get lost in my thoughts, doodle on my bedside notebook or read something that takes my interest and brings joy to my heart!\n",
      "I think about love and relationships. It might sound quite cliché but I'm a hopeless romantic who dreams of having a great relationship with the man of her dreams.\n",
      "I love movies, so I would say one of my all-time favourites is The Lord Of The Rings trilogy!\n",
      "The Hobbit (both animated and live-action) are also among my favourite films!\n",
      "A few other favourites that come to mind would be Harry Potter & Percy Jackson.\n",
      "I'm not too sure whether I have any particular favorite books, but if it had to be one... then I would say Pride And Prejudice by Jane Austen. It's really a wonderful story! And even though I haven't read all of them yet, the Harry Potter series is also one that you should check out.\n",
      "I like to listen to old songs from the 1920's and 30's as they are super beautiful!\n",
      "What does the word 'love' mean to you\n",
      "llama_print_timings:        load time =  1782.74 ms\n",
      "llama_print_timings:      sample time =   319.79 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time =  3058.74 ms /   265 tokens (   11.54 ms per token)\n",
      "llama_print_timings:        eval time = 18912.29 ms /   510 runs   (   37.08 ms per token)\n",
      "llama_print_timings:       total time = 22332.62 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866704\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x146c26550\n",
      "ggml_metal_init: loaded kernel_mul                            0x146c27a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x146c28f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x146c29270\n",
      "ggml_metal_init: loaded kernel_silu                           0x146c29ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x146c27fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x146c2a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x146c2b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x146c2c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x146c2c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x146c2bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x146c2d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x146c2e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x146c2db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x146c2ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x146c2f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x146c30170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x146c31220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x146c31b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x146c327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x146c331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x146c33bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x146c34760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x146c34f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x146c35af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x146c36610\n",
      "ggml_metal_init: loaded kernel_rope                           0x146c37230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x146c37db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x146c38a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I want to have a good life and enjoy it as much as possible. It seems like an obvious thing to say, but it's not always easy to do.\n",
      "It’s not about having all the money in the world or being successful at everything you put your hand to. If I can make people happy then that’s great. I feel very lucky and privileged to have met so many wonderful people during my life.\n",
      "I would like to spend time with friends, enjoy my family and continue working on new ideas for movies and other projects. I want to be a positive influence in the world if possible. It's also important to give back to the community somehow, whether it's through charity work or helping people out when they need you.\n",
      "I have never been one of those people that wants to be famous or in the public eye all the time. I've always just wanted to do my job and not let fame get to me. It was a lot harder when I was younger because I didn’t think about it so much, but nowadays you can’t avoid it. There are days when I wish there were less expectations on me and that people weren't looking at everything I do all the time.\n",
      "I don't have any regrets in life yet. I try to live a healthy lifestyle, exercise regularly and eat well, but I love food as much as anyone else so it can be hard sometimes.\n",
      "It was difficult being away from my family when I first moved to America, especially at Christmas time but you just have to get used to it after a while. They come over here quite a lot now which is great because I love seeing them and they can experience life in LA with me. We get a bit of snow around Christmas here so that’s always a nice change from the heat in Australia!\n",
      "When I was young my favourite movies were The Lord Of The Rings, Jurassic Park, Star Wars and Back To The Future. I loved them all but I think it would have to be Star Wars for me now because of its lasting impact on popular culture.\n",
      "My first job was working in a pub when I was 16 years old. It wasn't the most exciting job but it paid well so that was good! I also worked at a local fish and chip shop, and did some odd jobs for my dad too.\n",
      "I have always\n",
      "llama_print_timings:        load time =  1751.61 ms\n",
      "llama_print_timings:      sample time =   320.16 ms /   512 runs   (    0.63 ms per token)\n",
      "llama_print_timings: prompt eval time =  3064.46 ms /   265 tokens (   11.56 ms per token)\n",
      "llama_print_timings:        eval time = 18945.21 ms /   510 runs   (   37.15 ms per token)\n",
      "llama_print_timings:       total time = 22368.60 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866728\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11ff26ef0\n",
      "ggml_metal_init: loaded kernel_mul                            0x11ff283e0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11ff29930\n",
      "ggml_metal_init: loaded kernel_scale                          0x11ff29c10\n",
      "ggml_metal_init: loaded kernel_silu                           0x11ff2a450\n",
      "ggml_metal_init: loaded kernel_relu                           0x11ff28980\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11ff2adc0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11ff2bd30\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11ff2d070\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11ff2d2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11ff2c580\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11ff2dc90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11ff2efe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11ff2e4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11ff2f720\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11ff301c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11ff30b10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11ff31bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11ff32520\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11ff33160\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11ff33b50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11ff34560\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11ff35100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11ff35930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11ff36490\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11ff36fb0\n",
      "ggml_metal_init: loaded kernel_rope                           0x11ff37bd0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11ff38750\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11ff39430\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to fulfill your potential as a human being.\n",
      "I believe that every single person has an inherent worth and dignity, regardless of skin color, gender, sexual orientation or any other physical, intellectual, emotional or psychological condition.\n",
      "I believe that all people have the potential for goodness. I believe we are more than our mistakes. I believe that no matter what you've done wrong – there is a way to make it right, and that everyone deserves a second chance...and maybe even a third!\n",
      "I believe in kindness. I believe in love. I believe in empathy, compassion, sympathy, and understanding. I believe in peace, joy, and happiness. I believe we are not perfect people, but striving to be the best that we can be. We make mistakes; we learn from them; we move on.\n",
      "I believe that all of us have the capacity for evil, but it is a dark power, not inherent in our nature. I believe we have the choice between good and evil, right and wrong. And I believe that there are consequences for both.\n",
      "I believe in the value of life - my own life, your life, everyone's life.\n",
      "I believe I was created by God for a reason; that every single person on this planet has a purpose in their lives. And I believe if we all find our purpose and use it to make the world a better place, the world will be an infinitely happier and more beautiful place than it is today.\n",
      "I believe we are each here to learn and grow, to love and to be loved. To create, inspire, build and teach. I believe we are all connected; that our words and actions affect others. And so I believe in the importance of choosing my words wisely, acting with integrity and speaking the truth.\n",
      "I believe in the power of prayer - both receiving and giving it. I believe there is a place for faith and religion in our lives, but also that they are not the only way to find God or spiritual fulfillment. I believe you can be spiritual without being religious; that we all have the capacity to connect with the divine.\n",
      "I believe that this life is about making choices – right and wrong. And it's about accepting responsibility for those choices, no matter how they turn out. It's not about finding fault, but about taking personal accountability for our actions and decisions; to live our lives as true leaders,\n",
      "llama_print_timings:        load time =  1800.72 ms\n",
      "llama_print_timings:      sample time =   317.57 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time =  5632.51 ms /   265 tokens (   21.25 ms per token)\n",
      "llama_print_timings:        eval time = 11672.65 ms /   510 runs   (   22.89 ms per token)\n",
      "llama_print_timings:       total time = 17663.80 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866748\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10242adc0\n",
      "ggml_metal_init: loaded kernel_mul                            0x10242c2b0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10242d800\n",
      "ggml_metal_init: loaded kernel_scale                          0x10242dae0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10242e320\n",
      "ggml_metal_init: loaded kernel_relu                           0x10242c850\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10242ec90\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10242fc00\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102430f40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1024311a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x102430450\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x102431b60\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x102432eb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x1024323b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1024335f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x102434090\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1024349e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102435a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1024363f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102437030\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102437a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x102438430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x102438fd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x102439800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x10243a360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x10243ae80\n",
      "ggml_metal_init: loaded kernel_rope                           0x10243baa0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10243c620\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10243d300\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living.\n",
      "I love the whole world, especially its people, but there are places I can't wait to visit.\n",
      "If you want to know who a person really is, don't look at their resume or their degrees. Look at what they do when nobody else is watching.\n",
      "You might be surprised by how much you like me if you got to know me.\n",
      "Writing helps me work through my emotions and get a better understanding of myself and the world around me.\n",
      "I love listening to other people's stories, reading their blogs, hearing what they have done or learned in life. I'm always curious about the story behind a person's smile.\n",
      "My life is a great adventure; I just haven't figured out how to take pictures of it yet.\n",
      "I love watching people interact with one another and I always wonder about their stories, especially if they look sad or lonely. It's amazing what we can learn by listening to the little details that make up someone else's life story.\n",
      "You have the choice every day whether you want to make your life better or worse than it was yesterday. The only way to grow is to try new things, take chances and fail sometimes.\n",
      "I don't think I will ever get tired of watching the stars dance in the sky...and I love how much they change each year.\n",
      "I believe you can find beauty in anything if you look hard enough.\n",
      "Love is something that we all want, need and deserve. But we cannot love others until we learn to love ourselves first.\n",
      "A person's smile is like a key that unlocks their soul for us to discover who they really are.\n",
      "My life doesn't have one specific thing or event that has happened so far that I would say was the worst, but there were several periods of time when I felt completely lost and overwhelmed by grief and emptiness. Those times made me realize how much I wanted to live my life with purpose and meaning.\n",
      "I'm a firm believer in taking chances, following your dreams and learning about the world around you.\n",
      "Happiness is not something that just happens. It takes work every day to make it happen, especially when things aren't going so well.\n",
      "Writing can be a way for me to express my feelings or ideas that I'm having trouble putting into words.\n",
      "\n",
      "llama_print_timings:        load time =  1093.72 ms\n",
      "llama_print_timings:      sample time =   318.85 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time =  5628.54 ms /   265 tokens (   21.24 ms per token)\n",
      "llama_print_timings:        eval time = 11694.68 ms /   510 runs   (   22.93 ms per token)\n",
      "llama_print_timings:       total time = 17680.74 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866767\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11e726530\n",
      "ggml_metal_init: loaded kernel_mul                            0x11e727a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11e728f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x11e7292a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11e729a30\n",
      "ggml_metal_init: loaded kernel_relu                           0x11e727e40\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11e72a300\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11e72b420\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11e72c6b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11e72c910\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11e72cfe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11e72d240\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11e72e630\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11e72db10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11e72ed70\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11e72f6b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11e72fff0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11e730ac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11e731bd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11e732810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11e733360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11e733b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11e7345c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11e734ff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11e735b10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11e736640\n",
      "ggml_metal_init: loaded kernel_rope                           0x11e737270\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11e737dc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11e738ab0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.” – Pablo Picasso\n",
      "“The question isn’t who is going to let me; it’s who is going to stop me.” – Ayn Rand\n",
      "“Don’t wait for extraordinary opportunities. Seize common occasions and make them great. Weakness can be turned into strength, disadvantage into opportunity.” – Napoleon Hill\n",
      "“I have learned over the years that when one’s mind is made up, this diminishes fear; knowing what must be done does away with fear.” – Rosa Parks\n",
      "“The price of anything is the amount of life you exchange for it.” – Henry David Thoreau\n",
      "“What we achieve inwardly will change outer reality” – Plutarch\n",
      "“Everyone has been made for some particular work, and the desire for that work has been put in every heart.” – Rumi\n",
      "“The only person you are destined to become is the person you decide to be.” – Ralph Waldo Emerson\n",
      "“It’s hard to beat a person who never gives up” – Babe Ruth\n",
      "“To live is the rarest thing in the world. Most people exist, that is all!” – Oscar Wilde\n",
      "“When everything seems to be going against you, remember that the airplane takes off against the wind, not with it.” – Henry Ford\n",
      "“Do one thing every day that scares you” – Anonymous\n",
      "“The difference between a successful person and others is not a lack of strength, not a lack of knowledge, but rather in a lack of will.” – Vince Lombardi\n",
      "“I’ve come to believe that all my past failure and frustration were actually laying the foundation for the understandings that have created the new level of living I now enjoy. And the secret of your future is hidden in your daily routine.” – Mike Murdock\n",
      "“We must believe that we are gifted for something, and that this thing, at whatever cost, must be attained.” – Marie Curie\n",
      "“It’s not the mountains ahead to climb that wear you out; it’s the pebble in your shoe” – Muhammad Ali\n",
      "“When I let go of what I am, I become what I might be.” – Lao Tzu\n",
      "“You can do anything but not everything.” – David Allen\n",
      "“The best years of your life are the ones in which you decide your problems are\n",
      "llama_print_timings:        load time =  1088.49 ms\n",
      "llama_print_timings:      sample time =   317.19 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time =  5665.04 ms /   265 tokens (   21.38 ms per token)\n",
      "llama_print_timings:        eval time = 11712.12 ms /   510 runs   (   22.96 ms per token)\n",
      "llama_print_timings:       total time = 17733.12 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866786\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13c78dd50\n",
      "ggml_metal_init: loaded kernel_mul                            0x13c78f4c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13c790b90\n",
      "ggml_metal_init: loaded kernel_scale                          0x13c790ea0\n",
      "ggml_metal_init: loaded kernel_silu                           0x13c7916e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13c78f9b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13c791f50\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13c793000\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13c793260\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13c794490\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13c793840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13c7960d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13c795750\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13c794dd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13c7969a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13c7972e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13c797c20\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13c7986f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13c799800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13c79a440\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13c79afa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13c79b7e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13c79c230\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13c79cc60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13c79d7b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x13c79e120\n",
      "ggml_metal_init: loaded kernel_rope                           0x13c79eef0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13c79fa60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13c7a0710\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.67 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in loving and being loved.\n",
      "\"I believe that every human has a right to love, to be loved, and to fulfill his or her potential as a uniquely creative individual.\"\n",
      "—Helen Keller\n",
      "In 1978, at age twenty-six, I had everything going for me: a career in journalism and a life filled with friends. A few years later, I was living on the streets of New York City, homeless, penniless, and hopeless.\n",
      "How did things change so radically? How could I have gone from being a successful journalist to being homeless, sleeping on park benches, eating out of trash cans, and begging for money on the streets of New York City? The simple answer is that I was depressed—deeply depressed.\n",
      "When I was twenty-six years old, I was diagnosed with bipolar disorder. What this meant to me was that my moods could swing from high (mania) to low (depression), and that I would experience periods of intense euphoria followed by periods of utter despondency.\n",
      "I have often said that bipolar disorder is like a roller coaster ride, but the difference between this ride and others is that the ups are very high and the downs are very low—and sometimes they happen all at once!\n",
      "When I was manic, I would spend money on things I didn't need. When depressed, I could not work or function normally. The years that followed were filled with periods of intense depression punctuated by brief moments of elation. But the highs never lasted long enough to make up for all the lows I had experienced, and the lows went from bad to worse.\n",
      "In 1986, after a few years living on the streets, I was arrested in New York City and taken to Bellevue Hospital, where I spent several months in a locked ward. I remember crying out in pain as my head pounded with such intensity that I felt like my brain would explode. The manic highs had become more violent than ever before: at times, I couldn't stop talking, and at other times, I could not move or speak—it was all so overwhelming!\n",
      "I remember the day they told me that I was going to be released from Bellevue. My family\n",
      "llama_print_timings:        load time =  5980.03 ms\n",
      "llama_print_timings:      sample time =   317.85 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time =  5622.72 ms /   265 tokens (   21.22 ms per token)\n",
      "llama_print_timings:        eval time = 35452.81 ms /   510 runs   (   69.52 ms per token)\n",
      "llama_print_timings:       total time = 41437.85 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866833\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11b619e50\n",
      "ggml_metal_init: loaded kernel_mul                            0x11b61b650\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11b61cd30\n",
      "ggml_metal_init: loaded kernel_scale                          0x11b61d040\n",
      "ggml_metal_init: loaded kernel_silu                           0x11b61d880\n",
      "ggml_metal_init: loaded kernel_relu                           0x11b61e070\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11b61bce0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11b61f1d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11b61f430\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11b620660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11b61f9d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11b6210f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11b622440\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11b621940\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11b622ce0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11b623630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11b623f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11b6248c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11b6259c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11b626620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11b627180\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11b627990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11b6283b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11b628d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11b6298e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11b62a410\n",
      "ggml_metal_init: loaded kernel_rope                           0x11b62b040\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11b62bbb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11b62c850\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.67 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a life worth living.\n",
      "I believe that I am here on this planet for a reason, and that my mission in life is to discover what that reason is. There are things I want to do with my life, and some of those things I have already done. Others, however, remain unaccomplished. I’m not sure why these goals are important, but they are important, and it is important for me to accomplish them before I die.\n",
      "I believe that the world revolves around people who choose to do something extraordinary with their lives. These individuals have a way of making a difference. They inspire others to make changes in their own lives. They create new systems, or change old ones. Their actions and thoughts cause ripples throughout the world. Some people see these ripples, while others don’t care about them at all. What makes you different from everyone else is what defines your life.\n",
      "I believe that I can live my life as an ordinary person, or I can choose to do something extraordinary with it. I believe that if I want to accomplish something in this world, then I need to act now. There’s no time like the present. If you want to make a difference in this world, then go out and do something worthwhile.\n",
      "I believe that every individual has the potential for greatness within them. It is up to us to discover it, and realize our true capabilities. I believe that if we can find what makes us special, and use it to help others, then we have a chance of becoming who we’re meant to be. We all have the power to choose which path our lives will take. We are in control of our own destiny. It is up to us to discover our purpose in life, and act upon it accordingly.\n",
      "I believe that I can accomplish anything if I am willing to put in the time and effort required for success. If you want something badly enough, then go out and do everything within your power to achieve it. Nothing worthwhile will ever be easy. The road is long and tiring, but the rewards are well worth the journey.\n",
      "I believe that there’s nothing wrong with dreaming big. I believe that we can all accomplish great things in this world, if only we put our minds to it. We are capable of achieving anything we want. It’s simply a matter of believing in yourself and your goals, and being willing to do whatever it takes to\n",
      "llama_print_timings:        load time =  4012.16 ms\n",
      "llama_print_timings:      sample time =   320.04 ms /   512 runs   (    0.63 ms per token)\n",
      "llama_print_timings: prompt eval time =  5633.40 ms /   265 tokens (   21.26 ms per token)\n",
      "llama_print_timings:        eval time = 35562.82 ms /   510 runs   (   69.73 ms per token)\n",
      "llama_print_timings:       total time = 41560.07 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866879\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102a289e0\n",
      "ggml_metal_init: loaded kernel_mul                            0x102a29ed0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x102a2b420\n",
      "ggml_metal_init: loaded kernel_scale                          0x102a2b700\n",
      "ggml_metal_init: loaded kernel_silu                           0x102a2bf40\n",
      "ggml_metal_init: loaded kernel_relu                           0x102a2a470\n",
      "ggml_metal_init: loaded kernel_gelu                           0x102a2c8b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x102a2d820\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102a2eb60\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x102a2edc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x102a2e070\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x102a2f780\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x102a30ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x102a2ffd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x102a31210\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x102a31cb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x102a32600\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102a336b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x102a34010\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102a34c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102a35640\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x102a36050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x102a36bf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x102a37420\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x102a37f80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x102a38aa0\n",
      "ggml_metal_init: loaded kernel_rope                           0x102a396c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x102a3a240\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x102a3af20\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.67 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have a good time!\n",
      "I am not sure what the meaning of life is, but I have found that having fun and helping others are some of the most important things in my life. Having a positive attitude will help you through any challenge you face. If you can have fun while doing it then you’ve got it made.\n",
      "A good time for me would be to spend quality time with family and friends, whether it be on vacation or having a BBQ at home. I also love camping with my daughter and husband in our trailer when we get the chance. It always seems like we are so busy that there is never enough time!\n",
      "I try to live life one day at a time so that I can appreciate every moment.\n",
      "Sometimes you have to take a step back from your problems, not run away from them, just let go for a little while and enjoy some fun times. Life is too short and precious to waste on being angry or sad.\n",
      "I love my job as an administrative assistant at the City of Nanaimo. I get to help everyone in the community and also work with some wonderful people here at the city. It’s not always about doing big things, sometimes it’s more important to do small acts that make a difference. If you can put a smile on someone else’s face then chances are they will pay it forward as well.\n",
      "I think if you are happy and comfortable in your own skin then the meaning of life has already been found.\n",
      "I have always been very lucky to have good friends and family who have always been there for me when I have needed help or just someone to talk to. I like to think I am a pretty positive person, but sometimes that can be hard when you are feeling negative. My daughter is also my best friend and we spend lots of time together laughing and having fun!\n",
      "I would love to travel more. I try to take advantage of the weekends as often as possible because it gives me a chance to get out of town for a short while. I am not sure where I would like to go, maybe just up island or maybe even Vancouver Island. The beauty here is stunning!\n",
      "The future? Well that’s a tough one; we all have our dreams and hopes. My life is pretty good right now so I guess the only thing I can hope for in the future is that it stays this way. If there are any changes coming my way then so be\n",
      "llama_print_timings:        load time =  3871.81 ms\n",
      "llama_print_timings:      sample time =   317.49 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time =  5639.20 ms /   265 tokens (   21.28 ms per token)\n",
      "llama_print_timings:        eval time = 35519.43 ms /   510 runs   (   69.65 ms per token)\n",
      "llama_print_timings:       total time = 41519.65 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6537f",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c67651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866924\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x126417c00\n",
      "ggml_metal_init: loaded kernel_mul                            0x1264190f0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12641a640\n",
      "ggml_metal_init: loaded kernel_scale                          0x12641a950\n",
      "ggml_metal_init: loaded kernel_silu                           0x12641b160\n",
      "ggml_metal_init: loaded kernel_relu                           0x126419690\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12641ba90\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12641ca70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12641ccd0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12641df50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12641d2c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12641fb90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x12641f070\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x126420470\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1264206d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x126420f50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x126421890\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x126422970\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1264232d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x126423f30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x126424a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x1264252d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x126425d20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x126426740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x126427290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x126427c30\n",
      "ggml_metal_init: loaded kernel_rope                           0x126428a00\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1264295c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12642a250\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.03 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and healthy. In order to achieve that you have to do something that makes you happy. I am passionate about being a physical therapist because it allows me to help others be happy with their bodies by relieving pain and promoting movement so they can live an active lifestyle.\n",
      "As a PT, I can make a difference in my patients' lives. I want to give them hope for the future and show that there is light at the end of the tunnel, even if it takes time to get there. Helping people gives me so much satisfaction and fulfillment; I couldn’t ask for anything more rewarding than that!\n",
      "I'm a New Yorker through and through. My husband and I are both from NYC and we live in Rockland County. We love the outdoors, whether it be hiking, camping or just spending some time at one of our local parks with our dog.\n",
      "Degree: Bachelor's degree in Biology (2011) - Wagner College. Doctorate degree in Physical Therapy (2014) - Touro College.\n",
      "I enjoy working out, hiking and camping as well as playing with my dog. I also like to read and spend time with family and friends.\n",
      "When I was a kid, I wanted to be an astronaut! I loved space so much that I even had a model of the solar system in my bedroom.\n",
      "My favorite movie is The Notebook. It’s such a beautiful love story that brings tears to my eyes every time (and yes, I have seen it multiple times). My husband and I have been together since high school and I feel like we are so lucky to have found each other at such a young age.\n",
      "My favorite season is Fall - the weather is perfect with a nice crisp breeze in the air and all of the beautiful colors. The smell of pumpkin spice lattes and fresh apples make me feel warm and cozy! It’s also my birthday month so I have an extra special reason to love it!\n",
      "My favorite food is definitely Chinese takeout, but I do love anything cheese and carbs (pizza, lasagna, pasta, mozzarella sticks…). If you ever want to get on my good side bring me some cheesy bread!\n",
      "I was born in the town\n",
      "llama_print_timings:        load time =  4315.60 ms\n",
      "llama_print_timings:      sample time =   321.07 ms /   512 runs   (    0.63 ms per token)\n",
      "llama_print_timings: prompt eval time = 13001.99 ms /   265 tokens (   49.06 ms per token)\n",
      "llama_print_timings:        eval time = 25234.79 ms /   510 runs   (   49.48 ms per token)\n",
      "llama_print_timings:       total time = 38597.38 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148d30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689866968\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1032265c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x103227ab0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x103229000\n",
      "ggml_metal_init: loaded kernel_scale                          0x103227d10\n",
      "ggml_metal_init: loaded kernel_silu                           0x103229a70\n",
      "ggml_metal_init: loaded kernel_relu                           0x10322a250\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10322ab40\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10322b3d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10322c700\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10322c960\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10322bc10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10322e4f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x10322d9f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x10322dc50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x10322ef60\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x10322f8a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1032301e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x103231290\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x103231bf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x103232830\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1032331f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x103233c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x1032347d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x103235010\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x103235b50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x103236680\n",
      "ggml_metal_init: loaded kernel_rope                           0x103237280\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x103237df0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x103238ad0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.03 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I've always been drawn to spirituality and religion, but I was never very religious as a child or young adult. However, when my first husband left me for another woman (and yes, she was pregnant), I started looking for answers and went back to church. While there, I had an epiphany: if God didn't want me to be happy in the marriage I so desperately wanted, then perhaps he didn't want me to be married at all. I began to believe that the meaning of life was to be happy - and if being single made me happy, then that was my purpose for living.\n",
      "Suddenly, everything changed. Instead of mourning the loss of a marriage that never really existed, I focused on finding ways to make myself happier. And it worked! The more I focused on making myself happy, the happier I felt.\n",
      "So what is happiness? It's a feeling - a very good feeling. It's a sense of joy and peace; an inner knowledge that everything is right with your world. Happiness comes from inside you. No one can make you happy or unhappy. The only person who can truly make you happy is yourself, because happiness is a state of mind.\n",
      "Happiness is the freedom to be yourself: to say what you feel and do what you want without guilt; to live your life according to your own values and priorities. Happiness is the knowledge that you are living your life authentically - with honesty, integrity, and sincerity.\n",
      "Happiness is a choice. You can choose to be happy or you can choose to be unhappy. It's up to you. And if you're not happy, then it's also up to you to change that!\n",
      "The best way to find happiness is by eliminating all the things that make you feel bad: the people who hurt you; the situations that are uncomfortable or unhealthy; and the thoughts and behaviors that keep you stuck. This site will help you do just that, step-by-step.\n",
      "The first step to finding happiness is by improving your self-esteem - which is essential for a happy life. It's important that you understand the difference between self-worth (which is fixed) and self-esteem (which can be improved). Self-esteem is just an opinion about yourself\n",
      "llama_print_timings:        load time =  2910.17 ms\n",
      "llama_print_timings:      sample time =   318.11 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time = 12867.28 ms /   265 tokens (   48.56 ms per token)\n",
      "llama_print_timings:        eval time = 25279.52 ms /   510 runs   (   49.57 ms per token)\n",
      "llama_print_timings:       total time = 38504.04 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867009\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100d26550\n",
      "ggml_metal_init: loaded kernel_mul                            0x100d27a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100d28f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x100d29270\n",
      "ggml_metal_init: loaded kernel_silu                           0x100d29ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x100d27fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100d2a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100d2b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100d2c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100d2c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100d2bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100d2d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x100d2e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x100d2db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x100d2ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x100d2f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x100d30170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100d31220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100d31b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100d327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100d331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x100d33bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x100d34760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x100d34f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x100d35af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x100d36610\n",
      "ggml_metal_init: loaded kernel_rope                           0x100d37230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100d37db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100d38a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.03 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your purpose — which depends on who you ask, but I define it as the intersection between what you’re good at and what you care about — and then give your all to developing that gift.\n",
      "You were given certain talents not for you to horde, but so that you can use them to help others achieve their potential. To help them fulfill their purpose.\n",
      "And in doing so, you will be fulfilling yours.\n",
      "When I think about the people who have changed my life, they’ve done it by giving me something of themselves — and often it was a skill or knowledge that I didn’t even know I needed until afterward.\n",
      "But when they did give it to me, I was better off for it. I was able to do more and be more than before. So in turn, my life was better because of the contribution those people made to my development.\n",
      "And that’s what fulfilling your purpose is all about: contributing to someone else’s.\n",
      "That’s why the best leaders don’t ask, “What can you do for me?” but instead ask, “How can I help you become who you’re meant to be?”\n",
      "They understand that their own success depends on the success of others. They know they won’t achieve anything great alone.\n",
      "So that’s what I think our purpose is: To find our gift and use it in service of helping others achieve their potential. To develop our ability so we can develop the abilities of those around us.\n",
      "And, ironically, the byproduct of doing this will be achieving your own potential.\n",
      "Because when you focus on developing others, it forces you to become better at what you do. When you spend your life helping other people grow and succeed, you’ll inevitably grow and succeed yourself. It’s impossible not to.\n",
      "So the next time you ask yourself, “What is my purpose?” maybe a better question would be: Who can I help today?\n",
      "Image courtesy of Thomas Leuthard.\n",
      "The Difference Between Givers and Takers\n",
      "A lot of people don’t understand the difference between giving and taking in business. For them, it’s about what they get out of every relationship or interaction. They might be polite on the surface but they have an underlying agenda.\n",
      "Ironically, this approach is self-limiting because it puts you at\n",
      "llama_print_timings:        load time =  2884.21 ms\n",
      "llama_print_timings:      sample time =   318.87 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time = 13088.46 ms /   265 tokens (   49.39 ms per token)\n",
      "llama_print_timings:        eval time = 25288.90 ms /   510 runs   (   49.59 ms per token)\n",
      "llama_print_timings:       total time = 38736.55 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb0e7",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867051\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x132626550\n",
      "ggml_metal_init: loaded kernel_mul                            0x132627a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x132628f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x132629270\n",
      "ggml_metal_init: loaded kernel_silu                           0x132629ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x132627fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13262a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13262b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13262c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13262c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13262bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13262d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13262e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13262db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13262ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13262f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x132630170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x132631220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x132631b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1326327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1326331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x132633bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x132634760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x132634f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x132635af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x132636610\n",
      "ggml_metal_init: loaded kernel_rope                           0x132637230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x132637db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x132638a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.72 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your bliss. And that doesn’t mean you’re not going to have suffering anymore. I think part of our spiritual path, part of following a spiritual path, is to acknowledge that there’s going to be pain and then you make friends with it. You don’t run away from it.\n",
      "You don’t try to find meaning in the suffering. What you do is say ‘I know this is here. It’s not going anywhere. So how can I make friends with this? How can I find a way to accept and be more with my pain? And what does that teach me about myself, what does it teach me about other people?’\n",
      "— Ram Dass (Finding Meaning in Suffering)\n",
      "Tags: BlissGiftLifeMeaning of LifePainSuffering\n",
      "Next story There Is No Such Thing As Failure\n",
      "Previous story The World is Not a Fairy Tale\n",
      "Wisdom is the Principle Thing…\n",
      "by Joseph B. Wirthlin · Published August 18, 2015 · Last modified January 27, 2016\n",
      "Sometimes you just have to say what everyone else is thinking.\n",
      "It’s amazing how much you can accomplish when it doesn’t matter who gets the credit.\n",
      "by Joseph B. Wirthlin · Published May 19, 2015 · Last modified January 27, 2016\n",
      "We don’t have to be perfect to experience God’s love. We just need to come as we are.\n",
      "God does not expect us to eliminate weaknesses or stop feeling overwhelmed by everyday life. In fact, the Lord often blesses us through our weaknesses. He said that when we are weak, then we are strong.\n",
      "The best way to prepare for tomorrow is to concentrate on doing today’s work superbly.\n",
      "We don’t have to be perfect to experience God’s love. We just need to come as...\n",
      "Don’t let the little things steal your joy.\n",
      "What we think determines what happens to us, so it stands to reason that if we...\n",
      "It is not by chance that our first article of faith begins with faith in God...\n",
      "There Is No Such Thing As Failure\n",
      "“There is no such thing as failure\n",
      "llama_print_timings:        load time = 15422.26 ms\n",
      "llama_print_timings:      sample time =   318.55 ms /   512 runs   (    0.62 ms per token)\n",
      "llama_print_timings: prompt eval time = 12715.84 ms /   265 tokens (   47.98 ms per token)\n",
      "llama_print_timings:        eval time = 76093.57 ms /   510 runs   (  149.20 ms per token)\n",
      "llama_print_timings:       total time = 89171.67 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867156\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104f269f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x104f27ee0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104f29430\n",
      "ggml_metal_init: loaded kernel_scale                          0x104f29710\n",
      "ggml_metal_init: loaded kernel_silu                           0x104f29f50\n",
      "ggml_metal_init: loaded kernel_relu                           0x104f28480\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104f2a8c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104f2b830\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104f2cb70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104f2cdd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104f2c080\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104f2d790\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x104f2eae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x104f2dfe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x104f2f220\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x104f2fcc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x104f30610\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104f316c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104f32020\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104f32c60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104f33650\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x104f34060\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x104f34c00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x104f35430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x104f35f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x104f36ab0\n",
      "ggml_metal_init: loaded kernel_rope                           0x104f376d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104f38250\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104f38f30\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.72 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give your best, every day.\n",
      "If you ask me what’s my biggest dream in life, it would be to have a family of my own and to make them proud.\n",
      "I am very interested in fashion and music. I love going out with friends!\n",
      "I work as a freelance social media manager for different brands and companies. My job gives me the opportunity to travel all over the world and meet many new people.\n",
      "My favorite part about being an Ambassador is that I can inspire other young women to give their best in life and go after their biggest dreams!\n",
      "I am a very open-minded person, who loves to help others with anything they need. Don’t be afraid to contact me!\n",
      "I would love to learn more about your goals and see if we can find ways to help each other out! I would love to have you in my network. We could even meet up for coffee sometime. Just let me know!\n",
      "Don’t forget – it is never too late to try something new in life, so give your best, every day!\n",
      "Amazing! Can’t wait to see what happens next with these two beautiful ladies. Congratulations!\n",
      "Thank you so much for the support, we love this!\n",
      "Gorgeous!! I love that you are both doing things you enjoy and pursuing dreams to make them happen! Keep up the great work!!\n",
      "Wow, I am very impressed by these two ambassadors! They have a lot of passion and they are going after their dreams. Love it!\n",
      "Thanks for your message! We love this too! We will reach out soon to see if we can meet sometime soon!\n",
      "This post is so inspirational. I’m loving the quotes you posted, especially the one about living life in a way that makes you proud at the end of each day. It’s such an important concept and something not enough people think about. Thank you for sharing and congratulations on being ambassadors.\n",
      "Thank you so much! We are really passionate about helping others reach their goals and be their best self. We love to inspire others through our posts and it really makes us feel great when we get messages from our readers saying they got inspired by something! Thank you again for your lovely message!\n",
      "This is very inspirational! I’m a big believer in giving my best each day. Not just because it\n",
      "llama_print_timings:        load time = 10756.77 ms\n",
      "llama_print_timings:      sample time =   325.19 ms /   512 runs   (    0.64 ms per token)\n",
      "llama_print_timings: prompt eval time = 12688.67 ms /   265 tokens (   47.88 ms per token)\n",
      "llama_print_timings:        eval time = 76022.55 ms /   510 runs   (  149.06 ms per token)\n",
      "llama_print_timings:       total time = 89080.90 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c4a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867256\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x126b26550\n",
      "ggml_metal_init: loaded kernel_mul                            0x126b27a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x126b28f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x126b29270\n",
      "ggml_metal_init: loaded kernel_silu                           0x126b29ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x126b27fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x126b2a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x126b2b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x126b2c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x126b2c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x126b2bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x126b2d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x126b2e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x126b2db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x126b2ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x126b2f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x126b30170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x126b31220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x126b31b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x126b327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x126b331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x126b33bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x126b34760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x126b34f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x126b35af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x126b36610\n",
      "ggml_metal_init: loaded kernel_rope                           0x126b37230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x126b37db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x126b38a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.72 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to enjoy it. It’s that simple. If you’re having a good time, you’re doing it right.\n",
      "Life should be filled with adventure. It should also be filled with love and companionship. You deserve to have both.\n",
      "I’m here to help you find your way back to the things in life that make you feel alive. Sometimes life gets complicated and we lose our footing. I can help you get it back so you can live the life you’ve always wanted.\n",
      "Life coaching is about helping people figure out what they really want in life, then figuring out how to get there. We all have a purpose for being here. And it’s not just to pay bills and die. It’s time to start living with purpose. The good news is that you don’t have to do this alone.\n",
      "I offer one-on-one coaching in person or over Skype, as well as group coaching. Contact me for more information on rates.\n",
      "It all starts with a free 30 minute consultation where we talk about what’s going on and figure out how I can help you. You don’t have to do this alone. Contact me today!\n",
      "I’m a certified life coach through the American Association of Christian Counselors. As part of my training, I worked with children in need. I also spent two years working at a crisis hotline, where I helped people who were in an emergency situation, as well as those who needed to talk but didn’t know where else to turn.\n",
      "I have experience teaching and mentoring young adults, from helping them through school to encouraging them to figure out what they want out of life.\n",
      "I also have my own personal struggles with depression and anxiety that I am able to relate to when working with clients. I know the pain that comes along with mental illnesses all too well, and it’s my goal to help people find their way back from there.\n",
      "Most importantly though, I want you to feel heard. I work on building a relationship with each of my clients so that they can be sure they are being heard and understood. It is my hope that through these relationships I am able to help others discover how to live their best life now.\n",
      "If you’re ready to start living your best life, contact me today for a free 30 minute consultation!\n",
      "I offer coaching\n",
      "llama_print_timings:        load time = 10810.46 ms\n",
      "llama_print_timings:      sample time =   336.11 ms /   512 runs   (    0.66 ms per token)\n",
      "llama_print_timings: prompt eval time = 12728.97 ms /   265 tokens (   48.03 ms per token)\n",
      "llama_print_timings:        eval time = 76161.18 ms /   510 runs   (  149.34 ms per token)\n",
      "llama_print_timings:       total time = 89271.16 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0bef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867356\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.91 MB\n",
      "llama_model_load_internal: mem required  = 38674.91 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x107726550\n",
      "ggml_metal_init: loaded kernel_mul                            0x107727a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x107728f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x107729270\n",
      "ggml_metal_init: loaded kernel_silu                           0x107729ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x107727fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10772a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10772b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10772c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10772c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10772bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10772d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x10772e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x10772db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x10772ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x10772f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x107730170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x107731220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x107731b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1077327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1077331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x107733bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x107734760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x107734f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x107735af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x107736610\n",
      "ggml_metal_init: loaded kernel_rope                           0x107737230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x107737db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x107738a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.92 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1536.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =  1024.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. In order to achieve this, you must first recognize that you are responsible for your own happiness and can’t rely on other people or external circumstances to make you happy. Happiness means different things to each person. For me, it involves spending quality time with my family and friends, being healthy, having a good job, and doing fun stuff like traveling.\n",
      "I was born in the Philippines and moved to the United States when I was 12 years old. My family and I immigrated because we wanted better opportunities for ourselves. We were blessed with a new life filled with possibilities!\n",
      "My career has been focused on financial services, and I have worked in various roles including customer service, sales, training, project management, quality assurance, compliance, data analytics, and risk management. My experience gives me an edge because I understand the importance of providing excellent customer service while making sure that all regulations are being met.\n",
      "My favorite vacation spot is Hawaii. The sun, sand, ocean, and laid-back atmosphere relaxes my mind and body!\n",
      "The most important thing to me in life is spending quality time with family and friends. I love going on vacations, attending sporting events, and watching movies. I’m a huge fan of the Green Bay Packers and Milwaukee Brewers—Go Pack Go!\n",
      "I enjoy learning new skills by taking online classes in data science (statistics), machine learning, and programming. I also like to read books about personal development, finance, and real estate investing.\n",
      "I love listening to music. My favorite artists are Ed Sheeran, Michael Buble, Jason Mraz, John Mayer, and Bruno Mars. My taste in music is quite eclectic. I enjoy pop, rock, rap, R&B, jazz, reggae, and country!\n",
      "The best advice I’ve received is to always do your best because you are doing it for yourself—not for others. People will judge you no matter what so don’t let that bother you.\n",
      "I am a morning person. I like waking up early and starting my day with a cup of coffee.\n",
      "My favorite place in the world is where I was born, the Philippines. It’s a beautiful country filled with amazing people!\n",
      "The most important thing to me in life is spending quality time with my family—it keeps me ground\n",
      "llama_print_timings:        load time =  8778.95 ms\n",
      "llama_print_timings:      sample time =   343.12 ms /   512 runs   (    0.67 ms per token)\n",
      "llama_print_timings: prompt eval time = 25342.97 ms /   265 tokens (   95.63 ms per token)\n",
      "llama_print_timings:        eval time = 46386.29 ms /   510 runs   (   90.95 ms per token)\n",
      "llama_print_timings:       total time = 72114.73 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867437\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.91 MB\n",
      "llama_model_load_internal: mem required  = 38674.91 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12c719500\n",
      "ggml_metal_init: loaded kernel_mul                            0x12c71a920\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12c71c000\n",
      "ggml_metal_init: loaded kernel_scale                          0x12c71c310\n",
      "ggml_metal_init: loaded kernel_silu                           0x12c71cb50\n",
      "ggml_metal_init: loaded kernel_relu                           0x12c71afd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12c71d4d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12c71e4d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12c71f770\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12c71f9d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12c720080\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12c7202e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x12c721720\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x12c720c00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x12c721e80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x12c722760\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x12c7230a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12c723b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12c724c80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12c7258c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12c726410\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x12c726dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x12c727610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x12c728060\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x12c728b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x12c729690\n",
      "ggml_metal_init: loaded kernel_rope                           0x12c72a2c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12c72ae40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12c72baf0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.92 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1536.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =  1024.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a happy, full-filled life with love around you.\n",
      "I don't think there is one specific meaning of life but I do know that you should live your life to the fullest and be happy.\n",
      "Meaning Of Life? I don’t know what it is. But all we have to do is make sure we stay alive and healthy. Don’t smoke, don’t drink or take drugs, and just make good choices.\n",
      "I think that meaning of life is to help others in need, to have fun, and enjoy each day as if its your last because you never know whats gonna happen the next second.\n",
      "The meaning of life is a good question with no right answer, but I believe that it's just finding happiness through experiences and memories. Everyone looks for different things in life: some people look for money, fame, love, or peace. You may not know what you want yet, but when you get there, you will.\n",
      "I think the meaning of life is to find your own purpose and goals that fit your character while making sure your actions are not hindering others. The meaning of life is a very broad subject with no right answer so I guess it all depends on what kind of person you are. If you believe in God, religion could be your ultimate goal. But if you don't there are still many other goals that can be found by just getting out there and living life.\n",
      "I think the meaning of life is to live a good happy life with your friends and family.\n",
      "The meaning of life is to do what makes you happy, and be loved.\n",
      "The meaning of life is to enjoy each day as if it was your last. You never know whats going to happen in the next second so just make the best of every moment.\n",
      "I think that we all have a purpose in life and we must live our lives to the fullest and when you die it's not the end of everything, but it's the beginning of another journey in your afterlife. I know that it may sound weird for most people, but I think that there is something better waiting on us after we pass away from this world.\n",
      "I believe that the meaning of life is to be happy and do whatever you want to do because you only live once so might as well go out with a bang.\n",
      "The meaning of life? I don't know what it is, but i feel like it\n",
      "llama_print_timings:        load time =  6179.72 ms\n",
      "llama_print_timings:      sample time =   343.23 ms /   512 runs   (    0.67 ms per token)\n",
      "llama_print_timings: prompt eval time = 25693.63 ms /   265 tokens (   96.96 ms per token)\n",
      "llama_print_timings:        eval time = 46372.44 ms /   510 runs   (   90.93 ms per token)\n",
      "llama_print_timings:       total time = 72451.52 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867516\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.91 MB\n",
      "llama_model_load_internal: mem required  = 38674.91 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1436229d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x143623ec0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x143625410\n",
      "ggml_metal_init: loaded kernel_scale                          0x143624120\n",
      "ggml_metal_init: loaded kernel_silu                           0x143625e80\n",
      "ggml_metal_init: loaded kernel_relu                           0x143626650\n",
      "ggml_metal_init: loaded kernel_gelu                           0x143626f40\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1436277b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x143628ae0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x143628d40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x143627ff0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x143629730\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x14362aa90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x143629f40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x14362b330\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x14362bc70\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x14362c5b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14362d690\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14362dff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14362ec50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14362f7b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x14362fff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x1436309e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1436313d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x143631f10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x143632a10\n",
      "ggml_metal_init: loaded kernel_rope                           0x143633640\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1436341c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x143634e90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.92 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1536.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =  1024.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give love and show kindness.\n",
      "I have a wonderful family, with lots of children, grandchildren, great-grandchildren, nieces and nephews, so I’m always involved in their lives.\n",
      "But sometimes you need a change, so when my sister told me about the opportunity to volunteer at a refugee camp in Calais, France, I jumped at it.\n",
      "I’ve worked in care homes for 20 years and volunteered with Age UK, but nothing could have prepared me for this experience.\n",
      "It was tough going out there - we were living in tents, sleeping on the floor with just a thin mat between us and the ground. The wind whistled through our tents at night and I’d lie awake listening to the rain pelting down on my tent. It rained for 10 days straight one week; water leaked into my tent and we had to move it to higher ground so we didn’t get flooded out.\n",
      "But all this paled in comparison to what the refugees were going through. They were living in much worse conditions, with no food or shelter. They had fled war-torn countries like Syria, Afghanistan and Iraq, sometimes leaving everything they owned behind.\n",
      "You could see how much they appreciated anything we did for them - even if it was just giving someone a cup of tea. One man who hadn’t eaten in 4 days broke down in tears when we gave him food.\n",
      "Sometimes people would come and have a chat with us about their situation or tell us what had happened to them. They were all very keen to help others, even though they didn’t know if they could survive themselves.\n",
      "I met some amazing volunteers while I was out there, including a couple from Texas who were on honeymoon! I’d never have thought of going somewhere like that for my honeymoon but their attitude was ‘why not?’ and it really made me think about the impact you can make by just changing your perspective.\n",
      "The refugees are living in a very difficult situation, but they’re still looking for ways to help others. They’ve lost everything and yet they want to be there helping people who have even less than them. That’s something we could all learn from.\n",
      "I hope I made some difference to their lives. A lot of the refugees haven’t been able to work,\n",
      "llama_print_timings:        load time =  6210.20 ms\n",
      "llama_print_timings:      sample time =   338.02 ms /   512 runs   (    0.66 ms per token)\n",
      "llama_print_timings: prompt eval time = 25393.97 ms /   265 tokens (   95.83 ms per token)\n",
      "llama_print_timings:        eval time = 46309.39 ms /   510 runs   (   90.80 ms per token)\n",
      "llama_print_timings:       total time = 72083.85 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce3e8",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5122b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867594\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x115011a70\n",
      "ggml_metal_init: loaded kernel_mul                            0x115012f80\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1150144d0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1150147e0\n",
      "ggml_metal_init: loaded kernel_silu                           0x115013380\n",
      "ggml_metal_init: loaded kernel_relu                           0x115014f70\n",
      "ggml_metal_init: loaded kernel_gelu                           0x115015830\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1150169d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x115016c30\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x115017e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x115017150\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x115019a10\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1150190b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x115018730\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11501a440\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11501adc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11501b700\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11501c7e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11501d140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11501dda0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11501e900\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11501f140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11501fb30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x115020520\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x115021070\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x115021b90\n",
      "ggml_metal_init: loaded kernel_rope                           0x1150227f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x115023360\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x115024030\n",
      "ggml_metal_add_buffer: buffer 'data' size 130574155776 is larger than buffer maximum of 115964116992\n",
      "llama_init_from_file: failed to add buffer\n",
      "llama_init_from_gpt_params: error: failed to load model './models/65B/ggml-model-f16.bin'\n",
      "main: error: unable to load model\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86588417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689867638\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and live life to its fullest.\n",
      "I believe everyone should have the right to their own opinions, and that everyone’s opinion counts equally.\n",
      "I believe in doing good deeds every day, and that we all need to help each other out more often.\n",
      "I believe the purpose of a soul is to experience life.\n",
      "These are just some things I believe in; they might sound cheesy or cliché, but they're my opinions, so I'm allowed to say them!\n",
      "In this blog I’ll post my personal thoughts on various topics, and hopefully you can relate to them as well. Thanks for stopping by!\n",
      "Sorry I haven't posted anything in a while... I was on holiday.\n",
      "I just thought I'd share some of the amazing photos we took when we were there.\n",
      "Above is a picture of my dad and brother looking at the waterfall, and below are some other pictures of me standing by it. We also saw some animals; above is a picture of some penguins we saw in the zoo! They're so cute. And below is a picture of some deer we saw on our last day there.\n",
      "I think these photos turned out really well, and I hope you enjoyed looking at them.\n",
      "Happy New Year to all my followers! I don’t really believe in new year resolutions because I think they can be set any time during the year. But anyway, here are some of mine:\n",
      "To get good grades for my GCSEs\n",
      "To lose weight (I need to do this a lot)\n",
      "To make more friends at school and on this blog\n",
      "To do more work experience\n",
      "To have fun and enjoy life more!\n",
      "What are your new year resolutions? I hope you all achieve them, and that 2014 is the best year of our lives. Happy New Year once again! :) xx\n",
      "Christmas and New Year are fast approaching, so I thought I'd make a Christmas-themed post. The other day my family and I put up the Christmas tree, and we decorated it with tinsel, baubles, lights and a star on top. We also put some presents underneath, which were for me and my brother.\n",
      "I have to say that our Christmas tree looks pretty good; the decorations are all colour-coordinated, and there are a lot of them.\n",
      "llama_print_timings:        load time = 54911.26 ms\n",
      "llama_print_timings:      sample time =   355.19 ms /   512 runs   (    0.69 ms per token)\n",
      "llama_print_timings: prompt eval time = 25514.76 ms /   265 tokens (   96.28 ms per token)\n",
      "llama_print_timings:        eval time = 300125.21 ms /   510 runs   (  588.48 ms per token)\n",
      "llama_print_timings:       total time = 326041.71 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e1e0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689868021\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live your life in such a way that it’s worth living.\n",
      "If you have a purpose, then you will be happy. Happiness doesn’t come from being rich or successful. Happiness comes from having a purpose and from loving people.\n",
      "I have been very blessed by the universe. I am able to do what I love for a living — it’s not just a job that pays the bills. I don’t think about work as “work” because I enjoy every day of my life!\n",
      "My purpose is to make the world a better place with my art. The more people can see and be inspired by my art, the happier and better they will feel. If my art makes people happy, then it makes me happy.\n",
      "There are so many things we can do in this world that can make us happy. Find your own meaning of life and live to your fullest! Love what you do and love others. The meaning of life is LOVE!!\n",
      "This entry was posted on Wednesday, October 24th, 2018 at 6:49 am\tand is filed under Uncategorized. You can follow any responses to this entry through the RSS 2.0 feed. Both comments and pings are currently closed.\n",
      "I totally agree with what you said!\n",
      "The world needs more people like you, who spread joy, laughter, love, creativity, hope, inspiration and happiness in their own unique ways! Love your artwork!!! Thank you for sharing your gift with the world!!\n",
      "Thank you so much for your lovely comment! I really appreciate that!\n",
      "I agree. The meaning of life is to be happy. And I’m glad you are very happy. We need more people like you in this world.\n",
      "Yes, we need more happiness in this world! Thank you for your kind words!\n",
      "Hi Dice!! I think my purpose in life is helping others as much as possible and being the best version of myself that I can be. Your art really speaks to me and makes me happy. Thank you!!!\n",
      "That’s so great, Laura! You are very talented too!\n",
      "Thank you for sharing your gift with us! I love all your work and it always cheers me up when I look at it!\n",
      "Thanks for sharing this beautiful message!\n",
      "I agree, the meaning of life is to be happy. It doesn’t matter\n",
      "llama_print_timings:        load time = 54707.12 ms\n",
      "llama_print_timings:      sample time =   351.90 ms /   512 runs   (    0.69 ms per token)\n",
      "llama_print_timings: prompt eval time = 25945.77 ms /   265 tokens (   97.91 ms per token)\n",
      "llama_print_timings:        eval time = 300758.15 ms /   510 runs   (  589.72 ms per token)\n",
      "llama_print_timings:       total time = 327102.49 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57dd0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689868405\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love, learn and grow. I have always enjoyed helping people in need and as a result I am pursuing my dream career as a teacher. While I am studying education, I continue to work part-time at an early childhood centre where I can put into practise all that I am learning.\n",
      "I believe that the best way to learn is by doing. That is why I make it my goal to create an inclusive environment in which children feel confident and happy to express themselves and their ideas, as well as try new things. In this sense, I focus on building positive relationships with both children and parents/caregivers to ensure every child’s needs are being met.\n",
      "I believe the best way for a child to learn is through play-based learning. Children have a natural curiosity that allows them to learn without even realising it through exploring their environment, experimenting, asking questions and building on prior knowledge. I also use intentional teaching to further support children in their interests and development.\n",
      "I love being around young children because of the way they view the world. They notice and appreciate all the little things that we often take for granted as adults. I like to think of myself as a mentor or role model to the children, showing them how to be kind, patient and respectful towards themselves and others.\n",
      "I am passionate about helping every child feel valued and appreciated for who they are. Everyone is different in their own way so I ensure that all children have equal opportunities to learn at their own pace in a safe environment.\n",
      "I believe that it’s important for children to have the opportunity to express themselves, and to be given the encouragement to do so in a creative manner. It is my goal to help develop the young minds of our future into happy individuals who care about others, who embrace individuality, and can express their feelings to those around them.\n",
      "I believe that children are the future leaders and I love being able to inspire them each day. I am committed to providing a safe environment for all children to learn in while helping them develop their own personalities and interests. I encourage each child to be confident, happy and to respect themselves and others.\n",
      "I believe that every individual should have the opportunity to reach their full potential through education. It is important that children are given the chance to grow as individuals, which includes developing new skills and self-confidence. I am committed to providing a safe environment where all children can learn at\n",
      "llama_print_timings:        load time = 53292.63 ms\n",
      "llama_print_timings:      sample time =   353.37 ms /   512 runs   (    0.69 ms per token)\n",
      "llama_print_timings: prompt eval time = 26513.07 ms /   265 tokens (  100.05 ms per token)\n",
      "llama_print_timings:        eval time = 300828.76 ms /   510 runs   (  589.86 ms per token)\n",
      "llama_print_timings:       total time = 327742.12 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
