{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     ggml-vocab.bin\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                     \u001b[31mtokenizer.model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "embd-input-test\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-metal.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders \n",
      "c++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078421\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.95 MB\n",
      "llama_model_load_internal: mem required  = 3949.95 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x125726fc0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1257284b0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x125729a00\n",
      "ggml_metal_init: loaded kernel_scale                          0x125728710\n",
      "ggml_metal_init: loaded kernel_silu                           0x12572a470\n",
      "ggml_metal_init: loaded kernel_relu                           0x12572ac50\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12572b540\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12572bdd0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12572d100\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12572d360\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12572c610\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12572eef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12572e3f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12572e650\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12572f960\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1257302a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x125730be0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x125731c90\n",
      "ggml_metal_init: loaded kernel_norm                           0x125732610\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x125733290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x125733ca0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x125734700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x125735100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x125735aa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1257365f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1257370a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x125737a80\n",
      "ggml_metal_init: loaded kernel_rope                           0x125738320\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1257392a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12573a4e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12573b030\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12573bbd0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB, ( 3648.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.41 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out who you really are. To be that person, and not live in fear.\n",
      "In a world where we’re all afraid of one thing or another, it’s important to keep your head high and to be the best version of yourself possible. So when you come across people who put you down for what you do, don’t listen to them – even if they’re someone close to you.\n",
      "I feel like I want to be a positive influence to others by showing them that it is possible to live your life with no regrets and always strive to be the best version of yourself.\n",
      "For me, being my own boss allows me to spend time with people I love, to explore new opportunities and to do things that make me happy – all while making an income at the same time.\n",
      "I know everyone’s path is different to mine, but it doesn’t mean yours can’t be fulfilling in its own way too. Be proud of yourself and who you are. And when someone comes along with a negative opinion of you, don’t listen to them – because they wouldn’t want to live your life either!\n",
      "Love this post? Check out The Best Way To Get Over Your Past Failures (And Stop Letting Them Affect You In The Future) and 10 Ways To Improve Your Self Esteem.\n",
      "What is the meaning of life to you? Comment below!\n",
      "17 thoughts on “The Meaning Of Life”\n",
      "Kiran Desai says:\n",
      "Love this post, thanks for sharing.\n",
      "Thank you so much, Kiran! Really glad you enjoyed it �� xx\n",
      "This was such a great read and I couldn’t agree with you more when you said that your path is different to others. If we all did the same thing then the world would be pretty boring! Xx\n",
      "Thank you so much, Emily! So glad you enjoyed it ��� xx\n",
      "I love this post! It’s such a positive and inspiring read!\n",
      "Thanks so much, Samantha! I really appreciate your kind words. xx\n",
      "This is such an honest and open post, well done!!\n",
      "Thank you so much, Kelly! That means a lot ���� xx\n",
      "I love this post it’s so true and I couldn’t agree with you more when you said that your path is different to others.\n",
      "llama_print_timings:        load time =   571.79 ms\n",
      "llama_print_timings:      sample time =   323.49 ms /   512 runs   (    0.63 ms per token,  1582.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3458.53 ms /   265 tokens (   13.05 ms per token,    76.62 tokens per second)\n",
      "llama_print_timings:        eval time =  6520.82 ms /   510 runs   (   12.79 ms per token,    78.21 tokens per second)\n",
      "llama_print_timings:       total time = 10342.86 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078432\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.95 MB\n",
      "llama_model_load_internal: mem required  = 3949.95 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100d27340\n",
      "ggml_metal_init: loaded kernel_mul                            0x100d28830\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100d29d80\n",
      "ggml_metal_init: loaded kernel_scale                          0x100d2a060\n",
      "ggml_metal_init: loaded kernel_silu                           0x100d2a880\n",
      "ggml_metal_init: loaded kernel_relu                           0x100d28dd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100d2b1e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100d2c150\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100d2d490\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100d2c800\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100d2dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100d2f270\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100d2e750\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100d2fb50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100d2fdb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100d30630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100d30f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100d32050\n",
      "ggml_metal_init: loaded kernel_norm                           0x100d329b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100d33680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100d34050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100d34a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100d35450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100d35e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100d36960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100d37270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100d37c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x100d38490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100d39410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100d39fb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100d3ab30\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100d3b690\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB, ( 3648.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.41 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love and be loved. I am a very down-to-earth, kind and considerate person who is looking for my soul mate to share my journey in life with.\n",
      "I’m 53 years old and live in Melbourne, Australia. Apart from that I have no other special qualities!\n",
      "I like many different things but don’t really enjoy going out. More than likely we will meet at the movies or a restaurant.\n",
      "I am looking for someone who is easy-going and can keep up with my quick wit. Someone who wants to enjoy life as much as I do.\n",
      "My ideal partner would be someone who has a good sense of humour, is considerate, caring and compassionate, honest, faithful, and open minded. The most important things are to have a good heart, and the ability to love me unconditionally and vice versa.\n",
      "I’m looking for my soul mate! I don’t like playing games, so if you’re not serious about finding true love then please go ahead and leave now. This is just too important to mess around with.\n",
      "My ideal partner would be someone who has a good sense of humour, is considerate, caring and compassionate, honest, faithful, and open minded. The most important things are to have a good heart, and the ability to love me unconditionally and vice versa. I’m looking for my soul mate!\n",
      "Love and be loved. Someone who has a good sense of humour, is considerate, caring and compassionate, honest, faithful, and open minded. The most important things are to have a good heart, and the ability to love me unconditionally and vice versa. I’m looking for my soul mate!\n",
      "Love and be loved. Someone who has a good sense of humour, is considerate, caring and compassionate, honest, faithful, and open minded. The most important things are to have a good heart, and the ability to love me unconditionally and vice versa. I’m looking for my soul mate! This is just too important to mess around with.\n",
      "I am looking for someone who has a good sense of humour, is considerate, caring and compassionate, honest, faithful, and open minded. The most important things are to have a good heart, and the\n",
      "llama_print_timings:        load time =   539.95 ms\n",
      "llama_print_timings:      sample time =   317.40 ms /   512 runs   (    0.62 ms per token,  1613.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3447.43 ms /   265 tokens (   13.01 ms per token,    76.87 tokens per second)\n",
      "llama_print_timings:        eval time =  6515.07 ms /   510 runs   (   12.77 ms per token,    78.28 tokens per second)\n",
      "llama_print_timings:       total time = 10320.46 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078443\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.95 MB\n",
      "llama_model_load_internal: mem required  = 3949.95 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104e27340\n",
      "ggml_metal_init: loaded kernel_mul                            0x104e28830\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104e29d80\n",
      "ggml_metal_init: loaded kernel_scale                          0x104e2a060\n",
      "ggml_metal_init: loaded kernel_silu                           0x104e2a880\n",
      "ggml_metal_init: loaded kernel_relu                           0x104e28dd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104e2b1e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104e2c150\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104e2d490\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104e2c800\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104e2dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104e2f270\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x104e2e750\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x104e2fb50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104e2fdb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x104e30630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104e30f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104e32050\n",
      "ggml_metal_init: loaded kernel_norm                           0x104e329b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104e33680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104e34050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104e34a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104e35450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104e35e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104e36960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104e37270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x104e37c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x104e38490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104e39410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104e39fb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104e3ab30\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x104e3b690\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB, ( 3648.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.41 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "I believe in love, and I'm not afraid to say \"I love you.\"\n",
      "I have a hard time writing about myself, but here goes: I am a 23-year-old woman living on her own for the first time. I recently graduated from college with a BA in English and a minor in Women's Studies (amongst other things). I currently work as a waitress, and have also done some freelance writing here and there. Writing is my passion, and I plan to continue doing it until I can do nothing else but write. My dream job would be working for a magazine or publication, either in an editorial position or on the creative side -- fiction or nonfiction (I'm still undecided). I also love music, film, photography, and art/design of all kinds.\n",
      "I am open to just about any kind of relationship: platonic, romantic, whatever. I value honesty above all else, so if you can't be honest with me and vice versa (which includes telling the truth), then we probably aren't meant for each other in the long run. I don't have a lot of time to date right now -- I mean that in terms of my schedule as well as my desire for a relationship, so if it takes more than one meeting to get to know you and vice versa, please go elsewhere.\n",
      "This is also where you can leave me a message or question here on the site; I will do my best to respond within 24 hours. (You can also send me an email at melissablack@hotmail.com.)\n",
      "Thanks for taking the time to read this and learn more about me! :)\n",
      "I'm an open-minded person who doesn't like to judge a book by its cover, so I'll say that if you think you have what it takes to make me happy, go ahead and leave me a message or question. If we don't get along, no hard feelings -- just keep it moving, and there are plenty of other people who will probably like you better! ;)\n",
      "I am not looking for a one-night stand with anyone. I have enough problems as it is without adding another one to the list. Please do not message me if that's all you plan on doing tonight -- it is very obvious from your messages and what you write, and I would rather spend\n",
      "llama_print_timings:        load time =   540.93 ms\n",
      "llama_print_timings:      sample time =   329.89 ms /   512 runs   (    0.64 ms per token,  1552.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3501.78 ms /   265 tokens (   13.21 ms per token,    75.68 tokens per second)\n",
      "llama_print_timings:        eval time =  6525.51 ms /   510 runs   (   12.80 ms per token,    78.15 tokens per second)\n",
      "llama_print_timings:       total time = 10398.43 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078454\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x15bfbd040\n",
      "ggml_metal_init: loaded kernel_mul                            0x15bfbe530\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x15bfbfa80\n",
      "ggml_metal_init: loaded kernel_scale                          0x15bfbe790\n",
      "ggml_metal_init: loaded kernel_silu                           0x15bfc04f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x15bfc0cd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15bfc15c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x15bfc1e40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x15bfc3180\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x15bfc33e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x15bfc3a70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x15bfc3e40\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x15bfc4460\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x15bfc46c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x15bfc5a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x15bfc6310\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x15bfc6c50\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x15bfc7d00\n",
      "ggml_metal_init: loaded kernel_norm                           0x15bfc8680\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x15bfc9300\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x15bfc9d10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x15bfca770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x15bfcb170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x15bfcbb10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x15bfcc660\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x15bfcd110\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x15bfcdaf0\n",
      "ggml_metal_init: loaded kernel_rope                           0x15bfce390\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x15bfcf310\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x15bfd0550\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x15bfd10a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x15bfd1c40\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.56 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live.\n",
      "I am a human being and I am not perfect at all, but I have my own value.\n",
      "I always try hard to give everything what I have in me for this beautiful world.\n",
      "Let's make the world more colorful!\n",
      "And also I love cooking, travel, music, and photography.\n",
      "I will post some of my works in the coming days.\n",
      "I hope you enjoy browsing!\n",
      "A very special thanks to my family & friends who always encouraged me to do this!\n",
      "Because without them, I wouldn't be here today.\n",
      "And also a great thanks for all those people who have been following me and cheering me up at the same time.\n",
      "I will try hard and make something that worth your time.\n",
      "Thank you very much!! It is a pleasure to share my world with you!\n",
      "I am so glad to see your postings. This is an amazing blog. You are such a talented person. I wish you good luck with it. Looking forward to future posts.\n",
      "Wow, it's really a nice blog! Thanks for sharing your beautiful life in this blog!\n",
      "Thanks for following my travel blog, please let me know if you have any questions about the places I visit, they are all over the world and from personal experience so I hope there is something that may help you.\n",
      "Thank you very much for leaving a comment on my blog!!\n",
      "I will always be here to check your posts!\n",
      "Hey! Thanks for following my blog :) Nice to meet you!\n",
      "My name is Pixy, from Philippines. I am happy to find you here in wordpress..\n",
      "Hi Pixy! thank you so much for your kind words and also the follow! I really appreciate it very much!!\n",
      "I have read some of your posts already and they are great!\n",
      "Thanks for visiting my blog. You've got a great one here!\n",
      "Hey, thanks for stopping by my blog! It means a lot to me that you read and like what I write. Thanks too for the follow - much appreciated! Have a good day.\n",
      "Hi. Your photos are gorgeous. My favourite is the one of the blue flowers.\n",
      "Thanks so much for visiting my blog, which is all about writing on life and travel. I hope you’ll be back in the future – I look forward to it.\n",
      "Thank you very much for stopping by!!\n",
      "I really appreciate\n",
      "llama_print_timings:        load time =  2949.73 ms\n",
      "llama_print_timings:      sample time =   370.47 ms /   512 runs   (    0.72 ms per token,  1382.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3700.61 ms /   265 tokens (   13.96 ms per token,    71.61 tokens per second)\n",
      "llama_print_timings:        eval time = 18745.17 ms /   510 runs   (   36.76 ms per token,    27.21 tokens per second)\n",
      "llama_print_timings:       total time = 22863.44 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078480\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x127091360\n",
      "ggml_metal_init: loaded kernel_mul                            0x127092b50\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x127094220\n",
      "ggml_metal_init: loaded kernel_scale                          0x127094530\n",
      "ggml_metal_init: loaded kernel_silu                           0x127094d80\n",
      "ggml_metal_init: loaded kernel_relu                           0x127093120\n",
      "ggml_metal_init: loaded kernel_gelu                           0x127095630\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x127096650\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x127097980\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x127097be0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x127096ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x127098440\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x127099940\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x127098df0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12709a210\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12709ab50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12709b490\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12709c570\n",
      "ggml_metal_init: loaded kernel_norm                           0x12709cef0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12709dbc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12709e590\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12709efa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12709f980\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1270a0380\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1270a0e60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1270a1600\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1270a1fd0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1270a29c0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1270a3940\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1270a44d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1270a5090\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1270a5c10\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.56 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. Happiness is an emotion, and emotions are not something that can be defined by a word or a sentence. Happiness has many forms; it comes in different shapes, colors, sizes and smells. It makes us feel good inside and puts a smile on our face.\n",
      "Happiness is what we seek in life to make ourselves feel better about life. Without happiness, there would be nothing to strive for, no reason to do something or even have a hope that things will get better. People are not happy because they have their own problems. These problems can be from family members, friends, society, etc. The way we see the world and what is happening in it, has an effect on how we feel about ourselves. If you have a good job, live in a nice home, have a beautiful family, etc., there will always be something that could make you unhappy. If you lose your job or someone close to you passes away, or even if you get into debt with credit cards and bills, there is still hope for happiness to come back again.\n",
      "Our emotions are part of the human mindset; however, we do not choose what emotions we feel or have in our lives. I believe it all comes from something that is outside of ourselves; such as God. We do not choose where we were born in life or even when we will be born. Some of us may live in a good neighborhood and some in a bad one. We also cannot control what happens to us while we are on this earth, but it does shape our beliefs about the world and how we see life.\n",
      "If we do not have happiness in life, then how can we make others happy? How can we expect people to be happy when we are not ourselves? You need to smile; it will put a smile on other peoples faces when you pass them by. Smiling is contagious and once one person smiles at you, you cannot help but smile back at that person. I believe if we put some happiness in our lives, others will follow.\n",
      "Happiness does not have to be a certain thing or someone; happiness can come from things around you such as the weather, your day, your friends and family members. It is important for people to make time for themselves so they can find what makes them happy. I believe we are all born with our own happiness inside of us, but sometimes we just need to find it again\n",
      "llama_print_timings:        load time =  1709.73 ms\n",
      "llama_print_timings:      sample time =   367.99 ms /   512 runs   (    0.72 ms per token,  1391.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3468.72 ms /   265 tokens (   13.09 ms per token,    76.40 tokens per second)\n",
      "llama_print_timings:        eval time = 18731.81 ms /   510 runs   (   36.73 ms per token,    27.23 tokens per second)\n",
      "llama_print_timings:       total time = 22613.32 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078504\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1529a2fa0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1529a4490\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1529a59e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1529a46f0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1529a6450\n",
      "ggml_metal_init: loaded kernel_relu                           0x1529a6c30\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1529a7520\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1529a7db0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1529a90e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1529a9340\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1529a85f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1529aaed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1529aa3d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1529aa630\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1529ab940\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1529ac280\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1529acbc0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1529adc70\n",
      "ggml_metal_init: loaded kernel_norm                           0x1529ae5f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1529af270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1529afc80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1529b06e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1529b10e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1529b1a80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1529b25d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1529b3080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1529b3a60\n",
      "ggml_metal_init: loaded kernel_rope                           0x1529b4300\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1529b5280\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1529b64c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1529b7010\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1529b7bb0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.56 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live and live it fully. And, in doing so, one must find what makes them happy, then do more of that thing. That’s a big part of my blogging philosophy; I write what I like to read and hope others will enjoy reading it too!\n",
      "I have two children, a son who is 12 years old and a daughter who is 8. They are the light of my life and give me so much joy. We are a very active family and love being outdoors. We play tennis together, go hiking, bicycle riding… you name it!\n",
      "I was born in Sweden but moved to England at age 6 with my parents and two older siblings. I have been living here ever since (with the exception of a short stint when I went to university in Canada). My husband is American so we also spend time there on vacation, and he loves visiting his relatives in Sweden for Christmas every year!\n",
      "My first job was working in a supermarket at age 16. It lasted about four months before my boss called me into his office and fired me because I kept talking to customers (in Swedish) instead of sticking to the script.\n",
      "I have a few degrees, including a BSc. in Physics from Imperial College London, an MBA from the University of Leicester in England and more recently a Certified Management Accountant designation. I worked as a management accountant for 10 years before starting my own company where I currently consult with businesses on financial matters and do tax preparations for individuals and small businesses.\n",
      "I’ve always loved writing, but it was only after my husband died in 2013 that I decided to start blogging as a creative outlet. I wrote a short story about our marriage called “A Perfect Love Story”. This is the first chapter. It’s not for everyone because it’s pretty sappy (and long), but if you like romance, you should give it a try!\n",
      "I write on various topics including parenting, travel and relationships. I love to tell my stories in blog posts, but also enjoy writing short fiction. My favorite genre is romance so that’s what most of my blog posts are about. However, I also like to write about life in general and the experiences we go through as human beings. If you have a minute, check out this collection of funny stories from\n",
      "llama_print_timings:        load time =  1625.42 ms\n",
      "llama_print_timings:      sample time =   330.97 ms /   512 runs   (    0.65 ms per token,  1546.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3595.85 ms /   265 tokens (   13.57 ms per token,    73.70 tokens per second)\n",
      "llama_print_timings:        eval time = 18664.02 ms /   510 runs   (   36.60 ms per token,    27.33 tokens per second)\n",
      "llama_print_timings:       total time = 22632.76 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078529\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.00 MB\n",
      "llama_model_load_internal: mem required  = 7390.00 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x105c1de80\n",
      "ggml_metal_init: loaded kernel_mul                            0x105c1f370\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x105c208c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x105c20bd0\n",
      "ggml_metal_init: loaded kernel_silu                           0x105c213e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x105c1f910\n",
      "ggml_metal_init: loaded kernel_gelu                           0x105c21d10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x105c22cf0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x105c22f50\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x105c241d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x105c23540\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x105c25e10\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x105c252f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x105c266f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x105c26950\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x105c271d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x105c27b10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x105c28bf0\n",
      "ggml_metal_init: loaded kernel_norm                           0x105c29570\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x105c2a200\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x105c2ac00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x105c2b620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x105c2c020\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x105c2c9f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x105c2d550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x105c2dfc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x105c2f060\n",
      "ggml_metal_init: loaded kernel_rope                           0x105c2fc40\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x105c307f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x105c313a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x105c31ef0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x105c32a50\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB, ( 7024.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 7036.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.45 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and useful.\n",
      "3. Invest in yourself, your career, your family and your friends.\n",
      "You will be amazed at how much you get back!\n",
      "4. Never lie down when it's time to stand up.\n",
      "5. Don't waste time thinking about the things that are out of your control.\n",
      "6. Learn to say no - graciously, but firmly.\n",
      "7. Be a part of something bigger than yourself.\n",
      "8. Love yourself and make good choices for you!\n",
      "9. Make happiness a habit, a priority.\n",
      "10. Don't wait until everything is perfect to enjoy your life.\n",
      "Take the risk and jump into the void.\n",
      "Because it might just be the greatest leap of faith you will ever take.\n",
      "12. You have a choice everyday whether or not you are going to try!\n",
      "13. Remember, you can't use up creativity - the more you use, the more you have.\n",
      "14. It is better to fail at something you love than to succeed at something you hate.\n",
      "15. Do not forget that your children watch everything you do!\n",
      "16. Be a good example for your children and your community.\n",
      "17. Remember, it's okay to be scared - it means you care about what you're doing.\n",
      "18. Love the life you live and you will live the life you love.\n",
      "19. Above all else, remember the most important person in the world is... YOU!\n",
      "20. You have a choice everyday whether or not you are going to try!\n",
      "21. Have faith and hope that everything happens for a reason.\n",
      "22. Learn from your mistakes but never regret them. It was experience that taught you something.\n",
      "23. Be thankful for what you have; you'll end up having more. If you concentrate on what you don't have, you will never, ever have enough.\n",
      "24. The best is yet to come and dreams really do come true!\n",
      "25. Above all else, remember the most important person in the world is... YOU!\n",
      "26. Remember that when you are kind to others it helps change your own life as well.\n",
      "27. If it is meant to be - it will happen - if not - it wasn't!\n",
      "28.\n",
      "llama_print_timings:        load time =  1704.00 ms\n",
      "llama_print_timings:      sample time =   318.88 ms /   512 runs   (    0.62 ms per token,  1605.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5823.02 ms /   265 tokens (   21.97 ms per token,    45.51 tokens per second)\n",
      "llama_print_timings:        eval time = 10404.93 ms /   510 runs   (   20.40 ms per token,    49.02 tokens per second)\n",
      "llama_print_timings:       total time = 16587.19 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078547\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.00 MB\n",
      "llama_model_load_internal: mem required  = 7390.00 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11ad27340\n",
      "ggml_metal_init: loaded kernel_mul                            0x11ad28830\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11ad29d80\n",
      "ggml_metal_init: loaded kernel_scale                          0x11ad2a060\n",
      "ggml_metal_init: loaded kernel_silu                           0x11ad2a880\n",
      "ggml_metal_init: loaded kernel_relu                           0x11ad28dd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11ad2b1e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11ad2c150\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11ad2d490\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11ad2c800\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11ad2dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11ad2f270\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11ad2e750\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11ad2fb50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11ad2fdb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11ad30630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11ad30f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11ad32050\n",
      "ggml_metal_init: loaded kernel_norm                           0x11ad329b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11ad33680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11ad34050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11ad34a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11ad35450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11ad35e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11ad36960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11ad37270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11ad37c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x11ad38490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11ad39410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11ad39fb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11ad3ab30\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11ad3b690\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB, ( 7024.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 7036.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.45 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living.\n",
      "I believe that living my life is an art, and as such it deserves the same care, dedication, and effort as any piece of artwork, whether it's a painting or sculpture. The beauty of art lies not so much in the product as the process of creating something from nothing. And so I see my life as something to shape with the same love and attention that I might give to clay when forming a pot.\n",
      "I believe that being alive is being human, and so I have come to realize that living fully means accepting all of who I am, including the parts of me that scare or embarrass me. That doesn't mean I accept everything about myself -- it just means that I accept my own humanity. And so I have come to love every part of myself with the same intensity that I might love a child -- even the parts that are difficult.\n",
      "I believe in loving life, and loving myself. This is how I know to be true to who I am. And it's one of the best ways I know to feel alive!\n",
      "What do you believe? Share your thoughts on the Meaning of Life or click here to write a Reader Review of this article.\n",
      "To learn more about my book, Click Here.\n",
      "Inspired by a true story, \"Awake in the Dark\" is a novel about how we can transform our lives through love and acceptance -- even if it means learning to live with loss, grief, illness or death.\n",
      "Click here for more information about \"Awake in the Dark.\" Click here for more information about my book, \"The Soul of Uncertainty: Choosing Your Own Path in Fear's Kingdom .\"\n",
      "For a complete list of all articles on this website, click here.\n",
      "Return from What Do You Believe to Awakening Self-Awareness Home Page.\n",
      "Return from What Do You Believe? to Personal Development for Business Succe$ss Home Page.\n",
      "Copyright © 1997 - 2018 by Dianne Marie. All rights reserved. If you would like to use any portion of this article, please contact me. I'd love to hear from you! In addition, if you find an error on this page, please let me know so that I can correct it immediately.\n",
      "Click here for more information about my book, \"Awake\n",
      "llama_print_timings:        load time =   982.19 ms\n",
      "llama_print_timings:      sample time =   317.25 ms /   512 runs   (    0.62 ms per token,  1613.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5936.68 ms /   265 tokens (   22.40 ms per token,    44.64 tokens per second)\n",
      "llama_print_timings:        eval time = 10403.44 ms /   510 runs   (   20.40 ms per token,    49.02 tokens per second)\n",
      "llama_print_timings:       total time = 16697.91 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078565\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.00 MB\n",
      "llama_model_load_internal: mem required  = 7390.00 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104f2d090\n",
      "ggml_metal_init: loaded kernel_mul                            0x104f2e580\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104f2fad0\n",
      "ggml_metal_init: loaded kernel_scale                          0x104f2e7e0\n",
      "ggml_metal_init: loaded kernel_silu                           0x104f30540\n",
      "ggml_metal_init: loaded kernel_relu                           0x104f30d10\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104f31600\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104f31e70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104f331a0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104f33400\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104f326b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104f33df0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x104f35150\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x104f34600\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104f359f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x104f36330\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104f36c70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104f37d50\n",
      "ggml_metal_init: loaded kernel_norm                           0x104f386d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104f39360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104f39d60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104f3a780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104f3b150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104f3bb50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104f3c630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104f3cfa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x104f3d940\n",
      "ggml_metal_init: loaded kernel_rope                           0x104f3e150\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104f3f130\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104f3fcb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104f40850\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x104f413d0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB, ( 7024.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 7036.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.45 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make a difference, and in that process you will find happiness.\n",
      "— WILLIAM G. BROWN, _Leaders Are Made, Not Born_\n",
      "**A** t the ripe old age of eighty-seven, Bill Brown, the former CEO of EDS (Electronic Data Systems), could rest on his laurels. But instead he was doing something that no one else in the world has ever done: giving away 99 percent of his fortune—and spending it all before he died.\n",
      "Brown's story is a great lesson for us all, because while we may not have $10 billion to give away like Bill Brown did, if we were to live by this philosophy, we could make an impact on the world that would be unparalleled. We can do it with our time and energy, as well as a little money here and there, but all of us together can truly make a difference in the lives of others—and ourselves.\n",
      "We are at an incredible point in history where we have access to information, technology, opportunity, and each other like never before. It is up to you to do something with it! The world needs what you have to offer: your ideas, your expertise, your time, energy, and money—and the people who need those things most are often right next door to where you live or work.\n",
      "But for some reason, many people just don't think about giving in terms of their own lives. They see it as something that has to be done by other people, while they go on with life as usual. It is not surprising then that the number one reason why most people do not give more of themselves or their money is that they feel they are too busy (or have no time) and that they don't have enough money. But even if you are broke, there are ways to make a difference with your life, as we will see in this chapter.\n",
      "There is also the assumption that you can only give when you have lots of money—but that is not true either! In fact, when you think about it, some of the most powerful gifts come from people who don't have much to give. Just ask a child what he has and would like to give away, or an elderly person with little else in life, and you will be astounded at how much they have to offer.\n",
      "The simple truth is that we all\n",
      "llama_print_timings:        load time =   985.98 ms\n",
      "llama_print_timings:      sample time =   321.67 ms /   512 runs   (    0.63 ms per token,  1591.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5903.94 ms /   265 tokens (   22.28 ms per token,    44.89 tokens per second)\n",
      "llama_print_timings:        eval time = 10430.40 ms /   510 runs   (   20.45 ms per token,    48.90 tokens per second)\n",
      "llama_print_timings:       total time = 16696.87 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078583\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1551a0a10\n",
      "ggml_metal_init: loaded kernel_mul                            0x1551a1f00\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1551a3450\n",
      "ggml_metal_init: loaded kernel_scale                          0x1551a2160\n",
      "ggml_metal_init: loaded kernel_silu                           0x1551a3ec0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1551a4690\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1551a4f80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1551a57f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1551a6b20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1551a6d80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1551a6030\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1551a7770\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1551a8ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1551a7f80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1551a9370\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1551a9cb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1551aa5f0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1551ab6d0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1551ac050\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1551acce0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1551ad6e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1551ae0d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1551aeaa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1551af480\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1551affa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1551b08c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1551b1290\n",
      "ggml_metal_init: loaded kernel_rope                           0x1551b1aa0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1551b2a20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1551b35c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1551b4150\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1551b4cb0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.69 MB, (24827.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, (24839.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.14 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living out your purpose. To find your calling and follow it with all your heart, no matter where that leads you.\n",
      "I’ve always been a fan of finding your passion and doing what you love for a career.\n",
      "But I also know that when you pursue something you are passionate about, it doesn’t always end up feeling like the perfect fit.\n",
      "We can grow to not enjoy our chosen path anymore or find we have changed in some way but haven’t taken the necessary steps to make an adjustment.\n",
      "Finding your purpose is a lifelong journey of discovery and re-evaluation. This can be both exciting and scary but if you go into it with an open mind, you will eventually end up where you were always meant to be.\n",
      "I’ve been thinking about this recently because I think we often get caught up in the day to day stuff that we lose sight of what is really important – our purpose!\n",
      "It’s easy to fall into that trap and start living your life for everyone else but it can be so fulfilling when you follow your own dreams. If you haven’t found yours yet I thought I would share some ways that have helped me discover mine.\n",
      "The first thing I had to do was to really ask myself what is important to me in my life?\n",
      "I like to write and I enjoy helping people so those were the two things that came up straight away.\n",
      "When you’re doing something you love it doesn’t feel much like ‘work’, but if you’ve been in a job for a long time or have never really loved what you do then maybe you need to start again from scratch?\n",
      "What would your perfect day look like if money was no object and you had total freedom? What do you want more of in your life? More time with friends, family, pets or something else entirely?\n",
      "I think most people have a good idea what their ideal day looks like already but they just need to listen to their heart. If you’re anything like me, that can be hard because there’s always been an expectation on me from my parents and other people in my life to do certain things. I had to learn to let go of those expectations so that I could make my own decisions about what was important to me.\n",
      "I am a self-confessed daydreamer and if I don’t have something to focus on then my mind will wander\n",
      "llama_print_timings:        load time =  5878.94 ms\n",
      "llama_print_timings:      sample time =   331.02 ms /   512 runs   (    0.65 ms per token,  1546.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6281.15 ms /   265 tokens (   23.70 ms per token,    42.19 tokens per second)\n",
      "llama_print_timings:        eval time = 35176.40 ms /   510 runs   (   68.97 ms per token,    14.50 tokens per second)\n",
      "llama_print_timings:       total time = 41831.56 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078631\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1004273a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x100428890\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100429de0\n",
      "ggml_metal_init: loaded kernel_scale                          0x100428af0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10042a850\n",
      "ggml_metal_init: loaded kernel_relu                           0x10042b030\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10042b920\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10042c1b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10042d4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10042d740\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10042c9f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10042f2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10042e7d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10042ea30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10042fd40\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100430680\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100430fc0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100432070\n",
      "ggml_metal_init: loaded kernel_norm                           0x1004329f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100433670\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100434080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100434ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1004354e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100435e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1004369d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100437480\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100437e60\n",
      "ggml_metal_init: loaded kernel_rope                           0x100438700\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100439680\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10043a8c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10043b410\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10043bfb0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.69 MB, (24827.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, (24839.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.14 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find yourself. But, I don't think you ever do it once and for all.\n",
      "I was an only child until I was 12 years old, which was very unusual in those days. Being alone so much gave me an inner confidence that really helped me as a writer.\n",
      "In the end, after all my years of searching, I don't want to talk about life anymore. All I want to do is live it. I have had enough of theory and abstraction: let me get on with living.\n",
      "The difference between fiction and reality? Fiction has to make sense.\n",
      "We are all so desperately hungry for approval, aren't we - even those who don't want us to know how much they care. I think it comes from our childhoods, when the only approval we could get was for being better than someone else.\n",
      "I believe that everything happens for a reason. People change so you can learn to let go, things go wrong so you appreciate them when they're right, you believe lies so you eventually learn to trust no one but yourself, and sometimes good things fall apart so better things could fall together.\n",
      "Life isn't fair. It never has been, it is not now, and it never will be. Do not fall into the great trap of expecting people or life to treat you fairly, or well, or even politely. They won't. People are interested in their own needs and desires, and they won't be bothered with yours.\n",
      "Women like me because I look as if I understand what they're going through. A woman wrote a book once about why women don't have affairs or fall for married men: she said we are looking for our fathers in the men we love - and so we unconsciously seek out the qualities that would have delighted our childhood self, but which no grown man could ever possess.\n",
      "I know people will say I cannot be a Christian and believe in reincarnation, but I think that is nonsense.\n",
      "There are many ways to write about the same thing... If I wanted to write just one book, it would never be published - there's no point doing that. You have to keep reinventing yourself if you want a long career.\n",
      "I am not ashamed or embarrassed at my failures, because I learned from them. I'm glad I made mistakes. They\n",
      "llama_print_timings:        load time =  3806.76 ms\n",
      "llama_print_timings:      sample time =   334.54 ms /   512 runs   (    0.65 ms per token,  1530.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6298.11 ms /   265 tokens (   23.77 ms per token,    42.08 tokens per second)\n",
      "llama_print_timings:        eval time = 35220.79 ms /   510 runs   (   69.06 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time = 41897.51 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078677\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x129733b00\n",
      "ggml_metal_init: loaded kernel_mul                            0x129734ff0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x129736540\n",
      "ggml_metal_init: loaded kernel_scale                          0x129735250\n",
      "ggml_metal_init: loaded kernel_silu                           0x129736fb0\n",
      "ggml_metal_init: loaded kernel_relu                           0x129737780\n",
      "ggml_metal_init: loaded kernel_gelu                           0x129738070\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1297388e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x129739c10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x129739e70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x129739120\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12973a860\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12973bbc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12973b070\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12973c460\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12973cda0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12973d6e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12973e7c0\n",
      "ggml_metal_init: loaded kernel_norm                           0x12973f140\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12973fdd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1297407d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1297411c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x129741b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x129742570\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x129743090\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1297439b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x129744380\n",
      "ggml_metal_init: loaded kernel_rope                           0x129744b90\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x129745b10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1297466b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x129747240\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x129747da0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.69 MB, (24827.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, (24839.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.14 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.14 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m simple: We are put on this earth to do good. It doesn't mean we all have to be Mother Teresa, but it does mean we try as best we can to love and serve others.\n",
      "Amen! The rest will take care of itself in time. :) Thanks for sharing.\n",
      "What a lovely post! I agree with you 100%.\n",
      "This is a wonderful post. You are such an inspiration to me. Thank you so much!\n",
      "I love this post. This is all that matters to me too.\n",
      "Happy Friday, and Happy Valentine's Day to you, too!\n",
      "What a lovely post... and I agree with everything you said. It is easy to get caught up in the hustle & bustle of life, but when we slow down, take time for others and focus on what matters most....that's when real joy comes from within. :) Thank you for sharing! Happy Friday!\n",
      "Amen!! So true - it was a beautiful post.\n",
      "What a lovely reminder to take the time out each day to do something kind or helpful for someone else, even if it is simply a smile. I believe that is what life is all about and we should strive to make our lives and others around us better everyday. Thanks for sharing! Have an awesome weekend!\n",
      "I can't think of a more important meaning in life than this one!\n",
      "That is really great advice!\n",
      "So true! And so simple, but too often overlooked or forgotten. It's easy to get caught up in the hustle and bustle and end up forgetting about the bigger picture. I think it's a good idea to sit down every now and then and take stock of your life as well as your actions. Great post!\n",
      "I love this post so much, and you have inspired me to do something kind for someone today. Thank you! Happy weekend :)!\n",
      "Love the simplicity of this post. It is true - we are here to help others in some way. That will be my goal today as well!\n",
      "Awesome post..so simple but soo powerful, and so true!!\n",
      "I love your blog! I have been reading it for a while now and just wanted to let you know that I really enjoy it :) Keep up the good work!\n",
      "So true - our lives are not about us, they're about everyone around us. We can\n",
      "llama_print_timings:        load time =  3811.54 ms\n",
      "llama_print_timings:      sample time =   467.19 ms /   512 runs   (    0.91 ms per token,  1095.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6406.05 ms /   265 tokens (   24.17 ms per token,    41.37 tokens per second)\n",
      "llama_print_timings:        eval time = 35541.06 ms /   510 runs   (   69.69 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time = 42466.65 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6537f",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16c67651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078723\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.04 MB\n",
      "llama_model_load_internal: mem required  = 17993.04 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1004117d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x100412cc0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100414210\n",
      "ggml_metal_init: loaded kernel_scale                          0x100412f20\n",
      "ggml_metal_init: loaded kernel_silu                           0x100414c80\n",
      "ggml_metal_init: loaded kernel_relu                           0x100415460\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100415d50\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1004165e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100417910\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100417b70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100416e20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100419700\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100418c00\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100418e60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10041a170\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10041aab0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10041b3f0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10041c4a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x10041ce20\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10041daa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10041e4b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10041ef10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10041f910\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1004202b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100420e00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1004218b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100422290\n",
      "ggml_metal_init: loaded kernel_rope                           0x100422b30\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100423ab0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100424cf0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100425840\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1004263e0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.05 MB, (17505.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (17521.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18303.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18519.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18775.50 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your gift is the meaning of life.\n",
      "When you give a gift, you’re not doing it out of obligation; you’re doing it out of love. You’re doing it because you see something in that person and you want to acknowledge it. When they open that present from you, you’re going to feel good, but more importantly, they’re going to feel amazing. That’s the feeling I get when I play music live.\n",
      "What’s my gift? It’s a love for people and life itself; that’s where my songs come from. When I’m on stage, it feels like I’m in church, because I’m with these amazing people who are all there to worship life through music. That connection—that energy exchange—is the whole reason we do this.\n",
      "I didn’t always feel like that, though. In fact, at one point during my college years, I didn’t even want to play music any more.\n",
      "I was 20 or 21 when I got to Berklee College of Music in Boston. At first, I wasn’t sure if it was the right place for me. It seemed like I was surrounded by brilliant musicians who had spent their entire lives studying and practicing. Meanwhile, I was just a kid from Detroit with no formal training.\n",
      "But then I found out that there were other kids like me in school—guys from the ‘hood who loved to play and could really do it. After a while, we bonded, and our little group became known as “The Ghetto Boys.” That was us: some of the baddest musicians at Berklee, but also some of the poorest. There was no money in our families for music lessons, so instead we’d been learning all along from records. We were great at playing what we loved—funk and soul and jazz and blues—but not so good at the things we didn’t love. We knew how to play, but we didn’t know the theory behind it.\n",
      "For me, that was a huge problem. I was already feeling like I wasn’t as good a musician as all of these other kids, and when I found out that they could also read music and I couldn’t? That did some serious damage to my self-esteem. It made me want to quit school\n",
      "llama_print_timings:        load time =  4153.14 ms\n",
      "llama_print_timings:      sample time =   333.53 ms /   512 runs   (    0.65 ms per token,  1535.10 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13153.89 ms /   265 tokens (   49.64 ms per token,    20.15 tokens per second)\n",
      "llama_print_timings:        eval time = 22836.87 ms /   510 runs   (   44.78 ms per token,    22.33 tokens per second)\n",
      "llama_print_timings:       total time = 36365.37 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "148d30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078764\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.04 MB\n",
      "llama_model_load_internal: mem required  = 17993.04 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x105e296c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x105e2abb0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x105e2c100\n",
      "ggml_metal_init: loaded kernel_scale                          0x105e2ae10\n",
      "ggml_metal_init: loaded kernel_silu                           0x105e2cb70\n",
      "ggml_metal_init: loaded kernel_relu                           0x105e2d340\n",
      "ggml_metal_init: loaded kernel_gelu                           0x105e2dc30\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x105e2e4a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x105e2f7d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x105e2fa30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x105e2ece0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x105e30420\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x105e31780\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x105e30c30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x105e32020\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x105e32960\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x105e332a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x105e34380\n",
      "ggml_metal_init: loaded kernel_norm                           0x105e34d00\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x105e35990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x105e36390\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x105e36dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x105e37790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x105e38180\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x105e38ce0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x105e397c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x105e3a190\n",
      "ggml_metal_init: loaded kernel_rope                           0x105e3b420\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x105e3bfe0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x105e3cb70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x105e3d6c0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x105e3e220\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.05 MB, (17505.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (17521.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18303.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18519.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18775.50 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to follow your dreams and be happy.\n",
      "If you are following your dreams, then you will be happy.\n",
      "You can see that happiness is a choice. It's not something that happens to you like winning the lottery or finding a pot of gold at the end of a rainbow. You have to choose to be happy.\n",
      "When we go through life, if all we do is complain and focus on what we don’t have, we will never achieve true happiness.\n",
      "I've found that when I put my attention on being grateful for what I already have, it opens the door for more happiness to come into my life.\n",
      "Follow your dreams and be happy!\n",
      "PS. If you need some help with learning how to follow your dreams and how to be happy, then check out this video below where I’m interviewed by Marketing Your Way To Millions. In this video I talk about following your dreams and being happy. I hope it helps and inspires you to go after your dreams!\n",
      "I have been very fortunate in my life to get a great education from the University of Florida as well as earning two master degrees one in Business Administration (MBA) and one in Real Estate (MSRE). In addition, I am also an MCC with Master Coach Certification.\n",
      "There are many different types of coaches but my focus is on life coaching which can be done over the phone or in person. My clients have been from all walks of life and they range from CEO’s to housewives.\n",
      "I am very proud to say that I have helped hundreds of people change their lives, so if you are ready for a change in your life, then feel free to contact me at 561-302-3890 or email me at john@johnmulryan.com and we can set up a complimentary consultation.\n",
      "Follow Your Dreams and Be Happy! It's Your Choice!\n",
      "I believe the meaning of life is to follow your dreams and be happy. If you are following your dreams, then you will be happy. You can see that happiness is a choice. It’s not something that happens to you like winning the lottery or finding a pot of gold at the end of a rainbow.\n",
      "I've found that when I put my attention on being grateful for what I have in my life\n",
      "llama_print_timings:        load time =  2801.58 ms\n",
      "llama_print_timings:      sample time =   365.01 ms /   512 runs   (    0.71 ms per token,  1402.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13278.20 ms /   265 tokens (   50.11 ms per token,    19.96 tokens per second)\n",
      "llama_print_timings:        eval time = 23078.81 ms /   510 runs   (   45.25 ms per token,    22.10 tokens per second)\n",
      "llama_print_timings:       total time = 36768.37 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078804\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.04 MB\n",
      "llama_model_load_internal: mem required  = 17993.04 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100f27340\n",
      "ggml_metal_init: loaded kernel_mul                            0x100f28830\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100f29d80\n",
      "ggml_metal_init: loaded kernel_scale                          0x100f2a060\n",
      "ggml_metal_init: loaded kernel_silu                           0x100f2a880\n",
      "ggml_metal_init: loaded kernel_relu                           0x100f28dd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100f2b1e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100f2c150\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100f2d490\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100f2c800\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100f2dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100f2f270\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100f2e750\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100f2fb50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100f2fdb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100f30630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100f30f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100f32050\n",
      "ggml_metal_init: loaded kernel_norm                           0x100f329b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100f33680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100f34050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100f34a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100f35450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100f35e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100f36960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100f37270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100f37c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x100f38490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100f39410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100f39fb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100f3ab30\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100f3b690\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.05 MB, (17505.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (17521.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18303.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18519.50 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18775.50 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your gift is the meaning of life.\n",
      "The most powerful tool you have in this lifetime is your mind, and when you learn how to use it, you can do anything.\n",
      "I believe everything happens for a reason. People change so that you can learn to let go. Things go wrong so that you appreciate them when they're right. You believe lies so you eventually learn to trust no one but yourself. And sometimes good things fall apart so better things can fall together.\n",
      "If you don't think your life is worth more than a six-pack and another night of partying, then it won't be. Don't let others bring you down to their level. Stand strong, stand tall. You are beautiful. Never doubt yourself.\n",
      "I believe that we all have the power within ourselves to make a difference in this world. It doesn't matter how old you are. I started when I was 12 years old and I got into it because I loved being outside with animals. And then I realized that there were actually things wrong with wildlife, like they were losing their habitats and getting poached. So from an early age I just wanted to make a difference.\n",
      "I want to be so much more than friends. But I'm afraid of being hurt.\n",
      "— Nina Dobrev\n",
      "We have the right as individuals to give away as much of our own money as we please in charity; but as members of Congress we have no right to appropriate a dollar of the public money.\n",
      "I've been to war, and I've seen firsthand the horrific capacity for humans to make themselves miserable. And my conclusion is that we don't need any more help in that regard. What we need is people who are truly humane and spiritual to rise above the din of this age of terrible conflict and violence.\n",
      "If you love something, you have to be willing to set it free. If it comes back to you, then it's yours forever. But if it doesn't, then it was never meant to be and you have to let it go.\n",
      "Never say anything about somebody that you wouldn't say to them.\n",
      "— George H. W. Bush\n",
      "Every man is a damn fool for at least five minutes every day; wisdom consists of not exceeding the limit.\n",
      "The trouble with being poor is that it takes up all your time.\n",
      "— Willem\n",
      "llama_print_timings:        load time =  2800.90 ms\n",
      "llama_print_timings:      sample time =   403.50 ms /   512 runs   (    0.79 ms per token,  1268.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13236.62 ms /   265 tokens (   49.95 ms per token,    20.02 tokens per second)\n",
      "llama_print_timings:        eval time = 23140.98 ms /   510 runs   (   45.37 ms per token,    22.04 tokens per second)\n",
      "llama_print_timings:       total time = 36830.62 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb0e7",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078843\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.72 MB\n",
      "llama_model_load_internal: mem required  = 62533.72 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x105527320\n",
      "ggml_metal_init: loaded kernel_mul                            0x105528810\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x105529d60\n",
      "ggml_metal_init: loaded kernel_scale                          0x10552a040\n",
      "ggml_metal_init: loaded kernel_silu                           0x10552a860\n",
      "ggml_metal_init: loaded kernel_relu                           0x105528db0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10552b1c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10552c130\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10552d470\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10552c7e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10552dd50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10552f250\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10552e730\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10552fb30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10552fd90\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x105530610\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x105530f50\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x105532030\n",
      "ggml_metal_init: loaded kernel_norm                           0x105532990\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x105533660\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x105534030\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x105534a40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x105535430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x105535e20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x105536940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x105537250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x105537c00\n",
      "ggml_metal_init: loaded kernel_rope                           0x105538470\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1055393f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x105539f90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10553ab10\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10553b670\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   406.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.73 MB, (62046.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (62062.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (62844.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (63060.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (63316.19 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live in harmony with each other and our environment.\n",
      "I believe in sharing my knowledge, experience, compassion and energy to help others achieve their goals and objectives.\n",
      "I believe that it is important to find your purpose and pursue it until you reach your full potential.\n",
      "I believe that it is possible for us all to live happy, healthy lives.\n",
      "I believe that together we can make a difference in the world by creating positive change.\n",
      "I believe in being honest with myself and others.\n",
      "I believe every day is an opportunity to learn something new.\n",
      "I believe that through self-reflection you can find your path and overcome challenges.\n",
      "I believe in following my dreams with faith and courage.\n",
      "I believe that we are all connected, and that each of us has the power to impact others lives in positive way.\n",
      "As a child I was very active. When I was 12 years old I participated in my first 5 km run. I loved the feeling of being fit, strong and healthy. Soon after I began weight training with my dad at home. I then joined my brother in playing baseball. We played for many years together and I also took on coaching young kids during summer breaks.\n",
      "In high school I was the only girl to play on a boys hockey team for two seasons. That experience taught me how to be tough, confident, strong and determined. When faced with adversity I learned that if you have faith in yourself anything is possible.\n",
      "When I was 19 years old I decided it was time to try something new so I joined the gym at our local community centre. I had a lot of fun learning how to use all the equipment and I really enjoyed working out with my friends. After a while I noticed a few trainers there and thought, “Wow that would be an amazing job!” Soon after I became certified as a personal trainer through Athletics Canada and began training clients at the YMCA in Toronto.\n",
      "Six years later I was invited to work with the Fitness Kickboxing Association of Canada (FKA). This organization is dedicated to helping fitness professionals create safe, effective, high energy, fun fitness kickboxing classes for people who want an alternative to traditional group exercise. In 2014 after many years of hard work and dedication I became the Canadian Master Trainer for the FKA.\n",
      "My passion is teaching\n",
      "llama_print_timings:        load time = 14989.84 ms\n",
      "llama_print_timings:      sample time =   573.95 ms /   512 runs   (    1.12 ms per token,   892.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14434.81 ms /   265 tokens (   54.47 ms per token,    18.36 tokens per second)\n",
      "llama_print_timings:        eval time = 77860.00 ms /   510 runs   (  152.67 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:       total time = 92928.18 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690078952\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.72 MB\n",
      "llama_model_load_internal: mem required  = 62533.72 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13d94a130\n",
      "ggml_metal_init: loaded kernel_mul                            0x13d94b620\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13d94cb70\n",
      "ggml_metal_init: loaded kernel_scale                          0x13d94ce50\n",
      "ggml_metal_init: loaded kernel_silu                           0x13d94d670\n",
      "ggml_metal_init: loaded kernel_relu                           0x13d94bbc0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13d94dfd0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13d94ef40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13d950280\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13d94f5f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13d950b60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13d952060\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13d951540\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13d952940\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13d952ba0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13d953420\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13d953d60\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13d954e40\n",
      "ggml_metal_init: loaded kernel_norm                           0x13d9557a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13d956470\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13d956e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13d957850\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13d958240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13d958c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13d959750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13d95a060\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13d95aa10\n",
      "ggml_metal_init: loaded kernel_rope                           0x13d95b280\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13d95c200\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13d95cda0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13d95d920\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13d95e480\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   406.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.73 MB, (62046.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (62062.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (62844.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (63060.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (63316.19 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and healthy. The way you live your life will determine how long you will live your life.\n",
      "What does it mean to be a good person? How do we know if someone is a good person or not? What qualities make up a good person?\n",
      "I agree with this 100%. I am not saying this just because I’m his wife, but my husband is a very good person. He has the heart of gold and would give you the shirt off his back if he thought it could help you out. Now don’t get me wrong, he does have his moments when I think he’s being an idiot or too trusting of people, but that’s just a small part of who he is.\n",
      "This post is really about looking at what makes up a good person and how they behave in different situations. Whenever I see someone helping another person out it always puts me in a better mood for the rest of the day. It doesn’t matter if they are holding the door open for you, giving you their seat on the bus or train, or offering to help you carry your groceries into your home, people doing good deeds makes me happy.\n",
      "I have been trying really hard lately to be a good person. I haven’t always been good at it, but I am definitely trying to improve my behavior and actions around others. If we all try to be just a little bit better each day, the world would be a much nicer place than it is now.\n",
      "So when you see someone doing something kind for another person, stop them and tell them how wonderful they are being. I am sure it will make their day as well.\n",
      "What makes up a good person in your opinion? Do you know anyone who you think is a good person? Share with us in the comments below!\n",
      "This post was inspired by The Daily Post. To see what other bloggers wrote about this prompt, click here.\n",
      "I believe you are a good person. I love how you care for others and that you have so much empathy for them.\n",
      "Being a good person isn’t all that hard if we just make the effort to do it.\n",
      "You are such an amazingly kind person. It always seems like you’re there when people need help. You’ve helped me out more than once and I know I can always count on you!\n",
      "This is what makes YOU a good person. \n",
      "llama_print_timings:        load time = 10669.34 ms\n",
      "llama_print_timings:      sample time =   323.97 ms /   512 runs   (    0.63 ms per token,  1580.40 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14371.79 ms /   265 tokens (   54.23 ms per token,    18.44 tokens per second)\n",
      "llama_print_timings:        eval time = 77399.75 ms /   510 runs   (  151.76 ms per token,     6.59 tokens per second)\n",
      "llama_print_timings:       total time = 92138.94 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71c4a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079055\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.72 MB\n",
      "llama_model_load_internal: mem required  = 62533.72 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x103f27340\n",
      "ggml_metal_init: loaded kernel_mul                            0x103f28830\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x103f29d80\n",
      "ggml_metal_init: loaded kernel_scale                          0x103f2a060\n",
      "ggml_metal_init: loaded kernel_silu                           0x103f2a880\n",
      "ggml_metal_init: loaded kernel_relu                           0x103f28dd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x103f2b1e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x103f2c150\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x103f2d490\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x103f2c800\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x103f2dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x103f2f270\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x103f2e750\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x103f2fb50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x103f2fdb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x103f30630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x103f30f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x103f32050\n",
      "ggml_metal_init: loaded kernel_norm                           0x103f329b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x103f33680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x103f34050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x103f34a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x103f35450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x103f35e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x103f36960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x103f37270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x103f37c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x103f38490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x103f39410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x103f39fb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x103f3ab30\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x103f3b690\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   406.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.73 MB, (62046.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (62062.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (62844.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (63060.19 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (63316.19 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be fulfilled by helping others.\n",
      "I am a retired teacher who loves my pets and family very much.\n",
      "I love meeting new people, learning about different cultures and traveling.\n",
      "I enjoy music and nature.\n",
      "Having lost my daughter in 2005 I decided that I wanted to live life to the fullest.\n",
      "I love teaching children and being around young people. They bring a smile to my face every day.\n",
      "I am very friendly and outgoing. I like meeting new people and learning about different cultures, especially if they are kind, funny and have interesting stories to tell.\n",
      "My favorite hobbies include music, reading and traveling.\n",
      "Some of the places I've traveled to are England, Austria, Canada, Mexico, Puerto Rico and Japan.\n",
      "I also enjoy sports. I love going on adventures with my family members.\n",
      "Music: Country, Rock, 80s pop/rock, Blues, Jazz.\n",
      "Literature: Favorite authors include Toni Morrison, Maya Angelou, Alice Walker, Isabel Allende, Barbara Kingsolver and William Shakespeare.\n",
      "Art/Film: I love movies that have happy endings. I also enjoy going to the museums with my family.\n",
      "I am interested in learning about other people's cultures and traditions.\n",
      "I can teach English as a second language. I am fluent in both Spanish and French.\n",
      "I can help you learn American customs, dances, songs and food.\n",
      "I love to cook! My specialties are baked goods (cookies, cakes, etc) and Mexican food.\n",
      "I would like to meet people who are kind, funny and interested in learning about other cultures.\n",
      "Austria, Canada, China, England, France, Germany, Greece, Japan, Mexico, Puerto Rico, Spain, United States\n",
      "Germany, Japan, United States\n",
      "English as a Second Language (ES... Moderator\n",
      "Teacher/Language Exchange Moderator\n",
      "TEFL TEACHERS AND STUDENTS... Moderator\n",
      "Vegetarians / Vegans in SAN DIEGO County Moderator\n",
      "Japanese People and Culture Moderator\n",
      "Coffee lovers in San Diego, CA Moderator\n",
      "Church of Jesus Christ of Latte... Moderator\n",
      "Teachers/Students of English\n",
      "llama_print_timings:        load time = 10952.90 ms\n",
      "llama_print_timings:      sample time =   331.56 ms /   512 runs   (    0.65 ms per token,  1544.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14418.54 ms /   265 tokens (   54.41 ms per token,    18.38 tokens per second)\n",
      "llama_print_timings:        eval time = 77379.18 ms /   510 runs   (  151.72 ms per token,     6.59 tokens per second)\n",
      "llama_print_timings:       total time = 92174.39 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0bef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079158\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.93 MB\n",
      "llama_model_load_internal: mem required  = 35839.93 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13b127350\n",
      "ggml_metal_init: loaded kernel_mul                            0x13b128840\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13b129d90\n",
      "ggml_metal_init: loaded kernel_scale                          0x13b128aa0\n",
      "ggml_metal_init: loaded kernel_silu                           0x13b12a800\n",
      "ggml_metal_init: loaded kernel_relu                           0x13b12afe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13b12b8d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13b12c160\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13b12d490\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13b12d6f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13b12c9a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13b12f280\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13b12e780\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13b12e9e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13b12fcf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13b130630\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13b130f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13b132020\n",
      "ggml_metal_init: loaded kernel_norm                           0x13b1329a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13b133620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13b134030\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13b134a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13b135490\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13b135e30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13b136980\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13b137430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13b137e10\n",
      "ggml_metal_init: loaded kernel_rope                           0x13b1386b0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13b139630\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13b13a870\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13b13b3c0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13b13bf60\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.94 MB, (35091.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.00 MB, (35115.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (36397.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (36738.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (37122.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love and be loved. We are given this opportunity to live, grow and experience all the ups and downs of life in order to learn how to love more fully and deeply. When we love our self we can find peace within our own being. Love of self brings us closer to a pure state of being that is unconditional love for all living beings on this planet and beyond. It is an honor, blessing and gift for each one of us to be alive in the physical body at this time of awakening consciousness for humanity.\n",
      "As you read this book you will discover a way to expand your viewpoint of life and how we are all connected through our hearts and souls. You will learn ways to help yourself heal from old wounds that keep you stuck in painful emotional patterns. These patterns continue to repeat themselves over and over again until they are acknowledged, accepted, and released with love.\n",
      "Everyone has their own story of what happened in life and how it affected them. We have all been through different experiences. Some people experience great trauma from childhood or adulthood abuse by a parent, sibling or lover. Others may be involved in war and come back with post-traumatic stress disorder (PTSD). Many of us go through the loss of someone we love deeply. Death is very difficult for most people to accept as it feels like your heart has been ripped out.\n",
      "When something happens that causes pain, you feel a sense of being wounded. The wound may be deep or light depending on what happened and how you perceived it in your mind. When you have a physical wound from an accident, cut or surgery the body begins to heal itself immediately. It stops the blood flowing out, forms a scab over the area that was wounded so new skin can grow and cover up the old scarred area.\n",
      "Most people are not aware of this truth because they have been taught to believe the body is separate from their mind and emotions. In reality, our entire being is connected through every cell in the physical body. The body feels the same sensations as the heart and soul do when something happens. When you feel a stabbing pain in your heart it is also felt by the cells of your body. It is like the cells are saying “Ouch” just as the emotional self would say that.\n",
      "I am a firm believer in allowing myself to gr\n",
      "llama_print_timings:        load time =  8523.05 ms\n",
      "llama_print_timings:      sample time =   341.89 ms /   512 runs   (    0.67 ms per token,  1497.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26691.36 ms /   265 tokens (  100.72 ms per token,     9.93 tokens per second)\n",
      "llama_print_timings:        eval time = 40171.65 ms /   510 runs   (   78.77 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:       total time = 67246.23 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079234\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.93 MB\n",
      "llama_model_load_internal: mem required  = 35839.93 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13d831130\n",
      "ggml_metal_init: loaded kernel_mul                            0x13d832620\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13d833b70\n",
      "ggml_metal_init: loaded kernel_scale                          0x13d832880\n",
      "ggml_metal_init: loaded kernel_silu                           0x13d8345e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13d834dc0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13d8356b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13d835f40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13d837270\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13d8374d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13d836780\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13d839060\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13d838560\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13d8387c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13d839ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13d83a410\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13d83ad50\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13d83be00\n",
      "ggml_metal_init: loaded kernel_norm                           0x13d83c780\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13d83d400\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13d83de10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13d83e870\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13d83f270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13d83fc10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13d840760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13d841210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13d841bf0\n",
      "ggml_metal_init: loaded kernel_rope                           0x13d842490\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13d843410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13d844650\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13d8451a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13d845d40\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.94 MB, (35091.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.00 MB, (35115.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (36397.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (36738.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (37122.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love God and our neighbor. To do this we need to have a relationship with Jesus Christ, which happens when we trust in Him for the forgiveness of sins by His death on the cross and resurrection from the dead. As we grow in our relationships with Jesus, we learn more about how to love God by obeying Him and loving others by serving them and putting their interests above our own.\n",
      "I am a disciple of Jesus Christ and a pastor at Harvest Bible Chapel in Omaha, Nebraska. I have been married to my wife since 2014, and we are blessed with two beautiful children. My passion is to see people know God’s Word for themselves. I believe the most important thing for Christians to do is to read their Bibles regularly and pray every day.\n",
      "I was born into a Christian home in Omaha, Nebraska, and attended church with my family all throughout my childhood. When I was 12 years old, I realized that my life did not reflect the things I said I believed about God. At that time, I began to ask myself if this thing called Christianity was really true or not.\n",
      "That same summer, my parents decided to send me to a church camp in Colorado where I heard the gospel preached several times over the course of one week. It was at that camp when I realized I was destined for hell unless something changed. That’s when God began to show me that Jesus Christ died on the cross in my place, taking all my sins upon Himself so that I would not have to suffer eternally in hell.\n",
      "I also learned that day that if I would simply trust in this Jesus and what He has done for me, I could be forgiven of all my sin and given new life through the power of God’s Holy Spirit. After hearing these things, I realized it was time to put my trust in Christ and begin following Him rather than myself.\n",
      "After that experience, I began to follow Jesus with all my heart, soul, mind, and strength. I continued attending a Bible-believing church where I learned more about God’s Word over the course of my teenage years.\n",
      "In 2013, I graduated from Moody Bible Institute in Chicago with a degree in Biblical Studies, and in 2016 I received my Masters of Divinity from The Master’s Seminary in\n",
      "llama_print_timings:        load time =  5981.98 ms\n",
      "llama_print_timings:      sample time =   359.93 ms /   512 runs   (    0.70 ms per token,  1422.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26869.60 ms /   265 tokens (  101.39 ms per token,     9.86 tokens per second)\n",
      "llama_print_timings:        eval time = 40405.01 ms /   510 runs   (   79.23 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time = 67678.42 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079308\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.93 MB\n",
      "llama_model_load_internal: mem required  = 35839.93 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x132711ad0\n",
      "ggml_metal_init: loaded kernel_mul                            0x132712fc0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x132714510\n",
      "ggml_metal_init: loaded kernel_scale                          0x132713220\n",
      "ggml_metal_init: loaded kernel_silu                           0x132714f80\n",
      "ggml_metal_init: loaded kernel_relu                           0x132715760\n",
      "ggml_metal_init: loaded kernel_gelu                           0x132716050\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1327168e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x132717c10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x132717e70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x132717120\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x132719a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x132718f00\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x132719160\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13271a470\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13271adb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13271b6f0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13271c7a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x13271d120\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13271dda0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13271e7b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13271f210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13271fc10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1327205b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x132721100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x132721bb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x132722590\n",
      "ggml_metal_init: loaded kernel_rope                           0x132722e30\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x132723db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x132724ff0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x132725b40\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1327266e0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.94 MB, (35091.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.00 MB, (35115.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (36397.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (36738.39 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (37122.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.” Pablo Picasso\n",
      "This site contains a little bit about me, and lots of things that have inspired me or captured my interest over time – I hope you enjoy reading it as much as I’ve enjoyed writing it!\n",
      "I am fascinated by the history of the human race, the world around us, the stars above us and what could be beyond. I love exploring different cultures and discovering how people live and work in other countries or times. As a Christian I believe that God is at work all over the globe – I would like to explore some of those areas with you!\n",
      "My name is David Cox, I was born in 1952 in South London and lived there for most of my life but moved to Devon in 2013. I have an amazing wife called Sue, three wonderful children (two boys and a girl) and five grandchildren! I’ve been a Christian since I was sixteen years old – so that makes it over fifty years now. I’m very interested in the history of the human race, the world around us, the stars above us and what could be beyond. I love exploring different cultures and discovering how people live and work in other countries or times.\n",
      "I attended a number of Primary Schools – not because my parents kept moving house but because they moved me to various schools so that I didn’t have such a broad South London accent! I was baptised into the Anglican Church when I was seven years old at St John’s Church, Waterloo.\n",
      "When I was thirteen I went to Henry Thornton Grammar School in Clapham for three years but then moved to Tulse Hill Comprehensive after the school had a fire! By this time my brother and sister were going to the same school which was handy as we could travel together – it was only about a mile from home.\n",
      "I wasn’t really academic at that stage of my life so I left school with just one qualification (O Level Geography) but then went on to complete an OND in Business Studies and Accountancy at Wandsworth College while working full time for the National Westminster Bank. I stayed with NatWest for six years, completing their management training course before deciding that banking wasn’t really what I wanted to do!\n",
      "It was then that I began\n",
      "llama_print_timings:        load time =  5951.80 ms\n",
      "llama_print_timings:      sample time =   333.72 ms /   512 runs   (    0.65 ms per token,  1534.23 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26788.12 ms /   265 tokens (  101.09 ms per token,     9.89 tokens per second)\n",
      "llama_print_timings:        eval time = 40203.59 ms /   510 runs   (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time = 67366.61 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce3e8",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5122b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079382\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.23 MB\n",
      "llama_model_load_internal: mem required  = 125274.23 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104f12cb0\n",
      "ggml_metal_init: loaded kernel_mul                            0x104f141a0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104f156f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x104f159d0\n",
      "ggml_metal_init: loaded kernel_silu                           0x104f161a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x104f14740\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104f16af0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104f17af0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104f17d50\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104f19120\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104f18330\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104f1ac30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10782b580\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10782c340\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10782c870\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10782d180\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10782dac0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10782e560\n",
      "ggml_metal_init: loaded kernel_norm                           0x10782f640\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x107905680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x107830700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x107830b00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1078315b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x107831f50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x107832aa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x107833480\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x107833e20\n",
      "ggml_metal_init: loaded kernel_rope                           0x107834810\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x107835880\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x107836440\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x107836ff0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x107837b80\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.25 MB, offs = 115439812608, (125025.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.00 MB, (125049.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (126331.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (126672.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (127056.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to learn, laugh and love.\n",
      "I am a single mother of two beautiful children ages 5 & 7.\n",
      "I have been working with children since I was 16 years old. I started out babysitting after school and on weekends. When I turned 18 I was hired as a nanny for a family of 3 (2 girls) and worked there for about a year until I decided to move back home to be closer to my own family. Once I returned, I was working at various daycares and after school programs when my children were young. When my daughter turned three, she started daycare and that’s when I went back to college full time.\n",
      "I attended the University of Wisconsin Oshkosh where I received my Bachelor’s degree in Human Services Leadership with a minor in Psychology. While attending UWO, I was also working at the YMCA as an after-school counselor for grades 5K-8th grade.\n",
      "After my children started school full time I began working for a foster care agency where I worked for about two years. During that time I decided to go back to school and pursue my Master’s degree in Professional Counseling. I attended Lakeland College, now known as Lakeland University, and received my Master’s Degree in December of 2014.\n",
      "I have been a foster parent for several years, which inspired me to work with kids who are struggling emotionally and behaviorally. It is very rewarding to watch them grow and learn new skills that help them cope with difficult situations. I enjoy working with children because they keep you grounded in the present moment and teach you how to laugh even when things get tough!\n",
      "I have been a foster parent for several years, which inspired me to work with kids who are struggling emotionally and behaviorally. It is very rewarding to watch them grow and learn new skills that help them cope with difficult situations. I enjoy working with children because they keep you grounded in the present moment and teach you how to laugh even when things get tough! I love being able to provide them with a safe environment where they can be themselves without judgement and feel accepted for who they are!\n",
      "My goal is to help each child reach their full potential while teaching them skills necessary to succeed. I believe that every child deserves a chance at\n",
      "llama_print_timings:        load time = 46905.76 ms\n",
      "llama_print_timings:      sample time =   337.60 ms /   512 runs   (    0.66 ms per token,  1516.61 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31941.10 ms /   265 tokens (  120.53 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:        eval time = 151560.02 ms /   510 runs   (  297.18 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:       total time = 183883.21 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86588417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079698\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.23 MB\n",
      "llama_model_load_internal: mem required  = 125274.23 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102529fe0\n",
      "ggml_metal_init: loaded kernel_mul                            0x10252b810\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10252cf10\n",
      "ggml_metal_init: loaded kernel_scale                          0x10252d220\n",
      "ggml_metal_init: loaded kernel_silu                           0x10252da70\n",
      "ggml_metal_init: loaded kernel_relu                           0x10252bec0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10252e400\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10252f330\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102530690\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1025308f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10252fba0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1025312e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13d707190\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13d707980\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13d708000\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13d708910\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13d7090f0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13d709bd0\n",
      "ggml_metal_init: loaded kernel_norm                           0x13d70a6a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1025327b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102532d50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102533750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x102534120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x102534af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x102535610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x106804530\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x106806940\n",
      "ggml_metal_init: loaded kernel_rope                           0x1068077c0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x106808490\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1068090a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x106809c30\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10680a7a0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.25 MB, offs = 115439812608, (125025.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.00 MB, (125049.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (126331.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (126672.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (127056.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, and that all humans are entitled to a fair share of happiness.\n",
      "I also believe that what makes us happy is different for every person, but the most consistent ingredient in being happy is having good relationships with other people.\n",
      "This blog post is about a little boy who brought me great joy today, and whose story inspires me to continue working on this project.\n",
      "Today I went back to the same orphanage as my first visit. This time, I had no translator (as she was not able to come with me), but the staff at the orphanage was really helpful in introducing me to children who were available for adoption. They brought me to two different groups of children - one group under 4 years old and a second group over 6 years old.\n",
      "I met many adorable kids, and I'll introduce you to one of them today. His name is Lukas, and he was born on July 25th, 1998. He has brown eyes and brown hair that is cut in a crew-cut style. He looks very cute!\n",
      "Lukas was about 7 years old when I met him, so I'm guessing that he is now around 9 or 10 years old by the time you read this story.\n",
      "Lukas has been living at the orphanage since he was a baby - 8 months old. His mother and father are divorced, and his mother left him there because she could not care for him on her own. Lukas is an only child.\n",
      "When I met him, Lukas had completed first grade in school and was doing well. He can speak very good English, and he likes to read books and play games (like soccer) with other children at the orphanage.\n",
      "Although Lukas was a little shy when I asked him questions, he opened up when we played together, and he has an adorable smile!\n",
      "At the time of this visit, the orphanage staff said that although they had not received any applications from prospective adoptive parents yet, they were hopeful that Lukas would find a family soon. They also indicated that Lukas really wanted to be adopted and have his own family - he was very excited about meeting me!\n",
      "His favorite subject in school is math. He is doing well at school, and his grades are above average\n",
      "llama_print_timings:        load time = 47674.90 ms\n",
      "llama_print_timings:      sample time =   329.26 ms /   512 runs   (    0.64 ms per token,  1555.02 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31776.30 ms /   265 tokens (  119.91 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:        eval time = 151831.93 ms /   510 runs   (  297.71 ms per token,     3.36 tokens per second)\n",
      "llama_print_timings:       total time = 183981.84 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e1e0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690079931\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.23 MB\n",
      "llama_model_load_internal: mem required  = 125274.23 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10320f640\n",
      "ggml_metal_init: loaded kernel_mul                            0x103210b30\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x103212080\n",
      "ggml_metal_init: loaded kernel_scale                          0x103210d90\n",
      "ggml_metal_init: loaded kernel_silu                           0x103212b80\n",
      "ggml_metal_init: loaded kernel_relu                           0x103213290\n",
      "ggml_metal_init: loaded kernel_gelu                           0x107104360\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1071051c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1071067c0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x107106a60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x107105700\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x107108740\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x103213f50\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1032141b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1032145b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x103214eb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1032157c0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1032168a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x103217200\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x103217e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x103218850\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x103219270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x107e18cf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x107e19a50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x107e1a000\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10321a140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10321a710\n",
      "ggml_metal_init: loaded kernel_rope                           0x10321aff0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10321c0c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10321cc40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10321d7c0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10321e360\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.25 MB, offs = 115439812608, (125025.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.00 MB, (125049.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (126331.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (126672.70 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (127056.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.” — Pablo Picasso\n",
      "In 2018, I started a new career as an Inbound Marketing Consultant for HubSpot. But before my journey with HubSpot, I was a business owner for over 35 years in the retail industry and have been involved with digital marketing since 2014.\n",
      "I founded and operated several successful companies during my career as an entrepreneur – including a chain of women’s clothing stores called The Clothes Line, and then later, a company that sold promotional products to the corporate world called Promo Marketing Solutions. In 2016, I sold my businesses so I could spend more time with my family.\n",
      "I have been married for over 35 years to my husband, Drew. We met while working in a retail store after college and got engaged two weeks later! We have three children: our son, Ryan; our daughter, Lauren; and our son, Joshua (he is the youngest of the three). Drew and I are also proud grandparents to six wonderful grandchildren.\n",
      "After selling my businesses in 2016, I had time on my hands for the first time ever. It was a strange feeling at first but I knew that I could fill this void with meaningful work. After much soul searching, I realized that I wanted to share what I’ve learned during my career to help other businesses succeed. As luck would have it, HubSpot was hiring consultants and I decided to apply for the position. My background in digital marketing was a good fit for this new role with HubSpot and I couldn’t be happier!\n",
      "I love working with small business owners because they are passionate about what they do. It is so rewarding to see them succeed after implementing an Inbound Marketing Strategy that includes SEO, blogging, social media, email marketing, lead generation and other inbound strategies. I am a firm believer that if you work hard enough at anything, success will follow.\n",
      "When I’m not working, my family is the most important part of my life. We take time to travel together every chance we get and enjoy spending quality time with our children and grandchildren.\n",
      "I enjoy exercising and reading books about business and self-\n",
      "llama_print_timings:        load time = 49542.58 ms\n",
      "llama_print_timings:      sample time =   346.30 ms /   512 runs   (    0.68 ms per token,  1478.49 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31863.94 ms /   265 tokens (  120.24 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:        eval time = 153099.35 ms /   510 runs   (  300.19 ms per token,     3.33 tokens per second)\n",
      "llama_print_timings:       total time = 185354.79 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959fbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
