{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     ggml-vocab.bin\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                     \u001b[31mtokenizer.model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccf376-26f5-41ac-9a76-843d3621de23",
   "metadata": {},
   "source": [
    "## Q4_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646666\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x133620700\n",
      "ggml_metal_init: loaded kernel_mul                            0x133621bf0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x133623140\n",
      "ggml_metal_init: loaded kernel_scale                          0x133621e50\n",
      "ggml_metal_init: loaded kernel_silu                           0x133623c40\n",
      "ggml_metal_init: loaded kernel_relu                           0x133624350\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1336245b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x133625540\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1336257a0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x133626a20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x133625d90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1336274a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1336287e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x133627cf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1336290b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x1336299f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13362a330\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13362b410\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13362bd70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13362c9d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13362d530\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13362dd70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13362e760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13362f150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13362fc90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x133630790\n",
      "ggml_metal_init: loaded kernel_rope                           0x1336313c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x133631f40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x133632c10\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "I have a passion for helping people and providing them with the tools they need to create the life they want. My goal is to help you be your best self, achieve your full potential and become unstoppable.\n",
      "My mission is to create an environment where we can work together as a team – you are not alone in this process! I will guide you through your journey with clarity, compassion and empathy. My clients have described me as warm, caring and non-judgmental.\n",
      "I know that life can be challenging; however, it is possible to live a happy, fulfilling and meaningful life even when things aren’t going your way. You don’t need to stay stuck!\n",
      "You may be feeling overwhelmed, anxious or depressed. I will help you gain the clarity you are looking for in order to make positive changes that will last a lifetime. I have a wide range of experience and training to address each individual’s needs.\n",
      "I am a registered psychologist with the College of Alberta Psychologists (#014695). My work is guided by the Canadian Code of Ethics for Psychologists, and is grounded in my commitment to ethical practice and professionalism. I adhere to an Integrative Model that blends various therapeutic approaches from a person-centered perspective and draws on the strengths of each client as we work together towards your goals.\n",
      "I have completed a Doctorate in Clinical Psychology with Specialization in Health Psychology at the University of Alberta, where I graduated cum laude (with honour). My doctoral dissertation was awarded the 2017 Graduate Research Award by the Canadian Association for Suicide Prevention.\n",
      "I am also a Certified Exercise Physiologist (CEP) specializing in exercise psychology and nutrition. I graduated from the University of Alberta with distinction and received the 2016 Excellence in Human Biology Award. As an undergraduate, I was awarded the Chancellor’s Scholarship for excellence in academics (with honours) by the University of Alberta.\n",
      "I am also a Certified Nutritionist and Dietitian. In 2014, I graduated with distinction from Mount Royal University with an Honors Bachelor Degree\n",
      "llama_print_timings:        load time =  1245.99 ms\n",
      "llama_print_timings:      sample time =   331.33 ms /   512 runs   (    0.65 ms per token)\n",
      "llama_print_timings: prompt eval time =  3341.30 ms /   265 tokens (   12.61 ms per token)\n",
      "llama_print_timings:        eval time =  7385.49 ms /   510 runs   (   14.48 ms per token)\n",
      "llama_print_timings:       total time = 11100.22 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646678\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x114d27770\n",
      "ggml_metal_init: loaded kernel_mul                            0x114d28c60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x114d2a1b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x114d2a490\n",
      "ggml_metal_init: loaded kernel_silu                           0x114d2acb0\n",
      "ggml_metal_init: loaded kernel_relu                           0x114d29200\n",
      "ggml_metal_init: loaded kernel_gelu                           0x114d2b610\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x114d2c580\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x114d2d8c0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x114d2cc30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x114d2e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x114d2f6a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x114d2eb80\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x114d2ff80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x114d301e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x114d30a60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x114d313a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x114d32480\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x114d326e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x114d33ad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x114d34550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x114d34f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x114d35970\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x114d363a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x114d36eb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x114d37850\n",
      "ggml_metal_init: loaded kernel_rope                           0x114d38450\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x114d38fc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x114d39ca0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to serve others.\n",
      "The world would be a better place if we all did that, not just in our work lives but in our personal lives as well.\n",
      "I am currently a PhD candidate at UCLA in the Department of Epidemiology (Division of Biostatistics). I study how genetic variants influence risk of disease and survival. One application of my research is to understand why some people develop diabetes while others do not, or vice versa.\n",
      "I am also a medical doctor by training but have been working in epidemiology for 4 years now. I like statistics because it allows me to learn more about the world and connect the dots between various areas of science including genetics, medicine, etc.\n",
      "In my spare time, I really enjoy playing tennis.\n",
      "I also love watching movies (of all kinds), doing puzzles, reading books, going on hikes, traveling, cooking, going to museums and art galleries, as well as meeting people from different cultures! One of the places that I have traveled to is India where my family lives.\n",
      "I am originally from Southern California but have lived in Los Angeles for nearly 10 years now. Prior to that, I was born and raised in Bangalore, India.\n",
      "If you would like to get in touch with me (and I do hope you do!), please email me at gowri@ucla.edu or connect with me on LinkedIn!\n",
      "I am very interested in collaborating and welcome all types of feedback.\n",
      "All content copyright Gowri Varadarajan ©2016 unless otherwise stated. All rights reserved. Images used are either my own, free for the use (CC-BY), or public domain. Please do not reproduce without permission. Thank you!\n",
      "I am also on Twitter (@gv_ucla) and Instagram (@gowri_ucla). Feel free to connect with me there too!\n",
      "You can download my CV here.\n",
      "Wow, great website and blog! I like the way you are able to link your blogs to your twitter account. Very nice layout as well. I am going to try it on my own blog. Keep up the good work.\n",
      "Thanks for stopping by my blog. �� I’m so glad that this post helped. The world would be a better place if we were all more empathetic\n",
      "llama_print_timings:        load time =  1022.10 ms\n",
      "llama_print_timings:      sample time =   343.11 ms /   512 runs   (    0.67 ms per token)\n",
      "llama_print_timings: prompt eval time =  3352.10 ms /   265 tokens (   12.65 ms per token)\n",
      "llama_print_timings:        eval time =  7421.41 ms /   510 runs   (   14.55 ms per token)\n",
      "llama_print_timings:       total time = 11158.57 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646690\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x103429920\n",
      "ggml_metal_init: loaded kernel_mul                            0x10342ae10\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10342c360\n",
      "ggml_metal_init: loaded kernel_scale                          0x10342b070\n",
      "ggml_metal_init: loaded kernel_silu                           0x10342cdd0\n",
      "ggml_metal_init: loaded kernel_relu                           0x10342d5a0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10342de90\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10342e700\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10342fa30\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10342fc90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10342ef40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x103430680\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1034319e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x103430e90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x103432280\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x103432bc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x103433500\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1034345e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x103434f40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x103435ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x103436700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x103436f40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x103437930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x103438320\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x103438e70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x103439990\n",
      "ggml_metal_init: loaded kernel_rope                           0x10343a5f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10343b160\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10343be30\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "I am an optimist by nature and I always take every experience as a lesson learnt. There was once a time when I used to get upset or feel low but that has changed now. I have learned to look at things from a positive angle, be it for myself or for someone else.\n",
      "My goal in life is to make this world a better place for all of us. And I believe the best way to do that is by inspiring people and motivating them to find their true selves.\n",
      "I have been into writing since my childhood days but I never had the courage to pursue it as a profession until recently. Now, with time on my hands, I am writing more than ever before in order to make this world a better place for all of us. In fact that is what my blog is all about – making this world a better place and inspiring people to find their true selves.\n",
      "I have always been an avid reader but the books I read earlier were mostly fictional novels. Today, however, most of the stuff I read are self-help books or motivational stuff. My favourite authors include Paulo Coelho, Sonia Choquette and Deepak Chopra. They all inspire me a lot.\n",
      "The best thing about my blog is that it has given me an opportunity to talk and connect with people from all around the world. I have met some really nice people on this blog which I am sure will be friends for life.\n",
      "I believe we should never give up on our dreams, no matter how difficult things may seem at times.\n",
      "My hobbies include reading, writing, dancing and listening to music (classical) but my favourite activity is spending time with my family.\n",
      "I am an optimist by nature and I always take every experience as a lesson learnt. There was once a time when I used to get upset or feel low but that has changed now. I have learned to look at things from a positive angle, be it for myself or for someone else. My goal in life is to make this world a better place for all of us. And I believe the best way to do that is by inspiring people and motivating them to find their true selves.\n",
      "I have always been an avid reader but the books I read earlier were mostly fictional novels. Today, however, most of the stuff I read is related to self-im\n",
      "llama_print_timings:        load time =  1041.00 ms\n",
      "llama_print_timings:      sample time =   330.54 ms /   512 runs   (    0.65 ms per token)\n",
      "llama_print_timings: prompt eval time =  3470.41 ms /   265 tokens (   13.10 ms per token)\n",
      "llama_print_timings:        eval time =  7359.15 ms /   510 runs   (   14.43 ms per token)\n",
      "llama_print_timings:       total time = 11200.40 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646703\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x134226530\n",
      "ggml_metal_init: loaded kernel_mul                            0x134227a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x134228f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x1342292a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x134229a30\n",
      "ggml_metal_init: loaded kernel_relu                           0x134227e40\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13422a300\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13422b420\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13422c6b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13422c910\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13422cfe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13422d240\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13422e630\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13422db10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13422ed70\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13422f6b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13422fff0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x134230ac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x134231bd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x134232810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x134233360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x134233b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x1342345c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x134234ff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x134235b10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x134236640\n",
      "ggml_metal_init: loaded kernel_rope                           0x134237270\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x134237dc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x134238ab0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m not just a question, but an answer as well.\n",
      "I believe that everyone's lives are connected to one another.\n",
      "I believe that we are all here for a reason.\n",
      "And I believe that everything happens for a reason.\n",
      "But, how can this be true? If God is in control of every part of our lives and our futures why do so many bad things happen? Why did my family lose our home to a flood? Why am I still waiting on the man I love to propose? Why are people sick and hurt all around me?\n",
      "I believe that God is in control, but I also believe that we have free will. Our choices lead us down paths of joy or pain and sometimes those paths become so hard that we think they were never worth starting. But if there was no struggle then the reward would be meaningless.\n",
      "But why do good people suffer? What did they ever do to deserve it? I believe that everyone here on earth has the potential to change lives for the better, but sometimes their stories are cut short by an untimely death or a disease that takes them from us too soon. I believe that these people are blessed because they have already lived fuller and more meaningful lives than many of us will ever get a chance to do.\n",
      "I believe that we were all created for a reason, but I don't think it is always obvious what that purpose is or why we must endure the things we go through. I believe in God and I believe that if everyone could just see the beauty that surrounds them every day then they would truly understand.\n",
      "I believe that life has meaning, even when it doesn't seem like it does at all.\n",
      "These are my beliefs. What do you believe?\n",
      "This is so beautifully written! Thank you for sharing your thoughts with us today!\n",
      "Aww thank you, I appreciate the comment!\n",
      "So well said! Thank you for this!\n",
      "Thanks for reading and thanks for the comment!!\n",
      "Wow, that was amazing. Very inspiring. I always feel better after reading what you write. Thank you for sharing yourself in such a positive way.\n",
      "Thank you so much for your kind words, they mean more than you know!\n",
      "Love this! Beautifully written! I agree with everything you said here. You're right-everything happens for a reason...I truly believe that.\n",
      "Thanks so much for reading and thanks for the comment!!\n",
      "Ab\n",
      "llama_print_timings:        load time =  2287.93 ms\n",
      "llama_print_timings:      sample time =   321.43 ms /   512 runs   (    0.63 ms per token)\n",
      "llama_print_timings: prompt eval time =  5955.86 ms /   265 tokens (   22.47 ms per token)\n",
      "llama_print_timings:        eval time = 11911.25 ms /   510 runs   (   23.36 ms per token)\n",
      "llama_print_timings:       total time = 18228.22 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646724\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11e827770\n",
      "ggml_metal_init: loaded kernel_mul                            0x11e828c60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11e82a1b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11e82a490\n",
      "ggml_metal_init: loaded kernel_silu                           0x11e82acb0\n",
      "ggml_metal_init: loaded kernel_relu                           0x11e829200\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11e82b610\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11e82c580\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11e82d8c0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11e82cc30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11e82e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11e82f6a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11e82eb80\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11e82ff80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11e8301e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11e830a60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11e8313a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11e832480\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11e8326e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11e833ad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11e834550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11e834f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11e835970\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11e8363a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11e836eb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11e837850\n",
      "ggml_metal_init: loaded kernel_rope                           0x11e838450\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11e838fc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11e839ca0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "We all need a reason to get up in the morning, or at least something that makes it worthwhile getting out of bed. We can’t just live for ourselves, as our own satisfaction will not suffice forever.\n",
      "I think we need something bigger than ourselves to aim for, and to strive for. It is about making a difference in some way. Whether it be through the work you do with your hands and mind every day, or through something that gives back to those less fortunate than yourself. Some give their time, others their money, but whatever way you can, make a difference and it will add meaning to your life, and purpose to everything you do.\n",
      "This entry was posted in Awareness, Change, Inspiration, Life, Meaning of Life, Mindfulness, Motivation, Perspective and tagged #MakingADifference, Give Back, Mental Health, Purpose by The V Pub Team. Bookmark the permalink.\n",
      "← Previous Previous post: The Power Of Your Thoughts!\n",
      "Next → Next post: What Do You Want To Be Remembered For?\n",
      "I’m a big fan of this quote! I agree that we all need meaning in our lives and one way to do that is by giving back. If you don’t have the money to give, giving your time can be just as important. Even if it’s only a few hours per month (I volunteer at my local animal shelter every 4th Sunday), you are making a difference in someone else’s life!\n",
      "Thank you for this post and encouraging us all to make a difference!\n",
      "Awesome, thank you so much for your comment, I am glad that it resonated with you. It really is about giving back, and doing our bit.\n",
      "I totally agree with you. We should focus on making the world better in some way or another because I think we all can help somehow. Thanks for sharing!\n",
      "Awesome quote there – I like it �� Thank you for this post and encouraging us all to make a difference!\n",
      "Thank you so much, it is my pleasure!\n",
      "I love the idea of making a difference in life! I believe that’s one of the best gifts we can give ourselves.\n",
      "Awesome, thank you very much for your comment, I am glad this resonated with you. It really is about giving\n",
      "llama_print_timings:        load time =  1764.07 ms\n",
      "llama_print_timings:      sample time =   494.40 ms /   512 runs   (    0.97 ms per token)\n",
      "llama_print_timings: prompt eval time =  5916.23 ms /   265 tokens (   22.33 ms per token)\n",
      "llama_print_timings:        eval time = 12321.67 ms /   510 runs   (   24.16 ms per token)\n",
      "llama_print_timings:       total time = 18788.62 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646744\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100327920\n",
      "ggml_metal_init: loaded kernel_mul                            0x100328e10\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10032a360\n",
      "ggml_metal_init: loaded kernel_scale                          0x10032a640\n",
      "ggml_metal_init: loaded kernel_silu                           0x10032ae80\n",
      "ggml_metal_init: loaded kernel_relu                           0x1003293b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10032b7f0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10032c760\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10032daa0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10032dd00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10032cfb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10032e6c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x10032fa10\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x10032ef10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x100330150\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x100330bf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x100331540\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1003325f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100332f50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100333b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100334580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x100334f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x100335b30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x100336360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x100336ec0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x1003379e0\n",
      "ggml_metal_init: loaded kernel_rope                           0x100338600\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100339180\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100339e60\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m the same for everyone: to be happy and to use all your talents.\n",
      "- Henri Poincaré\n",
      "I've been blessed with a lot, so it makes sense for me to be giving back. - Paul Walker\n",
      "The world is full of nice people. If you can't find one, be one. - Miriam Hodgman\n",
      "Everything that we have given up in order to gain the things that seem important to us will turn out to be meaningless in the end anyway; it is only what we give up for the sake of Christ and His kingdom that will last forever. - Paul Washer\n",
      "I don't believe in astrology, I am a Sagittarius. - Bill Vaughan\n",
      "I didn't get to where I am by thinking about it or dreaming about it. I got there by doing it. - Estee Lauder\n",
      "The most successful people are those who think up things for the rest of the world to keep busy at. - Adlai Stevenson II\n",
      "There is nothing so easy as self-deceit. For what each man wishes, that he also believes to be true. - Demosthenes\n",
      "If I had eight hours to chop down a tree, I'd spend six sharpening my axe. - Abraham Lincoln\n",
      "If you are going through hell keep going. - Winston Churchill\n",
      "Success is a science; if you have the conditions, you get the result. - Oscar Wilde\n",
      "Sometimes I wonder whether men or women suffer more in this world. Then I make it a rule never to argue with myself. - Robert Browning\n",
      "What lies behind us and what lies before us are tiny matters compared to what lies within us. - Ralph Waldo Emerson\n",
      "Our business in life is not to succeed, but to continue to fail, in good spirits. - Stephen Leacock\n",
      "He knows nothing; and he thinks he knows everything. That points clearly to a political career. - Aristophanes\n",
      "You cannot plough a field by turning it over in your mind. - Irish Proverb\n",
      "To have little is one thing; to be in want of little is another. - Alexander Pope\n",
      "A leader leads by example, not by force. - Sun Tzu\n",
      "The best way to make your dreams come true is to wake up. - Paul Valery\n",
      "You can always tell when you're dealing with a white\n",
      "llama_print_timings:        load time =  1767.39 ms\n",
      "llama_print_timings:      sample time =   541.72 ms /   512 runs   (    1.06 ms per token)\n",
      "llama_print_timings: prompt eval time =  5955.30 ms /   265 tokens (   22.47 ms per token)\n",
      "llama_print_timings:        eval time = 12425.20 ms /   510 runs   (   24.36 ms per token)\n",
      "llama_print_timings:       total time = 18981.00 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646765\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104d27170\n",
      "ggml_metal_init: loaded kernel_mul                            0x104d28680\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104d29bd0\n",
      "ggml_metal_init: loaded kernel_scale                          0x104d29ee0\n",
      "ggml_metal_init: loaded kernel_silu                           0x104d2a670\n",
      "ggml_metal_init: loaded kernel_relu                           0x104d28a80\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104d2af40\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104d2c060\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104d2d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104d2d550\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104d2dc20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104d2de80\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x104d2f270\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x104d2e750\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x104d2f9b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x104d302f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x104d30c30\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104d31700\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104d32810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104d33450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104d33fa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x104d347b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x104d35200\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x104d35c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x104d36750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x104d37280\n",
      "ggml_metal_init: loaded kernel_rope                           0x104d37eb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104d38a00\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104d396f0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.03 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and enjoy your every day.\n",
      "I am a 30 year old single mother of three boys. I am looking for someone who has a good heart, honest, caring, compassionate and loves kids. Someone that doesn't mind a little bit of chaos but can still have some peace with me.\n",
      "Someone who isn't afraid to do new things! Someone that is willing to give and receive love!\n",
      "I am looking for someone who has the same values as I do, who will be there through thick and thin and is willing to give and receive love without expecting anything in return!\n",
      "AshleyLove hasn't asked any friends to write a recommendation yet.\n",
      "AshleyLove has not yet answered any of the optional questions that would tell you more about her.\n",
      "AshleyLove has not yet connected with anyone.\n",
      "Posted by BlessedMom on May 21, 2016. Brought to you by meetup.\n",
      "Posted by Ashley L. on March 16, 2015. Brought to you by yahoolocal.\n",
      "My name is Ashley, I am a single mother of three boys! They are my life and they are the best thing that has ever happened to me! My oldest son just turned ten and he's my little helper and he tells me everyday how much he loves me, it melts my heart. My middle son is seven years old and his smile lights up a room and he's so full of life, I love the way he sees everything in life. And last but not least is my little guy, he just turned two and he has a huge personality and he loves to dance! They are my everything and they make me laugh all day long! My boys give me reason to live and when they are happy that makes me happy! I am looking for someone who can accept me the way I am. I am a single mother of three boys so it's not always easy to be out in public with them, sometimes we have meltdowns and tantrums but that is part of being a mom and you just roll with the punches and enjoy every minute even when they are crying!\n",
      "I work full time as an executive assistant for a company called Allied Solutions. I am very close to my family and love spending time with them whenever\n",
      "llama_print_timings:        load time =  5219.19 ms\n",
      "llama_print_timings:      sample time =   634.37 ms /   512 runs   (    1.24 ms per token)\n",
      "llama_print_timings: prompt eval time = 13239.95 ms /   265 tokens (   49.96 ms per token)\n",
      "llama_print_timings:        eval time = 26469.96 ms /   510 runs   (   51.90 ms per token)\n",
      "llama_print_timings:       total time = 40406.54 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c19ee1f8-96fb-42d2-9736-6da7e7678b94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646811\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x150026780\n",
      "ggml_metal_init: loaded kernel_mul                            0x150027c70\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1500291c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1500294a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x150029ce0\n",
      "ggml_metal_init: loaded kernel_relu                           0x150028210\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15002a650\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x15002b5c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x15002c900\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x15002cb60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x15002be10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x15002d520\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x15002e870\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x15002dd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x15002efb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x15002fa50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1500303a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x150031450\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x150031db0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1500329f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1500333e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x150033df0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x150034990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1500351c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x150035d20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x150036840\n",
      "ggml_metal_init: loaded kernel_rope                           0x150037460\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x150037fe0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x150038cc0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.03 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift.\n",
      "To find your gift is the meaning of life.\n",
      "I think that’s it, ya know?\n",
      "If you agree, then do me a favor and answer this question: What is YOUR gift?\n",
      "It doesn’t have to be big or earth-shattering. It just has to be yours.\n",
      "But here’s the tricky part. Sometimes you can’t see your gift for what it is. And that makes giving and receiving gifts difficult.\n",
      "I’ve always had a hard time accepting compliments from people who say they like my voice, or that I’m funny, or just tell me I have a good sense of humor. In fact, I used to be in complete denial when someone told me this.\n",
      "My standard response would be, “Oh no, you don’t understand. I’m not funny.”\n",
      "But the truth is that, yes, I am funny. Or at least people think I am. And I can tell them what they want to hear and be nice about it. But if they say something like, “You’re so good with people,” or “You’ve got a great smile,” well then you can bet your last dollar that I will try my best to convince them how wrong they are.\n",
      "Why? Because I don’t see these qualities as gifts at all! In fact, it would never occur to me to call them gifts. To me, funny and good with people are things I do naturally, without even thinking about them. They just come out of me. So how could they be called a gift?\n",
      "I’m not alone in this. I have friends who say the same thing when people compliment them. “Oh no, you don’t understand. It comes so easy to me.”\n",
      "But here’s what I’ve learned: If it’s easy for you and hard for others, then by definition, that means it’s a gift. Why? Because other people don’t have the same amount of talent or skill in this area as you do.\n",
      "And so when someone says to me, “I love your sense of humor,” I have to ask myself if my reflexive response of denial is actually hurting me in some way.\n",
      "For instance, how does it make me feel? Or how does it affect other people’s perception of me? Do they think I’m\n",
      "llama_print_timings:        load time =  4144.45 ms\n",
      "llama_print_timings:      sample time =   536.82 ms /   512 runs   (    1.05 ms per token)\n",
      "llama_print_timings: prompt eval time = 13073.78 ms /   265 tokens (   49.34 ms per token)\n",
      "llama_print_timings:        eval time = 26145.11 ms /   510 runs   (   51.26 ms per token)\n",
      "llama_print_timings:       total time = 39814.30 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eed0b357-9339-4acf-90a2-491e8da51bd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646855\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x114f45aa0\n",
      "ggml_metal_init: loaded kernel_mul                            0x114f46f90\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x114f484e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x114f487c0\n",
      "ggml_metal_init: loaded kernel_silu                           0x114f49000\n",
      "ggml_metal_init: loaded kernel_relu                           0x114f47530\n",
      "ggml_metal_init: loaded kernel_gelu                           0x114f49970\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x114f4a8e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x114f4bc20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x114f4be80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x114f4b130\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x114f4c840\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x114f4db90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x114f4d090\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x114f4e2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x114f4ed70\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x114f4f6c0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x114f50770\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x114f510d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x114f51d10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x114f52700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x114f53110\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x114f53cb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x114f544e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x114f55040\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x114f55b60\n",
      "ggml_metal_init: loaded kernel_rope                           0x114f56780\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x114f57300\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x114f57fe0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.03 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and fulfilled, and to do that you need to understand yourself. You need to know what you want in your life and how to get it.\n",
      "I am a psychologist from Brussels with over 10 years experience. I have worked for the Belgian social services, teaching at university level and as an independent psychologist.\n",
      "If you are interested in improving your quality of life or solving specific problems in your life, feel free to contact me. I offer individual support on how to live a more fulfilling life. We can work together either in-person or online.\n",
      "I speak English and French fluently. As for Dutch, it is at a conversational level.\n",
      "You can find me on the following platforms: LinkedIn, Twitter, Instagram, Tumblr.\n",
      "If you want to talk about what I do, please get in touch with me using this form or by emailing me at elodie[at]elodieleblanc.com.\n",
      "My blog is about things that interest me in a professional and personal level. It includes posts on life hacks, psychology, self-help, books I read, movies I like. You can expect articles related to the following: wellbeing, happiness, success, healthy living, self-help, psychology, positive psychology, mental health, motivation, self-esteem and anything that will help you live a better life!\n",
      "I am also interested in other blogs, especially those about personal development. If you are into it too and have a great blog, feel free to contact me. I would be happy to write an article on your blog or to interview you on my blog.\n",
      "Please note that this is not a commercial website – I do not make money with this blog. I share information I have gathered during my work as psychologist and also things I find interesting for personal use. If you want to contact me about anything, please get in touch using the form below or by emailing me at elodie[at]elodieleblanc.com.\n",
      "If you would like to advertise on this blog, please contact me for more information using the form below or by emailing me at elodie[at]elodieleblanc.com.\n",
      "Please note that I will only consider ads and sponsored posts related to life hacks, psychology, self-help, motivation, wellbeing and\n",
      "llama_print_timings:        load time =  5203.76 ms\n",
      "llama_print_timings:      sample time =   617.62 ms /   512 runs   (    1.21 ms per token)\n",
      "llama_print_timings: prompt eval time = 12858.88 ms /   265 tokens (   48.52 ms per token)\n",
      "llama_print_timings:        eval time = 26262.80 ms /   510 runs   (   51.50 ms per token)\n",
      "llama_print_timings:       total time = 39801.26 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b1651",
   "metadata": {},
   "source": [
    "### 65B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04957af-e22b-4394-88fc-653530808c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646901\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.91 MB\n",
      "llama_model_load_internal: mem required  = 38674.91 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12b61bd30\n",
      "ggml_metal_init: loaded kernel_mul                            0x12b61d220\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12b61e770\n",
      "ggml_metal_init: loaded kernel_scale                          0x12b61d480\n",
      "ggml_metal_init: loaded kernel_silu                           0x12b61f270\n",
      "ggml_metal_init: loaded kernel_relu                           0x12b61f980\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12b61fbe0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12b620b70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12b620dd0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12b622050\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12b6213c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12b622ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x12b623e10\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x12b623320\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x12b6246e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x12b625020\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x12b625960\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12b626a40\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12b6273a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12b628000\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12b628b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x12b6293a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x12b629d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x12b62a780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x12b62b2c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x12b62bdc0\n",
      "ggml_metal_init: loaded kernel_rope                           0x12b62c9f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12b62d570\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12b62e240\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.92 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1536.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =  1024.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a meaning.\n",
      "In this sense, it’s our duty to find something that makes us happy and fulfilled – and try to help others do the same.\n",
      "I’m not talking about some kind of hippie nonsense here; I’m talking about real life.\n",
      "Real work for real money in a career you love. For most people, doing what they want every day is a pipe dream. It seems like something reserved only for the lucky few who get to be rock stars and movie directors.\n",
      "But, it doesn’t have to be that way. I believe anyone can earn a living doing something they’re passionate about – or at least find work which allows them enough time outside of it to pursue their passions.\n",
      "I’m writing this blog to share with you the knowledge and experience I’ve gained on my journey so far, as well as what I learn in the future. I hope that you too can create a lifestyle for yourself which makes you happy and fulfilled – and gives your life meaning.\n",
      "Hi, I'm Sean. I help young people achieve more in their careers. If you're interested in finding work you love (or at least work which allows you to do what you love), then join 20,000+ subscribers by entering your email below. You'll also get a free copy of my eBook on how to start your career!\n",
      "I’m not an expert – I’m just sharing knowledge and experience gained from reading books, articles and blogs, listening to podcasts, watching videos, speaking with friends and family members as well as mentors, and most importantly making mistakes.\n",
      "So take what you read here with a grain of salt. If it helps you out – great! But always be aware that there are many different ways to achieve success in your life.\n",
      "I’m not a professional writer or blogger; I just love writing (and learning) about these topics and hope to help others along the way.\n",
      "What You Should Be Doing Right Now If You Want To Succeed In Life – 7 practical tips for young people who want to find work they love and live a fulfilling life.\n",
      "How To Get Ahead In Your Career (Part 1: Self-Awareness) – Find out how self-awareness can help you perform better in your job and identify which type\n",
      "llama_print_timings:        load time =  9434.66 ms\n",
      "llama_print_timings:      sample time =   737.87 ms /   512 runs   (    1.44 ms per token)\n",
      "llama_print_timings: prompt eval time = 25587.99 ms /   265 tokens (   96.56 ms per token)\n",
      "llama_print_timings:        eval time = 48259.98 ms /   510 runs   (   94.63 ms per token)\n",
      "llama_print_timings:       total time = 74653.90 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b290b17-1c8d-4923-be68-fd80052cc841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689646985\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.91 MB\n",
      "llama_model_load_internal: mem required  = 38674.91 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10242fa80\n",
      "ggml_metal_init: loaded kernel_mul                            0x102430f70\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1024324c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1024327a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x102432fe0\n",
      "ggml_metal_init: loaded kernel_relu                           0x102431510\n",
      "ggml_metal_init: loaded kernel_gelu                           0x102433950\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1024348c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x102435c00\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x102435e60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x102435110\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x102436820\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x102437b70\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x102437070\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1024382b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x102438d50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1024396a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10243a750\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10243b0b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10243bcf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10243c6e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x10243d0f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x10243dc90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x10243e4c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x10243f020\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x10243fb40\n",
      "ggml_metal_init: loaded kernel_rope                           0x102440760\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1024412e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x102441fc0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.92 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1536.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =  1024.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "The Dalai Lama says that “the purpose of our lives is to be happy.”\n",
      "We’re all trying to achieve happiness, but what does it mean?\n",
      "Is it something you get from outside sources, such as money, fame or success? Or is there a way to find happiness within yourself?\n",
      "Happiness is something everyone wants. No one wakes up and says “I want to be miserable today.” Most of us would choose happiness over misery if we had the option. And yet it seems that many of us do not know how to create it for ourselves, or how to hang on to it once we have a taste of it.\n",
      "How can we maintain happiness and keep our lives in balance?\n",
      "Achieving and maintaining a happy life requires some effort and commitment, but the results are worth it! It is easy to find things that make us unhappy, but much more difficult to change those things. It’s often easier to be a victim of circumstances than to take control over our lives. But if we don’t do something to fix things, no one else will.\n",
      "We all have the ability to be happy, and the power to do it is within each one of us. Happiness is not a goal or an accomplishment; it is simply a way of living. It isn’t dependent on external forces, but comes from within you. Happiness doesn’t require any special skills or talents—it is available to everyone.\n",
      "What Doesn’t Make Us Happy?\n",
      "We cannot be happy if we are constantly worried about something. Worrying won’t change anything, and it will only make us feel worse. No matter how hard we try, there are some things we just can’t control. Once you accept that fact, you will find your life becomes a lot less stressful.\n",
      "There is nothing wrong with wanting to be successful or wealthy—if that is what makes you happy. But if you believe those two things alone will make you happy, you’re kidding yourself. Being rich and successful does not guarantee happiness. There are plenty of people who have all the money in the world, but they still feel unfulfilled.\n",
      "Sometimes we think things like our job or a relationship is making us unhappy. We think that if only that one thing would change, then we could be happy. But that’s not how it works. If you are\n",
      "llama_print_timings:        load time =  6746.94 ms\n",
      "llama_print_timings:      sample time =   655.41 ms /   512 runs   (    1.28 ms per token)\n",
      "llama_print_timings: prompt eval time = 25438.90 ms /   265 tokens (   96.00 ms per token)\n",
      "llama_print_timings:        eval time = 47580.36 ms /   510 runs   (   93.29 ms per token)\n",
      "llama_print_timings:       total time = 73738.36 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ba24b4b-59b0-489b-8227-76fd81c2fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647066\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.91 MB\n",
      "llama_model_load_internal: mem required  = 38674.91 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104b2e760\n",
      "ggml_metal_init: loaded kernel_mul                            0x104b2fc50\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104b311a0\n",
      "ggml_metal_init: loaded kernel_scale                          0x104b2feb0\n",
      "ggml_metal_init: loaded kernel_silu                           0x104b31c10\n",
      "ggml_metal_init: loaded kernel_relu                           0x104b323e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104b32cd0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104b33540\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104b34870\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104b34ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104b33d80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104b354c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x104b36820\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x104b35cd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x104b370c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x104b37a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x104b38340\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104b39420\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104b39d80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104b3a9e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104b3b540\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x104b3bd80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x104b3c770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x104b3d160\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x104b3dca0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x104b3e7a0\n",
      "ggml_metal_init: loaded kernel_rope                           0x104b3f3d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104b3ff70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104b40c10\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.92 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1536.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =  1024.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "What are you? A person, a writer, or a reader?\n",
      "I’m still here at Penn State University for the 5th annual Pennwriters Conference, but since I won’t be home until late Sunday night and have a book club meeting on Monday, I decided to go ahead and post this week’s column.\n",
      "Last week’s prompt elicited some excellent responses from writers who are also readers. This week’s prompt is for those of you who consider yourselves writers first and readers second (or third, fourth….). What would you do if you suddenly lost the ability to write? How would that affect your life and how would you eventually move past it?\n",
      "You can post your responses as a comment or email them to me. I will include as many of them as I have room for in next week’s column.\n",
      "Last week’s prompt was: You are in the bookstore browsing the shelves when a person walks up to you, introduces him/herself and informs you that s/he is an author. What happens next? The following responses came via email from those who took up the challenge of this prompt.\n",
      "“My name’s Bob,” he said, thrusting out his hand, “And I’m a writer.”\n",
      "I shook it limply, thinking I must be in the Twilight Zone. How could someone so ordinary look like such an extraordinary jerk? I mean, did he really think that introducing himself as a writer was going to impress me or something? What, just because we’re both in a bookstore browsing for best sellers that makes us friends all of the sudden?\n",
      "“I don’t read much,” I said. “So forgive me if I sound stupid, but what exactly is it that you write?”\n",
      "He grinned smugly and said, “Stories about people like you.”\n",
      "That did not come out right. What was I thinking? He must have thought I was insulting him because he turned away abruptly and walked to another aisle.\n",
      "I stood there for a second trying to decide whether to follow or just forget the whole thing. But then, I couldn’t help myself. I had to know what kind of stories this guy wrote. So I followed him.\n",
      "As soon as he noticed me again,\n",
      "llama_print_timings:        load time =  6797.09 ms\n",
      "llama_print_timings:      sample time =   347.02 ms /   512 runs   (    0.68 ms per token)\n",
      "llama_print_timings: prompt eval time = 25412.07 ms /   265 tokens (   95.89 ms per token)\n",
      "llama_print_timings:        eval time = 46515.63 ms /   510 runs   (   91.21 ms per token)\n",
      "llama_print_timings:       total time = 72316.98 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31303842",
   "metadata": {},
   "source": [
    "## F16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07d3dcd1-627b-4544-88a1-9e68647c61a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647145\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x111a26c80\n",
      "ggml_metal_init: loaded kernel_mul                            0x111a28170\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x111a296c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x111a283d0\n",
      "ggml_metal_init: loaded kernel_silu                           0x111a2a130\n",
      "ggml_metal_init: loaded kernel_relu                           0x111a2a910\n",
      "ggml_metal_init: loaded kernel_gelu                           0x111a2b200\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x111a2ba90\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x111a2cdc0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x111a2d020\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x111a2c2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x111a2ebb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x111a2e0b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x111a2e310\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x111a2f620\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x111a2ff60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x111a308a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x111a31950\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x111a322b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x111a32ef0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x111a338b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x111a342f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x111a34e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x111a356d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x111a36210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x111a36d40\n",
      "ggml_metal_init: loaded kernel_rope                           0x111a37940\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x111a384b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x111a39190\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find happiness and live a purposeful, positive life. I’m not a religious person, but if there was one thing that religion teaches us, it is that we should live this way.\n",
      "In order to really understand the meaning of life, you have to take out all the distractions in your life: money, family ties, work obligations… and think about what life would be like without them. What do YOU want? It’s probably not a big house, fancy car or new clothes. If that is what you really want, then go ahead and pursue it – but for most people, those things don’t make us truly happy.\n",
      "The meaning of my life lies in helping others, being creative and sharing my knowledge with the world.\n",
      "I believe that everyone has a purpose on this earth and we should all try to find ours. I think it is also our responsibility to help others discover their purpose if they are struggling.\n",
      "If you want to know what your purpose is, just ask yourself “What makes me happy?” Then write down everything that comes to mind. These are your passions! Once you have identified them, start focusing on the things that bring you joy and happiness. Focus on those things and make a conscious effort to do them more often.\n",
      "It’s okay to not know what your purpose is yet. You don’t need to figure it out right now. Maybe you just need a break from your daily routine in order to get some clarity – that’s what I did when my life fell apart… and I wrote about it here.\n",
      "If you feel like there is something more out there, start asking questions and seek answers – but don’t be afraid of the silence. If you find yourself asking the same question over and over again in your mind, then it’s probably not going to go away unless you do something about it. So… what can you do?\n",
      "If you are feeling stuck or lost in life, you need a change. I am a firm believer that change is good for us if we accept the fact that we don’t know everything and we should be open to new ideas and perspectives. We might not like every aspect of the changes we go through – but they will make our lives better eventually.\n",
      "So… what could you do? You can start by changing your attitude towards life (more on this here). Maybe you need to move away from everything that is familiar\n",
      "llama_print_timings:        load time =  3523.02 ms\n",
      "llama_print_timings:      sample time =   444.79 ms /   512 runs   (    0.87 ms per token)\n",
      "llama_print_timings: prompt eval time =  3136.07 ms /   265 tokens (   11.83 ms per token)\n",
      "llama_print_timings:        eval time = 19552.85 ms /   510 runs   (   38.34 ms per token)\n",
      "llama_print_timings:       total time = 23182.83 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ecbe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647172\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1135286a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x113529b90\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11352b0e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x113529df0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11352bb50\n",
      "ggml_metal_init: loaded kernel_relu                           0x11352c320\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11352cc10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11352d480\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11352e7b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11352ea10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11352dcc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11352f400\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x113530760\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11352fc10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x113531000\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x113531940\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x113532280\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x113533360\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x113533cc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x113534920\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x113535480\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x113535cc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x1135366b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1135370a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x113537bf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x113538710\n",
      "ggml_metal_init: loaded kernel_rope                           0x113539370\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x113539ee0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11353abb0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I am a teacher, writer and speaker. My work is about finding and sharing our gifts with others and making the world a better place by doing so. My mission is to help you discover your unique talents, develop them, and use them to make an impact on the lives of those around you.\n",
      "My journey began in 2015 when I was invited to join The Greatness Project as a facilitator for their workshops. Since then, I have attended many other courses and seminars including one with the Dalai Lama. My teaching experience comes from my role as a teacher trainer at the British Council in Budapest where I worked alongside teachers from all over Europe.\n",
      "I am passionate about inspiring others to discover their unique talents and finding ways of sharing them with the world. In my workshops, I help people get clear on what they are naturally good at doing. We then explore how to make a meaningful contribution using our gifts which is what helps us feel fulfilled in life.\n",
      "My background as an International School Teacher gives me a unique perspective into teaching and learning. My own experiences of living abroad have allowed me to gain insight into the challenges that people face when moving to new countries or cultures. I believe that everyone has something important to contribute to their communities. I am passionate about helping others discover their gifts, develop them, and use them to make an impact on the lives of those around them.\n",
      "I have a love for nature and animals which is why you can find me walking my dog in beautiful places whenever I get the chance! I also enjoy reading, cooking and playing sports such as tennis or volleyball.\n",
      "My greatest joy comes from being with my family. My husband and two children are my inspiration. In my free time, I like to spend it with them.\n",
      "I have a Masters in Education from the University of Bath, UK, and a certificate in Coaching from Cambridge University. I am also an accredited trainer for The Greatness Project.\n",
      "Find out more about my coaching services here.\n",
      "Find out more about my workshops here.\n",
      "Follow me on Facebook or Instagram to keep up with what’s happening!\n",
      "I have known Agnes since 2016 when we both worked together at the British Council. I immediately noticed how calm and positive she was towards everyone\n",
      "llama_print_timings:        load time =  2513.93 ms\n",
      "llama_print_timings:      sample time =   407.48 ms /   512 runs   (    0.80 ms per token)\n",
      "llama_print_timings: prompt eval time =  3190.55 ms /   265 tokens (   12.04 ms per token)\n",
      "llama_print_timings:        eval time = 19518.75 ms /   510 runs   (   38.27 ms per token)\n",
      "llama_print_timings:       total time = 23163.64 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c5ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647198\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x15302a720\n",
      "ggml_metal_init: loaded kernel_mul                            0x15302bc10\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x15302d160\n",
      "ggml_metal_init: loaded kernel_scale                          0x15302be70\n",
      "ggml_metal_init: loaded kernel_silu                           0x15302dbd0\n",
      "ggml_metal_init: loaded kernel_relu                           0x15302e3b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15302eca0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x15302f530\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x153030860\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x153030ac0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x15302fd70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x153032650\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x153031b50\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x153031db0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1530330c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x153033a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x153034340\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1530353f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x153035d50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x153036990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x153037350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x153037d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x153038930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x153039170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x153039cb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x15303a7e0\n",
      "ggml_metal_init: loaded kernel_rope                           0x15303b3e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x15303bf50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x15303cc30\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a life that has purpose. If you can't figure out what that is, then find something to do and stick with it until you feel like you have found your passion or calling in life. This will make you happy!\n",
      "I am a firm believer that when we feel good about the things that we do, we are more likely to give our best effort to those things which leads to success. We all have a purpose and if we can find it, then I believe most of us will be happier and successful in life. It is hard to get out of bed every morning and go to work if you don't feel like what you are doing has any meaning at all.\n",
      "I know that there are many people who have jobs that they do not like but still do them because they need the money or their family depends on it, but I think we can all agree that most of us would prefer to do something that we enjoy and have some feeling of accomplishment when our day is over. Some people seem to be able to find meaning in things that other people think are mundane and others who work at jobs they love find themselves unfulfilled after a while because their \"job\" becomes too comfortable.\n",
      "Sometimes we need outside influences to remind us of what matters the most, for example when someone is close to death it sometimes seems like all of sudden life takes on different meaning than before and people become thankful for things that they may have taken for granted in the past. I believe that everyone has a purpose and if you are fortunate enough to find yours then you should stick with it as long as it makes you happy!\n",
      "I think that each one of us is born with a certain amount of intelligence, talent, abilities or whatever it may be that fits our personality and helps us to feel fulfilled. When we are in the process of finding ourselves I believe that each of us has some inkling about what that is. We have all heard people say \"I just know\" when they are talking about something important in their life, but many times we don't listen very well or we don't pay attention to our feelings because we are told to do things differently by other people around us.\n",
      "Sometimes it takes a major event like the death of a loved one for us to realize what is really important and sometimes even that doesn't happen until after we have already passed away ourselves. It seems like every time there is a birth in the\n",
      "llama_print_timings:        load time =  2388.66 ms\n",
      "llama_print_timings:      sample time =   680.01 ms /   512 runs   (    1.33 ms per token)\n",
      "llama_print_timings: prompt eval time =  3131.60 ms /   265 tokens (   11.82 ms per token)\n",
      "llama_print_timings:        eval time = 19898.39 ms /   510 runs   (   39.02 ms per token)\n",
      "llama_print_timings:       total time = 23774.21 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b47f5",
   "metadata": {},
   "source": [
    "### 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bbe825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647224\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x128826550\n",
      "ggml_metal_init: loaded kernel_mul                            0x128827a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x128828f90\n",
      "ggml_metal_init: loaded kernel_scale                          0x128829270\n",
      "ggml_metal_init: loaded kernel_silu                           0x128829ab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x128827fe0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12882a420\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12882b390\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12882c6d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12882c930\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12882bbe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12882d2f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x12882e640\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x12882db40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x12882ed80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x12882f820\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x128830170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x128831220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x128831b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1288327c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1288331b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x128833bc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x128834760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x128834f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x128835af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x128836610\n",
      "ggml_metal_init: loaded kernel_rope                           0x128837230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x128837db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x128838a90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.67 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find happiness, and joy, in whatever you do. I’m not saying that everyone should be smiling at all times, but that you should enjoy what you are doing, have fun, smile every once in a while, and appreciate what you have been given in this world.\n",
      "I believe that you should always try to make the best of your situation, even if it is bad. You might not end up with what you were hoping for, but at least you will know that you tried. Don’t live with regrets. That way, when all is said and done, you can look back on your life and say that you didn’t let fear get in the way of success.\n",
      "I believe that everyone should always have a dream, no matter how big or small. If you don’t, then what are you living for? I also believe that you should be willing to accept that your dream might not come true, and be content with it. As long as you try hard, then you can hold your head up high.\n",
      "I believe that people shouldn’t worry about the little things in life, because no matter how much they stress over them, nothing will change. Instead, I think we should spend more time appreciating what we have and not taking it for granted. Because tomorrow isn’t promised to anyone, you might wake up one day and realize that all of your worries were pointless.\n",
      "I believe in doing what makes you happy, even if it means breaking the mold or being different from everyone else around you. It doesn’t matter if everyone is against you; you do what you want to do, and let them deal with it. Life is too short to live someone else’s life for them.\n",
      "I believe that people should be more patient. We all have something that we are waiting for, so why rush? Things will happen when they need to happen, and you can only wait until then. If you try to hurry it along, then things might not happen the way you want them to. Patience is a virtue; if you can learn it, then you’ll be on your way to living a happier life.\n",
      "I believe that everyone should have some sort of faith. No matter what religion you are or what you believe in, just know that there is something out there bigger than all of us. I don’t think we will ever truly understand what it is until the day we die, but I think that\n",
      "llama_print_timings:        load time =  6550.70 ms\n",
      "llama_print_timings:      sample time =   953.12 ms /   512 runs   (    1.86 ms per token)\n",
      "llama_print_timings: prompt eval time =  5579.82 ms /   265 tokens (   21.06 ms per token)\n",
      "llama_print_timings:        eval time = 36872.55 ms /   510 runs   (   72.30 ms per token)\n",
      "llama_print_timings:       total time = 43488.03 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ba9404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647275\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104e27770\n",
      "ggml_metal_init: loaded kernel_mul                            0x104e28c60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104e2a1b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x104e2a490\n",
      "ggml_metal_init: loaded kernel_silu                           0x104e2acb0\n",
      "ggml_metal_init: loaded kernel_relu                           0x104e29200\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104e2b610\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104e2c580\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104e2d8c0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104e2cc30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104e2e1a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104e2f6a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x104e2eb80\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x104e2ff80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x104e301e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x104e30a60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x104e313a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104e32480\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104e326e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104e33ad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104e34550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x104e34f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x104e35970\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x104e363a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x104e36eb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x104e37850\n",
      "ggml_metal_init: loaded kernel_rope                           0x104e38450\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104e38fc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104e39ca0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.67 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "—Pablo Picasso\n",
      "_Dear Future Generation,  \n",
      "This is how we did it_.\n",
      "—Message written on a piece of wood by someone trying to survive in the aftermath of the great fire that destroyed Chicago in 1871\n",
      "In this chapter, I'll talk about why and how I do what I do. This includes my work with the Pachamama Alliance (Pachamama), my personal life mission, and my businesses. I'm sharing these insights because they are part of a story that has led me to where I am today.\n",
      "When I was twenty-eight years old, I lived in a little town called Boulder Creek, California, in the Santa Cruz Mountains outside San Francisco. It was a magical time for me personally and professionally. I had just finished building my dream house: a 170-square-foot cabin that I designed and built myself on an eight-acre parcel of land I owned. I had a small business as a consultant to Fortune 500 companies, which I'd started two years before. And most importantly, I was in love.\n",
      "I met my husband, Scott, while we were working at the same company, a computer hardware manufacturer. We had both been in the industry for several years already and had worked together a few times on some of the largest business deals the company had ever done. He had recently moved to California from upstate New York with his wife and three children and was looking for work. I hired him into my group as soon as he walked through the door, and we spent several months working closely together—without realizing that we were falling in love.\n",
      "I remember distinctly one day when I looked over at Scott and realized how amazing he was. He seemed so intelligent, capable, and interesting, but then I also noticed that he was deeply spiritual, which surprised me because most people I knew who lived on the West Coast weren't very religious or spiritual. It wasn't long before we started dating.\n",
      "We were married in 1987 at a gorgeous little church in the mountains of North Carolina. As part of our wedding ceremony, Scott and I took a vow to do something together that would bring us closer as a couple—something that would be good for both of us and\n",
      "llama_print_timings:        load time =  4767.22 ms\n",
      "llama_print_timings:      sample time =  1025.81 ms /   512 runs   (    2.00 ms per token)\n",
      "llama_print_timings: prompt eval time =  5540.96 ms /   265 tokens (   20.91 ms per token)\n",
      "llama_print_timings:        eval time = 37075.68 ms /   510 runs   (   72.70 ms per token)\n",
      "llama_print_timings:       total time = 43731.14 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5d178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647323\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x152e8be30\n",
      "ggml_metal_init: loaded kernel_mul                            0x152e8d320\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x152e8e870\n",
      "ggml_metal_init: loaded kernel_scale                          0x152e8d580\n",
      "ggml_metal_init: loaded kernel_silu                           0x152e8f2e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x152e8fab0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x152e903a0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x152e90c10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x152e91f40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x152e921a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x152e91450\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x152e92b90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x152e93ef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x152e933a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x152e94790\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x152e950d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x152e95a10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x152e96af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x152e97450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x152e980b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x152e98c10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x152e99450\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x152e99e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x152e9a830\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x152e9b370\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x152e9be80\n",
      "ggml_metal_init: loaded kernel_rope                           0x152e9ca90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x152e9d630\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x152e9e300\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.67 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "Giving one's own life purpose and meaning; this is a fundamental characteristic that defines human beings, the species that has evolved so as to have the capacity for such thoughts and actions.\n",
      "In a sense, we are all creators of our own world.\n",
      "If you look at humanity in this context, then it becomes apparent that there is no one \"right\" or \"wrong\" way to give life meaning: every person's journey (or path) is different. And even though these paths may be divergent and unique from each other, it is a fact of nature that some will overlap with others on certain points, and it is in our nature as human beings, we seek out such overlaps, to establish a community of sorts.\n",
      "For example, the overlapping point between the paths of John Smith and Jane Doe may have something to do with religion (such as Christianity or Islam), politics (liberalism or conservatism) or science (the Big Bang Theory or Intelligent Design). These things are not mutually exclusive, however. Indeed, there is no reason why one cannot be both a Christian and an evolutionist at the same time; this is possible because what we learn from the sciences can have meaning for those who have faith in God, too.\n",
      "I believe that all of these \"overlapping points\" are fundamental to the lives of most people (if not all), simply because it defines our common humanity - and thus our shared purpose as human beings. These things are also what separate us from other animals: even though we have similar needs as other species, such as food, water and shelter, a dog will never be able to understand the significance of the Big Bang Theory, for example.\n",
      "It is with this in mind that I write this blog, which focuses on my personal experiences and thoughts - things that I believe are important and relevant to my life and thus have value to others as well. If you read some of my previous posts, such as Why I Write or 21 Years (Or: The Journey So Far), then it is apparent why I feel this way about giving meaning to the world around me - by writing about what I think is important in life and sharing those thoughts with others through this medium.\n",
      "I hope that you find my musings useful, thought-provoking and maybe even entertaining at times!\n",
      "I write because I have something to say...\n",
      "llama_print_timings:        load time =  4750.63 ms\n",
      "llama_print_timings:      sample time =  1189.15 ms /   512 runs   (    2.32 ms per token)\n",
      "llama_print_timings: prompt eval time =  5566.23 ms /   265 tokens (   21.00 ms per token)\n",
      "llama_print_timings:        eval time = 37313.71 ms /   510 runs   (   73.16 ms per token)\n",
      "llama_print_timings:       total time = 44168.46 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d60297",
   "metadata": {},
   "source": [
    "### 30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e3b4886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647372\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13c912480\n",
      "ggml_metal_init: loaded kernel_mul                            0x13c913970\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13c914ec0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13c9151a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x13c9159e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13c913f10\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13c916350\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13c9172c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13c918600\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13c918860\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13c917b10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13c919220\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13c91a570\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13c919a70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13c91acb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13c91b750\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13c91c0a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13c91d150\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13c91dab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13c91e6f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13c91f0e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13c91faf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13c920690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13c920ec0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13c921a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x13c922540\n",
      "ggml_metal_init: loaded kernel_rope                           0x13c923160\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13c923ce0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13c9249c0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.72 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "—Pablo Picasso\n",
      "I'm not a parent, but I have always wanted to be one. In fact, during my first semester at Princeton University, I wrote an essay entitled \"I Want to Become a Father\" for a class called \"Parenthood.\" There was no textbook, and the professor didn't teach us how to be parents. Instead, he guided us in exploring the meaning of parenthood. And as we worked through the class, I fell deeper and deeper in love with this idea that I could one day create life and then serve it unconditionally.\n",
      "I was raised by my mother, who sacrificed everything for me. She taught me what it meant to be a parent before I ever had kids of my own. And as much as I loved her, there were moments when we clashed, because I didn't understand her decisions. But now that I have my own company—a company where I serve a team of 60 people and am responsible for the well-being of their families—I can see how hard it is to make choices that are best for everyone, even if they are not always perceived as such by your children.\n",
      "My hope is that one day I will be blessed with children who teach me as much about life as my mother did and as our team members do today at SoulCycle. For now, though, the team at SoulCycle is my family, and it is their well-being I think of every single morning when I wake up—even on Sundays.\n",
      "It has been a journey to get here, from writing that essay in college through the ups and downs of building this business with my cofounder, Julie Rice, to where we are today as SoulCycle's chief executive officer (CEO) and chief brand officer (CBO), respectively. But I would not change a thing about our story; it has taught me who I am and how I want to live.\n",
      "As I sat down to write this book, I wanted to share with you not only what we have learned in building SoulCycle into the brand it is today but also the lessons that Julie and I have gleaned from our own personal journeys as mothers, as business partners, and as daughters. While we both began our careers on Wall\n",
      "llama_print_timings:        load time = 16691.74 ms\n",
      "llama_print_timings:      sample time =   927.26 ms /   512 runs   (    1.81 ms per token)\n",
      "llama_print_timings: prompt eval time = 12511.00 ms /   265 tokens (   47.21 ms per token)\n",
      "llama_print_timings:        eval time = 78180.29 ms /   510 runs   (  153.29 ms per token)\n",
      "llama_print_timings:       total time = 91703.42 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c05a4923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647481\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x101a2b2f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x101a2c800\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x101a2dd50\n",
      "ggml_metal_init: loaded kernel_scale                          0x101a2e060\n",
      "ggml_metal_init: loaded kernel_silu                           0x101a2e7f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x101a2cc00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x101a2f0c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x101a301e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x101a31470\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x101a316d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x101a31da0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x101a32000\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x101a333f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x101a328d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x101a33b30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x101a34470\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x101a34db0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x101a35880\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x101a36990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x101a375d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x101a38120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x101a38930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x101a39380\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x101a39db0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x101a3a8d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x101a3b400\n",
      "ggml_metal_init: loaded kernel_rope                           0x101a3c030\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x101a3cb80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x101a3d870\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.72 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your gift is\n",
      "to find your purpose. The meaning of life, therefore, is to fulfill your\n",
      "purpose. If you are blessed enough to have found your purpose in life then\n",
      "you will be forever happy. This is because when you do what you love, you will\n",
      "be doing it with all your heart and soul. You will be giving 100% of yourself to it\n",
      "and not holding back. When you do this you will have a feeling of fulfillment in life.\n",
      "This is the meaning of life to me. It is also what I want out of my own life, which\n",
      "is to find my purpose and use that gift to help others. This is what I am trying to do through my blogs. I hope that my writing can inspire other people who may be going through the same things that I have gone through in the past.\n",
      "Previous Post: How To Find Your Purpose In Life\n",
      "Next Post: What Is The Meaning Of Life Part 2\n",
      "What is the meaning of life? I think we all should find out what it means for ourselves. Our purpose can be different from one another, but if you do figure out your purpose then that’s when you know what the meaning of life is to you. This post makes me think about a lot of things and even though I’m not quite done figuring everything out, this has helped in my journey. Thank you so much for sharing!\n",
      "Hi, Krystle. Thanks for your comment. I agree with you that we should all find our own meaning of life because it is different for everyone. Finding your purpose is very important to find the meaning of life, but there are other things too. The journey can be hard to figure out what your purpose is, but it is possible. If you need any help or encouragement just let me know. Thanks again and have a great day!\n",
      "Thank you for this very motivating article. It’s so true that we should all try to find our own meaning of life. I was always trying hard in school to be good at everything (which is impossible) but now after some time, when I realized my purpose of life, it has become much easier to work on the things I love and do best.\n",
      "Hi, Sharon. Thanks for your comment. The journey can be hard sometimes to find your purpose, which is why I have created my site. You can always come here\n",
      "llama_print_timings:        load time = 16699.46 ms\n",
      "llama_print_timings:      sample time =   846.47 ms /   512 runs   (    1.65 ms per token)\n",
      "llama_print_timings: prompt eval time = 12381.73 ms /   265 tokens (   46.72 ms per token)\n",
      "llama_print_timings:        eval time = 77985.11 ms /   510 runs   (  152.91 ms per token)\n",
      "llama_print_timings:       total time = 91291.74 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d004b329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647590\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x132a265f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x132a27b00\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x132a29050\n",
      "ggml_metal_init: loaded kernel_scale                          0x132a29360\n",
      "ggml_metal_init: loaded kernel_silu                           0x132a29af0\n",
      "ggml_metal_init: loaded kernel_relu                           0x132a27f00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x132a2a3c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x132a2b4e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x132a2c770\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x132a2c9d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x132a2d0a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x132a2d300\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x132a2e6f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x132a2dbd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x132a2ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x132a2f770\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x132a300b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x132a30b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x132a31c90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x132a328d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x132a33420\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x132a33c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x132a34680\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x132a350b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x132a35bd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x132a36700\n",
      "ggml_metal_init: loaded kernel_rope                           0x132a37330\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x132a37e80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x132a38b70\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.72 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "Sometimes I think we all forget that.\n",
      "We get caught up in things, chasing our dreams and trying to make a living and just surviving from day to day. Sometimes it’s overwhelming.\n",
      "But when you stop and really look at your life, are you happy? Do you feel like you have a purpose? Are you fulfilled?\n",
      "I believe we all should be searching for happiness. It doesn’t mean that everything needs to be perfect in our lives for us to achieve it. We can find happiness in the little things. The everyday things. Moments spent with your family and friends and even by yourself.\n",
      "We don’t need big, grand gestures to make us happy. Sometimes we just need to take a step back and appreciate the beauty that is our lives.\n",
      "I was reminded of this on Sunday when I took my son for a walk at the park. We walked through the woods along the creek, stopping occasionally to throw rocks into the water or pick up sticks. He would look up at me with his big smile and laugh and tell me about how much fun he was having.\n",
      "I could have been feeling stressed because I had laundry to do or dishes piled in my sink. But instead I looked around at all of the beauty surrounding us. The cool water sparkling as it rushed over rocks. The trees, green and lush, reaching high into the blue sky. The birds singing happily from their nests. The warm sun shining down on our skin.\n",
      "It was a perfect moment. A beautiful day that reminded me of what life is all about. It’s about spending time with the people you love and making memories. It’s about appreciating nature and being outside. It’s about living in the present, not worrying about the past or future.\n",
      "I believe happiness is a choice that we make for ourselves. If you look hard enough, I know you can find it. Even on your darkest days when everything seems to be going wrong. Look around at all of the beauty surrounding you and appreciate life. Be thankful for everything you have. Focus on the good instead of the bad.\n",
      "I am trying my best to live a happy life. To make decisions that will bring me joy, not sadness. To look forward instead of backward. To focus on what’s important and let go of\n",
      "llama_print_timings:        load time = 12703.11 ms\n",
      "llama_print_timings:      sample time =   498.38 ms /   512 runs   (    0.97 ms per token)\n",
      "llama_print_timings: prompt eval time = 12821.61 ms /   265 tokens (   48.38 ms per token)\n",
      "llama_print_timings:        eval time = 77248.43 ms /   510 runs   (  151.47 ms per token)\n",
      "llama_print_timings:       total time = 90624.45 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a7536",
   "metadata": {},
   "source": [
    "### 65B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "965d4c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647693\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12820d080\n",
      "ggml_metal_init: loaded kernel_mul                            0x12820e570\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12820fac0\n",
      "ggml_metal_init: loaded kernel_scale                          0x12820e7d0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1282105c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x128210cd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x128210f30\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x128211ec0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x128212120\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1282133a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x128212710\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x128213e20\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x128215160\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x128214670\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x128215a30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x128216370\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x128216cb0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x128217d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1282186f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x128219350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x128219eb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x12821a6f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x12821b0e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x12821bad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x12821c610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x12821d110\n",
      "ggml_metal_init: loaded kernel_rope                           0x12821dd40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12821e8f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12821f580\n",
      "ggml_metal_add_buffer: buffer 'data' size 130574155776 is larger than buffer maximum of 115964116992\n",
      "llama_init_from_file: failed to add buffer\n",
      "llama_init_from_gpt_params: error: failed to load model './models/65B/ggml-model-f16.bin'\n",
      "main: error: unable to load model\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb392d",
   "metadata": {},
   "source": [
    "### 65B on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "588abdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689647737\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and spread positivity.\n",
      "I am 27 years old from Mumbai, India. My favorite quote from a book that I love reading is “Wherever you go, go with all your heart.” And my favorite movie is 50 First Dates. I believe the meaning of life is to be happy and spread positivity.\n",
      "I am an actor (film and television), stand-up comedian, singer, and radio jockey. I have also worked in corporate sales for eight years. I also work as a voice-over artist and host events such as awards nights.\n",
      "My biggest achievement so far has been hosting the 62nd Filmfare Awards with Karan Johar (a popular Bollywood director) and my stand-up comedy shows.\n",
      "I was born with a cleft lip that wasn’t treated until I was four years old due to economic difficulties faced by my family at the time. Since then, I have undergone several corrective surgeries, including one last year for scar revision surgery.\n",
      "At first, it was very difficult to adjust to my new appearance and I had a lot of insecurities. However, with age and maturity, I grew comfortable in my own skin. Today, I am proud that I have overcome these challenges and achieved success as an artist. In fact, my cleft lip has become my identity and people recognize me due to my unique smile.\n",
      "I believe the biggest challenge after surgery is facing society when it comes to dating. I have been fortunate enough to not face any discrimination in my career or from friends and family, but I do feel that sometimes people are scared of making a commitment because they don’t know what the future holds for them.\n",
      "I want to tell everyone out there who is facing similar challenges that you should love yourself first before expecting anyone else to do so. And it is important to have positive and supportive people around you at all times. Your happiness depends on how you view life, not by how others perceive you!\n",
      "My dream for the future is that no child born with a cleft lip or palate should ever feel shy about their appearance because there is nothing wrong with being different. It is my goal to spread awareness and create a positive impact in people’s lives. And I want to continue working hard towards achieving my goals as an artist and inspire others.\n",
      "\n",
      "llama_print_timings:        load time = 52244.54 ms\n",
      "llama_print_timings:      sample time =   360.88 ms /   512 runs   (    0.70 ms per token)\n",
      "llama_print_timings: prompt eval time = 26120.77 ms /   265 tokens (   98.57 ms per token)\n",
      "llama_print_timings:        eval time = 293148.68 ms /   510 runs   (  574.80 ms per token)\n",
      "llama_print_timings:       total time = 319676.77 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca999889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689648186\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift and the purpose of life is to give it away.\n",
      "I believe God loves us all, no exceptions.\n",
      "I believe that we can do anything we put our minds to, with the right resources.\n",
      "I believe in the power of choice; we are free to choose our own way.\n",
      "I believe that you attract what you think about most of the time.\n",
      "I believe in the power of love and faith.\n",
      "And I believe that the way to a better world is through gratitude, kindness and compassion towards all living things.\n",
      "My name is Julie A. Fast and I am an author, speaker, coach and consultant specializing in bipolar disorder, schizoaffective disorder and schizophrenia. My goal is to help people with mood disorders live the lives they want using mainstream and alternative treatment options.\n",
      "I have two books on Amazon: Loving Someone with Bipolar Disorder: Understanding and Helping Your Partner, Take Charge of Bipolar Disorder: A Four Step Plan for You and Your Loved Ones to Manage the Illness and Create Lasting Stability. I also co-wrote a book with John Preston called The Health Cards Treatment System for Bipolar Disorder: WIN your battle against bipolar disorder using the 10 health cards system.\n",
      "I have a DVD on YouTube called Bipolar Disorder and Schizophrenia: Helping Loved Ones with Mental Illness. You can watch it right here! Please subscribe to my channel at Julie Fast Youtube Channel and click on the bell for updates. And please leave comments. I love them!\n",
      "My books are available around the world in bookstores, libraries and online retailers such as Amazon.com.\n",
      "I’m a regular blogger on bpHope Magazine. You can read my recent articles here: Julie Fast Blog bphope.\n",
      "I also have a regular column with Healthy Place Bipolar Network called The Stability Zone that focuses on family and relationship issues as well as self-help techniques for those with mood disorders. This article series is available in all major languages.\n",
      "I work with clients worldwide via phone and Skype. I also travel internationally to speak at conferences, hospitals and events. I am a frequent guest expert on\n",
      "llama_print_timings:        load time = 51041.51 ms\n",
      "llama_print_timings:      sample time =   362.04 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 26050.99 ms /   265 tokens (   98.31 ms per token)\n",
      "llama_print_timings:        eval time = 300075.53 ms /   510 runs   (  588.38 ms per token)\n",
      "llama_print_timings:       total time = 326535.23 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72124b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689648566\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.21 MB\n",
      "llama_model_load_internal: mem required  = 128109.21 MB (+ 5120.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away. - Pablo Picasso\n",
      "I want to thank you for taking the time to visit my site and for your interest in me as a wedding photographer. My goal is to be able to provide you with an experience that will make you say, \"We LOVE our photos!\"\n",
      "I believe that each person has their own unique story to tell. And I would love to capture yours through beautiful, timeless images. My style can best be described as natural, documentary-style photography with a bit of classic elegance thrown in for good measure. I strive to create photographs that will be cherished by you and your families for generations to come.\n",
      "I'm very blessed to have the opportunity to do what I love - wedding photography! I would love to talk more about it with you. Please feel free to contact me directly at 417-503-6298 or by email at jim@jimgarnerphotographer.com.\n",
      "I look forward to hearing from you. Have a great day!\n",
      "Jim Garner Photography is based in Springfield, Missouri and serves the surrounding areas of Ozark, Nixa, Branson and Joplin. Jim has also traveled throughout the United States photographing weddings on location in St. Louis, Kansas City, Chicago, Nashville, Memphis, Destin, Florida, and Scottsdale, Arizona.\n",
      "Jim is available for destination wedding photography worldwide! He has photographed weddings in Belize, Mexico, Costa Rica, Canada, New Zealand, Australia, Ireland and England. And he would love to talk with you about your travel plans. Please give him a call at 417-503-6298 or email him directly at jim@jimgarnerphotographer.com.\n",
      "Member of the Professional Photographers of America (PPA) and Missouri Professional Photographers Association (MPPA).\n",
      "Copyright © Jim Garner Photography | Springfield, MO Wedding Photographer | Design by La Lune Creative|ProPhoto Photo Theme by NetRivet Websites\n",
      "1485 E Woodland St, Ozark, MO 65721 - 417-503-6298 - jim\n",
      "llama_print_timings:        load time = 50200.25 ms\n",
      "llama_print_timings:      sample time =   357.28 ms /   512 runs   (    0.70 ms per token)\n",
      "llama_print_timings: prompt eval time = 26025.51 ms /   265 tokens (   98.21 ms per token)\n",
      "llama_print_timings:        eval time = 298108.99 ms /   510 runs   (  584.53 ms per token)\n",
      "llama_print_timings:       total time = 324538.38 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
