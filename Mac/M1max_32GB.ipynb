{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     ggml-vocab.bin          \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191dea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "embd-input-test\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-metal.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders \n",
      "c++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074695\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.95 MB\n",
      "llama_model_load_internal: mem required  = 3949.95 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x129634dc0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1296362b0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x129637800\n",
      "ggml_metal_init: loaded kernel_scale                          0x129636510\n",
      "ggml_metal_init: loaded kernel_silu                           0x129638300\n",
      "ggml_metal_init: loaded kernel_relu                           0x129638a10\n",
      "ggml_metal_init: loaded kernel_gelu                           0x129638c70\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x129639c00\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x129639e60\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12963b0e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12963a450\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12963bb60\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12963cea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12963c3b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12963d770\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12963e0b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12963e9f0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12963fad0\n",
      "ggml_metal_init: loaded kernel_norm                           0x129640450\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1296410e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x129641ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1296424d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x129642ea0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x129643880\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1296443a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x129644cc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x129645690\n",
      "ggml_metal_init: loaded kernel_rope                           0x129645ea0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x129646e20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1296479c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x129648550\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1296490b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB, ( 3648.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.41 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be fully alive and happy. There are different ways of achieving this, but they all lead me back to my love for people, animals, and nature.\n",
      "I was born in the south of Romania, in a small town called Ploiesti. My parents named me Ioana (Jona), because it’s the name of an angel that brings hope. I am very glad they chose this name, as my life has been full of opportunities to be happy and to give happiness to others.\n",
      "My father was a mechanical engineer who loved to take us out in nature, and he taught me how to make a fire without matches (I still can). My mother was an accountant who never worked outside home but she managed the house very well and took great care of us. I am so happy that my parents raised me with values like being kind and caring for others; it is one of the most important things in life.\n",
      "My favorite pastime as a child was to ride my bicycle, play with my dolls, read, draw, and go camping with my parents. I had many pets: dogs, cats, hamsters, birds…I loved them all! As I grew up I wanted to help people in need. This is why I am studying medicine, as well as I am volunteering for a charity that helps homeless people (The Homeless Foundation).\n",
      "My first love was my boyfriend of 3 years, but we broke up after he cheated on me with another girl. When I met Jesse in November 2016, I felt like I had found the man who understood me and loved me for who I am. We dated long distance (I lived in London and he was in LA) until we could live together in LA this past April.\n",
      "Jesse is one of my best friends and I love him very much. He loves animals just as much as I do. Together, we have 2 dogs, a cat, a cockatiel, and a bearded dragon. We love to go camping, hiking, swimming…we like outdoor activities, especially when they involve nature and animals.\n",
      "I am very happy with my life and I hope to live it to the fullest!\n",
      "Hannah is a 26 year old photographer who lives in Los Angeles, CA. She has been shooting weddings for 5 years now and her passion lies\n",
      "llama_print_timings:        load time =  1191.70 ms\n",
      "llama_print_timings:      sample time =   534.92 ms /   512 runs   (    1.04 ms per token,   957.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4864.92 ms /   265 tokens (   18.36 ms per token,    54.47 tokens per second)\n",
      "llama_print_timings:        eval time = 12070.17 ms /   510 runs   (   23.67 ms per token,    42.25 tokens per second)\n",
      "llama_print_timings:       total time = 17530.17 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074714\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.95 MB\n",
      "llama_model_load_internal: mem required  = 3949.95 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x14f81f3a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x14f820890\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x14f821de0\n",
      "ggml_metal_init: loaded kernel_scale                          0x14f820af0\n",
      "ggml_metal_init: loaded kernel_silu                           0x14f822850\n",
      "ggml_metal_init: loaded kernel_relu                           0x14f823020\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14f823910\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14f824180\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14f8254b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14f825710\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14f8249c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14f826100\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14f827460\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14f826910\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14f827d00\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14f828640\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14f828f80\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14f82a060\n",
      "ggml_metal_init: loaded kernel_norm                           0x14f82a9e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14f82b670\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14f82c070\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14f82ca60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14f82d430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14f82de10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x14f82e930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x14f82f250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x14f82fc20\n",
      "ggml_metal_init: loaded kernel_rope                           0x14f830430\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x14f8313b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14f831f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14f832ae0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14f833640\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB, ( 3648.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.41 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift, make a living out of it and then give back.\n",
      "I am a graduate from New York University with a degree in Economics and Finance; and have worked as an analyst at Goldman Sachs. After being laid off during the financial crisis, I became a consultant for other firms on Wall Street. However, my heart was never on Wall Street; it was always in Africa.\n",
      "In 2013, a friend who studied abroad in Kenya introduced me to a charity that helped young girls with school fees. This inspired me to raise money for a cause I cared about: education. In the following year, I managed to collect over $5000 which was used to sponsor 25 girls attending high school in Kenya.\n",
      "In March of 2014, I decided to go to Ghana on my own to see how the charity worked and what else I could do there. While there, I saw first-hand the plight of children living in orphanages and those who were abused by their caretakers. This inspired me to start up an NGO called “The Gathering.”\n",
      "“The Gathering” is a charity whose mission is to help provide education and opportunities for underprivileged children in Africa. We work closely with the orphanages on the ground to help ensure that they are financially stable so that they can continue to take care of our beneficiaries. In addition, we also raise money for them by organizing events here in the US such as benefit dinners and charity concerts featuring African music artists.\n",
      "To date, “The Gathering” has helped sponsor the education of over 100 children across three orphanages in Ghana. We have raised funds for them through our various fundraising activities. For example, we hosted a benefit dinner at a prominent restaurant in New York City and also held two African music concerts.\n",
      "To learn more about “The Gathering,” click here: thegatheringafrica.org\n",
      "Follow “The Gathering” on Twitter @thegathering_afr\n",
      "Learn More About Ola\n",
      "Visit TheGatheringAfrica.org\n",
      "Contact Ola at olad@thegatheringafrica.org or call 646-237-9057\n",
      "“The Gathering\n",
      "llama_print_timings:        load time =   836.33 ms\n",
      "llama_print_timings:      sample time =   573.25 ms /   512 runs   (    1.12 ms per token,   893.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4797.41 ms /   265 tokens (   18.10 ms per token,    55.24 tokens per second)\n",
      "llama_print_timings:        eval time = 12189.42 ms /   510 runs   (   23.90 ms per token,    41.84 tokens per second)\n",
      "llama_print_timings:       total time = 17626.13 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074732\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.95 MB\n",
      "llama_model_load_internal: mem required  = 3949.95 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x15271f6a0\n",
      "ggml_metal_init: loaded kernel_mul                            0x152720b90\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1527220e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x152720df0\n",
      "ggml_metal_init: loaded kernel_silu                           0x152722b50\n",
      "ggml_metal_init: loaded kernel_relu                           0x152723330\n",
      "ggml_metal_init: loaded kernel_gelu                           0x152723c20\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1527244a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1527257e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x152725a40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1527260d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1527264a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x152726ac0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x152726d20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x152728060\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x152728970\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1527292b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x15272a360\n",
      "ggml_metal_init: loaded kernel_norm                           0x15272ace0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x15272b960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x15272c370\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x15272cdd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x15272d7d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x15272e170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x15272ecc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x15272f770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x152730150\n",
      "ggml_metal_init: loaded kernel_rope                           0x1527309f0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x152731970\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x152732bb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x152733700\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1527342a0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB, ( 3648.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, ( 3658.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.41 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.41 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give purpose to one’s existence and find some kind of happiness in this life.\n",
      "I also believe that we all have different purposes, and should never forget to do something for someone else or at least help someone if you can.\n",
      "The above are my beliefs; not really a religion – more like an attitude about life. I am not a Christian, but I do believe strongly in God and Jesus. But I’m also very open-minded and have no wish to convert anyone to any particular faith or viewpoint.\n",
      "I don’t think that it is for me to impose my views on anyone else unless they ask me about them; and then if they ask, I will give my view in a non-confrontational manner – but I won’t try to change their mind if they are happy with their religion or belief system.\n",
      "I am a very open person and I value the views of others regardless of their religious beliefs. But I don’t think that any religion can be totally right, and I do not agree with those who say that anyone who is not in their religion must go to hell for eternity.\n",
      "I also believe that we need to have a healthy respect for each other’s views; but above all else, I believe that love is the most important thing of all – and that people should always try to be kind and considerate to others at all times.\n",
      "And one more thing: I am very proud to be an Australian, and I think Australia has a great future as long as it stays true to its roots. We must never forget where we come from, or how lucky we are to have been born in this great country.\n",
      "I believe that I was put on this earth for a reason; and I will use my gifts wisely, I hope, so that some good may be able to come out of them for others.\n",
      "So that is the long version of what I believe. But I also want to make it clear that this is not a site for proselytising or ‘preaching’, and I will only ever share my beliefs here if someone asks me about them first – and even then, I won’t be too pushy.\n",
      "I am just an ordinary guy who has lived through some interesting times in life so far; but I have also seen the good side of human nature in many ways. And I will continue to share my experiences with you as they happen.\n",
      "So please feel\n",
      "llama_print_timings:        load time =   836.39 ms\n",
      "llama_print_timings:      sample time =   580.60 ms /   512 runs   (    1.13 ms per token,   881.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4807.96 ms /   265 tokens (   18.14 ms per token,    55.12 tokens per second)\n",
      "llama_print_timings:        eval time = 12165.18 ms /   510 runs   (   23.85 ms per token,    41.92 tokens per second)\n",
      "llama_print_timings:       total time = 17618.63 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074751\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x122730bb0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1227320a0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1227335f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x122732300\n",
      "ggml_metal_init: loaded kernel_silu                           0x1227340f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x122734800\n",
      "ggml_metal_init: loaded kernel_gelu                           0x122734a60\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1227359f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x122735c50\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x122736ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x122736240\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x122737950\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x122738c90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1227381a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x122739560\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x122739ea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12273a7e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12273b8c0\n",
      "ggml_metal_init: loaded kernel_norm                           0x12273c240\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12273ced0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12273d8d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12273e2c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12273ec90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12273f670\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x122740190\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x122740ab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x122741480\n",
      "ggml_metal_init: loaded kernel_rope                           0x122741c90\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x122742c10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1227437b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x122744340\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x122744ea0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.56 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. Happiness can also be defined as “the state of being happy,” which is “a feeling of great pleasure, joy or happiness.” This definition describes what we expect from living: an enjoyable experience filled with positive emotions and excitement.\n",
      "My mom used to say “Life is a box of chocolates, you never know what you’re gonna get.” I would always take that literally, so when I got to high school, I was prepared for the unexpected and had no expectations. My goal in life was just to make it through each day with as much laughter as possible, so when I started looking at colleges, I wanted a place where there were fun activities going on all the time that would keep me from getting bored.\n",
      "The school also has an amazing athletic program and great facilities for my sports of choice: basketball and volleyball. Last but definitely not least is the music department, which is what originally attracted me to the University of Rhode Island. Music is one of my passions. I’ve been playing clarinet since sixth grade, and in high school, I played with the wind ensemble, symphonic band, jazz band, concert band, pep band and marching band.\n",
      "I was also a member of The Rhody Express, URI’s co-ed a cappella group, for three years. I had an amazing time singing on stage every week and making memories with my fellow Rams, but this semester I decided to take some time off from performing in order to do more playing. Being around so many talented musicians has motivated me to practice more, which is something that I have been wanting to do for a while now.\n",
      "My parents always said “It’s not about what you know, it’s about who you know.” My mom had a friend who graduated URI and told my mom all the amazing things she had learned from different classes such as psychology, sociology and history. That is how I decided on my major: psychology. One of the greatest perks of being in this major is that it gives me time to do what I love most: play music.\n",
      "This past summer, I was fortunate enough to be a part of URI’s band, which traveled all over New England and even went as far as Boston, Massachusetts for a concert with the Boston Pops Orchestra. My favorite memory from this trip is when\n",
      "llama_print_timings:        load time =  3836.72 ms\n",
      "llama_print_timings:      sample time =  1186.86 ms /   512 runs   (    2.32 ms per token,   431.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5281.73 ms /   265 tokens (   19.93 ms per token,    50.17 tokens per second)\n",
      "llama_print_timings:        eval time = 34448.87 ms /   510 runs   (   67.55 ms per token,    14.80 tokens per second)\n",
      "llama_print_timings:       total time = 41038.00 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074796\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12164fc40\n",
      "ggml_metal_init: loaded kernel_mul                            0x121651430\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x121652b00\n",
      "ggml_metal_init: loaded kernel_scale                          0x121652e10\n",
      "ggml_metal_init: loaded kernel_silu                           0x121653660\n",
      "ggml_metal_init: loaded kernel_relu                           0x121651a00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x121653f10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x121654f30\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x121656260\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1216564c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1216557b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x121656e90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x121658200\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x121657700\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x121658a90\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1216593d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x121659d50\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12165ae30\n",
      "ggml_metal_init: loaded kernel_norm                           0x12165b790\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12165c460\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12165ce80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12165d890\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12165e270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12165ec70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12165f750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12165fef0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1216608d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1216612c0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x121662240\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x121662dc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x121663980\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1216644f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.56 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. To experience as much of this wondrous world and being alive as you can. When we stop and think about what we want out of life, we are thinking too much. The best way to be happy in life is not by looking ahead, but by living in the moment; making every day count.\n",
      "I always thought my purpose was to help people, but I now believe that this may only be a part of it. I can do so many different things in life: work with animals, help the environment, make money, travel… There are endless opportunities out there and the best way to find your own meaning is by simply trying new things until you stumble upon something you love. Do not waste time worrying about what you ‘should be doing’ or how other people perceive you; just live each day as if it were your last, so that when your days are over, you can say: I lived my life to the fullest.\n",
      "What is your purpose in life? Let me know below!\n",
      "Previous Post What is Success?\n",
      "Next Post My Favourite Things\n",
      "4 thoughts on “How do you find your purpose?”\n",
      "I’ve asked myself this question a few times too but I think it is so hard to answer because there are so many different answers. But when you say the best way is just to live in the moment, that’s very true! We should not worry about our future or what we want for our lives or where we should be and just focus on being happy and enjoying each day ����\n",
      "I think it’s hard to find your purpose because sometimes you are so caught up in what other people want from you that you forget that we have the power to shape our own life. But I agree, being in the moment is probably the best thing ever! Xo\n",
      "Alyssa @ The Alx & Aly Show says:\n",
      "I love this post! It’s so true – there are endless opportunities and it’s important not to waste time worrying about what we “should be doing” or how other people perceive us. Do what makes you happy! ����\n",
      "Thank you so much for your kind words! I agree, there are so many choices in life that it’s easy to get lost and forget what we really want to do. But when we live in the moment we can find ourselves again. Xo\n",
      "I think everyone wants\n",
      "llama_print_timings:        load time =  3735.38 ms\n",
      "llama_print_timings:      sample time =  1266.42 ms /   512 runs   (    2.47 ms per token,   404.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5336.24 ms /   265 tokens (   20.14 ms per token,    49.66 tokens per second)\n",
      "llama_print_timings:        eval time = 34512.90 ms /   510 runs   (   67.67 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time = 41242.59 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074842\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x14d88caa0\n",
      "ggml_metal_init: loaded kernel_mul                            0x14d88df90\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x14d88f4e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x14d88e1f0\n",
      "ggml_metal_init: loaded kernel_silu                           0x14d88ffe0\n",
      "ggml_metal_init: loaded kernel_relu                           0x14d8906f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14d890950\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14d891930\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14d891b90\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14d891fd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14d892230\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14d893880\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14d894bf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14d8940f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14d895330\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14d895c90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14d8965c0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14d897030\n",
      "ggml_metal_init: loaded kernel_norm                           0x14d897ad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14d898dd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14d8997f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14d89a210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14d89abd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14d89b5b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x14d89c0c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x14d89c860\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x14d89d200\n",
      "ggml_metal_init: loaded kernel_rope                           0x14d89dbc0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x14d89ebd0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14d89f720\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14d8a02a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14d8a0e00\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.56 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it, and that means living in the moment. It’s easy for me to be a happy person because I enjoy every second of my day-to-day activities. My goal is to not take anything for granted.\n",
      "My favorite part about being at Clemson has been getting to know all of my fellow students who are so passionate and driven. I am fortunate enough to have gotten a job working in the Office of Undergraduate Admissions, which allows me to connect with prospective students and their families from around the world.\n",
      "I chose Clemson because it was the perfect fit for my needs as an international student pursuing a degree in business. I wanted the academic challenge Clemson offered while still being able to get involved with activities on campus. There is no better place than Clemson University!\n",
      "My favorite class at Clemson has been Principles of Management, because it taught me how to be an effective leader and manager. The coursework for this class allowed me to learn to lead my peers in a fun way. I have really enjoyed being able to apply the lessons I learned in that class to my work with admissions.\n",
      "I am looking forward to completing my last year at Clemson, learning as much as I can from all of the courses I’m taking this semester and being able to travel abroad next summer on a study abroad trip. After graduation I hope to get back into international business, either through an MBA or by working for a multinational company with a focus in Asia.\n",
      "My advice for incoming freshmen would be to learn as much as you can about the university before attending and take advantage of all that Clemson has to offer while on campus. Also, make sure to get involved in different activities around campus! There are so many opportunities waiting here for you, and you never know who you will meet or where your interests may lead you.\n",
      "I chose the CU150 theme because it is a year of celebration at Clemson University, and I am excited to take part in events that showcase all the amazing things our university has accomplished over the past 150 years. It has been an honor to be a part of this celebration!\n",
      "My favorite moment since being at CU was when I performed with the University Choir for the first time as a freshman. As someone who did not participate in marching band or choir\n",
      "llama_print_timings:        load time =  3692.69 ms\n",
      "llama_print_timings:      sample time =  1247.29 ms /   512 runs   (    2.44 ms per token,   410.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5341.30 ms /   265 tokens (   20.16 ms per token,    49.61 tokens per second)\n",
      "llama_print_timings:        eval time = 34390.16 ms /   510 runs   (   67.43 ms per token,    14.83 tokens per second)\n",
      "llama_print_timings:       total time = 41099.03 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074887\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.00 MB\n",
      "llama_model_load_internal: mem required  = 7390.00 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x129691160\n",
      "ggml_metal_init: loaded kernel_mul                            0x129692650\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x129693ba0\n",
      "ggml_metal_init: loaded kernel_scale                          0x129693e80\n",
      "ggml_metal_init: loaded kernel_silu                           0x129694670\n",
      "ggml_metal_init: loaded kernel_relu                           0x129692bf0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x129695040\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x129695fe0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x129697320\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x129696690\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x129697c30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x129699130\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x129698780\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x129699a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12969a310\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12969ac50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12969b5b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12969bec0\n",
      "ggml_metal_init: loaded kernel_norm                           0x12969c840\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12969dd00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12969e720\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12969f110\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12969faf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1296a0610\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1296a0f40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1296a18e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1296a22d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1296a2eb0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1296a3a80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1296a4630\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1296a51a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1296a5d20\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB, ( 7024.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 7036.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.45 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I'm in a good place, mentally, and I want to keep it that way. And with me, I feel like if I'm not training hard or if I'm not working as much as I can, then I get into trouble.\n",
      "In the future, there will be no female Mick Jaggers or Keith Richards, because they are men. They have a Y chromosome, so their hormones rage, and that is what creates rock 'n roll. You take those hormones away, and you're not going to get it from women.\n",
      "I love my work and I always want to do something new.\n",
      "My father used to tell me man provides, God provides - woman just complains.\n",
      "I'm a very ambitious person. I like the world to be in order because when things are in their place, then you can focus on other things.\n",
      "When I go out with my girlfriends now, we're like, 'What did you do today?' And they say, 'Oh, I took my kid here and there.' And I'm like, 'Where? What for?' They don't know what to tell me because I've never been a mother.\n",
      "I'd like to be remembered as a human being who cared about other human beings.\n",
      "God gave us women intuition and femininity. But he also gave us intellect. So, I say, use both sides of the brain.\n",
      "The biggest tragedy in my life was Celine Dion. She ruined my image of myself.\n",
      "You need to have people around you that you trust and love. That's the only way for a person to grow.\n",
      "I am in favor of legalizing drugs. According to my values system, if people want to kill themselves, they have every right to do so; accordingly, if they want to get high, they can get high.\n",
      "Sometimes I think I should be a little more like Madonna and do what she does, make herself the center of attention all the time, but that's just not me.\n",
      "I don't really believe in luck. I believe everything is a consequence - either good or bad - of something you do or didn't do. So when I was singing, it was because I wanted to be discovered. It was my decision to go\n",
      "llama_print_timings:        load time =  2079.99 ms\n",
      "llama_print_timings:      sample time =  1151.24 ms /   512 runs   (    2.25 ms per token,   444.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8728.21 ms /   265 tokens (   32.94 ms per token,    30.36 tokens per second)\n",
      "llama_print_timings:        eval time = 20779.10 ms /   510 runs   (   40.74 ms per token,    24.54 tokens per second)\n",
      "llama_print_timings:       total time = 30778.71 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074920\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.00 MB\n",
      "llama_model_load_internal: mem required  = 7390.00 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13e728110\n",
      "ggml_metal_init: loaded kernel_mul                            0x13e729600\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13e72ab50\n",
      "ggml_metal_init: loaded kernel_scale                          0x13e729860\n",
      "ggml_metal_init: loaded kernel_silu                           0x13e72b5c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13e72bd90\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13e72c680\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13e72cef0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13e72e220\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13e72e480\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13e72d730\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13e72ee70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13e7301d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13e72f680\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13e730a70\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13e7313b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13e731cf0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13e732dd0\n",
      "ggml_metal_init: loaded kernel_norm                           0x13e733750\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13e7343e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13e734de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13e7357d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13e7361a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13e736b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13e7376a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13e737fc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13e738990\n",
      "ggml_metal_init: loaded kernel_rope                           0x13e7391a0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13e73a120\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13e73acc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13e73b850\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13e73c3b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB, ( 7024.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 7036.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.45 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and content. Happiness comes from within; it is not dependent on external things or people except insofar as they may act as a mirror to our selves, showing us what we are doing right, or wrong. We know when we have been true to ourselves and others because we feel good about ourselves and the way we've behaved, no matter what the circumstances or who was involved.\n",
      "# The Meaning of True Happiness\n",
      "The meaning of life is to be happy. Contentment, or happiness—not just in a superficial sense but at a deep level—is what we all want. We all have moments when we feel good and happy, but these are only temporary. Our deeper nature is one of blissful joy. This is the true meaning of life: to be happy.\n",
      "True happiness has nothing to do with egoism or being self-centered; it has everything to do with seeing reality for what it is—the play of existence, and realizing that we are part of this existence. True happiness does not depend on things going a certain way, because if they don't go the way we want them to, then we will get upset. We can't control everything in life, but there is one thing we can control—our own mind and our attitude toward things. This is where true happiness begins: when we are able to look at life as it really is, without judging or evaluating what happens to us.\n",
      "A friend of mine once told me a story about the Dalai Lama that illustrates this beautifully. He said that when he was younger, he had an opportunity to speak with His Holiness, and he asked him how Buddhism differs from other religions. The Dalai Lama replied, \"We can't control the external world, but we can always control our own minds.\" This is the essence of being happy—it has nothing to do with external things.\n",
      "# Happiness Is Not Just a Feeling\n",
      "Happiness is not just a feeling; it is deeper than that. It comes from knowing who we truly are and what our true nature really is. When our minds become clear, we will see life as it really is: the play of existence. This understanding gives us great insight into how to live in the world without becoming attached to external things or people.\n",
      "This does not mean that everything will be perfect from now on; far from it!\n",
      "llama_print_timings:        load time =  1566.90 ms\n",
      "llama_print_timings:      sample time =   981.06 ms /   512 runs   (    1.92 ms per token,   521.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8623.49 ms /   265 tokens (   32.54 ms per token,    30.73 tokens per second)\n",
      "llama_print_timings:        eval time = 20795.39 ms /   510 runs   (   40.78 ms per token,    24.52 tokens per second)\n",
      "llama_print_timings:       total time = 30499.39 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074952\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.00 MB\n",
      "llama_model_load_internal: mem required  = 7390.00 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1411773e0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1411788d0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x141179e20\n",
      "ggml_metal_init: loaded kernel_scale                          0x141178b30\n",
      "ggml_metal_init: loaded kernel_silu                           0x14117a890\n",
      "ggml_metal_init: loaded kernel_relu                           0x14117b060\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14117b950\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14117c1c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14117d4f0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14117d750\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14117ca00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14117e140\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14117f4a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14117e950\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14117fd40\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x141180680\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x141180fc0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1411820a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x141182a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1411836b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1411840b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x141184ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1411854b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x141185ea0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x141186a00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1411874e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x141187eb0\n",
      "ggml_metal_init: loaded kernel_rope                           0x141189140\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x141189d00\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14118a890\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14118b3e0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14118bf40\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB, ( 7024.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 7036.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.45 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\"\n",
      "\"You've gotta know what you want... And then you've got to keep after it, no matter how hard it gets.\"\n",
      "\"A business has a responsibility that goes beyond the bottom line.\"\n",
      "~ James Goldsmith\n",
      "\"The only thing worse than training an employee and having him leave is not training an employee and having him stay.\"\n",
      "\"We don't rise to our level of expectations, we fall to our level of training. \"\n",
      "~ Archilochus\n",
      "\"Any man who can drive safely while kissing a pretty girl is simply not giving the kiss the attention it deserves.\"\n",
      "\"I know I have no talent for writing, but I can always think of a good title.\"\n",
      "\"The difference between genius and stupidity is that genius has its limits.\"\n",
      "~ Alexander Pushkin\n",
      "\"If you are going to be thinking, you may as well think big!\"\n",
      "\"A bank is a place where they lend you an umbrella in fair weather and demand it back when it begins to rain. \"\n",
      "~ Robert Frost\n",
      "\"Whenever I climb I am followed by a dog called 'Ego'.\"\n",
      "\"The best time to plant a tree was 20 years ago. The second best time is now.\"\n",
      "\"If we don't succeed, we run the risk of failure.\"\n",
      "\"I think it is possible for ordinary people to choose to be extraordinary.\"\n",
      "~ Elie Weisel\n",
      "\"It doesn't matter what people say about you or how they treat you. What matters is your reaction to them. If you react with bitterness, you defeat yourself. It's not the people who hurt you that count-it's the people who help and comfort you.\"\n",
      "~ Ayn Rand\n",
      "\"It is never too late to be what you might have been.\"\n",
      "\"What lies behind us and what lies before us are tiny matters compared to what lives within us\".\n",
      "\"Don't judge a man until you've walked a mile in his shoes. After that, who cares? He's a mile away and you've got his shoes.\"\n",
      "~ Billy Connolly\n",
      "\"When the wind changes direction, some people just build a new shed.\"\n",
      "~ Anonymous (but I like it!)\n",
      "\"A pessimist is one who makes difficulties of opportunities\n",
      "llama_print_timings:        load time =  1548.40 ms\n",
      "llama_print_timings:      sample time =   917.20 ms /   512 runs   (    1.79 ms per token,   558.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8592.96 ms /   265 tokens (   32.43 ms per token,    30.84 tokens per second)\n",
      "llama_print_timings:        eval time = 21172.66 ms /   510 runs   (   41.52 ms per token,    24.09 tokens per second)\n",
      "llama_print_timings:       total time = 30774.01 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690074985\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1377082e0\n",
      "ggml_metal_init: loaded kernel_mul                            0x137709b40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13770b210\n",
      "ggml_metal_init: loaded kernel_scale                          0x13770b520\n",
      "ggml_metal_init: loaded kernel_silu                           0x13770bd20\n",
      "ggml_metal_init: loaded kernel_relu                           0x13770a030\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13770c620\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13770d6f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13770e9d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13770ec30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13770f300\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13770f560\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x147724850\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1477255e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x147725b40\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x147726560\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x147726e90\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x147727810\n",
      "ggml_metal_init: loaded kernel_norm                           0x147728160\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x147729890\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14772a290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14772acf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14772b6b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14772c090\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1330143f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x133014e20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1330153a0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1330164d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x133017500\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x133017760\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1330182b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x133018e20\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  8755.20 MB, offs =  16852172800, (25139.66 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, (25151.66 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25553.66 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25715.66 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25907.66 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33mggml_metal_graph_compute: command buffer 5 failed with status 5\n",
      "GGML_ASSERT: ggml-metal.m:997: false\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690075010\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give yourself fully to something.\n",
      "Even if it means giving up your dreams, or your goals, or your pride. Even if it hurts.\n",
      "Even if you have to let go of relationships that are hurting you. Or leave behind the past so you can embrace a new future. Even if it’s inconvenient. Even if it changes everything. And even if it makes you feel vulnerable, or weak.\n",
      "Giving yourself fully means giving up on fear and doubt. It means being open to love. It means being willing to take risks. It means taking care of your body and your mind so that you can be the best possible version of yourself. It means making time for people who bring out the best in you, even if it means spending less time with those who don’t.\n",
      "It means loving your life, even when it’s hard, or uncomfortable. Even when you feel like giving up, or like nothing is going right. It means being present and grateful for every single day that you are alive on this planet.\n",
      "It means making a difference in the world. It means following your passions and doing what makes you happy. It means finding purpose in your pain. And it means trusting God with all of your heart.\n",
      "Giving yourself fully doesn’t mean giving up control, or giving into the fears that hold us back from truly living our lives.\n",
      "It means being brave enough to let go and give it all you’ve got. It means daring to dream big and fight for what matters most. It means loving with everything you have and not holding anything back. And it means having faith in the process of life, even when times are tough.\n",
      "Giving yourself fully is about living without regrets. Giving yourself fully is about being the best version of yourself that you can possibly be. So don’t give up on your dreams. Don’t give up on love. And don’t give up on yourself.\n",
      "Because if there’s one thing I know for sure, it’s this: Life is short. Make every day count and keep giving yourself fully to the things that matter most.\n",
      "And when you do, even though life isn’t perfect, it will be a whole lot better than you ever dreamed possible.\n",
      "I love this so much! You are always such an inspiration! I wish my mom would have read this to me\n",
      "llama_print_timings:        load time = 24057.63 ms\n",
      "llama_print_timings:      sample time =   365.10 ms /   512 runs   (    0.71 ms per token,  1402.37 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13178.03 ms /   265 tokens (   49.73 ms per token,    20.11 tokens per second)\n",
      "llama_print_timings:        eval time = 120952.86 ms /   510 runs   (  237.16 ms per token,     4.22 tokens per second)\n",
      "llama_print_timings:       total time = 134552.51 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690075170\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to understand and to be understood by others.\n",
      "Love is not a game or a competition, it's not about winning, it's about sharing your heart with someone who will love you in return.\n",
      "True friendship can survive long periods of time and distance.\n",
      "I have been searching for something, not knowing what I was looking for. Now that I found it, I know that is love.\n",
      "I don't want to be one of those people who gets married and then looks back on their wedding day with regret. So I'm going to take my time and make sure everything is perfect!\n",
      "We may not have it all together, but together we have it all.\n",
      "When you realize that the person that you love is not loving you back, stop torturing yourself by trying to convince them that they do. Just let them go.\n",
      "In a relationship, I don't mind being the one who makes more money or having the bigger house. I just want us to be equal.\n",
      "Give me something to look forward to every day and you will never see me cry.\n",
      "All good relationships are built on mutual trust and respect.\n",
      "Life is too short for long-term grudges, so forgive today, love tomorrow.\n",
      "I don't need a boyfriend. I have two cats at home that make much better lovers than any man ever could. They always want to be pet and never complain when I eat the last slice of pizza. Best of all, they are there when I get home from work every day.\n",
      "I don't care what people think about me. People are going to talk regardless, so I might as well just do whatever makes me happy.\n",
      "We will not be lovers if we can only meet once a year; but where the hearts of two people are united by a noble cause, nothing is impossible. - Josephine Baker\n",
      "A true friend knows your weaknesses but shows you your strengths; feels your fears but fortifies your faith; sees your anxieties but frees your spirit; recognizes your disabilities but emphasizes your possibilities.\n",
      "You know we're going to be friends for a long time when I stop trying to impress you and start trying to impress myself instead.\n",
      "I don't need anyone else to make me happy, I just need someone like you.\n",
      "A good relationship is one in which each person appoints the other as the guard\n",
      "llama_print_timings:        load time = 23443.06 ms\n",
      "llama_print_timings:      sample time =   364.44 ms /   512 runs   (    0.71 ms per token,  1404.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17079.17 ms /   265 tokens (   64.45 ms per token,    15.52 tokens per second)\n",
      "llama_print_timings:        eval time = 116888.66 ms /   510 runs   (  229.19 ms per token,     4.36 tokens per second)\n",
      "llama_print_timings:       total time = 134387.89 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9469f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690075328\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.68 MB\n",
      "llama_model_load_internal: mem required  = 25192.68 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "I believe that it is important for people to be good and kind and honest and generous, but never self-righteous or smug about it.\n",
      "I believe in a God who lives everywhere – in the sky and in my heart – and in the hearts and minds of every person I meet. And I believe that this same God is a God who loves us very much; and wants only what’s best for us.\n",
      "And so, if we try to do whatever we can to be good people, to live our lives with love as a way of life, then the way will open before us.\n",
      "We need not worry about anything – because there is nothing to worry about.\n",
      "God knows everything and loves everything that happens in life; and God has already written our story. All we have to do is let go – and let the story unfold as it should.\n",
      "I believe that love makes all things possible, and so I choose to be kind and generous and honest and good to everyone I meet because I am always hoping for an opportunity to show someone a little kindness which might just change their lives forever.\n",
      "In all my years of living on this earth, the most important thing that I have learned is that it is never too late. There is always time to make things right; there is always a chance to let go and move forward – because what happens in our life isn’t really about us. It just seems like it is when we are in the middle of the chaos, but as soon as it has passed, then we can see that all along we were being guided towards something beautiful and wonderful which we didn’t even know was there.\n",
      "All we have to do is trust in God – and love each other.\n",
      "I believe that life is a miracle; and that every moment is an opportunity to be kind and generous and honest and good, because the world needs all the kindness and honesty and generosity and goodness it can get. And I have found that when we give these things away unconditionally then they come back to us tenfold – so that we are never giving away anything at all, but merely passing along the love which God has already given to us.\n",
      "I believe in miracles because I know that there is a God who loves every living thing on this earth – and who knows every moment of our lives as it unfolds before him. And if you need proof\n",
      "llama_print_timings:        load time = 23844.60 ms\n",
      "llama_print_timings:      sample time =   364.85 ms /   512 runs   (    0.71 ms per token,  1403.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12679.63 ms /   265 tokens (   47.85 ms per token,    20.90 tokens per second)\n",
      "llama_print_timings:        eval time = 115627.87 ms /   510 runs   (  226.72 ms per token,     4.41 tokens per second)\n",
      "llama_print_timings:       total time = 128729.21 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690076782\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.04 MB\n",
      "llama_model_load_internal: mem required  = 17993.04 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13e726d70\n",
      "ggml_metal_init: loaded kernel_mul                            0x13e728260\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13e7297b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13e7284c0\n",
      "ggml_metal_init: loaded kernel_silu                           0x13e72a220\n",
      "ggml_metal_init: loaded kernel_relu                           0x13e72a9f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13e72b2e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12e695a40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12e695670\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12e68a560\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12e68af40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12e68dae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13e72be60\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13e72c4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13e72ce00\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13e72d5c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13e72ded0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13e72e9b0\n",
      "ggml_metal_init: loaded kernel_norm                           0x13e72f320\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13e7300f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13e730b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13e7315a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13e731f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13e733170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13e733b40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13e733da0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13e734ed0\n",
      "ggml_metal_init: loaded kernel_rope                           0x13e735c00\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13e7367a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13e737340\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13e737eb0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13e7389f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1287.70 MB, offs =  17005117440, (17672.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (17688.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18470.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18686.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18942.16 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your purpose - which, believe it or not, is\n",
      "often hiding inside something you consider a weakness right now. The question then becomes: Are you willing to look for it?\n",
      "A lot of us are afraid to look because we think that when we find our purpose, we'll have to completely change our lives, start a new career, and become someone different. We think we're supposed to turn into a perfect person who never gets angry or cries or makes mistakes. But that's just not true.\n",
      "You don't have to be perfect to find your purpose! You don't even need to be the same person you are right now to start on this journey - you can start today, in fact, with all of your flaws and imperfections intact. It doesn't matter how old you are or what kind of shape you're in. It doesn't matter if you have a criminal record or a PhD. You don't need to be rich or famous or beautiful.\n",
      "I'm not saying that the search for your purpose will be easy, because it won't be. I believe that our gifts are buried beneath layers of fear and doubt - and excuses, too - and that uncovering them is an act of courage in itself. The fact is, you need to find your gift if you want to live the life of your dreams - so the question becomes: Are you willing to do what it takes?\n",
      "In this book, I'm going to walk you through a process I call the \"Miracle Morning.\" This isn't just any morning. It's a special morning that can change your life forever. The Miracle Morning is built around a simple concept: If you wake up earlier than everyone else, you can get one extra hour in your day! But it doesn't stop there; the real \"miracle\" is not just the time you gain, but what you do with that time.\n",
      "I firmly believe that your life's purpose is to serve others, and that you discover what that looks like by waking up earlier than everyone else to work on yourself - for yourself. It is the most selfless thing you can do; when you improve yourself, you improve the world. So in the Miracle Morning I'm going to show you how to build a morning routine that will transform your life before everyone else wakes\n",
      "llama_print_timings:        load time =  8969.64 ms\n",
      "llama_print_timings:      sample time =  1002.17 ms /   512 runs   (    1.96 ms per token,   510.89 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21795.41 ms /   265 tokens (   82.25 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:        eval time = 45627.27 ms /   510 runs   (   89.47 ms per token,    11.18 tokens per second)\n",
      "llama_print_timings:       total time = 68536.82 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690077029\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.04 MB\n",
      "llama_model_load_internal: mem required  = 17993.04 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x159e16640\n",
      "ggml_metal_init: loaded kernel_mul                            0x159e17b50\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x159e190f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x159e18990\n",
      "ggml_metal_init: loaded kernel_silu                           0x159e19e30\n",
      "ggml_metal_init: loaded kernel_relu                           0x159e17f50\n",
      "ggml_metal_init: loaded kernel_gelu                           0x159e1a770\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x159e1b7e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x159e1ba40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x159e1ce30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x159e1c020\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x159e1d790\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x159e1eac0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x159e1dfa0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x159e1f350\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x159e1fca0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x159e20600\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x159e216e0\n",
      "ggml_metal_init: loaded kernel_norm                           0x159e21940\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x159e22cd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x159e236e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x149f1fad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x149f208b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x149f20e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x159e243c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x159e24940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x159e253d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x159e25c10\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x159e26b90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x159e27730\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x159e282b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x159e28e30\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1287.70 MB, offs =  17005117440, (17672.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (17688.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18470.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18686.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18942.16 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. Everyone must find the path he has chosen, and follow it. We have no choice but to follow our destiny.\n",
      "Coco Chanel, (1883-1971), French fashion designer\n",
      "Everything that has been will be, everything that will be is. Everything ends everything begins again.\n",
      "Ecclesiastes 1:9, Bible\n",
      "And it’s the end of our world, and I feel fine.\n",
      "Radiohead, “The Bends”\n",
      "It might seem to some that life is over for me. But I intend to fight. For as long as possible, I refuse to remain down. I am very tired, but I am a lot tougher than cancer. I will try my best to be there when you graduate from high school and college. Continue pursuing your dreams, find what you love doing, and do it.\n",
      "Steve Jobs, “Stanford Commencement Address” (12 June 2005)\n",
      "It is not death that a man should fear, but he should fear never beginning to live.\n",
      "Marcus Aurelius Antoninus, Meditations\n",
      "It’s the end of my world as I know it, and I feel fine.\n",
      "REM, “It’s the End of the World”\n",
      "I have lived so long, that I can remember when the only means of communicating with friends was by letter, or in person; neither of which methods would be tolerated in this age.\n",
      "Thomas Jefferson (1743-1826), Letter to Msgr. Jean-Baptiste Say, Paris, 5 August, 1801\n",
      "The most important thing is to enjoy your life – to be happy – it’s all that matters.\n",
      "Audrey Hepburn (1929-1993), actress\n",
      "I don’t want to achieve immortality through my work… I want to achieve it by not dying.\n",
      "Woody Allen, “Without Feathers” (1975)\n",
      "I am the one who is living, and I am grateful for every breath in and out. I celebrate this gift called life with enthusiasm, joy, and gratitude…\n",
      "Ruth Fishel, Kitchen Table Wisdom: Stories that Heal (2003)\n",
      "I’m not afraid of death\n",
      "llama_print_timings:        load time =  8755.45 ms\n",
      "llama_print_timings:      sample time =  1061.31 ms /   512 runs   (    2.07 ms per token,   482.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21572.47 ms /   265 tokens (   81.41 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:        eval time = 45798.67 ms /   510 runs   (   89.80 ms per token,    11.14 tokens per second)\n",
      "llama_print_timings:       total time = 68548.23 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690077107\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.04 MB\n",
      "llama_model_load_internal: mem required  = 17993.04 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12e64b5d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x12e64ca60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12e64df80\n",
      "ggml_metal_init: loaded kernel_scale                          0x12e64e260\n",
      "ggml_metal_init: loaded kernel_silu                           0x12e64eab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x12e64d000\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12e64f3b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12e6503d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12e650630\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12e6518a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12e650c10\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12e652330\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12e653660\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12e652b40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12e653ef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12e654870\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12e6551b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12e656260\n",
      "ggml_metal_init: loaded kernel_norm                           0x12e656c10\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12e6578e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12e6582d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12e658d10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12e6596b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12e65a090\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12e65aba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12e65b340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12e65bd10\n",
      "ggml_metal_init: loaded kernel_rope                           0x12e65c6d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12e65d6b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12e65e260\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12e65ee00\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12e65f980\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1287.70 MB, offs =  17005117440, (17672.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (17688.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18470.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18686.16 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18942.16 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. But what makes me so? Not much nowadays, for in reality, almost everyone has problems of some kind or another whether they are health problems, financial problems, relationship problems, and many others. These problems get in the way of our being happy. Why can’t we all just be contented with life as it is, without wanting something better or different?\n",
      "These questions came to mind one day as I was on my daily walk around a nearby lake. As I walked, I looked at the water and saw that there were no waves. But then, suddenly, there was a ripple in the water where an insect had just landed. Then, after some time and distance, I noticed another ripple and then another as more and more insects touched down on the water’s surface. After walking around the lake for about 30 minutes, I saw that the lake was now filled with lots of ripples. The question came to mind: Are these ripples any different from the waves that were there originally?\n",
      "These are just some thoughts that came into my head one day as I walked around a peaceful lake in the middle of a busy city.\n",
      "This is my first book so please take it easy on me. I would appreciate comments or suggestions if you have anything to say about it. Thanks for taking the time to read this. It really means a lot to me.\n",
      "I hope you will enjoy reading about my thoughts as much as I enjoyed writing them.\n",
      "Thank you in advance for any kind words that come your way.\n",
      "Thanks again for reading this, and take care!\n",
      "Happy Days! A Simple Guide To Living a Happy Life is now available at Amazon.com (http://www.amazon.com/dp/B00L6YDZNM) as well as in the iTunes store (https://itunes.apple.com/us/book/happy-days!/id893789543?mt=11).\n",
      "My book, Happy Days!, is now available at Amazon.com and other eBook retailers.\n",
      "Happy Days! A Simple Guide To Living a Happy Life is my first book, so please take it easy on me. I would appreciate comments or suggestions if you have anything to say about it. Thanks for taking the time to read this. It really means a lot to me. I hope you will enjoy reading about my\n",
      "llama_print_timings:        load time =  8306.12 ms\n",
      "llama_print_timings:      sample time =  1077.87 ms /   512 runs   (    2.11 ms per token,   475.01 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21770.16 ms /   265 tokens (   82.15 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:        eval time = 45710.45 ms /   510 runs   (   89.63 ms per token,    11.16 tokens per second)\n",
      "llama_print_timings:       total time = 68666.88 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d974327",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 880 (b9b7d94)\n",
      "main: seed  = 1690077184\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.72 MB\n",
      "llama_model_load_internal: mem required  = 62533.72 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "—PABLO PICASSO\n",
      "My name is Marissa Mayer and I am a computer scientist. This means that I study computers, I design software for them, and I write code. Computers are everywhere now. They're in cell phones, cars, watches, refrigerators, airplanes, and more. And they are used by almost everyone. Many people use computers every day to do their jobs. Computer programmers like me use computers to create new things that help people. We have fun doing it too!\n",
      "I wrote this book because I wanted a way for children to learn about programming. My hope is that this book can be useful for all kinds of people who want to learn how to code, not just kids. It's true that kids are learning computer coding in schools now, but they aren't the only ones. Plenty of adults find themselves needing to learn about computers too.\n",
      "I hope you will use this book as a guide to teach yourself how to program. I'll tell you what programming is and why it's important. You can read about the different kinds of languages that people use when they program, and we'll talk about the tools you'll need to get started. After that, we'll jump right in with a coding project.\n",
      "I am going to teach you how to create a website using the language HTML and the programming tool called Notepad++ (or another text editor). You can visit my website at www.learnwithmarissa.com or download the code to see what your first program will look like. We are going to build this simple site together, step by step, in the pages of this book.\n",
      "Once you have finished this book and made your own website, there are lots of other things to learn about coding. Here is a list of some great places for learning more:\n",
      "  * Code.org—www.code.org\n",
      "This is a nonprofit organization that provides programs, tools, and resources to get people excited about programming and teach them how to do it! There are online courses you can take for free at any time, including one designed especially for kids called Hour of Code. You can also host or participate in an event with your friends and family.\n",
      "  * Khan Academy—www.khanacademy.org/computing/computer\n",
      "llama_print_timings:        load time = 157815.14 ms\n",
      "llama_print_timings:      sample time =   957.58 ms /   512 runs   (    1.87 ms per token,   534.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 384839.44 ms /   265 tokens ( 1452.22 ms per token,     0.69 tokens per second)\n",
      "llama_print_timings:        eval time = 93789788.75 ms /   510 runs   (183901.55 ms per token,     0.01 tokens per second)\n",
      "llama_print_timings:       total time = 94177930.77 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu (over one min per token)\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
