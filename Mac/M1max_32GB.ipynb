{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[1m\u001b[36m7B\u001b[m\u001b[m                      \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m30B\u001b[m\u001b[m                     ggml-vocab.bin          \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701054\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12d717bd0\n",
      "ggml_metal_init: loaded kernel_mul                            0x12d7190c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12d71a610\n",
      "ggml_metal_init: loaded kernel_scale                          0x12d719320\n",
      "ggml_metal_init: loaded kernel_silu                           0x12d71b110\n",
      "ggml_metal_init: loaded kernel_relu                           0x12d71b820\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12d71ba80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12d71ca10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12d71cc70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12d71def0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12d71d260\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12d71e970\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x12d71fcb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x12d71f1c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x12d720580\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x12d720ec0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x12d721800\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12d7228e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12d723240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12d723ea0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12d724a00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x12d725240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x12d725c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x12d726620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x12d727160\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x12d727c60\n",
      "ggml_metal_init: loaded kernel_rope                           0x12d728890\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12d729410\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12d72a0e0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to know yourself deeply. To find your purpose and then live it with everything you've got!\n",
      "I was not a good student, but what I did very well at school was sports: Soccer, volleyball, track and field, swimming, horseback riding, etc. I wasn’t the most popular kid in class (nor the least), but I had a lot of friends because I was always down to try anything new!\n",
      "My mother is from El Salvador and my father is from Mexico. I grew up with my parents and one brother in the suburbs of Los Angeles, California. We were a very tight-knit family who went on many adventures together. My dad would take us on weekend camping trips to explore the mountains of Southern California and my mom would take us swimming at the beach or hiking in the park. As I got older, I began exploring cities like San Francisco and New York with friends. These are some great memories that have shaped who I am today!\n",
      "I’ve always been adventurous; so much so that my parents were concerned. They thought I was going to go off the deep end one day and not return home again, haha. But what they didn’t know is that being adventurous meant I wanted to be an explorer like my dad.\n",
      "My father took me on many trips when I was young, and we always talked about how important it was for me to continue studying so I could have a good job and support myself in the future. But, at 18 years old, before entering college, I decided I wanted to go out into the world and experience life as an adult rather than waste those precious years stuck in the classroom.\n",
      "I thought about how many people would like to do what I’m doing—traveling around the world and living life on my own terms—but they can’t because of fear, money or just simply not having the courage to take that leap. I started to realize it was time for me to step up and show others how to be more confident in themselves and take control of their own lives so they could live out their dreams too!\n",
      "I left my home in Los Angeles and moved to San Francisco. With no job, no place to stay and just a backpack full of clothes, I began traveling around the world. It has been one crazy adventure after another, with me always\n",
      "llama_print_timings:        load time =  1398.51 ms\n",
      "llama_print_timings:      sample time =   937.39 ms /   512 runs   (    1.83 ms per token)\n",
      "llama_print_timings: prompt eval time =  4621.62 ms /   265 tokens (   17.44 ms per token)\n",
      "llama_print_timings:        eval time = 16316.04 ms /   510 runs   (   31.99 ms per token)\n",
      "llama_print_timings:       total time = 21972.92 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701077\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x111a1f170\n",
      "ggml_metal_init: loaded kernel_mul                            0x111a20660\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x111a21bb0\n",
      "ggml_metal_init: loaded kernel_scale                          0x111a208c0\n",
      "ggml_metal_init: loaded kernel_silu                           0x111a22620\n",
      "ggml_metal_init: loaded kernel_relu                           0x111a22e00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x111a236f0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x111a23f80\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x111a252b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x111a25510\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x111a247c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x111a270a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x111a265a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x111a26800\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x111a27b10\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x111a28450\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x111a28d90\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x111a29e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x111a2a7a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x111a2b3e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x111a2bda0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x111a2c7e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x111a2d380\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x111a2dbc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x111a2e700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x111a2f230\n",
      "ggml_metal_init: loaded kernel_rope                           0x111a2fe30\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x111a309a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x111a31680\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your calling and live it. When you have the ability to do what you love with a passion, and then help others, we are at our happiest, healthiest, most fulfilled state. It’s when we are in that space where miracles happen!\n",
      "I believe that everyone has a unique purpose which makes them indispensable. When they discover it and live it out, their lives will be transformed forever!\n",
      "I believe that the greatest gift you can give is yourself. It may sound cliche, but when we give of ourselves in service to others – it’s one of the most rewarding acts we could ever do.\n",
      "When I was growing up, my Dad always told me – “It doesn’t matter where you came from or how much money your parents had. What matters is that you find out what you love doing and then go after it!”\n",
      "His words have stuck with me for as long as I can remember. And they are so true!\n",
      "Life is too short to waste time, and we really only get one shot at it…so why not go for it and do something meaningful in the process? That’s what makes life worth living!\n",
      "Amy Rees Anderson\ton December 28, 2010 at 4:57 pm\n",
      "I love your dad’s words so much. I have to say that you are an inspiration to me and I feel so lucky to have met you in person (at the Elev8 event). You radiate such a warmth & energy, and it really is contagious! Your blog has made its way into my favorites list, and I look forward to reading your posts.\n",
      "Thank you Amy – I am so glad that we met too! It was such an honor to be able to help out at Elev8! You are a truly amazing woman! ����\n",
      "Melanie\ton December 29, 2010 at 3:57 pm\n",
      "I like your dad’s words. I agree with them too.\n",
      "Thank you Melanie – they sure are a good way to live life and feel fulfilled!! ���\n",
      "I believe the meaning of life is to find your calling and live it. When you have the ability to do what you love with a passion, and then help others, we are at our happiest.\n",
      "That’s right on\n",
      "llama_print_timings:        load time =   995.39 ms\n",
      "llama_print_timings:      sample time =  1015.79 ms /   512 runs   (    1.98 ms per token)\n",
      "llama_print_timings: prompt eval time =  4627.93 ms /   265 tokens (   17.46 ms per token)\n",
      "llama_print_timings:        eval time = 16407.87 ms /   510 runs   (   32.17 ms per token)\n",
      "llama_print_timings:       total time = 22162.39 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701101\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x117010cd0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1170121c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x117013710\n",
      "ggml_metal_init: loaded kernel_scale                          0x117012420\n",
      "ggml_metal_init: loaded kernel_silu                           0x117014180\n",
      "ggml_metal_init: loaded kernel_relu                           0x117014960\n",
      "ggml_metal_init: loaded kernel_gelu                           0x117015250\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x117015ad0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x117016e10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x117017070\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x117017700\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x117017ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1170180f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x117018350\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x117019690\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x117019fa0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11701a8e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11701b990\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11701c2f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11701cf30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11701d8f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11701e330\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11701eed0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11701f710\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x117020250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x117020d80\n",
      "ggml_metal_init: loaded kernel_rope                           0x117021980\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1170224f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1170231d0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live life. And when you do that with a sense of purpose, you’re going to have a pretty good time.\n",
      "The word “purpose” has been on my mind lately.\n",
      "First of all because of the movie The Purpose-Driven Life which I watched recently. Secondly, I’ve been trying to find my own personal purpose in life. And thirdly, it seems as if we are living in a world where many people have lost their sense of purpose and identity. People who seem to be without direction or goal. Many who don’t know what they want to do with their lives…\n",
      "It is not easy to find your own personal purpose in life. It takes time, work and effort. But it is definitely worth it!\n",
      "When you have a clear sense of your own purpose in life, you’ll never give up!\n",
      "You will live happily ever after!\n",
      "Purpose Quotes:\n",
      "“Your ultimate goal should be to find the purpose that best suits you.”\n",
      "– Richard St. John\n",
      "“Finding your purpose is like finding a diamond. You need to dig deep, and work hard for it!”\n",
      "“You are here with a purpose. There’s a reason why you were born. Discover your purpose in life. Go after what you want. And never give up.”\n",
      "– Steve Maraboli\n",
      "“The only thing of value that we can offer the world is ourselves. When we express our own authenticity, we are contributing to the greater good because we are each a unique expression of God.”\n",
      "“You’re already living your purpose when you’re being yourself with authenticity and honesty in every situation.”\n",
      "– Dr. Steve Maraboli\n",
      "“There is nothing on this earth more powerful than an idea whose time has come.”\n",
      "“Your purpose here on earth is to make a positive difference, even if it’s just for one person at a time.”\n",
      "– Michael Beckwith\n",
      "“Everything you do should have a higher purpose. Your job, your relationships, and everything in between. Find out what makes you tick. And then give yourself the gift of meaningful living.”\n",
      "“The thing that gives me my greatest pleasure is to see somebody else succeed who deserves it.”\n",
      "– George Burns\n",
      "“People with a sense of purpose feel more satisfied, productive, and happy in life.”\n",
      "“If you don’t know what your purpose\n",
      "llama_print_timings:        load time =   981.81 ms\n",
      "llama_print_timings:      sample time =  1005.25 ms /   512 runs   (    1.96 ms per token)\n",
      "llama_print_timings: prompt eval time =  4641.48 ms /   265 tokens (   17.52 ms per token)\n",
      "llama_print_timings:        eval time = 16391.71 ms /   510 runs   (   32.14 ms per token)\n",
      "llama_print_timings:       total time = 22142.27 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701124\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13c66ec40\n",
      "ggml_metal_init: loaded kernel_mul                            0x13c670130\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13c671680\n",
      "ggml_metal_init: loaded kernel_scale                          0x13c670390\n",
      "ggml_metal_init: loaded kernel_silu                           0x13c672180\n",
      "ggml_metal_init: loaded kernel_relu                           0x13c672890\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13c672af0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13c673a80\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13c673ce0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13c674f60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13c6742d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13c6759e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13c676d20\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13c676230\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13c6775f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13c677f30\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13c678870\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13c679950\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13c67a2b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13c67af10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13c67ba70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13c67c2b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13c67cca0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13c67d690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13c67e1d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x13c67ecd0\n",
      "ggml_metal_init: loaded kernel_rope                           0x13c67f900\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13c680480\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13c681150\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and find your true love.\n",
      "I am a positive person and have a very good sense of humor.\n",
      "My friends say that I’m a kind, caring and reliable girl. I try to make other people smile and cheer them up.\n",
      "I value my family more than anything else in the world. When I have free time I like to spend it with my parents and children. They are the most important things for me. I also enjoy spending time outdoors. We go on picnics, camping or just walking. Sometimes we go to the movies together or play board games.\n",
      "I am a very active person so I really like sports. I try to do sport every day and sometimes my friends and I meet at the gym for training together. I also enjoy going in for fitness dancing, which helps me keep fit.\n",
      "My favorite sport is volleyball. I used to play it professionally before but now I just play it for fun.\n",
      "I love cooking and baking a lot of delicious food. My specialty is vegetarian cuisine so my friends often ask me if they can come to dinner. When we’re together with our families, I always make sure that there are plenty of different dishes. Everyone loves it!\n",
      "I also like to knit and do needlework. You know, every woman needs something to do at home when she’s alone. It helps me keep busy and not get bored. As a result, my family is always surprised by the new things I make for them and they really appreciate it.\n",
      "As you can see, I like doing different creative activities. This is because I like making gifts to my friends and relatives. For example, I love to make scented candles with cute names. And every year I bake a lot of cookies for the holidays. Everyone in my family loves them!\n",
      "I think it’s important for a woman to be beautiful and well-groomed but not too much emphasis on beauty is necessary. Of course, everyone wants to look good but it shouldn’t take up all your time. A good personality is what really matters. I always try to smile at people because I think that doing this makes you look younger and more attractive.\n",
      "My friends say that I have a very kind heart and I always help them with their problems. Sometimes they come to me for advice when they\n",
      "llama_print_timings:        load time =  3896.75 ms\n",
      "llama_print_timings:      sample time =  1209.71 ms /   512 runs   (    2.36 ms per token)\n",
      "llama_print_timings: prompt eval time =  4617.43 ms /   265 tokens (   17.42 ms per token)\n",
      "llama_print_timings:        eval time = 34553.53 ms /   510 runs   (   67.75 ms per token)\n",
      "llama_print_timings:       total time = 40502.06 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701169\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1170144b0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1170159a0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x117016ef0\n",
      "ggml_metal_init: loaded kernel_scale                          0x117015c00\n",
      "ggml_metal_init: loaded kernel_silu                           0x1170179f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x117018100\n",
      "ggml_metal_init: loaded kernel_gelu                           0x117018360\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1170192f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x117019550\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11701a7d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x117019b40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11701b250\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11701c590\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11701baa0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11701ce60\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11701d7a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11701e0e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11701f1c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11701fb20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x117020780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1170212e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x117021b20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x117022510\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x117022f00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x117023a40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x117024540\n",
      "ggml_metal_init: loaded kernel_rope                           0x117025170\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x117025cf0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1170269c0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live well, to enjoy and appreciate the moments. To be good to people, and to animals. And to leave a legacy behind for future generations.\n",
      "I think that most people have a pretty good idea of what they want out of their lives. But we all need some help along the way.\n",
      "I believe that you can get there from here by focusing on your life’s purpose, and creating a vision to which you are committed. I believe you should be your own champion in this regard.\n",
      "And I believe you will live well if you have good friends around you, and you love what you do for a living.\n",
      "I hope that all of these things come together for each one of us. And that we can leave something better than we found it to the next generation.\n",
      "To learn more about my life’s work, click here. To read my blog, click here.\n",
      "If you would like me to speak at your church or business, please contact me. I am available for book signings and speaking engagements. My books are also available for purchase through Amazon.com. Click the buttons below to order.\n",
      "If you’d rather talk with me, call me at 931-707-4800. Or send an email to jim@jameslester.net. I look forward to hearing from you!\n",
      "Jim Lester has had a full and varied career. He was the first person on television in Columbia, South Carolina, worked as a disc jockey at WIS, wrote for The State Newspaper and WBTV-Charlotte, owned radio stations and was the editor of “The Voice” magazine.\n",
      "His best selling book, “When Life Is Hard to Take,” is used in churches, prisons and recovery programs around the country. He is also a keynote speaker at meetings and conventions, and has appeared on television talk shows in several cities. His column appears weekly in The Times-Free Press newspaper.\n",
      "Jim Lester was inducted into the “Who’s Who of Broadcasting” for his work as an announcer and host of a nationally syndicated program. He is a member of Kappa Alpha Psi Fraternity, Inc., Delta Phi Omega Sorority, The National Academy of Television Arts & Sciences (EMMY), The Tennessee Radio Hall of Fame, The Columbia Music Educators Association and\n",
      "llama_print_timings:        load time =  4349.16 ms\n",
      "llama_print_timings:      sample time =  1071.27 ms /   512 runs   (    2.09 ms per token)\n",
      "llama_print_timings: prompt eval time =  4584.34 ms /   265 tokens (   17.30 ms per token)\n",
      "llama_print_timings:        eval time = 34340.02 ms /   510 runs   (   67.33 ms per token)\n",
      "llama_print_timings:       total time = 40106.53 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701213\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11f01e470\n",
      "ggml_metal_init: loaded kernel_mul                            0x11f02a680\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11f02bd50\n",
      "ggml_metal_init: loaded kernel_scale                          0x11f02c060\n",
      "ggml_metal_init: loaded kernel_silu                           0x11f02c8b0\n",
      "ggml_metal_init: loaded kernel_relu                           0x11f02d0a0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11f02ad10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11f02e270\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11f02e4d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11f02f6a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11f02e9f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11f030140\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11f031490\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11f030990\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11f031bd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11f032510\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11f032fb0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11f033900\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11f034a00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11f035640\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11f036180\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11f036ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11f0375a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11f037fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11f038b10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11f039610\n",
      "ggml_metal_init: loaded kernel_rope                           0x11f03a8e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11f03b5a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11f03c0f0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love. To be loved is a blessing, but loving is an even greater blessing.\n",
      "Love is a blessing. Loving is the greatest blessing of all.\n",
      "The key to happiness and success in life isn’t about what we do or how much we get done, it’s about how well we love those around us. The more love we give out, the bigger the payoff. So don’t ever hesitate to be kind to someone else because you never know when that person may need your help the most.\n",
      "Most of the time our greatest blessings are found right in front of us. We just have to open our eyes and see them. The meaning of life is not about what we do, but who we love!\n",
      "Love is all around us. It’s on social media, in books, movies… it’s all over the place!\n",
      "And when you find someone who loves you just as much as you love them, then you’ll know that this is something special. You won’t ever want to let go of it.\n",
      "The meaning of life is not about what we do, but who we love! Sometimes it takes a while to see the bigger picture and to recognize the value in all the little things that make your heart beat faster. But when you finally realize it, nothing else will matter anymore because you’ll be happy!\n",
      "What are some other things that make life worth living? Share with us below!\n",
      "I believe the meaning of life is to love. To be loved is a blessing, but loving is an even greater blessing. Love is a blessing. Loving is the greatest blessing of all. The key to happiness and success in life isn’t about what we do or how much we get done, it’s about how well we love those around us. The more love we give out, the bigger the payoff. So don’t ever hesitate to be kind to someone else because you never know when that person may need your help the most. Most of the time our greatest blessings are found right in front of us. We just have to open our eyes and see them. The meaning of life is not about what we do, but who we love! Love is all around us. It’s on social media, in books, movies… it’s all over the place! And when you find someone who loves you with all their heart,\n",
      "llama_print_timings:        load time =  4139.33 ms\n",
      "llama_print_timings:      sample time =   966.31 ms /   512 runs   (    1.89 ms per token)\n",
      "llama_print_timings: prompt eval time =  4861.58 ms /   265 tokens (   18.35 ms per token)\n",
      "llama_print_timings:        eval time = 34298.50 ms /   510 runs   (   67.25 ms per token)\n",
      "llama_print_timings:       total time = 40228.05 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701258\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11d7c9870\n",
      "ggml_metal_init: loaded kernel_mul                            0x11d7cad60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11d7cc2b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11d7cafc0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11d7ccdb0\n",
      "ggml_metal_init: loaded kernel_relu                           0x11d7cd4c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11d7cd720\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11d7ce6b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11d7ce910\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11d7cfb90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11d7cef00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11d7d0610\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11d7d1950\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11d7d0e60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11d7d2220\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11d7d2b60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11d7d34a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11d7d4580\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11d7d4ee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11d7d5b40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11d7d66a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11d7d6ee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11d7d78d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11d7d82c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11d7d8e00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11d7d9910\n",
      "ggml_metal_init: loaded kernel_rope                           0x11d7da520\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11d7db0c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11d7dbd90\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give it.\n",
      "—Peter Buffett, American musician and composer 1953–2018\n",
      "# PREFACE BY CRAIG PETERSON\n",
      "> In 2017, Craig Peter­son was named one of Canada's Top 40 Under 40 by _The Globe and Mail_ newspaper. He is the founder and CEO of Kik Interactive. A pioneer in the world of social media, he launched Kik as a university student. By 2017, Kik had more than 300 million registered users, an app store with millions of downloads, and a $1 billion valuation. Peter­son graduated from Harvard Business School in 2016, receiving the Baker Scholar Award for outstanding leadership. He is also active in charitable causes. In May 2017, he co-chaired a gala that raised nearly $3 million for SickKids Foundation.\n",
      "For millions of young people today, social media has become an essential tool and a major part of their lives. It's not just for keeping up with friends anymore; it can help us connect with new people, explore new ideas and passions, and discover the world around us in ways that were impossible only 10 years ago.\n",
      "Social media has also created a whole new set of challenges. How do we balance social media with work? How do we make sure our kids aren't spending too much time online? What are appropriate boundaries for ourselves and for our families? And how can we be good parents, teachers, leaders, and citizens in this fast-moving digital world?\n",
      "These questions are at the heart of _Social Media in Canada_ , a collection of essays from some of our nation's leading thinkers. The essays offer new ways to explore and understand social media—with all its potential for good and bad. What do we do when a president uses it as a tool to spread fake news? How can we use social media to create positive change in the world? And how can we teach our kids to be better online citizens than we are ourselves?\n",
      "The essays in this book offer new perspectives on these and other questions. They show us that, while the tools of social media are constantly changing, human nature remains constant—and there is much wisdom to be found in ancient philosophies like Stoicism\n",
      "llama_print_timings:        load time =  2350.32 ms\n",
      "llama_print_timings:      sample time =  1009.71 ms /   512 runs   (    1.97 ms per token)\n",
      "llama_print_timings: prompt eval time =  8685.31 ms /   265 tokens (   32.77 ms per token)\n",
      "llama_print_timings:        eval time = 29116.98 ms /   510 runs   (   57.09 ms per token)\n",
      "llama_print_timings:       total time = 38918.55 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701299\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x14be2abe0\n",
      "ggml_metal_init: loaded kernel_mul                            0x14be2c0d0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x14be2d620\n",
      "ggml_metal_init: loaded kernel_scale                          0x14be2d900\n",
      "ggml_metal_init: loaded kernel_silu                           0x14be2e0f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x14be2c670\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14be2eac0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14be2fa60\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14be30da0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14be30110\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14be316b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14be32bb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x14be32200\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x14be33480\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x14be33d90\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x14be346d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x14be35030\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14be35940\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14be365e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14be371a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14be382b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x14be38c90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x14be396d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x14be3a220\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x14be3ab90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x14be3b5b0\n",
      "ggml_metal_init: loaded kernel_rope                           0x14be3c170\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14be3ce30\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14be3d940\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love.\n",
      "There are many different kinds of love, and all have their place in our lives. But as far as love for God goes, it involves an entirely new kind of love: Love that has no strings attached. It’s a strange concept, really. The only way to understand it is to experience it firsthand.\n",
      "In the Old Testament book of Deuteronomy, Moses speaks about what he calls “love feasts,” or parties, in which the Israelites are commanded to celebrate and worship God with their whole hearts. He writes:\n",
      "At the end of every seven years you must cancel debts. How well this is to be done has been explained to you; take care to do it. There will, however, be cases where a man becomes poor and his means fail so that he is unable to support himself or his family; he may then call on his brothers and they must help him as though with the money of a foreigner—they are not to charge interest. He shall hold the position of a foreigner among you and shall be treated like one; but you are to love him as yourself, for you were foreigners in Egypt. I am the Lord your God (Deuteronomy 15:1-3).\n",
      "Aren’t these verses beautiful? They speak of a way of life that is so different from our own. Too often we try to “pay back” or “collect on debts,” but, as Moses points out, that’s not how it works with God. He loves us freely and unconditionally. We can do nothing to repay him for his love or make up for all the wrong things we have done in our lives; still he comes after us with love anyway.\n",
      "I know there are those of you who would like to see something more tangible from God, but that is not how it works: “God will not be a man and lie, nor a son of man and deceive” (Numbers 23:19). He doesn’t lie; he always keeps his promises. But what is most amazing to me in these verses is the way Moses tells us we are to love God. We are told that “you shall serve him with all your heart and soul,” but then he says, “You shall love him with all your strength” (Deuteronomy 15:12).\n",
      "The Hebrew word for “strength” is\n",
      "llama_print_timings:        load time =  1770.39 ms\n",
      "llama_print_timings:      sample time =   919.01 ms /   512 runs   (    1.79 ms per token)\n",
      "llama_print_timings: prompt eval time =  8508.13 ms /   265 tokens (   32.11 ms per token)\n",
      "llama_print_timings:        eval time = 29081.61 ms /   510 runs   (   57.02 ms per token)\n",
      "llama_print_timings:       total time = 38615.26 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701340\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x15c98a1f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x15c98b6e0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x15c98cc30\n",
      "ggml_metal_init: loaded kernel_scale                          0x15c98b940\n",
      "ggml_metal_init: loaded kernel_silu                           0x15c98d730\n",
      "ggml_metal_init: loaded kernel_relu                           0x15c98de40\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15c98e0a0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x15c98f030\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x15c98f290\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x15c990510\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x15c98f880\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x15c990f90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x15c9922d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x15c9917e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x15c992bd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x15c993510\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x15c993e20\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x15c994f00\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x15c995880\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x15c9964e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x15c997020\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x15c997a00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x15c998410\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x15c998e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x15c999970\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x15c99a310\n",
      "ggml_metal_init: loaded kernel_rope                           0x15c99af00\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x15c99bac0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x15c99c780\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have fun, but that's not what they teach in school.\n",
      "In fact, when you look at school as a whole, it becomes pretty clear. The main goal of schools is to get the kids ready for college and then eventually work – or, if you're lucky enough, a job with little actual \"work\" involved.\n",
      "I am not one of those people who believe that college is a waste of time; I think it's a great opportunity to learn new things about yourself and make friends. However, the school system has become incredibly competitive which means that a lot of stress is put on students. For some, this means pushing themselves harder, while for others, it causes anxiety and depression.\n",
      "I am a freshman in high school right now, so I'm not exactly sure how to \"make life fun,\" but I do know what would help. When you think about it, there are really only two times that we have a break from our responsibilities: the weekends and summer vacation. The problem is, we spend those times doing everything but having fun.\n",
      "Some people go out every Friday night while others just chill with their friends and eat pizza. Some people spend all of their summers going to different places around the world, while others use it to catch up on sleep or homework (I'm looking at you, AP kids). While these are all good things to do, we should also take time out of our schedules in order to relax and enjoy life.\n",
      "There is no rule that says you have to spend every weekend with your friends or family – sometimes it's nice to hang out alone, read a book, watch a movie, walk your dog, or whatever floats your boat. It doesn't even have to be an hour-long activity; I promise it will do wonders for your mental health.\n",
      "In addition, it is also important that we take time off from our studies. We should not try to squeeze every last bit of knowledge out of our brains at the expense of our sanity. Some people have a \"I can't stop studying until I get an A\" mentality and they refuse to take breaks even when they are getting burnt out.\n",
      "It is important that we allow ourselves some down time so we don't lose sight of what really matters: having fun and being happy. So, if you have some spare time this summer\n",
      "llama_print_timings:        load time =  1709.02 ms\n",
      "llama_print_timings:      sample time =   983.96 ms /   512 runs   (    1.92 ms per token)\n",
      "llama_print_timings: prompt eval time =  8649.63 ms /   265 tokens (   32.64 ms per token)\n",
      "llama_print_timings:        eval time = 29120.10 ms /   510 runs   (   57.10 ms per token)\n",
      "llama_print_timings:       total time = 38871.13 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701381\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x139638d10\n",
      "ggml_metal_init: loaded kernel_mul                            0x13963a540\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13963bc40\n",
      "ggml_metal_init: loaded kernel_scale                          0x13963bf50\n",
      "ggml_metal_init: loaded kernel_silu                           0x13963c7a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13963abf0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13963d120\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13963e140\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13963e3a0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13963e760\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13963fce0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x139641220\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1396408a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x139641af0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x139642430\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x139642d70\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1396436b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x139644180\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x139644c90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x139645f60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x139646990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x139647340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x139647d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1396488f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x139649270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x139649c20\n",
      "ggml_metal_init: loaded kernel_rope                           0x13964a880\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13964b510\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13964c020\n",
      "ggml_metal_add_buffer: buffer 'data' size 26032652288 is larger than buffer maximum of 17179869184\n",
      "llama_init_from_file: failed to add buffer\n",
      "llama_init_from_gpt_params: error: failed to load model './models/13B/ggml-model-f16.bin'\n",
      "main: error: unable to load model\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701396\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m not to be discovered only in the future sands of time, but lies in us now.\n",
      "I’m not sure about you, but when I first heard that message, it took me a little while before I could even wrap my mind around it. It seemed so simple and yet so profound at the same time! The meaning of life is not “out there” for each of us to discover in some faraway place, but rather lies within us right now—in this very moment!\n",
      "The problem with most people is that they try to find the meaning of their lives in a myriad of different places. They think the key to finding the meaning of life is through fame or fortune, or in a relationship or a job or a career. While these are all important parts of our lives, no one thing can give us the meaning we so desperately seek.\n",
      "The only true source of meaning in this world is found within ourselves. It’s who you are and what you do with your life that will define its meaning—not what you do or how much money you make! So many people out there think that the key to happiness is found in material possessions, but it’s really not about having things—it’s about being happy.\n",
      "There have been many times in my own life when I was working hard and making a lot of money, but I wasn’t very happy because I did not feel like I had any real purpose to what I was doing. On the other hand, there were several occasions where I felt so blessed because I could see God’s plan for me in everything that was happening—even if it seemed hard at the time!\n",
      "I believe the reason our lives have meaning is because we are a gift from God to each other. We are here on this earth to show love and compassion toward one another, and to make the world a better place. And while I know that there are many bad things going on in our society today, it doesn’t mean you can’t help change those things for the better!\n",
      "There is so much goodness in each of us—and we have to start with ourselves by believing in who we are and what we can do to make a difference. If everyone took the time to love themselves and believe in their own potential, just think how different this world would be today! You could be the one person that someone else is looking up to, so don’t give up on your dreams or\n",
      "llama_print_timings:        load time = 24217.16 ms\n",
      "llama_print_timings:      sample time =   368.72 ms /   512 runs   (    0.72 ms per token)\n",
      "llama_print_timings: prompt eval time = 11468.05 ms /   265 tokens (   43.28 ms per token)\n",
      "llama_print_timings:        eval time = 118611.72 ms /   510 runs   (  232.57 ms per token)\n",
      "llama_print_timings:       total time = 130504.53 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701551\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have a purpose, a cause.\n",
      "A Purpose to be loved, a purpose to love others. To inspire and make our lives matter.\n",
      "When we have that purpose, all other goals become so much easier to achieve.\n",
      "I'm not saying that you can get your dream job on your first try, but you will have more clarity about what you want in life. So you will be able to move forward towards it with a clear mind and heart.\n",
      "If we are all going through the same experiences of life and if we all want something different from this life, then how come everyone doesn't get what they want?\n",
      "Well, because most people don't know what they want in life. They just follow their instincts or what others tell them to do. But what happens when that is not enough? When the only thing you are good at is following orders and being told what to do? Well... You have a boring existence. I believe this is how most people live their lives.\n",
      "If we all followed our hearts and intuition, then we would be living in harmony with ourselves, and therefore with others too!\n",
      "My passion is helping people become the best versions of themselves through awareness and by using the power of thought to manifest a better life for them.\n",
      "I have worked my way from being unemployed, depressed and unhappy, to becoming an entrepreneur in less than 6 months, without any experience or prior knowledge about how I was going to do it.\n",
      "I went from being an artist at 15 years old, thinking that art is all there is for me in this life, to now having a passion for teaching and helping people find their true path towards happiness.\n",
      "My purpose is to inspire others by showing them that they have the power inside of themselves to manifest the life they want.\n",
      "I believe it's not about what you do or how much money you make, I just want to help people become happy, so they can be the best version of themselves and achieve all their dreams!\n",
      "I am a writer, speaker, teacher, spiritual coach and entrepreneur.\n",
      "I love life and living my purpose!\n",
      "​If you want me to speak at your event or for any consultation please contact me through the form below!\n",
      "\"Our deepest fear is not that we are inadequate. Our deepest fear is that we are powerful beyond measure. It is our light,\n",
      "llama_print_timings:        load time = 22560.15 ms\n",
      "llama_print_timings:      sample time =   365.16 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 11974.79 ms /   265 tokens (   45.19 ms per token)\n",
      "llama_print_timings:        eval time = 117040.02 ms /   510 runs   (  229.49 ms per token)\n",
      "llama_print_timings:       total time = 129437.15 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9469f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689701704\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to plant trees, under whose shade you do not expect to sit.\n",
      "I think the most important single thing, beyond discipline and creativity is daring to dare.\n",
      "The only way to make sense out of change is to plunge into it, move with it, and join the dance.\n",
      "Don't worry when you are not recognized but strive to be worthy of recognition.\n",
      "I don't know who my grandfather was; I am much more concerned to know what his grandson will be.\n",
      "A man can fail many times, but he isn't a failure until he begins to blame somebody else.\n",
      "If you have built castles in the air, your work need not be lost that is where they should be. Now put the foundations under them.\n",
      "The only way of finding the limits of the possible is by going beyond them into the impossible.\n",
      "One man cannot hold another man down in the ditch without remaining down in the ditch with him.\n",
      "If you are always trying to be normal, you will never know how amazing you can be.\n",
      "You don't always need a plan. Sometimes you just need to breathe, trust, let go and see what happens.\n",
      "A leader is like a shepherd. He stays behind the flock, letting the most nimble go out ahead, whereupon the others follow, not realizing that all along they are being directed from behind.\n",
      "We do not need magic to transform our world. We carry all of the power we need inside ourselves already. We have the power to imagine better.\n",
      "A leader is best when people barely know he exists, not so good when people obey and acclaim him, worse when they despise him. But of a good leader who talks little, when his work is done, his aim fulfilled, they will say: We did it ourselves.\n",
      "In the confrontation between the stream and the rock, the stream always wins not through strength but by perseverance.\n",
      "It's hard to fail, but it's worse never to have tried to succeed. In this life we get nothing save by effort.\n",
      "A man who has done his best has done everything.\n",
      "The sun itself shines clearest in the middle of winter.\n",
      "Never doubt that a small group of thoughtful, committed citizens can change the world; indeed it's the only thing that ever has.\n",
      "Hope is not a dream, but\n",
      "llama_print_timings:        load time = 23290.52 ms\n",
      "llama_print_timings:      sample time =   364.66 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 11553.39 ms /   265 tokens (   43.60 ms per token)\n",
      "llama_print_timings:        eval time = 116405.19 ms /   510 runs   (  228.25 ms per token)\n",
      "llama_print_timings:       total time = 128372.24 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689703384\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x125e96d80\n",
      "ggml_metal_init: loaded kernel_mul                            0x125e98290\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x125e997b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x125e99ab0\n",
      "ggml_metal_init: loaded kernel_silu                           0x125e9a300\n",
      "ggml_metal_init: loaded kernel_relu                           0x125e98830\n",
      "ggml_metal_init: loaded kernel_gelu                           0x125e9ac70\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x125e9bc10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x125e9be70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x125e9d090\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x125e9c3e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x125e9db70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x125e9eee0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x125e9e3e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x125e9f610\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x125e9ff40\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x135f0da30\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x135f0e150\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x135f0e890\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x135f14be0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x125ea0a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x125ea18a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x125ea23f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x125ea2e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x125ea3960\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x125ea4480\n",
      "ggml_metal_init: loaded kernel_rope                           0x125ea50e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x125ea5c20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x125ea6930\n",
      "ggml_metal_add_buffer: buffer 'data' size 18355355648 is larger than buffer maximum of 17179869184\n",
      "llama_init_from_file: failed to add buffer\n",
      "llama_init_from_gpt_params: error: failed to load model './models/30B/ggml-model-q4_0.bin'\n",
      "main: error: unable to load model\n"
     ]
    }
   ],
   "source": [
    "!./main --color -n 128 -c 512 --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689703393\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m...\n",
      "To have a good time.\n",
      "Life has no meaning. It's just what we make it to be.\n",
      "I believe the meaning of life is... More Login\n",
      "Re:What? No \"42\" option? (Score:5, Funny)\n",
      "by Nerdfest ( 867930 ) writes: on Tuesday October 16, 2012 @08:44AM (#41667033)\n",
      "Because in most of the western world it's not 42:00 yet. It's still 41:59.\n",
      "Re:I believe the meaning of life is... (Score:5, Funny)\n",
      "by rwa2 ( 4381 ) * writes: on Tuesday October 16, 2012 @09:33AM (#41667599) Homepage Journal\n",
      "I believe the meaning of life is...\n",
      "To get a mod point.\n",
      "Re:Wrong options (Score:3, Funny)\n",
      "by Xaedalus ( 1192463 ) writes: <Xaedalys&yahoo,com> on Tuesday October 16, 2012 @10:35AM (#41668275)\n",
      "Would it be wrong to say \"none of the above\"?\n",
      "If you're not doing anything fun, then yeah, I guess so. But I can only speak for myself here...\n",
      "Re:I believe the meaning of life is... (Score:5, Interesting)\n",
      "by sribe ( 304414 ) writes: on Tuesday October 16, 2012 @10:58AM (#41668517)\n",
      "I think that the meaning of life is to be happy.\n",
      "For me, I am happiest when creating things and sharing them with others - this is true whether it is writing a poem or short story, or drawing something, or writing a computer program, or designing an electronic circuit board or a piece of furniture. When I'm creating things, and especially when other people find what I create useful to them, then I am happy.\n",
      "I think that the meaning of life is to be happy. For me,\n",
      "llama_print_timings:        load time = 10764.50 ms\n",
      "llama_print_timings:      sample time =   365.14 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 21216.64 ms /   265 tokens (   80.06 ms per token)\n",
      "llama_print_timings:        eval time = 91880.06 ms /   510 runs   (  180.16 ms per token)\n",
      "llama_print_timings:       total time = 113510.36 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689703518\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I believe that, in general, people are bad at knowing what will make them happy.\n",
      "I believe that a lot of the things we do to try and please ourselves are actually making us unhappy.\n",
      "I believe that most of my life decisions are driven by fear and not love.\n",
      "I believe that it is our responsibility to be happy.\n",
      "If you’re still reading, then I think these beliefs are worth exploring further. Let’s take them one at a time:\n",
      "1) I believe the meaning of life is to be happy.\n",
      "I don’t have a lot of proof for this other than it just feels right. To me, it makes sense that our purpose in life would be to find joy and satisfaction within ourselves. It seems like this is what we spend most of our time doing anyway. I know that I am constantly asking myself whether or not something will make me happy. If I get a promotion at work, will I be happier? If I buy an expensive car, will it make me happy? If I get married and have kids, will it make me happy?\n",
      "Our life decisions are based on what we think will bring us happiness. We do things for other people because we want to feel good about ourselves (which in itself would be a decision driven by our desire to be happy).\n",
      "2) I believe that, in general, people are bad at knowing what will make them happy.\n",
      "This is why the first statement is so important. If you believe the meaning of life is to be happy and you also believe it’s hard to know what will make you happy, then it’s clear how this can become a problem.\n",
      "How do we know if something will make us happy? We don’t! This is where the importance of exploration comes in. We need to explore our own emotions to learn what makes us happy and what doesn’t. The more you understand about your own happiness, the better decisions you can make in your life.\n",
      "3) I believe that a lot of the things we do to try and please ourselves are actually making us unhappy.\n",
      "This is something I have recently learned about myself. I used to think I was doing all these things to bring me joy but after exploring my own emotions, I discovered that many of these actions were not beneficial at all. In fact, they made me feel much worse than if I had done nothing!\n",
      "I believe\n",
      "llama_print_timings:        load time = 10902.35 ms\n",
      "llama_print_timings:      sample time =   367.42 ms /   512 runs   (    0.72 ms per token)\n",
      "llama_print_timings: prompt eval time = 21213.85 ms /   265 tokens (   80.05 ms per token)\n",
      "llama_print_timings:        eval time = 95613.95 ms /   510 runs   (  187.48 ms per token)\n",
      "llama_print_timings:       total time = 117244.92 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77454d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689703646\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to experience and learn everything we can. It's not a test, it's just about living.\n",
      "Find on Amazon: Cory Booker\n",
      "About, Everything, Experience, Life, Learn, Living, Meaning, Meaning Of Life, Test\n",
      "I want to be remembered as someone who did his best to help and do for others. I will never forget where I came from or those who helped me along the way - and that is how I will live my life. Cory Booker\n",
      "Life, Best, Live, Me, Help, Want\n",
      "There's an incredible power in humility. It allows you to keep your eyes open, your head up, and your mind wide-open. Humility is a virtue; arrogance is a cancer. Cory Booker\n",
      "Power, Eyes, Mind, Arrogance, You\n",
      "We all know how changeable weather can be. It's no less so in the world of politics. Cory Booker\n",
      "Politics, World, Weather, Know, How\n",
      "When you are running a campaign, when you're running for office, you need to make sure that every single day is about making progress and ensuring that your time is being used effectively. And I think we were able to do that in my campaigns. Cory Booker\n",
      "Time, Day, Progress, Think, You, Make\n",
      "The more cynical you become, the less likely it is you'll become president. Cory Booker\n",
      "You, President, Become, More, Less\n",
      "In the political world, I think we need to do a better job of explaining what we mean by freedom - which is not necessarily democracy in our case and not even necessarily elections. Cory Booker\n",
      "Freedom, Job, World, Democracy, Think\n",
      "I believe that my work as a mayor prepares me for more complex issues than being a community organizer or a senator. The job of mayor requires you to deal with every conceivable issue. You're dealing with 320,000 portfolios of issues; every single day, something is coming through your office that requires you to make decisions. Cory Booker\n",
      "Work, Job, Community, Day, Me, Believe\n",
      "When you're living in a community where violence is a fact of life, you can either pretend it doesn't exist or you can accept it and deal with it.\n",
      "llama_print_timings:        load time = 11137.24 ms\n",
      "llama_print_timings:      sample time =   367.39 ms /   512 runs   (    0.72 ms per token)\n",
      "llama_print_timings: prompt eval time = 21001.81 ms /   265 tokens (   79.25 ms per token)\n",
      "llama_print_timings:        eval time = 92532.75 ms /   510 runs   (  181.44 ms per token)\n",
      "llama_print_timings:       total time = 113950.70 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d974327",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689703772\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m about love"
     ]
    }
   ],
   "source": [
    "# try cpu (over one min per token)\n",
    "!./main --color -n 128 -c 512 --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
