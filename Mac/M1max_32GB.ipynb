{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     ggml-vocab.bin          \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer.model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817750\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1598bc850\n",
      "ggml_metal_init: loaded kernel_mul                            0x1598bdd60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1598be160\n",
      "ggml_metal_init: loaded kernel_scale                          0x1598beaf0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1598bf340\n",
      "ggml_metal_init: loaded kernel_relu                           0x1598bfd40\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1598c05c0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1598c0f70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1598c16a0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1598c1900\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1598c2ce0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1598c32b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1598c4a10\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x1598c3ef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1598c52a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x1598c5bf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1598c6550\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1598c6e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1598c7f90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1598c8bd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1598c9720\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x1598ca190\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x1598cab70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1598cb590\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x1598cc0a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x1598ccbe0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1598cde30\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1598ceb10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1598cf680\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be as much like Jesus as we can. We cannot do this without God’s help.\n",
      "I believe that the Bible is the inspired word of God, and it explains who he is, how he wants us to live our lives, what will happen if we don’t follow his plan for us, and so on.\n",
      "The Bible also tells me that I need to be baptised – to be washed clean from my sins. It says this at Acts 2:38-41, Peter said “Repent & be baptized in the name of Jesus Christ for the forgiveness of your sins; and you will receive the gift of the Holy Spirit”.\n",
      "I believe that I am saved by God’s grace as a gift from him. He did it all – my salvation is not mine to take any credit for, but his. It is what he does in me – not me doing anything. As Jesus said “I am the vine, you are the branches”.\n",
      "I believe that I must keep following Jesus and keep trying to be like him, by studying the Bible and praying.\n",
      "The Bible says that we are to go into all the world and preach the gospel – which is God’s good news about his son who was sent to take away our sins, so that we might have eternal life with him in heaven when we die. This is the best news there ever could be!\n",
      "I believe that Jesus has commissioned us (the church) as his ambassadors on earth and tells us to go into all the world to spread this good news of salvation through him. We are not meant to be a building or institution – but the people in it.\n",
      "The Bible says we need to live our lives so that others can see Jesus, and be attracted by his love for them. It also says that I will not be saved unless I am obedient to God’s word.\n",
      "I believe that Jesus is coming back at some point – and that when he does the world as we know it now will end. We are to make sure we have a relationship with him before then, so that we can spend eternity in heaven. I believe that if I die without this, I will go to hell where there will be no hope of salvation.\n",
      "I am thankful to God for his amazing love and grace that I’ve received, and I don’t want to waste it – so I will\n",
      "llama_print_timings:        load time =  1382.59 ms\n",
      "llama_print_timings:      sample time =   857.10 ms /   512 runs   (    1.67 ms per token)\n",
      "llama_print_timings: prompt eval time =  4609.39 ms /   265 tokens (   17.39 ms per token)\n",
      "llama_print_timings:        eval time = 16197.57 ms /   510 runs   (   31.76 ms per token)\n",
      "llama_print_timings:       total time = 21757.83 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817774\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13de94490\n",
      "ggml_metal_init: loaded kernel_mul                            0x13de95980\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13de96ed0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13de95be0\n",
      "ggml_metal_init: loaded kernel_silu                           0x13de97940\n",
      "ggml_metal_init: loaded kernel_relu                           0x13de98120\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13de98a10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13de992a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13de9a5d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13de9a830\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13de99ae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13de9c3c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13de9b8c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13de9bb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13de9ce30\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13de9d770\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13de9e0b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13de9f160\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13de9fac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13dea0700\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13dea10c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13dea1b00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13dea26a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13dea2ee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13dea3a20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x13dea4550\n",
      "ggml_metal_init: loaded kernel_rope                           0x13dea5150\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13dea5cc0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13dea69a0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it as fully and richly as possible.\n",
      "I’m a 20-something writer, editor, blogger and aspiring digital nomad who has been living in Sydney since March 2015.\n",
      "In my free time I enjoy writing fiction (mainly contemporary romance) novels, doing yoga, swimming, going to the movies, cooking, travelling and watching documentaries.\n",
      "I’m a self-confessed bookworm who loves reading books in almost any genre – but especially romance, women’s fiction and crime thrillers. I also love watching TV series (my favourites are Breaking Bad and The Walking Dead) and watching movies at the cinema or on my couch at home.\n",
      "I can’t wait to see what life has in store for me next!\n",
      "Read more about me here: About Me.\n",
      "Follow @mimivision on Instagram to see some of my travel adventures, as well as some food, fitness and fashion pics too!\n",
      "For all enquiries please use the form below or email me at mimi@mimmaymay.com.au. Thanks!\n",
      "What is one thing you would want to change about yourself?\n",
      "Your email address will not be published on your site.\n",
      "We respect your privacy and promise to never share your information with anyone, ever. You can read our full Privacy Policy here.\n",
      "10 thoughts on “About”\n",
      "I’ve just nominated you for a Liebster Award!\n",
      "For the rules please see my page at http://www.jessicahutchins.com/liebster-award/.\n",
      "Thanks for nominating me, that was so sweet of you! ��� I’ll check it out soon, and pass on the nomination to other bloggers ���� Thanks again!\n",
      "Hi Mimi, great blog and I love your photo! When I started reading the about me page, I thought “oh no! another travel blogger” but then you mentioned you live in Sydney so I persevered. Your writing is engaging and refreshingly honest. Looking forward to following you on the journey ����\n",
      "Thanks for your lovely comment, that’s really sweet of you! ��� Thanks for following too – it’s much appreciated! Hopefully we can continue to meet\n",
      "llama_print_timings:        load time =  1034.78 ms\n",
      "llama_print_timings:      sample time =   898.89 ms /   512 runs   (    1.76 ms per token)\n",
      "llama_print_timings: prompt eval time =  4572.21 ms /   265 tokens (   17.25 ms per token)\n",
      "llama_print_timings:        eval time = 16367.77 ms /   510 runs   (   32.09 ms per token)\n",
      "llama_print_timings:       total time = 21936.31 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817797\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.94 MB\n",
      "llama_model_load_internal: mem required  = 5439.94 MB (+ 1026.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x148714c80\n",
      "ggml_metal_init: loaded kernel_mul                            0x148716190\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x148717780\n",
      "ggml_metal_init: loaded kernel_scale                          0x1487170c0\n",
      "ggml_metal_init: loaded kernel_silu                           0x148718290\n",
      "ggml_metal_init: loaded kernel_relu                           0x148716730\n",
      "ggml_metal_init: loaded kernel_gelu                           0x148718be0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x148719b80\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14871aec0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14871a230\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14871b7d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14871cc80\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x14871c160\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x14871d520\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x14871d780\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x14871e000\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x14871e940\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14871fa20\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x148720380\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x148720fc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x148721b20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x148722360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x148722db0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1487237e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x148724320\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x148724ca0\n",
      "ggml_metal_init: loaded kernel_rope                           0x148725a70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x148726630\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1487272c0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.95 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it with purpose. We need to take a stand and choose what we want our lives to represent, whether that's through work or hobbies.\n",
      "If you’re reading this, you are most likely like me: you enjoy being outside, you love nature and beauty, and you crave adventure and excitement in your life. You probably spend a lot of time on the go, either physically going somewhere new, or mentally thinking about where you want to be next, or what’s happening around the world.\n",
      "It’s easy to get caught up in the fast-paced lifestyle we live today, and it can become all too easy to lose sight of why we live the way we do. We have been programmed for so long that work is priority number one; that money is the most important thing; that material possessions are essential for happiness.\n",
      "I believe there’s more to life than this. I believe the meaning of life is to live it with purpose. We need to take a stand and choose what we want our lives to represent, whether that’s through work or hobbies—we need to have a plan. What I mean by this is that we need to make choices that are in line with our values; that are aligned with who we are as people.\n",
      "I believe the meaning of life is to live it with purpose. We need to take a stand and choose what we want our lives to represent, whether that’s through work or hobbies.\n",
      "There is no right way to do this; you can choose whatever it may be for you: to have more children, to travel the world, to become an entrepreneur, to dedicate your life to a charity, to pursue a career in business, etcetera—the possibilities are endless!\n",
      "However, I believe we need to have a purpose or direction in order to be happy. We all crave meaning and fulfillment, but it’s not something that is automatically given to us, so we need to go out there and make it for ourselves. We must take control of our lives and decide what we want them to represent; this will define how we spend our time from now on until the end of our days.\n",
      "It’s easy to forget sometimes that we have a choice in how we live, but I believe it is vital to remember this. It gives us power over our own happiness and future and makes us less likely to\n",
      "llama_print_timings:        load time =  1030.88 ms\n",
      "llama_print_timings:      sample time =  1052.74 ms /   512 runs   (    2.06 ms per token)\n",
      "llama_print_timings: prompt eval time =  4608.55 ms /   265 tokens (   17.39 ms per token)\n",
      "llama_print_timings:        eval time = 16650.27 ms /   510 runs   (   32.65 ms per token)\n",
      "llama_print_timings:       total time = 22428.75 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817820\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x132625dd0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1326272c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x132628810\n",
      "ggml_metal_init: loaded kernel_scale                          0x132627520\n",
      "ggml_metal_init: loaded kernel_silu                           0x132629310\n",
      "ggml_metal_init: loaded kernel_relu                           0x132629a20\n",
      "ggml_metal_init: loaded kernel_gelu                           0x132629c80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13262ac10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13262ae70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13262c0f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13262b460\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13262cb70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13262deb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13262d3c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13262e780\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13262f0c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13262fa00\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x132630ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x132631440\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1326320a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x132632c00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x132633440\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x132633e30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x132634820\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x132635360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x132635e60\n",
      "ggml_metal_init: loaded kernel_rope                           0x132636a90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x132637610\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1326382e0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I believe that happiness comes from giving love and expressing kindness to others.\n",
      "I believe in being honest with people; when you’re good, say so; if you make a mistake, admit it quickly – be brave and take responsibility for your actions.\n",
      "I believe in the importance of self-esteem. If you don’t believe in yourself, how do expect other people to believe in you?\n",
      "I believe that true love comes from accepting someone as they are.\n",
      "I believe that when we feel good about ourselves, it makes us more attractive to others and increases our chances for success.\n",
      "And I believe that if we are happy with our lives, we can do anything – no matter how difficult the task may seem!\n",
      "Previous Post How Much Do You REALLY Enjoy Your Work? Next Post What Are You Living For?\n",
      "How much does it cost to live in your world?\n",
      "What Are You Living For?\n",
      "How Much Do You REALLY Enjoy Your Work?\n",
      "Follow @@johnmccarthy050 on Twitter!\n",
      "Copyright © John McCarthy - Life Coach, Motivational Speaker and Author of \"The Path to Happiness\". All Rights Reserved. Designed by: WP Themes Unlimited. Based on the WordPress Theme: Twenty Ten by Chris Pearson. Powered by WordPress | Theme by Bluchic.\n",
      "This website contains affiliate links, which means we receive a commission when you click through and make a purchase. This allows us to continue to provide free content for our readers. Thank you!\n",
      "Join my email list for the latest information about upcoming events, workshops, podcasts and more! You can also connect with me on Facebook by clicking here.\n",
      "John is an international best-selling author, life coach and motivational speaker who helps people overcome their challenges to achieve success and happiness in all areas of their lives. He has coached hundreds of clients from around the world. His book \"The Path to Happiness\" was published in November 2017.\n",
      "To learn more about John or schedule a free consultation, visit JohnMcCarthy.ca. You can connect with him on Facebook by clicking here. To subscribe to his newsletter, click here!\n",
      "John McCarthy is an international best-selling author, life coach and motivational\n",
      "llama_print_timings:        load time =  7403.28 ms\n",
      "llama_print_timings:      sample time =  1749.79 ms /   512 runs   (    3.42 ms per token)\n",
      "llama_print_timings: prompt eval time =  4686.76 ms /   265 tokens (   17.69 ms per token)\n",
      "llama_print_timings:        eval time = 35858.74 ms /   510 runs   (   70.31 ms per token)\n",
      "llama_print_timings:       total time = 42445.33 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817871\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x137e3e0d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x137e3f5c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x137e40b10\n",
      "ggml_metal_init: loaded kernel_scale                          0x137e3f820\n",
      "ggml_metal_init: loaded kernel_silu                           0x137e41610\n",
      "ggml_metal_init: loaded kernel_relu                           0x137e41d20\n",
      "ggml_metal_init: loaded kernel_gelu                           0x137e41f80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x137e42f10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x137e43170\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x137e443f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x137e43760\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x137e44e70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x137e461b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x137e456c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x137e46a80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x137e473c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x137e47d00\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x137e48de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x137e49740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x137e4a3a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x137e4af00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x137e4b740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x137e4c130\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x137e4cb20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x137e4d660\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x137e4e160\n",
      "ggml_metal_init: loaded kernel_rope                           0x137e4ed90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x137e4f930\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x137e505d0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and do good.\n",
      "I love to read, to learn new things and spend time with my family. I have two children. My daughter is 20 years old and works as a doctor in Moscow. She has already graduated from university and is now studying at the postgraduate school. My son is studying in the second grade of school. He is an active boy, he can swim very well, plays football, volleyball.\n",
      "I am interested in cooking, I like to make different dishes. In my free time I go shopping or walk with my dog. I enjoy reading books and going to the cinema.\n",
      "In my spare time I like spending time with my family - we often do things together: go on picnics, watch movies, go swimming in summer. We also like travelling. Last year we went to Barcelona for a holiday, next year we are going to visit Venice. I hope that you will be interested in me and write me letters!\n",
      "My dream is to find a man from another country who can understand my feelings and thoughts, who wants to love and be loved for all the life. A man with sense of humor, kind heart, who is not afraid of difficulties. He should love children as I do. I want him to have big family. I am looking for a serious relationship, which will lead to marriage.\n",
      "I hope that one day you will find me and we will be happy together! I look forward to your letter!\n",
      "Sometimes it is difficult to describe myself in few words but I'll try to do it.\n",
      "I like reading books, doing sports (go swimming, walk or jogging), watching movies and going out with friends.\n",
      "My dream is to find a man from another country who can understand my feelings and thoughts, who wants to love and be loved for all the life. A man with sense of humor, kind heart, who is not afraid of difficulties. He should love children as I do. I want him to have big family. I am looking for a serious relationship, which will lead to marriage. I hope that one day you will find me and we will be happy together!\n",
      "I look forward to your letter!\n",
      "Hi, my name is Elena. I was born in Moscow but live now in the city of Vyborg.\n",
      "I love to read books, do sports (go swimming, walk or jogging), watching movies and going out\n",
      "llama_print_timings:        load time =  4169.48 ms\n",
      "llama_print_timings:      sample time =  1686.54 ms /   512 runs   (    3.29 ms per token)\n",
      "llama_print_timings: prompt eval time =  4624.40 ms /   265 tokens (   17.45 ms per token)\n",
      "llama_print_timings:        eval time = 35629.18 ms /   510 runs   (   69.86 ms per token)\n",
      "llama_print_timings:       total time = 42090.09 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817917\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.09 MB\n",
      "llama_model_load_internal: mem required  = 14645.09 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x145f27760\n",
      "ggml_metal_init: loaded kernel_mul                            0x145f28c50\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x145f2a1a0\n",
      "ggml_metal_init: loaded kernel_scale                          0x145f28eb0\n",
      "ggml_metal_init: loaded kernel_silu                           0x145f2aca0\n",
      "ggml_metal_init: loaded kernel_relu                           0x145f2b3b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x145f2b610\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x145f2c5a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x145f2c800\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x145f2da80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x145f2cdf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x145f2e500\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x145f2f840\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x145f2ed50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x145f30110\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x145f30a50\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x145f31390\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x145f32470\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x145f32dd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x145f33a30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x145f34590\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x145f34dd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x145f357c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x145f361b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x145f36cf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x145f377f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x145f38420\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x145f38fa0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x145f39c70\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.09 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out what you're best at and do it better than anyone else, and do it for as long as you can.\n",
      "I don't think there's a single thing that's good or bad about acting. It makes you stronger mentally, because you have to face a lot of rejection, and I think that helps in life.\n",
      "I don't think it's ever going to be easy for the human race as long as we can see each other. The way things are going now in the world, it's only getting more difficult. There will always be wars, because people want power. And there will always be people wanting to have a better life than others.\n",
      "I don't think that anyone should look back on their lives and wish they had done something differently unless they feel regret for doing it - as long as you can say, 'Yeah, I did the right thing,' then all is well.\n",
      "I don't think there are any hard and fast rules to acting, and I wouldn't want them if there were. I wouldn't want to do a different job; I just want to be an actor. The only rule that I would say is to try to work with good directors and writers so you can learn as much as possible from them.\n",
      "I don't think this generation should take the blame for all the things that have gone wrong in the world - it was a lot of other people before them. We just need to do what we can to help fix things, and not get too overwhelmed by all the bad stuff going on.\n",
      "If you want to be an actor, then just go out there and do it. There's no right or wrong way to act - it's a very personal thing, so if you like it, then go for it. And don't give up until you get your first job!\n",
      "I love doing movies, but I also enjoy doing stage work too. In the theatre, there is no room for error; I can't be late or forget my lines. It keeps me on my toes.\n",
      "I try not to read a script until I have got it - because then I know what you want from me and that ruins things. But if I don't get the part, I will read it afterwards and see how I could have done something different with the character.\n",
      "If you are going out on auditions, just go\n",
      "llama_print_timings:        load time =  4389.26 ms\n",
      "llama_print_timings:      sample time =  1617.94 ms /   512 runs   (    3.16 ms per token)\n",
      "llama_print_timings: prompt eval time =  4627.86 ms /   265 tokens (   17.46 ms per token)\n",
      "llama_print_timings:        eval time = 35613.05 ms /   510 runs   (   69.83 ms per token)\n",
      "llama_print_timings:       total time = 42006.85 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689817964\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11f011370\n",
      "ggml_metal_init: loaded kernel_mul                            0x11f012860\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11f013db0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11f014090\n",
      "ggml_metal_init: loaded kernel_silu                           0x11f014860\n",
      "ggml_metal_init: loaded kernel_relu                           0x11f012e00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11f0151b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11f0161b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11f016410\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11f0177e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11f0169f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11f0192f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x11f018940\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x11f019bc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x11f01a4d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x11f01ae10\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x11f01b750\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11f01c220\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11f01cd50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11f01e000\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11f01e9e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x11f01f400\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x11f01fde0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x11f020940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x11f0212a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x11f021cb0\n",
      "ggml_metal_init: loaded kernel_rope                           0x11f0228c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11f0235b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11f024100\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living out the purpose for which God created me.\n",
      "My name is Cheryl Ladd, and I am here to help you find your purpose and live it out successfully. You have a lot going on—you’re busy, overwhelmed, stressed, and at times, discouraged about what lies ahead of you. You don’t always feel like you’re moving forward or making progress in the right direction.\n",
      "I am here to help you overcome these feelings. I am also here to help you identify your God-given purpose, and then show you how to live it out every day. I want you to find your true worth so that you can move from where you are now—to a place of abundance in life.\n",
      "My book is available through Amazon or any Christian Bookstore. You can also buy the book at my website. Click here to go to the store and order.\n",
      "I was born on April 15, 1943, as Beverly Ann Mitchell to parents, John (Jack) E. Mitchell and Edna Mae (Rose) Mitchell in Los Angeles, California, but I became known by my stage name, Cheryl Ladd. My birth order was the sixth of seven children born in our family.\n",
      "My father was a professional football player for the Rams (1937-40). He eventually became an engineer and worked at Lockheed. My mother went to work as a secretary after my dad retired, which is when she met my stepfather, Bill Ladd. They were married in 1952, and I moved to Palo Alto with them that same year.\n",
      "My family was quite dysfunctional, but there were some positive things about it too—like the music. We sang constantly!\n",
      "I grew up in Palo Alto, California, where my parents had a restaurant and I worked as a waitress while in high school. At age 17, I attended Miss Golden State beauty pageant where I won the title. This helped me get noticed by Hollywood agents and led to an acting career for me.\n",
      "I became known professionally for the role of Kris Munroe on the hit TV series “Charlie’s Angels” from 1976-81, which later spawned a successful movie franchise. I also appeared in many other movies and television series throughout my career.\n",
      "llama_print_timings:        load time =  2355.26 ms\n",
      "llama_print_timings:      sample time =  1479.62 ms /   512 runs   (    2.89 ms per token)\n",
      "llama_print_timings: prompt eval time =  8507.56 ms /   265 tokens (   32.10 ms per token)\n",
      "llama_print_timings:        eval time = 30240.06 ms /   510 runs   (   59.29 ms per token)\n",
      "llama_print_timings:       total time = 40357.77 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818007\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13b21d780\n",
      "ggml_metal_init: loaded kernel_mul                            0x13b21ec70\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13b2201c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13b2204a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x13b220cc0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13b21f210\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13b221620\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13b222590\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13b2238d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13b222c40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13b2241b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13b2256b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x13b224b90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x13b225f90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x13b2261f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x13b226a70\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x13b2273b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13b228490\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13b2286f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13b229ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13b22a560\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x13b22af70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x13b22b980\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x13b22c3b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x13b22cec0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x13b22d860\n",
      "ggml_metal_init: loaded kernel_rope                           0x13b22e460\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13b22efd0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13b22fcb0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out who you are and what you are supposed to be doing.\n",
      "I have always believed that writing a book is a journey with many paths. We may start off in one direction, but after a while we realize it’s not the right path for us. We back up and find another. In the end the most important thing is that we get to our destination: the finished first draft of a manuscript.\n",
      "It takes time to write a novel. It takes patience. You have to be prepared for lots of road blocks, dead ends, and wrong turns. But if you keep your mind open and focus on the goal, you will eventually reach it. And when you do … well, it’s an exhilarating feeling.\n",
      "So, in that spirit, I will continue writing this blog as a way to stay accountable for my journey of discovery into who I am and what it is I’m supposed to be doing.\n",
      "I started out thinking I was on the wrong path. My first draft wasn’t working. I could see it. It felt forced. It didn’t fit. But I kept going because that is what you do when you are writing a novel, right? You keep going until it feels right! And so I did.\n",
      "Then one day I realized I was on the wrong path. My first draft wasn’t working and it was time to back up and find another way in. So I started over with my characters. I wanted them to be more realistic, and I wanted their interactions to come off as more natural. This meant a lot of character work that required me to re-read the book I read on writing characters (The Emotional Craft of Fiction by Donald Maass) and watch more episodes of The Office than was healthy for my brain.\n",
      "After all this character work, however, I realized there was still something missing. My plot felt forced and unnatural. It was time to back up again and find another way in. So I wrote a synopsis. This helped me get back on track with the storyline (which is a lot harder than it sounds!).\n",
      "I’m now at 35,000 words–halfway through my second draft! And this one is feeling pretty good so far. It’s moving along nicely, and I think I’ve found the right path for this manuscript. I have another 40,000 words to go before it’s done\n",
      "llama_print_timings:        load time =  1766.71 ms\n",
      "llama_print_timings:      sample time =  1574.83 ms /   512 runs   (    3.08 ms per token)\n",
      "llama_print_timings: prompt eval time =  8633.67 ms /   265 tokens (   32.58 ms per token)\n",
      "llama_print_timings:        eval time = 30481.61 ms /   510 runs   (   59.77 ms per token)\n",
      "llama_print_timings:       total time = 40816.47 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818049\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7023.99 MB\n",
      "llama_model_load_internal: mem required  = 9071.99 MB (+ 1608.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x142f4fad0\n",
      "ggml_metal_init: loaded kernel_mul                            0x142f50fc0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x142f52510\n",
      "ggml_metal_init: loaded kernel_scale                          0x142f51220\n",
      "ggml_metal_init: loaded kernel_silu                           0x142f53010\n",
      "ggml_metal_init: loaded kernel_relu                           0x142f53720\n",
      "ggml_metal_init: loaded kernel_gelu                           0x142f53980\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x142f54910\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x142f54b70\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x142f55df0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x142f55160\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x142f56870\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x142f57bb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x142f570c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x142f58480\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x142f58dc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x142f59700\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x142f5a7e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x142f5b140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x142f5bda0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x142f5c900\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x142f5d140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x142f5db30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x142f5e520\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x142f5f060\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x142f5fb60\n",
      "ggml_metal_init: loaded kernel_rope                           0x142f60790\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x142f61310\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x142f61fe0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have an experience. As we all know, we are spiritual beings having a human experience. We are here to learn and grow. And we each get to choose our own experiences and how we experience them.\n",
      "Let me tell you about my choice!\n",
      "In June 2016 I made the decision to pursue yoga certification through Yoga International with Annie Carpenter. I had been teaching on and off for years but never really thought much of myself as a teacher, which is why I never went after it. However, one day my husband and I were talking about how we wanted to travel and make a living at the same time. So I told him I'd look into yoga certification programs that would allow me to do both. The first company I looked into was Yoga International (YI). It sounded like it could work for our life, so I applied.\n",
      "While I waited for their response I read through their website and watched one of Annie's videos on how she teaches a class from start to finish. Then I thought about the many yoga classes I had taken over the years with various teachers. And I realized that my experience was quite different than what I was seeing in most yoga classes I took, even those taught by some pretty well-known instructors.\n",
      "I saw myself as a student of life...and so it goes for me. I'm here to learn and grow. And I decided then and there that I would have my own experience, an Annie Carpenter experience! So I signed up immediately!\n",
      "So here is the story about how it all began...\n",
      "The first thing I learned was that yoga certification was not an easy process. Not only did I have to learn everything from scratch but I had to put my heart and soul into it every day for three months, which meant that I had to give up almost everything else in my life (including my own practice) because this is what it takes!\n",
      "I began with the basics of yoga philosophy. This was a very new experience for me and although I felt like I knew these things already, I soon realized how much more I needed to learn and that there were many layers to each topic. I spent hours every day reading about various topics such as ahimsa (non-violence), aparigraha (non-hoarding), satya (truthfulness), yamas (restraints) etc.,\n",
      "llama_print_timings:        load time =  1756.20 ms\n",
      "llama_print_timings:      sample time =  1148.13 ms /   512 runs   (    2.24 ms per token)\n",
      "llama_print_timings: prompt eval time =  9489.58 ms /   265 tokens (   35.81 ms per token)\n",
      "llama_print_timings:        eval time = 29697.39 ms /   510 runs   (   58.23 ms per token)\n",
      "llama_print_timings:       total time = 40442.25 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818092\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12f010180\n",
      "ggml_metal_init: loaded kernel_mul                            0x12f0116e0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12f012c00\n",
      "ggml_metal_init: loaded kernel_scale                          0x12f012f10\n",
      "ggml_metal_init: loaded kernel_silu                           0x12f0136c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x12f011c80\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12f013ff0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12f014fe0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12f016340\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12f0165a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12f015850\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12f016f60\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x12f0182d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x12f0177b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x12f018a10\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x12f019350\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x12f019c90\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12f01a760\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12f01b870\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12f01c4b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12f01d010\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x12f01d820\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x12f01e240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x12f01ec20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x12f01f790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x12f020290\n",
      "ggml_metal_init: loaded kernel_rope                           0x12f020e90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12f021a30\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12f022700\n",
      "ggml_metal_add_buffer: buffer 'data' size 26032652288 is larger than buffer maximum of 17179869184\n",
      "llama_init_from_file: failed to add buffer\n",
      "llama_init_from_gpt_params: error: failed to load model './models/13B/ggml-model-f16.bin'\n",
      "main: error: unable to load model\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818107\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give.\n",
      "Life is to be given away. Away from selfishness and self-concern. Away from hatred and anger. Away from bitterness and sorrow. Life is to be given in love, in joy, in peace, in kindness.\n",
      "A life of giving means that I have to be a servant. A servant of God first, then others. My life has to be about serving; to serve others and to serve the world around me. I have to be willing to give. I have to be willing to let go of some things so I can help someone else.\n",
      "I am reminded of this every time we take a trip to the grocery store, or do any shopping for that matter. We will always see people who are less fortunate than we are. They ask us for money or food. It is then that we have to make the choice: do I give?\n",
      "I believe it isn’t so much about whether you give these people money or not (though a lot of times we should), but more what your attitude towards them is. We can be kind, or we can be mean and tell them to go away and leave us alone. We can help others in need, or we can ignore their needs altogether.\n",
      "I know some people believe you shouldn’t give money to the homeless because it will only enable them to stay on the streets rather than getting a job and a home. I am all for helping the homeless get back on their feet and into jobs, but that doesn’t mean they should have to suffer until then.\n",
      "I think we need to be giving in every aspect of our lives, and not just to those less fortunate than us. We also need to give to others who are more fortunate: money, time, attention. Giving is the key to a happy life.\n",
      "We all have something we can give. You don’t have to be rich or famous to live a life of giving. It could be just your smile, your kindness, your love. It doesn’t matter what it is as long as you are willing to let go of some of yourself and share it with someone else.\n",
      "We are all called to give in our own unique way. I know for me it will always include helping the homeless. I don’t care if they spend my money on drugs or alcohol, the fact is that they need a little something extra\n",
      "llama_print_timings:        load time = 23714.92 ms\n",
      "llama_print_timings:      sample time =   365.25 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 11378.73 ms /   265 tokens (   42.94 ms per token)\n",
      "llama_print_timings:        eval time = 116933.82 ms /   510 runs   (  229.28 ms per token)\n",
      "llama_print_timings:       total time = 128742.15 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818260\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find our purpose.\n",
      "We are all born with a reason for being here and that reason is unique to us, as it is part of who we are.\n",
      "There may be many reasons why some people never find this; perhaps they don’t even know what their “reason” is because it has been hidden from them or maybe they have become so busy running through life that they haven’t taken the time to stop and think about what it could possibly be.\n",
      "I believe our purpose is not just one thing, but many things – all of which combine to form something much greater than ourselves. It is part of us as individuals, who we are, but it also has a broader effect on people, animals, the world around us and beyond.\n",
      "Our purpose doesn’t have to be big or dramatic; in fact it isn’t always a grand gesture that will change the lives of millions. It could simply be something as small as a smile to someone who needs one.\n",
      "It is our job to find it, whether we do this consciously or not. Finding and following your purpose can bring you fulfilment, peace and happiness. However, it may also bring you challenges and trials, but this is all part of the journey.\n",
      "I believe that we are each here for a reason, to make a difference and to help others; our true purpose will always be there waiting patiently for us to find it.\n",
      "The question is…what’s yours?\n",
      "Previous Post 10 Reasons why I love being an Intuitive Coach\n",
      "Next Post What is Intuition?\n",
      "One thought on “What is the meaning of life?”\n",
      "Pingback: The power of a vision board – Life in Colourful Clarity | The Intuitive Coach Blog\n",
      "Leave a Reply to Precious S.\tCancel reply\n",
      "The Intuitive Coach Blog What is Intuition?\n",
      "Copyright © 2019 Life in Colourful Clarity. All Rights Reserved. Designed by bavotasan.com.\n",
      "This website uses cookies to improve your experience. We'll assume you're ok with this, but you can opt-out if you wish.Accept Read More Privacy & Cookie Policy. ACCEPT Reject REJECT Privacy & Cookie Policy\n",
      "Website by Bavotasan.com\n",
      "The Intuitive Coach B\n",
      "llama_print_timings:        load time = 23924.58 ms\n",
      "llama_print_timings:      sample time =   364.64 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 11702.01 ms /   265 tokens (   44.16 ms per token)\n",
      "llama_print_timings:        eval time = 118364.16 ms /   510 runs   (  232.09 ms per token)\n",
      "llama_print_timings:       total time = 130490.63 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9469f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818415\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.67 MB\n",
      "llama_model_load_internal: mem required  = 26874.67 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a life with purpose and passion. That doesn’t mean it has to be perfect, or even easy, but that you can take on any challenge and make something beautiful out of it… that every moment is an opportunity for growth and joy.\n",
      "In my photography, this means creating images that are both real & timeless. Images that capture the emotion and details of each individual’s story – their love, connection, and beauty. I want the photos from your wedding day to be a reflection of you as a couple – the real you!\n",
      "Everyone has heard a photographer say they “capture the moments”…but what does that really mean? To me it means taking the time to connect with my couples during their engagement session and on their wedding day. It’s about understanding your vision & being able to help you create it!\n",
      "I believe in the power of connection through photography, and I feel privileged when I get to share these moments with my couples; both in front of the camera as well as behind-the-scenes.\n",
      "So, thank you for visiting my site! I hope you love your experience here as much as I loved creating it!\n",
      "I'm just a girl with a camera who likes capturing real emotions & authentic moments in time.\n",
      "© 2019 Kaitlin Boucher Photography|ProPhoto Site by NetRivet, Inc.\n",
      "Hello there! It’s so nice to meet you. I am an Oregon native born and raised on the Oregon Coast. My husband Andrew (also an Oregon native) and I have spent most of our lives here along with our adorable son, Noah. We are currently living in Astoria, Oregon where we enjoy hiking & camping on the beautiful Oregon coast!\n",
      "I was a wedding coordinator for 10 years before deciding to follow my heart and pursue photography full-time. I am truly passionate about what I do – creating authentic images that capture the real moments of your wedding day and tell the story of you & your love. When I’m not photographing weddings, I enjoy spending time with my family, reading, hiking, camping, or baking!\n",
      "I am a self-taught photographer who has been pursuing photography full-time since 2015. Since becoming fully invested in the art\n",
      "llama_print_timings:        load time = 22867.67 ms\n",
      "llama_print_timings:      sample time =   365.16 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 11196.25 ms /   265 tokens (   42.25 ms per token)\n",
      "llama_print_timings:        eval time = 118566.00 ms /   510 runs   (  232.48 ms per token)\n",
      "llama_print_timings:       total time = 130190.62 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818957\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x114e62710\n",
      "ggml_metal_init: loaded kernel_mul                            0x114e63c20\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x114e64020\n",
      "ggml_metal_init: loaded kernel_scale                          0x114e649b0\n",
      "ggml_metal_init: loaded kernel_silu                           0x114e65200\n",
      "ggml_metal_init: loaded kernel_relu                           0x114e66520\n",
      "ggml_metal_init: loaded kernel_gelu                           0x114e65b40\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x114e66f20\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x114e68930\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x114e67e40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x114e67940\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x114e6a6f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x114e69d70\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x114e693f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x114e6afc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x114e6b900\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x125812b30\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1258138f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x125814060\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1258151e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x114e6cf00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x114e6d320\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x114e6dda0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x114e6e7f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x114e6f310\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x114e6fe30\n",
      "ggml_metal_init: loaded kernel_rope                           0x114e70a90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1153040d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1153068e0\n",
      "ggml_metal_add_buffer: buffer 'data' size 18355355648 is larger than buffer maximum of 17179869184\n",
      "llama_init_from_file: failed to add buffer\n",
      "llama_init_from_gpt_params: error: failed to load model './models/30B/ggml-model-q4_0.bin'\n",
      "main: error: unable to load model\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689818966\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I know that happiness comes from within, not from external sources. It comes from how we choose to see things.\n",
      "I also know that there are many ways in which to be happy and that every person has a different journey to get there. Everyone’s path is unique.\n",
      "What I don’t know is why people believe they have the right to decide what makes you happy or not. Or how others think they can tell us how we should be living our lives.\n",
      "I’m talking about friends and family who love us, but that somehow feel entitled to judge us. That somehow find it acceptable to dictate what makes us happy. Who somehow believe they know better than we do when it comes to our happiness.\n",
      "How is it possible for a person to not understand the concept of happiness? How can someone be so self-absorbed and careless as to decide the direction of another’s life without asking their opinion?\n",
      "And most importantly, how can they believe that their way is the only way when it comes to our happiness?\n",
      "I remember I used to have a friend who would often say: “Do what you want. Just don’t think you are doing it for me.” It always made me laugh and it never came across as mean or condescending, rather the opposite. It was always said with love and good intentions.\n",
      "But in the last few years I have noticed a lot of people around me who, while loving us, still decide to take control of our happiness by taking decisions on our behalf. And they do so without asking for our opinion or listening to what we are saying.\n",
      "I know sometimes it’s hard to make your voice heard over the noise that comes from others telling you how you should live your life. I also know it can be difficult to find your own path when everyone around you is trying to convince you to do things their way and not yours.\n",
      "So what can we do? How can we break free of other people’s expectations? How can we live our lives according to the principles that make us happy without having to feel guilty or ashamed?\n",
      "I believe it all starts in understanding who we are and why we want certain things. It also comes from knowing which goals and dreams are truly meaningful for us and what are those others would like us to have. And then, most importantly, it comes from having the courage to go after our dreams despite other people\n",
      "llama_print_timings:        load time = 10484.25 ms\n",
      "llama_print_timings:      sample time =   365.04 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 21048.95 ms /   265 tokens (   79.43 ms per token)\n",
      "llama_print_timings:        eval time = 93203.20 ms /   510 runs   (  182.75 ms per token)\n",
      "llama_print_timings:       total time = 114666.35 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689819092\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in the simple pleasures. Like a walk in the park, or a nap on the sofa... and let's not forget ice cream.\n",
      "Matthew McConaughey is an American actor whose first leading role was in 1996’s “A Time to Kill.” He then went on to appear in many movies throughout the early 2000s, including “Contact,” “U-571” and “Reign of Fire.” In 2005, he starred in “Sahara,” which was a commercial flop. After that, McConaughey decided to take some time off and spend it with his family. He returned to acting in the mid 2000s, starring in “We Are Marshall,” “Fool’s Gold” and “The Ghosts of Girlfriends Past.” In 2010, he received critical acclaim for his role in “The Lincoln Lawyer” and won a Best Actor award at the Cannes Film Festival for “Killer Joe.” He then starred in the film adaptation of John Grisham’s novel “The Paperboy,” which premiered at the 2012 Cannes Film Festival. In 2014, he appeared as HIV positive Ron Woodroof in \"Dallas Buyers Club.\" His performance earned him an Academy Award for Best Actor.\n",
      "In 2015, McConaughey starred in two films: the thriller “Interstellar” and a crime drama called “The Dark Tower.” In 2016, he appeared in the film adaptation of “Gold,” which is based on a true story about the Bre-X mining scandal. He also made his directorial debut with a documentary about American football coaches entitled “J.K. Livin Foundation.”\n",
      "In addition to his acting career, McConaughey has served as the celebrity spokesman for “The Dodge Ram” campaign since 2013. Through his work with them, he’s helped raise $1 million dollars for The J.K. Livin Foundation. He is married to Brazilian model Camila Alves. The couple wed in June 2012 after dating for six years. They have three children together.\n",
      "Matthew McConaughey Filmography\n",
      "M\n",
      "llama_print_timings:        load time = 10930.63 ms\n",
      "llama_print_timings:      sample time =   366.14 ms /   512 runs   (    0.72 ms per token)\n",
      "llama_print_timings: prompt eval time = 21380.55 ms /   265 tokens (   80.68 ms per token)\n",
      "llama_print_timings:        eval time = 93640.40 ms /   510 runs   (  183.61 ms per token)\n",
      "llama_print_timings:       total time = 115435.93 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77454d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689819218\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.03 MB\n",
      "llama_model_load_internal: mem required  = 19809.03 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find\n",
      "your gift is the meaning of life.\n",
      "When we were children, it was easy. What was my gift?\n",
      "I’m a writer. I knew that even before I could read and write.\n",
      "What about you? When you were eight years old, what did you love doing every day? That was your gift, your purpose. You are here to do just that.\n",
      "If you have lost track of your gift, it is time to find it again.\n",
      "And how do we find our gifts? By doing the work.\n",
      "What is the work you can’t not do? Whatever it is—that’s your gift. That’s what you are here to do.\n",
      "You have something unique to offer this world.\n",
      "Your purpose is encoded in who you really are, and it is your sacred responsibility to find out what that is and live that truth every day.\n",
      "What if I told you it is possible—and even easy—to rediscover your gift?\n",
      "It’s called The Artist’s Way, and this book will help you do just that.\n",
      "I know firsthand how important it is to discover our gifts. As a young child, I was always writing stories and plays. Even before I could write my stories down, I would act them out in the backyard with my friends. When I was ten years old, I started to write my own songs on the piano.\n",
      "I thought everybody had these experiences, so it never occurred to me that they were special. They were just what made me feel alive.\n",
      "In high school, however, I stopped writing. The idea of being a writer seemed too weird and scary. Instead, I was a cheerleader and a student council president. I wrote my college application essay about how much I loved sports—and then promptly gave up all athletics once I got to college.\n",
      "As I struggled in my sophomore year at Duke University, I realized that I needed to find something to make me come alive again. One afternoon, as I sat on a bench outside of the dorm, I started to write down all of my interests and things I loved doing every day. At the top of this list was writing.\n",
      "I decided then and there that I would go back to being a writer. I wrote plays, poetry, short stories—and eventually, novels. I started a humor column in the school newspaper, where I\n",
      "llama_print_timings:        load time = 10968.58 ms\n",
      "llama_print_timings:      sample time =   365.14 ms /   512 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 21135.12 ms /   265 tokens (   79.76 ms per token)\n",
      "llama_print_timings:        eval time = 94983.05 ms /   510 runs   (  186.24 ms per token)\n",
      "llama_print_timings:       total time = 116532.20 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d974327",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 681 (a09f919)\n",
      "main: seed  = 1689819346\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.70 MB\n",
      "llama_model_load_internal: mem required  = 64349.70 MB (+ 3124.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "– Pablo Picasso\n",
      "What we call the secret of happiness is no more a secret than our willingness to choose life.\n",
      "– Leo Buscaglia\n",
      "Hope deferred makes the heart sick, but a longing fulfilled is a tree of life.\n",
      "– Proverbs 13:12 (NIV)\n",
      "I believe that the very purpose of life is to be happy. From the very core of our being, we desire contentment. In my own limited experience I have found that the more we care for the happiness of others, the greater is our own sense of well-being. Cultivating a close, warmhearted feeling for others automatically puts the mind at ease. It helps remove whatever fears or insecurities we may have and gives us the strength to cope with any obstacles we encounter. It is the principal source of success in life. Since we are not solely material creatures, it is a mistake to place all our hopes for happiness on external development alone. The key is to develop inner peace.\n",
      "– Dalai Lama\n",
      "# Introduction: My Story\n",
      "I have been looking for my purpose since I was twelve years old and I'm going to tell you how I found"
     ]
    }
   ],
   "source": [
    "# try cpu (over one min per token)\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
