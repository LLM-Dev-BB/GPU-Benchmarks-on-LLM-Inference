{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95918bf-d7b4-45ca-8235-49d1f7a61bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 03:42:50 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe           On | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0               43W / 300W|      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "model name\t: AMD EPYC 7543 32-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       1056751276 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1a46a0-1dfa-4a0d-aebb-68b02d4b959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edaeff4-e9d1-4f4d-8e7d-de0d546347d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7495179-6d7a-4020-a78f-668e34a18a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float, float)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kllama.cpp:2208:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koperation on ‘\u001b[01m\u001b[Kt_start_sample_us\u001b[m\u001b[K’ may be undefined [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsequence-point\u0007-Wsequence-point\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2208 |     int64_t t_start_sample_us = \u001b[01;35m\u001b[Kt_start_sample_us = ggml_time_us()\u001b[m\u001b[K;\n",
      "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f94d37-8050-4635-a642-5a7229fcd514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565392\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love, and to be loved.\n",
      "I was born in a small village called Dera Ismail Khan in Pakistan on 12th August 1964.\n",
      "My parents were very poor but they used to teach me, my younger brother, sister & cousins. I was the only one who could read from the whole family. My parents were so proud of me and my teachers that they could not afford to send me to a proper school. My mother had to work as a maid in another family’s home, and my father used to work in fields outside our village.\n",
      "We did not have any shoes or clothes but we were happy with what we had. We would go to the nearest mosque for prayers and there used to be a big garden at the back of it. Whenever I was feeling sad, I used to go to this garden and sit on the grass. Someone told me that if I was sitting in front of God, he would definitely listen to my prayer.\n",
      "I did not know about Islam, but I used to see Muslims praying towards Mecca (Kaaba). Once I asked a Muslim man who was also praying towards Kaaba why all the people were facing to the East and not to the West. He taught me that it is a Holy place in Arabia where the Prophet Muhammad, peace be upon him, received the message of God from the angel Jibraeel (Gabriel). It was very important for Muslims to face Kaaba and pray. I became very sad because I could not find out why we were all facing east instead of west.\n",
      "I used to go to the market on Friday mornings with my mother. Everyone would be in their own way praying towards Mecca, so I asked my mother why everyone was praying towards Kaaba and she told me that it is a Holy place where Prophet Muhammad peace be upon him received the message of God from Angel Jibraeel.\n",
      "I started to read my copy of Holy Qur’an and one day when I was reading Surah Al-Fatiha, I felt as if someone was talking to me. When I tried to follow that voice it took me to Kaaba where people were praying towards Mecca.\n",
      "In the evening the whole family went to a mosque for Eid prayers. During Jummah (Friday) prayer we all faced east and not west\n",
      "llama_print_timings:        load time =  6854.50 ms\n",
      "llama_print_timings:      sample time =   273.83 ms /   512 runs   (    0.53 ms per token,  1869.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   293.91 ms /   265 tokens (    1.11 ms per token,   901.64 tokens per second)\n",
      "llama_print_timings:        eval time =  5920.58 ms /   510 runs   (   11.61 ms per token,    86.14 tokens per second)\n",
      "llama_print_timings:       total time =  6592.11 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5116e2-78be-4cd0-94a1-c019b99c9ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565408\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose, live it and let yourself be guided by it.\n",
      "To me, purpose means using my skills for a greater good.\n",
      "My purpose on this earth is to help people and organizations move forward to make the world better place. For the last 25 years, I had the opportunity to lead a company that designs and manufactures high-performance industrial power supplies, as well as to be part of several startups where I helped them to create products and services for their customers that have made our lives easier and more enjoyable.\n",
      "I love my work. It keeps me active and alive.\n",
      "To inspire others to find their purpose, live it and let themselves be guided by it is what motivates me.\n",
      "That’s why I am writing this blog. My goal is to share with you the principles that guide me and hope they will help you to find your purpose in life and to achieve success in all areas of your life (health, family, work, personal growth).\n",
      "As an experienced professional and businessman, I know how hard it can be when we are faced with challenges. The reason for these challenges is usually the gap between our vision and reality, or as Napoleon Hill put it: “What the mind of man can conceive and believe, it can achieve.”\n",
      "I have spent many years to learn and practice the art of self-management that enabled me to be successful in business. I will share with you what I’ve learned about how we create our reality by using our minds and I will teach you the skills that will help you to develop a strong mindset that will guide you on your journey through life.\n",
      "Here is an example of my approach:\n",
      "I was going through a rough period in 2014 when I started noticing how much time, money and effort we spent at our house to keep it clean and organized. It occurred to me that by investing more of these precious resources into something with more meaning – charity work, for example – we could help people in need and make the world a better place.\n",
      "After some thought, I came up with an idea for a new business: to create a cleaning company that donates 20% of its profits to charities. We named this company “Clean and Give”.\n",
      "Here are my thoughts on what was driving me at that time:\n",
      "I am a caring person who deeply values family, friendship and community. I believe in the\n",
      "llama_print_timings:        load time =  1306.14 ms\n",
      "llama_print_timings:      sample time =   277.05 ms /   512 runs   (    0.54 ms per token,  1848.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   267.78 ms /   265 tokens (    1.01 ms per token,   989.63 tokens per second)\n",
      "llama_print_timings:        eval time =  5887.57 ms /   510 runs   (   11.54 ms per token,    86.62 tokens per second)\n",
      "llama_print_timings:       total time =  6533.14 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c96cc5c-ce16-4af7-b700-86fafeee2c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565418\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live life, and we must do that as if this were our last day on earth.\n",
      "This is what I mean by live your life like it’s your last day on earth. If you are going to live life with a passion, then you need to know what you want in this world. You have to know the things that would make you happy and fulfilled. The things that would make you say wow, these were the best days of my life.\n",
      "What are your goals? What is it that you really want out of life?\n",
      "If you don’t know what you want, then your life will be like a leaf floating with the wind – wherever it takes you. You won’t have any control over your future. You could end up in a place that you really didn’t want to go.\n",
      "I believe we all start out our lives without knowing exactly where we are headed. We don’t know what our goals are, and when we do set them they can change from day to day or even hour to hour. It is almost like trying to find your way in a maze with no map – you just go where the wind blows you.\n",
      "I think this is why people spend their lives drifting from one thing to another without ever finding peace, happiness and fulfillment. They don’t have a map to guide them and if they are not careful, they won’t even know what they really want out of life.\n",
      "This entry was posted in Uncategorized on March 19, 2017 by Bill.\n",
      "← The Secret To Success – Don't Give Up Just Keep Moving Forward 5 Simple Steps To Increase Your Wealth →\n",
      "One thought on “How to Live Life Like It’s Your Last Day on Earth”\n",
      "Pingback: How to Live Life Like It’s Your Last Day on Earth | Billionaire's Club Australia\n",
      "The Secret To Success – Don't Give Up Just Keep Moving Forward\n",
      "5 Simple Steps To Increase Your Wealth\n",
      "How to Live Life Like It’s Your Last Day on Earth\n",
      "Purpose Driven Living\n",
      "Are You a Victim of Modern Society?\n",
      "What Makes You Happy? How to Reach Your True Potential\n",
      "The Secret Of The Millionaires Is Out! Are You Taking Advantage?\n",
      "How To Live Life Like It's Your Last Day On Earth\n",
      "\n",
      "llama_print_timings:        load time =  1091.38 ms\n",
      "llama_print_timings:      sample time =   272.89 ms /   512 runs   (    0.53 ms per token,  1876.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   265.57 ms /   265 tokens (    1.00 ms per token,   997.84 tokens per second)\n",
      "llama_print_timings:        eval time =  5859.77 ms /   510 runs   (   11.49 ms per token,    87.03 tokens per second)\n",
      "llama_print_timings:       total time =  6498.40 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8ba9f3-7dd2-46fa-9646-895a003cef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565427\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m found when we find ourselves on our knees.\n",
      "I am a Christian who has made a lot of mistakes in my life, and I have come to accept that God’s love is unconditional and forgiving. I have been blessed with a wonderful wife, two beautiful children, and many great friends. My greatest desire is for you to know Jesus Christ as your personal Lord and Savior.\n",
      "I would like to be of assistance in any way possible. If it’s something that you need, but don’t see on this page; please do not hesitate to send me a message using the form below. I will get back to you as soon as possible. Thanks for visiting!\n",
      "Thanks for visiting my site. Please feel free to reach out if there is anything that I can help with! God Bless You and Yours!\n",
      "The information on this website may not be published, broadcast, rewritten or redistributed without the express written permission of the owner. All rights reserved. The owner of this blog makes no representations as to accuracy, completeness, correctness, suitability or validity of any information contained in this article and will not be liable for any errors, omissions, or delays in this information or any losses, injuries, or damages arising from its display or use. All information is provided on an as-is basis.\n",
      "The owner of this blog makes no representations as to accuracy, completeness, correctness, suitability or validity of any information contained in this article and will not be liable for any errors, omissions, or delays in this information or any losses, injuries, or damages arising from its display or use. All information is provided on an as-is basis.\n",
      "Copyright © 2019 · Timothy J. Brown All rights reserved.\n",
      "Timothy J. Brown is a participant in the Amazon Services LLC Associates Program, and earns a percentage of qualifying purchases made through their affiliate links.\n",
      "If you are interested in supporting my ministry and helping me reach out to people all over the world with the gospel of Jesus Christ, please consider making an online donation today! Your contribution will help us to continue reaching people for Christ, as well as developing resources like this website, books, and other materials that can be used by Christians around the world. I pray that God will bless you for your generosity in supporting\n",
      "llama_print_timings:        load time = 12526.65 ms\n",
      "llama_print_timings:      sample time =   281.30 ms /   512 runs   (    0.55 ms per token,  1820.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   437.24 ms /   265 tokens (    1.65 ms per token,   606.07 tokens per second)\n",
      "llama_print_timings:        eval time =  8847.09 ms /   510 runs   (   17.35 ms per token,    57.65 tokens per second)\n",
      "llama_print_timings:       total time =  9665.18 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09344c78-0880-4283-a06f-dc8fd19cf08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565452\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m not to be discovered only in the future. It can be found even now, by paying attention to the present moment and by directing our awareness within ourselves.\n",
      "> \n",
      "> — THICH NHAT HANH, ZEN BUDDHIST MONK\n",
      "Most of us have never been taught how to live in the Now. We usually think that we have to do something before we can be happy or fulfilled. We think that our life will start when we finish school, get a job, meet someone special, fall in love, get married . . . you name it!\n",
      "The whole idea of \"life\" is a big illusion; it's as though we are living in a dream and the only way to wake up from this dream is to bring more awareness to the present moment. This is not just true for some individuals, but actually all human beings!\n",
      "I used to think that I had to find another person first before I could feel happy or fulfilled; I was always looking for something or someone outside myself to complete my happiness. But this way of thinking does not work because it's based on the illusion that we are separate individuals. All of us have been raised in a culture where we think that we need to achieve and accumulate as many things as possible before we can be happy, but if you look around you, you will see that some people who own lots of stuff—money, cars, big houses, expensive clothes—are often the most unhappy people.\n",
      "I used to think that I needed to have a partner in my life first before I could feel fulfilled or happy; I was always looking for someone outside myself to complete my happiness. But this way of thinking does not work because it's based on the illusion that we are separate individuals. All of us have been raised in a culture where we think that we need to achieve and accumulate as many things as possible before we can be happy, but if you look around you, you will see that some people who own lots of stuff—money, cars, big houses, expensive clothes—are often the most unhappy people.\n",
      "We are all looking for the same thing, which is to feel fulfilled and alive. But most of us have no idea how to live in the present moment. We always think that we need to do something or achieve a goal before we can be happy, but there is nothing outside yourself that will\n",
      "llama_print_timings:        load time =  2256.22 ms\n",
      "llama_print_timings:      sample time =   275.29 ms /   512 runs   (    0.54 ms per token,  1859.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =   437.39 ms /   265 tokens (    1.65 ms per token,   605.86 tokens per second)\n",
      "llama_print_timings:        eval time =  8886.36 ms /   510 runs   (   17.42 ms per token,    57.39 tokens per second)\n",
      "llama_print_timings:       total time =  9698.40 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28417a09-eea0-410b-8bb5-ea48686988c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565466\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and to make others happy.\n",
      "Happiness is not something readymade. It comes from your own actions.\n",
      "I was born in Bombay, India on 15 February 1926. My father was a lawyer by training but worked for his brother's firm engaged in the import/export business. My mother was a homemaker and my eldest sister was already married at that time. I am the youngest of three children; I have two older brothers. My mother died when I was only twelve years old and so my upbringing was primarily by my father and grandmother, with the support of my uncles and aunts as well as cousins.\n",
      "My father had been admitted to the Bombay High Court but he moved to Hyderabad in 1928 because his brother's business was based there. I grew up in Hyderabad where, at that time, we were few English speaking people. Since my mother died when I was only twelve years old, my aunt had come to look after me and she spoke Telugu. As you know the official language of Andhra Pradesh is Telugu, not Hindi or Urdu.\n",
      "I studied in an English medium school where we were taught by British teachers who were very stern, although they were also kind. I was a good student and enjoyed going to school because it meant that I could leave home early and return later! After passing my matriculation examinations in 1942, I enrolled at the University of Osmania (then undergraduate college) where I studied Arts for two years before moving on to study law. My father wanted me to continue with the family business but my interests were more academic and so I completed a Master's degree in Law from the University in 1948.\n",
      "At that time, there was very little scope for women entering professions like medicine or engineering. There were even fewer options if one was married. My father was not happy at all when I told him of my intention to pursue a career and he even tried to dissuade me by suggesting that I teach in a school.\n",
      "By 1950, after finishing law college, I had also completed my post-graduate degree (L.LB) so I was free to choose whatever field attracted me. That is when I decided to join the Indian Administrative Service (IAS), which\n",
      "llama_print_timings:        load time =  1476.54 ms\n",
      "llama_print_timings:      sample time =   276.77 ms /   512 runs   (    0.54 ms per token,  1849.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   436.73 ms /   265 tokens (    1.65 ms per token,   606.78 tokens per second)\n",
      "llama_print_timings:        eval time =  8845.16 ms /   510 runs   (   17.34 ms per token,    57.66 tokens per second)\n",
      "llama_print_timings:       total time =  9658.42 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eed6291-7115-42f4-8164-7960fad07247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565480\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live as long and prosperous a life as you can.\n",
      "I do not think that there is much point in making predictions. Even if these could be relied upon, they would represent a substantial business advantage for those who could foresee them.\n",
      "I see no reason why we should not have both good science and good religion, because each gives strength to the other.\n",
      "If you look at any human endeavor - whether it's political, social, economic, artistic - you will find that it is driven by a desire of people to live in community with each other.\n",
      "If you want to be an entrepreneur, my best advice is: Don't pay attention to anybody else who tells you how hard it’s going to be. It’s not easy, but there are plenty of things that are worse than failing as an entrepreneur.\n",
      "In science and in life the real thing never really looks like a theory.\n",
      "It was my mother who first introduced me to science, when I was very young. She had studied biology at Stanford, and she taught me about the natural world around us: our local flora and fauna, the weather, the stars.\n",
      "Many people have a misconception that math is somehow foreign or abstract, but it’s not. It’s an integral part of everything we do. Math is not for scientists only; it is for all of us. So I would encourage everyone to take their children to the zoo, and let them count the animals – and then count them again!\n",
      "Math is not for scientists only; it is for all of us. So I would encourage everyone to take their children to the zoo, and let them count the animals – and then count them again!\n",
      "My father was a mathematician, so I grew up with formulas in my homework.\n",
      "No matter what the problem is, we need to look at it as an opportunity to do something better than it has ever been done before.\n",
      "People always say that you can’t please everybody. I think that’s a cop-out. Why not attempt it? ‘Cause think of all the people you will please if you try.\n",
      "Science is a way of trying not to fool yourself. The first principle is that you must not fool yourself, and you are the easiest person to fool.\n",
      "Technology is fun, but we have to use it wisely.\n",
      "The idea that you can\n",
      "llama_print_timings:        load time = 28802.18 ms\n",
      "llama_print_timings:      sample time =   273.58 ms /   512 runs   (    0.53 ms per token,  1871.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   958.45 ms /   265 tokens (    3.62 ms per token,   276.49 tokens per second)\n",
      "llama_print_timings:        eval time = 17997.84 ms /   510 runs   (   35.29 ms per token,    28.34 tokens per second)\n",
      "llama_print_timings:       total time = 19334.41 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebe2d01-c1e0-40e9-b511-3640347af132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565532\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "If a person can’t find happiness inside themself, they will always feel lost and confused. I believe this is because in our society, we are told that things will make us happy. We are told that a new car, or better yet, a new house, or maybe even a new spouse will make us happy.\n",
      "I think it is important to understand what happiness means on an individual level. For some people, having a lot of money and possessions makes them feel happy and accomplished; for others, it’s not about the material things in life that make them happy, but rather it’s about being able to spend quality time with family and friends, or even just being able to pursue hobbies that they like.\n",
      "I believe happiness is a feeling that should come from inside you. It shouldn’t be something that you go out of your way to find – instead, I think we should look at the things we already have in our lives and ask ourselves if those things make us happy. If not, then why do we keep them around?\n",
      "I feel that this question is one that many people don’t think about often enough; it is so easy to get caught up in the materialistic aspects of life that we forget about what truly makes us feel fulfilled and complete. We become obsessed with the idea that our jobs, or our cars, or the latest gadgets are going to make us happy when really those things should be secondary to our happiness.\n",
      "I think if we all took a step back in our lives and asked ourselves this question, we would be able to figure out what truly makes us happy; whether it’s spending time with family and friends or just doing the things that you love. I believe that once we can answer these questions, then we will have truly found happiness within ourselves.\n",
      "Posted in: Life | Tagged: Happiness, Life, Meaning of life, Philosophy\n",
      "One thought on “The Meaning of Life”\n",
      "I think that too. It is easy to get caught up in the materialistic aspects of life and not think about what truly makes us happy. I have been thinking a lot recently about this exact thing. I think that the things we buy should be secondary to our happiness as well. Thanks for sharing! ����\n",
      "~Tara (http://www.lifeislikeaboxofchocolates.com/)\n",
      "Leave a Reply to\n",
      "llama_print_timings:        load time =  3511.78 ms\n",
      "llama_print_timings:      sample time =   273.80 ms /   512 runs   (    0.53 ms per token,  1869.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   944.52 ms /   265 tokens (    3.56 ms per token,   280.57 tokens per second)\n",
      "llama_print_timings:        eval time = 17933.41 ms /   510 runs   (   35.16 ms per token,    28.44 tokens per second)\n",
      "llama_print_timings:       total time = 19253.41 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c69678d-185b-4590-888a-21f6e6123fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565557\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I believe that if you are not happy, you have every right to change something in your life.\n",
      "I believe that if you don’t like your job, you should find a new one.\n",
      "I believe in the freedom to live a life that makes you happy.\n",
      "We often let other people define what success is and then we feel bad about our lives because it doesn't look like theirs. I have been there and done that. It took me years to figure out that my life needed to be mine and not someone else’s.\n",
      "So, I am going to share with you a few things that worked for me. I hope they help you too!\n",
      "I spent an entire year doing nothing but trying to figure out what makes me happy.\n",
      "I wrote down everything that made me feel good, even a little bit of good. Then I did more of those things.\n",
      "At the end of the year, I had a list of all the things that make me happy and my life started to change. I was doing most of these things every day and it became easier to be happy.\n",
      "For example, at the top of my list was dancing with my daughter. So, almost every day we would dance together until we were out of breath.\n",
      "I made sure I did something from this list every day. It’s not enough that you know what makes you happy; you have to actually do it.\n",
      "When my life started to fall apart, I had no choice but to start over. I moved to a new city. I got a new job as an actress and I lived with my parents for three months while I found my feet.\n",
      "I was scared to death. In fact, I cried every day on the way to work because it was so hard. But I had no choice; I didn’t have any money and I needed to live somewhere.\n",
      "There is nothing wrong with starting over. It’s a great chance for a fresh start. You can do things differently this time around.\n",
      "I spent years trying to figure out what made me happy and how I could change my life. Then one day I realized, it doesn't matter!\n",
      "I have had many people in my life who are very unhappy with the way their lives are going and yet they won’t do anything about it. They spend all of their energy trying to figure out how to fix things instead of actually doing something about it.\n",
      "So get busy living or get\n",
      "llama_print_timings:        load time =  2942.29 ms\n",
      "llama_print_timings:      sample time =   280.53 ms /   512 runs   (    0.55 ms per token,  1825.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   949.18 ms /   265 tokens (    3.58 ms per token,   279.19 tokens per second)\n",
      "llama_print_timings:        eval time = 17983.19 ms /   510 runs   (   35.26 ms per token,    28.36 tokens per second)\n",
      "llama_print_timings:       total time = 19315.72 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3e647d-27e0-42d4-8040-d97064ef3efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565583\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "To do what you love, and what makes you feel good!\n",
      "I've learned that money can buy us a lot of things but it cannot buy happiness. The happiest people I know are not the richest, they just have a wonderful attitude toward life and spread joy everywhere they go.\n",
      "Sometimes we get so caught up in the little things, like what someone said or did to us, or when our favorite show will be on next. We forget that there is much more to life than these small details. There are people out there who would love nothing more than just to wake up tomorrow and have a normal day...to be able to see their family one last time....to be with the people they love again.\n",
      "When you look at things this way, it really puts things into perspective. We need to remember what's important in life! The little details aren't so significant anymore when we think of how short and fragile our lives can be. Life is too short to sweat over the small stuff. We need to live every day like it could be our last because honestly, you really never know.\n",
      "To truly be happy, we must realize what makes us happy...and that's different for everyone. I love singing and acting so much because when I do those things, all of my problems seem to fade into the background. I am truly happy doing them!\n",
      "My biggest passion is helping others. I love being able to make someone smile or laugh. It brightens up both our days and makes me feel like I've made a difference in someone else's life. I also love to help out at my church and community, because it makes me feel good knowing that I am making a positive impact on the world around me!\n",
      "We must always remember what we are truly thankful for...to be grateful for everything we have. We must do whatever it is that makes us smile! And never let anyone else tell you otherwise....we must follow our own hearts and intuition. Life is an incredible journey, and there's no better time to start living than today.\n",
      "I hope these thoughts will inspire people everywhere to live their best lives. Let's stop waiting around for happiness to find us...let's go out and find it ourselves! I hope we all have a wonderful day and do whatever makes you happy today!!!\n",
      "Labels: be happy, happiness, life is short, live life now, love life\n",
      "llama_print_timings:        load time = 56761.07 ms\n",
      "llama_print_timings:      sample time =   288.39 ms /   512 runs   (    0.56 ms per token,  1775.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1722.87 ms /   265 tokens (    6.50 ms per token,   153.81 tokens per second)\n",
      "llama_print_timings:        eval time = 34745.76 ms /   510 runs   (   68.13 ms per token,    14.68 tokens per second)\n",
      "llama_print_timings:       total time = 36858.57 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c5f47a-3adc-46e7-8aaa-2a727e4702d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565683\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I think that's what we're all trying to do, so if you do something you love and it makes you happy then you are living a great life!\n",
      "What is more important: the journey or the destination?\n",
      "I've always believed that the journey is just as important as the destination. I also believe in order to have a good destination you must enjoy the journey.\n",
      "How do you know when something is really over for you?\n",
      "As cliché as it sounds, when your gut tells you that it's time to move on, then it probably is. When things are right and meant to be, they feel easy, but if not you will always have this nagging feeling like a weight on your shoulders. Trust yourself enough to know what is best for you.\n",
      "I don't think there is a \"right\" way to live life, because everyone has their own definition of what a good life is for them. The only real wrong way to live life is to try and be something or someone you are not. Live your truth and the rest will come naturally.\n",
      "What do you believe about God? What do you suppose God thinks of you?\n",
      "I believe in a higher power, but I don't like to put labels on what that is because it is personal for everyone. I just know that when I am at my lowest point or saddest times there is always something that reminds me to keep the faith and not to give up. So whatever you want to call that presence, I believe it's always got your back!\n",
      "What three things are vital to being \"successful\" in life?\n",
      "Three things that will lead to happiness:\n",
      "1) Being able to support yourself financially so you can live the way you desire.\n",
      "2) Doing what makes you happy and bringing joy to others.\n",
      "3) Surrounding yourself with people who lift you up, and not tear you down.\n",
      "I think success is a very personal thing for everyone, but if you are able to achieve these three things in life then I believe you have lived a successful one!\n",
      "What do you most look forward to?\n",
      "My future! I am excited about the possibilities that the universe has in store for me and where my journey will take me.\n",
      "I've always had a love of fashion, but it wasn't until I moved from Chicago to L.A. when I was 17 that I realized there were\n",
      "llama_print_timings:        load time =  6616.10 ms\n",
      "llama_print_timings:      sample time =   281.33 ms /   512 runs   (    0.55 ms per token,  1819.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1721.66 ms /   265 tokens (    6.50 ms per token,   153.92 tokens per second)\n",
      "llama_print_timings:        eval time = 34719.76 ms /   510 runs   (   68.08 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time = 36826.24 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3014b1-68c7-4a1a-89cc-568e9780e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565730\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "I have to be honest, when I was first diagnosed with cancer a little over 2 years ago I thought: “What’s the point?” My thoughts were that if life has to end – why bother?\n",
      "After much deliberation and soul searching I decided there wasn’t any logic in thinking like this. I have to be honest, it is a daily struggle to keep myself motivated but, if you are feeling low about your situation, try to remember how many people would love to see what you see – feel what you feel – hear what you hear.\n",
      "I believe we are here for the journey and not necessarily the outcome. We all have a limited time on this planet so embrace every single second of it – even if that means having to deal with a disability!\n",
      "To read more about my story, click here: https://www.youtube.com/watch?v=RWZcPt2-qYE&feature=youtu.be or you can visit my blog for more information www.bryanadams35.wordpress.com.\n",
      "Tags: Attitude, Guest Blogger, Inspiration, Motivation, Purpose, What's Your Why?\n",
      "This entry was posted on Wednesday, May 18th, 2016 at 9:27 am and is filed under Inspiration. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.\n",
      "I was diagnosed with multiple myeloma in October of last year. I have been on chemo treatments since. I am still working full time and trying to keep up with my grandson’s soccer schedule! It is good to hear another story about someone staying so active despite their circumstances! Keep it up Bryan!\n",
      "Thank you so much for sharing your experience with us, Bryan. You are an inspiration and we wish you all the best in your journey!\n",
      "What a great attitude you have. I am inspired by your positive outlook on life despite some of the challenges you face. Thank you for sharing!\n",
      "Thanks for sharing Bryan, it’s really encouraging to hear about someone with such a positive attitude. It makes me want to do more and be better at what I do. You have an inspiring story!\n",
      "Bryan – thank\n",
      "llama_print_timings:        load time =  6631.01 ms\n",
      "llama_print_timings:      sample time =   271.82 ms /   512 runs   (    0.53 ms per token,  1883.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1724.29 ms /   265 tokens (    6.51 ms per token,   153.69 tokens per second)\n",
      "llama_print_timings:        eval time = 34714.14 ms /   510 runs   (   68.07 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time = 36812.01 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7d3aa-fd50-4a3f-9024-861d3986063b",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28020a7d-80b6-400c-ac71-5fa064d72bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565778\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out what we are here for and do that thing. That’s how I define my meaning in life now, but not at 16 or younger.\n",
      "I think this question is very hard to answer. For me it would be a combination between being able to help others, living a happy fulfilling life and being successful (whatever success means).\n",
      "My mother. She has always been there for me and supported me in everything I do.\n",
      "It’s not something that you can find answers to or achieve overnight. You have to experience it yourself.\n",
      "I hope so. It gives us the ability to look at things from a different perspective, understand what other people are going through, empathise etc..\n",
      "I was inspired by my mother who has always worked very hard and succeeded in everything she did. She is still an inspiration for me today.\n",
      "My daughter. I’m sure she will be able to do anything she wants to, so hopefully she will have no limits.\n",
      "I would like to see more opportunities for women to work in areas that are seen as male dominated (like construction). I also think it is important to break the “glass ceiling”. Women should not feel that they have to conform to the stereotypes of what a woman can or cannot do.\n",
      "I believe that if we all work together and support each other, our country will be even more successful than it already is.\n",
      "My mother. She has always been there for me and supported me in everything I do. My daughter – she is my inspiration.\n",
      "The best thing about being a woman is that I know how to listen to people and understand what they are going through, empathise etc.. The worst thing? The prejudice people have against women, especially when it comes to leadership positions.\n",
      "I would like to see more opportunities for women to work in areas that are seen as male dominated (like construction). I also think it is important to break the “glass ceiling”. Women should not feel that they have to conform to the stereotypes of what a woman can or cannot do. My mother – she has always been there for me and supported me in everything I do.\n",
      "Being able to help others, living a happy fulfilling life and being successful (whatever success means). Also having a great family life is very important to me.\n",
      "My daughter. She is my inspiration. The best thing about being a woman is\n",
      "llama_print_timings:        load time = 21018.24 ms\n",
      "llama_print_timings:      sample time =   268.91 ms /   512 runs   (    0.53 ms per token,  1904.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   268.38 ms /   265 tokens (    1.01 ms per token,   987.41 tokens per second)\n",
      "llama_print_timings:        eval time =  7274.37 ms /   510 runs   (   14.26 ms per token,    70.11 tokens per second)\n",
      "llama_print_timings:       total time =  7910.98 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54e9ca9-c4eb-4fb8-b639-646d2d87633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565809\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, and to make others as happy as you can.\n",
      "I think that is why God has given us this beautiful world where we have so much to do and see and feel.\n",
      "I’m not sure but there are so many things that God has blessed us with and I believe it’s our responsibility to make the best out of what he gave us.\n",
      "This entry was posted in 365 Days of Gratitude, Inspirational Thoughts & Stories on April 24, 2017 by Carl A. Cofer.\n",
      "I don’t believe that one can be truly happy unless they are grateful for what God has given them – the good and bad times included.\n",
      "This entry was posted in 365 Days of Gratitude, Inspirational Thoughts & Stories on April 21, 2017 by Carl A. Cofer.\n",
      "We should never take life for granted because it could be taken away from us at any moment.\n",
      "This entry was posted in 365 Days of Gratitude, Inspirational Thoughts & Stories on April 19, 2017 by Carl A. Cofer.\n",
      "Being able to share my passion about this amazing world with other people is one of the most rewarding things I enjoy doing in life.\n",
      "Sharing our experiences and learnings makes me feel great!\n",
      "This entry was posted in 365 Days of Gratitude, Inspirational Thoughts & Stories on April 17, 2017 by Carl A. Cofer.\n",
      "I believe that we are the ones who define what happiness means to us and for me it’s not about having a lot of money or fame but finding our true passion in life and doing whatever it takes to make this world a better place!\n",
      "This entry was posted in 365 Days of Gratitude, Inspirational Thoughts & Stories on April 15, 2017 by Carl A. Cofer.\n",
      "I love the feeling that I get when I am able to help other people.\n",
      "That is one reason why I enjoy my work as a travel consultant so much!\n",
      "This entry was posted in 365 Days of Gratitude, Inspirational Thoughts & Stories on April 14, 2017\n",
      "llama_print_timings:        load time =  1976.98 ms\n",
      "llama_print_timings:      sample time =   274.02 ms /   512 runs   (    0.54 ms per token,  1868.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   268.00 ms /   265 tokens (    1.01 ms per token,   988.79 tokens per second)\n",
      "llama_print_timings:        eval time =  7284.39 ms /   510 runs   (   14.28 ms per token,    70.01 tokens per second)\n",
      "llama_print_timings:       total time =  7927.22 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa691e89-fc14-478c-99f7-3250b7554ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565822\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and live peacefully. This includes, but isn't limited to:\n",
      "1) Being a good person with strong morals in order to help others achieve happiness as well.\n",
      "2) Learning from those around you and sharing what you learn (or have learned).\n",
      "3) Living in harmony with the world and all it has to offer us, whether that be nature or technology.\n",
      "4) Making sure your relationships are healthy and fulfilling.\n",
      "5) Appreciating everyday life, which includes both the good and bad aspects of living.\n",
      "6) Being able to laugh at yourself and others around you because there is no such thing as a foolproof idea.\n",
      "7) Accepting the things you cannot change about your life; this will make you happier in the long run.\n",
      "8) Appreciating what you have, which includes physical, mental, and spiritual aspects of one's life.\n",
      "9) Having faith that everything will work out the way it should (even if you have no clue how or when).\n",
      "10) Being able to say \"I tried,\" without regret because there are many times in life where we can only do our best.\n",
      "I would like my funeral and tombstone to be this:\n",
      "\"Happy people, living peacefully.\"\n",
      "That is what I believe the meaning of life is...what about you?\n",
      "Posted by Jillian Mcclure at 10:23 AM No comments:\n",
      "In honor of St. Patrick's Day, here are some fun facts about leprechauns!\n",
      "Leprechaun Facts:\n",
      "*Leprechauns were originally thought to be fairies by the Irish people. They were known as cluricaune (\"little bodies\") and were said to have been a race of dwarves (much like how elves are in other cultures). Many stories were told about them, but no one was sure if they actually existed or not.\n",
      "*The mythical leprechaun is thought to be between 4-6 feet tall. He has red hair and pointy ears that stick out a little bit from his head. His face is usually described as being wrinkled, with a bushy red beard. Leprechauns are also known for their baggy clothes (usually green), which they make from the leaves of the clover plant.\n",
      "*Leprechaun\n",
      "llama_print_timings:        load time =  1944.89 ms\n",
      "llama_print_timings:      sample time =   277.49 ms /   512 runs   (    0.54 ms per token,  1845.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   268.12 ms /   265 tokens (    1.01 ms per token,   988.38 tokens per second)\n",
      "llama_print_timings:        eval time =  7280.67 ms /   510 runs   (   14.28 ms per token,    70.05 tokens per second)\n",
      "llama_print_timings:       total time =  7925.88 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5862253-bac0-4d47-ad5d-de167e019874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565834\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a meaning.\n",
      "The problem with the rat race is that even if you win, you’re still a rat.\n",
      "This entry was posted in Quotes and tagged Albert Einstein, Carl Sandburg, Charles M Schulz, Dylan Thomas, Friedrich Nietzsche, George Carlin, Henry David Thoreau, Hunter S Thompson, Jarod Kintz, Kurt Vonnegut, Mark Twain, Oliver Goldsmith. Bookmark the permalink.\n",
      "I’ll check out your blog. Thanks for stopping by.\n",
      "Awesome collection of quotes that I will definitely use in my blog post.\n",
      "Thanks for sharing and keep up the good work. You are an inspiration to many!\n",
      "Lol, thanks for the inspiration.\n",
      "Excellent post, and awesome quotes. I’m a fan of Charles Bukowski, if you’ve never read him, check him out.\n",
      "I love it that your list is so varied… and in such great company!\n",
      "That was my intention – variety is the spice of life. Thanks for stopping by.\n",
      "Thanks for sharing this list. I’m a big fan of quotes and have been known to use them from time to time on my blog.\n",
      "This post made me smile. It reminds me of what is really important in life, not money or fame but the little things like family and friends that make us happy!\n",
      "Glad it brought a smile to your face. Thanks for stopping by.\n",
      "I am so glad I came across this! Thank you very much!!\n",
      "You’re welcome – thanks for stopping by.\n",
      "Great post, some really good quotes in there. You could easily take a quote from here and make an entire blog post out of it, and have one that can be related to everyone.\n",
      "Thanks for the comment. I’m glad you enjoyed my post. Thanks again for reading my blog.\n",
      "That is the best collection of quotes that I have seen in a long time. Thank you so much!\n",
      "You’re welcome – thanks for stopping by.\n",
      "Thank you for this wonderful list. I love it.\n",
      "Thanks for stopping by and your comment on my blog.\n",
      "I hope life gives you lots of inspiration to write more posts like these, they are really great.\n",
      "I hope that you will enjoy them. Let me know if you have any suggestions or if there is anything in particular that you would like to\n",
      "llama_print_timings:        load time = 39487.76 ms\n",
      "llama_print_timings:      sample time =   301.28 ms /   512 runs   (    0.59 ms per token,  1699.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   435.87 ms /   265 tokens (    1.64 ms per token,   607.98 tokens per second)\n",
      "llama_print_timings:        eval time = 12026.73 ms /   510 runs   (   23.58 ms per token,    42.41 tokens per second)\n",
      "llama_print_timings:       total time = 12863.35 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d280e91b-88ee-4a0e-99ae-6bdfd5c90ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565890\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find a way to live that reflects what you love the most.\n",
      "I think that everyone wants the best for themselves and their families, but sometimes we get wrapped up in all the things we have to do or don’t have time to do – so much so that we lose sight of our purpose. It is important to remember why we are doing something every day. It is also very important to find what you love most about life and then let your actions reflect that.\n",
      "In this episode, I talk about how I found my passion for art and design by looking back on the things that brought me happiness while growing up. If we can remember our happiest moments, I believe we will be able to find a way to make those same things happen now – even if they are in different forms.\n",
      "I hope this episode inspires you to reflect on your life or even better yet – to create a moment of reflection for someone else. This world is so much better when we can share our experiences with each other!\n",
      "If you have ever wanted to find your passion, but were not sure how to do it, I think you’ll be surprised by the simple things that brought me joy while growing up – and how those same things can bring you joy today.\n",
      "I also talk about the importance of helping others remember their past and their happiest memories! Sharing these moments with other people is so important because we all want to feel like we are not alone in this world and that our lives matter.\n",
      "If you have a story of reflection you would like to share – please do it! You never know who might need your words at the right moment.\n",
      "It’s my hope that when you reflect, you will remember what makes life beautiful – and then make time for those things every day.\n",
      "I want my artwork to inspire others to find their passion and bring joy back into their lives as well! I hope this podcast does just that.\n",
      "This is a really good episode of The Passionate Journey Podcast. I loved the discussion about creating reflection moments for others. Reflection moments can be very rewarding for everyone involved, so they definitely deserve to get more attention. Thanks for sharing these ideas!\n",
      "I’m glad you enjoyed it. It is something I think we all need to do more of, especially in this day and age with social media and the constant connection to technology. We have lost our sense of reflection and appreciation of life – let alone our own\n",
      "llama_print_timings:        load time =  4152.73 ms\n",
      "llama_print_timings:      sample time =   279.14 ms /   512 runs   (    0.55 ms per token,  1834.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   436.11 ms /   265 tokens (    1.65 ms per token,   607.65 tokens per second)\n",
      "llama_print_timings:        eval time = 11977.45 ms /   510 runs   (   23.49 ms per token,    42.58 tokens per second)\n",
      "llama_print_timings:       total time = 12794.72 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95081d00-4c19-4fc9-a3e3-5d1d8bacefe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565911\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m simple. Life can have many meanings, but ultimately it is a gift to be able to live and breathe.\n",
      "Life can have many ups and downs, and some days will feel like you are on an emotional roller coaster. But in the end, all we really need to do is find something that makes us happy. The meaning of life should not be taken so seriously and we should try to take each day as it comes, rather than worrying about what may happen tomorrow or next week or even five years from now. We never know what’s going to happen in the future, so we need to just focus on the present moment, and enjoy life while it lasts.\n",
      "When I think of the meaning of life, I think about all the things that bring me joy. These are simple things like spending time with friends and family, enjoying a good book or movie, working out, being active outside, eating healthy foods, and making a positive impact on those around me. I feel so fortunate to be alive and for these opportunities that I have been given in my life, so I want to make the most of them, by doing what makes me happy.\n",
      "I think the meaning of life is to give back as much as possible to the world. This could mean helping someone get home safely, or donating money to a charity or an organization that helps those in need. It could also be something simple like giving your time to help out at a soup kitchen for a few hours each week. Or maybe it means spending quality time with family and friends to make them feel loved.\n",
      "I think the meaning of life is whatever you want to make of it. If you choose to spend it living an unhappy or unfulfilled life, that’s your choice. But if you want to find happiness, then do what makes you happy. Whatever that may be.\n",
      "The meaning of life is a matter of perspective and everyone has their own opinions about it, but no one truly knows the answer. Everyone also has different ideas on how they should live their lives. But we all have to keep in mind that there is only one you, so try to find what brings you happiness and fulfillment and make the most of your life!\n",
      "I believe the meaning of life is to be happy and content with where we are at, no matter the situation. It’s not always easy, but having an open mind helps us to\n",
      "llama_print_timings:        load time =  4282.93 ms\n",
      "llama_print_timings:      sample time =   284.39 ms /   512 runs   (    0.56 ms per token,  1800.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   434.97 ms /   265 tokens (    1.64 ms per token,   609.24 tokens per second)\n",
      "llama_print_timings:        eval time = 11981.03 ms /   510 runs   (   23.49 ms per token,    42.57 tokens per second)\n",
      "llama_print_timings:       total time = 12801.43 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e415ffbc-2cb0-401d-9511-6d0024e17333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689565931\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have fun, enjoy every minute and never look back.\n",
      "Today I am grateful for my health. I have been having a lot of pain in my body that has gotten worse over the past few weeks. I did not realize how bad it was until yesterday when I was walking. Today I had an appointment with my doctor. She took one look at me and ordered xrays and then she sent me for blood tests. She said there is no way I can work with this pain which I have been working through for two months. I can feel the difference already from taking an anti-inflammatory drug.\n",
      "I am grateful that my doctor took care of this right away and did not make me wait weeks or months for a treatment plan. My next appointment is in 2 weeks to see how things are going.\n",
      "I am also grateful that I have good medical insurance through my husband’s work. It is an expense but it allows me to get the care I need when I need it.\n",
      "My gratitude list has gotten so large since I started this daily practice. Today I am grateful for my health, my family and friends, food in my cupboards, and a roof over my head. What are you grateful for today?\n",
      "I had an MRI yesterday on my shoulder because of the pain I have been having. The report said there is moderate to severe tendonitis and bursitis in my right shoulder. I am relieved that they found something wrong with me so we can work on a treatment plan. I will get the results of the blood tests on Friday when I go back to see my doctor.\n",
      "I went to the health food store yesterday and bought some glucosamine for my joints. I will also be taking an anti-inflammatory drug from the doctor. I am hoping that we can get this under control soon so that I can sleep again at night.\n",
      "Today I am grateful for the rain we have had today. We really need it here in California to make up for all of the water we lost last year.\n",
      "I hope you are having a great week and staying dry!\n",
      "Today I am grateful for family time. My husband’s brother and wife came over yesterday. It was so nice to see them again and spend some quality time with them.\n",
      "We enjoyed a delicious BBQ dinner cooked by my wonderful hubby. He is really good at the grill. We\n",
      "llama_print_timings:        load time = 107788.10 ms\n",
      "llama_print_timings:      sample time =   278.23 ms /   512 runs   (    0.54 ms per token,  1840.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   941.86 ms /   265 tokens (    3.55 ms per token,   281.36 tokens per second)\n",
      "llama_print_timings:        eval time = 26566.43 ms /   510 runs   (   52.09 ms per token,    19.20 tokens per second)\n",
      "llama_print_timings:       total time = 27893.69 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c2d2781-d616-4c01-9f8f-72473f17627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689566073\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. It’s hard for me to imagine anything that would make me feel more sad and empty than feeling like I was living a life in which I wasn’t truly being myself.\n",
      "I think that happiness, or rather, true contentment, comes from within and is not dependent on external factors. Of course it’s great to have things like good food, love, shelter, etc., but no matter how much we have it will never feel like enough if we don’t have peace in our hearts.\n",
      "If you haven’t seen the movie “Inside Out,” I highly recommend it! The main character is a little girl who has just moved to a new place and started at a new school, leaving her friends behind. She goes through all of these different emotions throughout the day as she deals with this big change in her life, and discovers that sometimes sadness can help us feel our joy more deeply. I think a lot of the time people see sadness or anger as something to be avoided at all costs, but it’s an important part of being human and is not necessarily bad.\n",
      "I often feel sad when I look around at how much pain there is in the world, both in people’s hearts and in our environment. But that doesn’t mean that I don’t believe there is goodness too, because I know there is. And I know that I can make a positive impact by being myself fully, and showing my love to others, while also doing the work that I feel called to do.\n",
      "I have learned that it’s essential for me to spend time in nature every day, and move my body as much as possible. This helps clear my mind and keeps me grounded, which allows more space for creativity and inspiration to flow in. It also makes me happier! When I don’t get enough of this, I can feel it physically and emotionally.\n",
      "For many years I was a vegan. It made me feel good to be following my values around food (and fashion), but I found that I didn’t have as much energy or creativity. After seeing an Ayurvedic doctor, she told me that based on my body type and constitution, being vegetarian would be best for me.\n",
      "I was sad at first because this was different than the dietary choices I had made before, but now I realize that it’s not about being perfect or right all of the time,\n",
      "llama_print_timings:        load time =  9337.40 ms\n",
      "llama_print_timings:      sample time =   278.73 ms /   512 runs   (    0.54 ms per token,  1836.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   958.08 ms /   265 tokens (    3.62 ms per token,   276.59 tokens per second)\n",
      "llama_print_timings:        eval time = 26674.40 ms /   510 runs   (   52.30 ms per token,    19.12 tokens per second)\n",
      "llama_print_timings:       total time = 28014.81 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f29c482b-190b-4b2f-87c4-0020c5b2807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689566116\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift, and the purpose of life is to give it away.\n",
      "In 1986, I was a graduate student at Stanford University in Palo Alto, California. One night during that school year, I received a phone call from my friend and business associate Bob Buford of Dallas, Texas. He said that he had been impressed by the work we were doing at Leadership Network to help churches and Christian nonprofits learn from each other.\n",
      "I told him that what we really needed was for someone with money and a strong interest in building up Christian leaders and organizations to put together an endowment fund that would give us a large amount of money annually. Bob told me he wanted to do it himself, and I replied that it sounded like a great idea but that I didn't think he had enough money! He paused for a moment and responded by saying, \"Well, not yet!\"\n",
      "I was stunned. But sure enough, he did just what he said he would. Bob Buford went on to become the founder of Leadership Network with an endowment fund that has helped us accomplish more than we could ever have imagined. Bob also wrote _Halftime_ , a book about his own \"midlife crisis\" and subsequent journey toward finding his purpose in life.\n",
      "I believe that everyone is searching for meaning in their lives, but I've found that the search for meaning can become even more urgent when you reach midlife. You begin to look back on your life so far, wondering what it has all meant. But that reflection can also turn into a fearful look forward as you realize there are fewer years ahead than there have been behind.\n",
      "I'm convinced that most people want their lives to count, but they don't always know how to move from merely being successful to being significant. In fact, one of the reasons I wrote this book is because I see so many people in their forties or fifties who are doing well financially and professionally but are still searching for a way to make a difference with what they have been given.\n",
      "I've written more than fifty books, but even though I'm now over eighty years old, I believe this book is one of the most important I've ever written. The reason? Because I want you—no matter your age or stage in life—to discover how to find God's purpose for your\n",
      "llama_print_timings:        load time =  9636.49 ms\n",
      "llama_print_timings:      sample time =   277.14 ms /   512 runs   (    0.54 ms per token,  1847.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   944.93 ms /   265 tokens (    3.57 ms per token,   280.45 tokens per second)\n",
      "llama_print_timings:        eval time = 26702.75 ms /   510 runs   (   52.36 ms per token,    19.10 tokens per second)\n",
      "llama_print_timings:       total time = 28029.03 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "928de9ba-abf8-475e-82aa-1ba6cb05eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689566160\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe056e8-7caf-4d0b-93c3-cfa501e2160f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
