{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Fri Dec 22 23:44:47 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:07:00.0 Off |                  Off |\n",
      "| 30%   33C    P8              23W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "model name\t: AMD Ryzen 7 5800X 8-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       65751016 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 14584, done.\u001b[K\n",
      "remote: Counting objects: 100% (4312/4312), done.\u001b[K\n",
      "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
      "remote: Total 14584 (delta 4201), reused 4102 (delta 4083), pack-reused 10272\u001b[K\n",
      "Receiving objects: 100% (14584/14584), 16.38 MiB | 15.20 MiB/s, done.\n",
      "Resolving deltas: 100% (10212/10212), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f20a13-e11c-4e56-98b9-4a896510f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   7024      0 --:--:-- --:--:-- --:--:--  7043\n",
      "Downloading tokenizer\n",
      "--2023-12-22 23:45:18--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2023-12-22 23:45:19 (15.8 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-12-22 23:45:19--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-22 23:45:19 (83.5 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-12-22 23:45:19--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[===================>]  12.55G  48.1MB/s    in 6m 3s   \n",
      "\n",
      "2023-12-22 23:51:24 (35.4 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-12-22 23:51:24--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-22 23:51:24 (108 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-22 23:51:24--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-22 23:51:26 (365 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-12-22 23:51:42--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  50.7MB/s    in 5m 6s   \n",
      "\n",
      "2023-12-22 23:56:49 (40.6 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-22 23:56:49--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  52.9MB/s    in 4m 18s  \n",
      "\n",
      "2023-12-23 00:01:08 (48.0 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-23 00:01:08--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 00:01:09 (21.2 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 00:01:09--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 00:01:09 (165 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-12-23 00:01:42--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  25.3MB/s    in 5m 49s  \n",
      "\n",
      "2023-12-23 00:07:32 (44.5 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 00:07:32--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  59.0MB/s    in 5m 3s   \n",
      "\n",
      "2023-12-23 00:12:36 (51.2 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 00:12:36--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  27.6MB/s    in 6m 58s  \n",
      "\n",
      "2023-12-23 00:19:34 (37.1 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 00:19:34--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  48.8MB/s    in 7m 9s   \n",
      "\n",
      "2023-12-23 00:26:44 (36.2 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 00:26:44--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 00:26:45 (83.6 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 00:26:45--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 00:26:45 (922 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-12-23 00:28:11--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  23.2MB/s    in 13m 8s  \n",
      "\n",
      "2023-12-23 00:41:21 (19.7 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 00:41:21--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  91.5MB/s    in 4m 19s  \n",
      "\n",
      "2023-12-23 00:45:40 (60.0 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 00:45:40--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  85.1MB/s    in 3m 44s  \n",
      "\n",
      "2023-12-23 00:49:25 (69.6 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 00:49:25--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  49.9MB/s    in 4m 3s   \n",
      "\n",
      "2023-12-23 00:53:29 (64.0 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 00:53:29--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  62.1MB/s    in 4m 24s  \n",
      "\n",
      "2023-12-23 00:57:54 (58.9 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 00:57:54--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  97.3MB/s    in 3m 7s   \n",
      "\n",
      "2023-12-23 01:01:02 (83.1 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 01:01:02--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  69.7MB/s    in 3m 41s  \n",
      "\n",
      "2023-12-23 01:04:43 (70.5 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 01:04:43--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  70.0MB/s    in 3m 25s  \n",
      "\n",
      "2023-12-23 01:08:09 (76.0 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 01:08:09--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 01:08:09 (50.9 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 01:08:09--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 01:08:10 (813 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B\t7B-v2\t\t\t  ggml-vocab-mpt.gguf\n",
      "13B-v2\tggml-vocab-aquila.gguf\t  ggml-vocab-refact.gguf\n",
      "30B\tggml-vocab-baichuan.gguf  ggml-vocab-stablelm-3b-4e1t.gguf\n",
      "65B\tggml-vocab-falcon.gguf\t  ggml-vocab-starcoder.gguf\n",
      "70B-v2\tggml-vocab-gpt-neox.gguf  tokenizer.model\n",
      "7B\tggml-vocab-llama.gguf\t  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# Obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376847f6-3509-4a11-b211-ec1fb0d73ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13283  100 13283    0     0  45560      0 --:--:-- --:--:-- --:--:-- 45489\n"
     ]
    }
   ],
   "source": [
    "# If you encounter the error \"does not appear to have a file named config.json\" when converting the models to ggml FP16 format, try to convert the model to huggingface format to get the config.json file.\n",
    "!curl -o convert_llama_weights_to_hf.py https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bee9eb-b358-40f2-8582-f93dad231f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ff2a39-7f7d-4285-9652-a89b9503005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp tokenizer.model 7B/\n",
    "!cp tokenizer.model 13B/\n",
    "!cp tokenizer.model 30B/\n",
    "!cp tokenizer.model 65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc57513-b8fa-46ba-8315-9f118b998c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.36.2)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: protobuf>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall accelerate # If you have this package, uninstall it first, then use `convert to hf model` to get the config.json.\n",
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636f3946-8a2b-4a9d-a23a-9e880e96ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/7B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/13B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/30B/.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/65B/.\n"
     ]
    }
   ],
   "source": [
    "# We don't need these models actually. We only need this to figure out the config.json error.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/7B/ --model_size 7B --output_dir models/7B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/13B/ --model_size 13B --output_dir models/13B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/30B/ --model_size 30B --output_dir models/30B/ # Surprisingly, it still solves the problem although you can't find the config.json file.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/65B/ --model_size 65B --output_dir models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbf7dba-9d3e-4c40-95c8-58ccc63d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit your params.json file if the \"vocab_size\" mismatch\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/7B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/7B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/13B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/13B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/30B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/30B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/65B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/65B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/7B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [4096]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 4096]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "skipping tensor blk.0.attn_rot_embd\n",
      "skipping tensor blk.1.attn_rot_embd\n",
      "skipping tensor blk.2.attn_rot_embd\n",
      "skipping tensor blk.3.attn_rot_embd\n",
      "skipping tensor blk.4.attn_rot_embd\n",
      "skipping tensor blk.5.attn_rot_embd\n",
      "skipping tensor blk.6.attn_rot_embd\n",
      "skipping tensor blk.7.attn_rot_embd\n",
      "skipping tensor blk.8.attn_rot_embd\n",
      "skipping tensor blk.9.attn_rot_embd\n",
      "skipping tensor blk.10.attn_rot_embd\n",
      "skipping tensor blk.11.attn_rot_embd\n",
      "skipping tensor blk.12.attn_rot_embd\n",
      "skipping tensor blk.13.attn_rot_embd\n",
      "skipping tensor blk.14.attn_rot_embd\n",
      "skipping tensor blk.15.attn_rot_embd\n",
      "skipping tensor blk.16.attn_rot_embd\n",
      "skipping tensor blk.17.attn_rot_embd\n",
      "skipping tensor blk.18.attn_rot_embd\n",
      "skipping tensor blk.19.attn_rot_embd\n",
      "skipping tensor blk.20.attn_rot_embd\n",
      "skipping tensor blk.21.attn_rot_embd\n",
      "skipping tensor blk.22.attn_rot_embd\n",
      "skipping tensor blk.23.attn_rot_embd\n",
      "skipping tensor blk.24.attn_rot_embd\n",
      "skipping tensor blk.25.attn_rot_embd\n",
      "skipping tensor blk.26.attn_rot_embd\n",
      "skipping tensor blk.27.attn_rot_embd\n",
      "skipping tensor blk.28.attn_rot_embd\n",
      "skipping tensor blk.29.attn_rot_embd\n",
      "skipping tensor blk.30.attn_rot_embd\n",
      "skipping tensor blk.31.attn_rot_embd\n",
      "Writing models/7B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 11/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 12/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 13/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 14/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 15/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 16/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 17/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 18/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 19/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 20/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 21/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 22/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 23/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 24/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 25/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 26/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 28/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 29/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 31/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 32/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 33/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 34/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 35/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 37/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 38/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 40/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 41/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 42/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 43/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 44/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 46/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 47/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 49/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 50/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 51/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 52/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 53/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 55/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 56/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 58/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 59/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 60/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 61/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 62/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 64/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 65/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 67/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 68/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 69/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 70/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 71/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 73/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 74/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 76/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 77/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 78/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 79/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 80/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 82/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 83/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 85/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 86/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 87/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 88/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 89/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 91/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 92/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 94/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 95/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 96/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 97/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 98/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   2\n",
      "[100/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   2\n",
      "[101/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   2\n",
      "[102/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[103/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[104/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[105/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[106/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   2\n",
      "[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   2\n",
      "[108/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   2\n",
      "[109/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   2\n",
      "[110/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   2\n",
      "[111/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[112/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[113/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[114/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   2\n",
      "[115/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   2\n",
      "[116/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   2\n",
      "[117/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   2\n",
      "[118/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   2\n",
      "[119/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[120/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[121/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[122/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[123/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[124/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[125/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[126/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[127/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[128/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[129/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[130/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[131/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[132/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[133/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[134/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[135/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[136/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[137/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[138/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[139/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[140/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[141/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[142/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[143/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[144/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[145/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[146/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[147/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[148/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[149/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[150/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[151/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[152/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[153/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[154/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[155/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[156/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[158/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[159/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[160/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[162/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[163/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[164/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[165/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[166/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[167/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[168/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[169/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[170/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[171/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[172/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[173/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[174/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[175/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[176/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[177/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[178/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[179/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[180/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[181/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[182/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[183/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[184/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[185/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[186/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[187/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[188/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[189/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[190/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[191/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[192/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[193/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[194/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[195/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[196/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[197/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[198/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[199/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[200/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[201/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[203/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[204/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[205/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[206/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[207/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[209/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[210/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[211/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[212/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[213/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[214/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[215/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[216/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[217/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[218/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[219/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[220/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[221/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[222/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[223/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[224/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[225/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[226/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[227/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[228/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[229/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[230/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[231/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[232/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[233/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[234/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[235/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[236/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[237/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[238/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[239/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[240/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[241/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[242/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[243/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[244/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[245/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[246/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[247/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[248/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[249/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[250/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[251/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[252/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[253/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[254/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[255/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[256/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[257/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[258/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[259/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[260/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[261/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[262/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[263/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[264/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[265/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[266/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[267/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[268/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[269/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[270/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[271/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[272/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[273/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[274/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[275/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[276/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[277/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[278/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[279/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[280/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[281/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[282/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[283/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[284/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[285/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[286/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[287/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[288/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[289/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[290/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[291/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "Wrote models/7B/ggml-model-f16.gguf\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "params = Params(n_vocab=32000, n_embd=5120, n_layer=40, n_ctx=2048, n_ff=13824, n_head=40, n_head_kv=40, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/13B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 5120]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [5120]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 5120]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [5120]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [5120]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [5120]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [5120]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [5120]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [5120]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [5120]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [5120]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [5120]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [5120]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [5120]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [5120]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [5120]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [5120]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [5120]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [5120]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [5120]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [5120]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [5120]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [5120]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [5120]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [5120]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [5120]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [5120]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [5120]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [5120]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [5120]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [5120]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [5120]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [5120]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [5120]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [5120]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [5120]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [5120]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [5120]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [5120]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [5120]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [5120]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [5120]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [5120]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [5120]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/13B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/363] Writing tensor token_embd.weight                      | size  32000 x   5120  | type F16  | T+   0\n",
      "[  2/363] Writing tensor output_norm.weight                     | size   5120           | type F32  | T+   0\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type F16  | T+   0\n",
      "[  4/363] Writing tensor blk.0.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   0\n",
      "[  5/363] Writing tensor blk.0.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   0\n",
      "[  6/363] Writing tensor blk.0.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   0\n",
      "[  7/363] Writing tensor blk.0.attn_output.weight               | size   5120 x   5120  | type F16  | T+   0\n",
      "[  8/363] Writing tensor blk.0.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[  9/363] Writing tensor blk.0.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   1\n",
      "[ 10/363] Writing tensor blk.0.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 11/363] Writing tensor blk.0.attn_norm.weight                 | size   5120           | type F32  | T+   1\n",
      "[ 12/363] Writing tensor blk.0.ffn_norm.weight                  | size   5120           | type F32  | T+   1\n",
      "[ 13/363] Writing tensor blk.1.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 14/363] Writing tensor blk.1.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 15/363] Writing tensor blk.1.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 16/363] Writing tensor blk.1.attn_output.weight               | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 17/363] Writing tensor blk.1.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 18/363] Writing tensor blk.1.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   1\n",
      "[ 19/363] Writing tensor blk.1.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 20/363] Writing tensor blk.1.attn_norm.weight                 | size   5120           | type F32  | T+   1\n",
      "[ 21/363] Writing tensor blk.1.ffn_norm.weight                  | size   5120           | type F32  | T+   1\n",
      "[ 22/363] Writing tensor blk.2.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 23/363] Writing tensor blk.2.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 24/363] Writing tensor blk.2.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 25/363] Writing tensor blk.2.attn_output.weight               | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 26/363] Writing tensor blk.2.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 27/363] Writing tensor blk.2.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   2\n",
      "[ 28/363] Writing tensor blk.2.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 29/363] Writing tensor blk.2.attn_norm.weight                 | size   5120           | type F32  | T+   2\n",
      "[ 30/363] Writing tensor blk.2.ffn_norm.weight                  | size   5120           | type F32  | T+   2\n",
      "[ 31/363] Writing tensor blk.3.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 32/363] Writing tensor blk.3.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 33/363] Writing tensor blk.3.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 34/363] Writing tensor blk.3.attn_output.weight               | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 35/363] Writing tensor blk.3.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 36/363] Writing tensor blk.3.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   3\n",
      "[ 37/363] Writing tensor blk.3.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 38/363] Writing tensor blk.3.attn_norm.weight                 | size   5120           | type F32  | T+   3\n",
      "[ 39/363] Writing tensor blk.3.ffn_norm.weight                  | size   5120           | type F32  | T+   3\n",
      "[ 40/363] Writing tensor blk.4.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 41/363] Writing tensor blk.4.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 42/363] Writing tensor blk.4.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 43/363] Writing tensor blk.4.attn_output.weight               | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 44/363] Writing tensor blk.4.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 45/363] Writing tensor blk.4.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   3\n",
      "[ 46/363] Writing tensor blk.4.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 47/363] Writing tensor blk.4.attn_norm.weight                 | size   5120           | type F32  | T+   4\n",
      "[ 48/363] Writing tensor blk.4.ffn_norm.weight                  | size   5120           | type F32  | T+   4\n",
      "[ 49/363] Writing tensor blk.5.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 50/363] Writing tensor blk.5.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 51/363] Writing tensor blk.5.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 52/363] Writing tensor blk.5.attn_output.weight               | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 53/363] Writing tensor blk.5.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 54/363] Writing tensor blk.5.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   4\n",
      "[ 55/363] Writing tensor blk.5.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 56/363] Writing tensor blk.5.attn_norm.weight                 | size   5120           | type F32  | T+   4\n",
      "[ 57/363] Writing tensor blk.5.ffn_norm.weight                  | size   5120           | type F32  | T+   4\n",
      "[ 58/363] Writing tensor blk.6.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 59/363] Writing tensor blk.6.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 60/363] Writing tensor blk.6.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 61/363] Writing tensor blk.6.attn_output.weight               | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 62/363] Writing tensor blk.6.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 63/363] Writing tensor blk.6.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   5\n",
      "[ 64/363] Writing tensor blk.6.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 65/363] Writing tensor blk.6.attn_norm.weight                 | size   5120           | type F32  | T+   5\n",
      "[ 66/363] Writing tensor blk.6.ffn_norm.weight                  | size   5120           | type F32  | T+   5\n",
      "[ 67/363] Writing tensor blk.7.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 68/363] Writing tensor blk.7.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 69/363] Writing tensor blk.7.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 70/363] Writing tensor blk.7.attn_output.weight               | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 71/363] Writing tensor blk.7.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 72/363] Writing tensor blk.7.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   6\n",
      "[ 73/363] Writing tensor blk.7.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 74/363] Writing tensor blk.7.attn_norm.weight                 | size   5120           | type F32  | T+   6\n",
      "[ 75/363] Writing tensor blk.7.ffn_norm.weight                  | size   5120           | type F32  | T+   6\n",
      "[ 76/363] Writing tensor blk.8.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 77/363] Writing tensor blk.8.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 78/363] Writing tensor blk.8.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 79/363] Writing tensor blk.8.attn_output.weight               | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 80/363] Writing tensor blk.8.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 81/363] Writing tensor blk.8.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   6\n",
      "[ 82/363] Writing tensor blk.8.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 83/363] Writing tensor blk.8.attn_norm.weight                 | size   5120           | type F32  | T+   6\n",
      "[ 84/363] Writing tensor blk.8.ffn_norm.weight                  | size   5120           | type F32  | T+   6\n",
      "[ 85/363] Writing tensor blk.9.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 86/363] Writing tensor blk.9.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 87/363] Writing tensor blk.9.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 88/363] Writing tensor blk.9.attn_output.weight               | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 89/363] Writing tensor blk.9.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 90/363] Writing tensor blk.9.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   7\n",
      "[ 91/363] Writing tensor blk.9.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 92/363] Writing tensor blk.9.attn_norm.weight                 | size   5120           | type F32  | T+   7\n",
      "[ 93/363] Writing tensor blk.9.ffn_norm.weight                  | size   5120           | type F32  | T+   7\n",
      "[ 94/363] Writing tensor blk.10.attn_q.weight                   | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 95/363] Writing tensor blk.10.attn_k.weight                   | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 96/363] Writing tensor blk.10.attn_v.weight                   | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 97/363] Writing tensor blk.10.attn_output.weight              | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 98/363] Writing tensor blk.10.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 99/363] Writing tensor blk.10.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+   8\n",
      "[100/363] Writing tensor blk.10.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+   8\n",
      "[101/363] Writing tensor blk.10.attn_norm.weight                | size   5120           | type F32  | T+   8\n",
      "[102/363] Writing tensor blk.10.ffn_norm.weight                 | size   5120           | type F32  | T+   8\n",
      "[103/363] Writing tensor blk.11.attn_q.weight                   | size   5120 x   5120  | type F16  | T+   8\n",
      "[104/363] Writing tensor blk.11.attn_k.weight                   | size   5120 x   5120  | type F16  | T+   8\n",
      "[105/363] Writing tensor blk.11.attn_v.weight                   | size   5120 x   5120  | type F16  | T+   8\n",
      "[106/363] Writing tensor blk.11.attn_output.weight              | size   5120 x   5120  | type F16  | T+   8\n",
      "[107/363] Writing tensor blk.11.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+   8\n",
      "[108/363] Writing tensor blk.11.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+   9\n",
      "[109/363] Writing tensor blk.11.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+   9\n",
      "[110/363] Writing tensor blk.11.attn_norm.weight                | size   5120           | type F32  | T+   9\n",
      "[111/363] Writing tensor blk.11.ffn_norm.weight                 | size   5120           | type F32  | T+   9\n",
      "[112/363] Writing tensor blk.12.attn_q.weight                   | size   5120 x   5120  | type F16  | T+   9\n",
      "[113/363] Writing tensor blk.12.attn_k.weight                   | size   5120 x   5120  | type F16  | T+   9\n",
      "[114/363] Writing tensor blk.12.attn_v.weight                   | size   5120 x   5120  | type F16  | T+   9\n",
      "[115/363] Writing tensor blk.12.attn_output.weight              | size   5120 x   5120  | type F16  | T+   9\n",
      "[116/363] Writing tensor blk.12.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+   9\n",
      "[117/363] Writing tensor blk.12.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+   9\n",
      "[118/363] Writing tensor blk.12.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  10\n",
      "[119/363] Writing tensor blk.12.attn_norm.weight                | size   5120           | type F32  | T+  10\n",
      "[120/363] Writing tensor blk.12.ffn_norm.weight                 | size   5120           | type F32  | T+  10\n",
      "[121/363] Writing tensor blk.13.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[122/363] Writing tensor blk.13.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[123/363] Writing tensor blk.13.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[124/363] Writing tensor blk.13.attn_output.weight              | size   5120 x   5120  | type F16  | T+  10\n",
      "[125/363] Writing tensor blk.13.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  10\n",
      "[126/363] Writing tensor blk.13.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  10\n",
      "[127/363] Writing tensor blk.13.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  10\n",
      "[128/363] Writing tensor blk.13.attn_norm.weight                | size   5120           | type F32  | T+  10\n",
      "[129/363] Writing tensor blk.13.ffn_norm.weight                 | size   5120           | type F32  | T+  10\n",
      "[130/363] Writing tensor blk.14.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[131/363] Writing tensor blk.14.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[132/363] Writing tensor blk.14.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[133/363] Writing tensor blk.14.attn_output.weight              | size   5120 x   5120  | type F16  | T+  10\n",
      "[134/363] Writing tensor blk.14.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  11\n",
      "[135/363] Writing tensor blk.14.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  11\n",
      "[136/363] Writing tensor blk.14.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  11\n",
      "[137/363] Writing tensor blk.14.attn_norm.weight                | size   5120           | type F32  | T+  11\n",
      "[138/363] Writing tensor blk.14.ffn_norm.weight                 | size   5120           | type F32  | T+  11\n",
      "[139/363] Writing tensor blk.15.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[140/363] Writing tensor blk.15.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[141/363] Writing tensor blk.15.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[142/363] Writing tensor blk.15.attn_output.weight              | size   5120 x   5120  | type F16  | T+  11\n",
      "[143/363] Writing tensor blk.15.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  11\n",
      "[144/363] Writing tensor blk.15.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  12\n",
      "[145/363] Writing tensor blk.15.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  12\n",
      "[146/363] Writing tensor blk.15.attn_norm.weight                | size   5120           | type F32  | T+  12\n",
      "[147/363] Writing tensor blk.15.ffn_norm.weight                 | size   5120           | type F32  | T+  12\n",
      "[148/363] Writing tensor blk.16.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[149/363] Writing tensor blk.16.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[150/363] Writing tensor blk.16.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[151/363] Writing tensor blk.16.attn_output.weight              | size   5120 x   5120  | type F16  | T+  12\n",
      "[152/363] Writing tensor blk.16.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  12\n",
      "[153/363] Writing tensor blk.16.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  12\n",
      "[154/363] Writing tensor blk.16.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  12\n",
      "[155/363] Writing tensor blk.16.attn_norm.weight                | size   5120           | type F32  | T+  13\n",
      "[156/363] Writing tensor blk.16.ffn_norm.weight                 | size   5120           | type F32  | T+  13\n",
      "[157/363] Writing tensor blk.17.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[158/363] Writing tensor blk.17.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[159/363] Writing tensor blk.17.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[160/363] Writing tensor blk.17.attn_output.weight              | size   5120 x   5120  | type F16  | T+  13\n",
      "[161/363] Writing tensor blk.17.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  13\n",
      "[162/363] Writing tensor blk.17.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  13\n",
      "[163/363] Writing tensor blk.17.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  13\n",
      "[164/363] Writing tensor blk.17.attn_norm.weight                | size   5120           | type F32  | T+  13\n",
      "[165/363] Writing tensor blk.17.ffn_norm.weight                 | size   5120           | type F32  | T+  13\n",
      "[166/363] Writing tensor blk.18.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[167/363] Writing tensor blk.18.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[168/363] Writing tensor blk.18.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[169/363] Writing tensor blk.18.attn_output.weight              | size   5120 x   5120  | type F16  | T+  13\n",
      "[170/363] Writing tensor blk.18.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  13\n",
      "[171/363] Writing tensor blk.18.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  14\n",
      "[172/363] Writing tensor blk.18.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  14\n",
      "[173/363] Writing tensor blk.18.attn_norm.weight                | size   5120           | type F32  | T+  14\n",
      "[174/363] Writing tensor blk.18.ffn_norm.weight                 | size   5120           | type F32  | T+  14\n",
      "[175/363] Writing tensor blk.19.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[176/363] Writing tensor blk.19.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[177/363] Writing tensor blk.19.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[178/363] Writing tensor blk.19.attn_output.weight              | size   5120 x   5120  | type F16  | T+  14\n",
      "[179/363] Writing tensor blk.19.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  14\n",
      "[180/363] Writing tensor blk.19.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  14\n",
      "[181/363] Writing tensor blk.19.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  15\n",
      "[182/363] Writing tensor blk.19.attn_norm.weight                | size   5120           | type F32  | T+  15\n",
      "[183/363] Writing tensor blk.19.ffn_norm.weight                 | size   5120           | type F32  | T+  15\n",
      "[184/363] Writing tensor blk.20.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[185/363] Writing tensor blk.20.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[186/363] Writing tensor blk.20.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[187/363] Writing tensor blk.20.attn_output.weight              | size   5120 x   5120  | type F16  | T+  15\n",
      "[188/363] Writing tensor blk.20.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  15\n",
      "[189/363] Writing tensor blk.20.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  15\n",
      "[190/363] Writing tensor blk.20.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  16\n",
      "[191/363] Writing tensor blk.20.attn_norm.weight                | size   5120           | type F32  | T+  16\n",
      "[192/363] Writing tensor blk.20.ffn_norm.weight                 | size   5120           | type F32  | T+  16\n",
      "[193/363] Writing tensor blk.21.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[194/363] Writing tensor blk.21.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[195/363] Writing tensor blk.21.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[196/363] Writing tensor blk.21.attn_output.weight              | size   5120 x   5120  | type F16  | T+  16\n",
      "[197/363] Writing tensor blk.21.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  16\n",
      "[198/363] Writing tensor blk.21.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  16\n",
      "[199/363] Writing tensor blk.21.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  16\n",
      "[200/363] Writing tensor blk.21.attn_norm.weight                | size   5120           | type F32  | T+  16\n",
      "[201/363] Writing tensor blk.21.ffn_norm.weight                 | size   5120           | type F32  | T+  16\n",
      "[202/363] Writing tensor blk.22.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[203/363] Writing tensor blk.22.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[204/363] Writing tensor blk.22.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[205/363] Writing tensor blk.22.attn_output.weight              | size   5120 x   5120  | type F16  | T+  16\n",
      "[206/363] Writing tensor blk.22.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  17\n",
      "[207/363] Writing tensor blk.22.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  17\n",
      "[208/363] Writing tensor blk.22.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  17\n",
      "[209/363] Writing tensor blk.22.attn_norm.weight                | size   5120           | type F32  | T+  17\n",
      "[210/363] Writing tensor blk.22.ffn_norm.weight                 | size   5120           | type F32  | T+  17\n",
      "[211/363] Writing tensor blk.23.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[212/363] Writing tensor blk.23.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[213/363] Writing tensor blk.23.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[214/363] Writing tensor blk.23.attn_output.weight              | size   5120 x   5120  | type F16  | T+  17\n",
      "[215/363] Writing tensor blk.23.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  17\n",
      "[216/363] Writing tensor blk.23.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  18\n",
      "[217/363] Writing tensor blk.23.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  18\n",
      "[218/363] Writing tensor blk.23.attn_norm.weight                | size   5120           | type F32  | T+  18\n",
      "[219/363] Writing tensor blk.23.ffn_norm.weight                 | size   5120           | type F32  | T+  18\n",
      "[220/363] Writing tensor blk.24.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[221/363] Writing tensor blk.24.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[222/363] Writing tensor blk.24.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[223/363] Writing tensor blk.24.attn_output.weight              | size   5120 x   5120  | type F16  | T+  18\n",
      "[224/363] Writing tensor blk.24.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  18\n",
      "[225/363] Writing tensor blk.24.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  18\n",
      "[226/363] Writing tensor blk.24.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  18\n",
      "[227/363] Writing tensor blk.24.attn_norm.weight                | size   5120           | type F32  | T+  18\n",
      "[228/363] Writing tensor blk.24.ffn_norm.weight                 | size   5120           | type F32  | T+  18\n",
      "[229/363] Writing tensor blk.25.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[230/363] Writing tensor blk.25.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[231/363] Writing tensor blk.25.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[232/363] Writing tensor blk.25.attn_output.weight              | size   5120 x   5120  | type F16  | T+  18\n",
      "[233/363] Writing tensor blk.25.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  19\n",
      "[234/363] Writing tensor blk.25.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  19\n",
      "[235/363] Writing tensor blk.25.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  19\n",
      "[236/363] Writing tensor blk.25.attn_norm.weight                | size   5120           | type F32  | T+  19\n",
      "[237/363] Writing tensor blk.25.ffn_norm.weight                 | size   5120           | type F32  | T+  19\n",
      "[238/363] Writing tensor blk.26.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[239/363] Writing tensor blk.26.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[240/363] Writing tensor blk.26.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[241/363] Writing tensor blk.26.attn_output.weight              | size   5120 x   5120  | type F16  | T+  19\n",
      "[242/363] Writing tensor blk.26.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  19\n",
      "[243/363] Writing tensor blk.26.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  20\n",
      "[244/363] Writing tensor blk.26.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  20\n",
      "[245/363] Writing tensor blk.26.attn_norm.weight                | size   5120           | type F32  | T+  20\n",
      "[246/363] Writing tensor blk.26.ffn_norm.weight                 | size   5120           | type F32  | T+  20\n",
      "[247/363] Writing tensor blk.27.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[248/363] Writing tensor blk.27.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[249/363] Writing tensor blk.27.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[250/363] Writing tensor blk.27.attn_output.weight              | size   5120 x   5120  | type F16  | T+  20\n",
      "[251/363] Writing tensor blk.27.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  20\n",
      "[252/363] Writing tensor blk.27.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  20\n",
      "[253/363] Writing tensor blk.27.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  21\n",
      "[254/363] Writing tensor blk.27.attn_norm.weight                | size   5120           | type F32  | T+  21\n",
      "[255/363] Writing tensor blk.27.ffn_norm.weight                 | size   5120           | type F32  | T+  21\n",
      "[256/363] Writing tensor blk.28.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[257/363] Writing tensor blk.28.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[258/363] Writing tensor blk.28.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[259/363] Writing tensor blk.28.attn_output.weight              | size   5120 x   5120  | type F16  | T+  21\n",
      "[260/363] Writing tensor blk.28.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  21\n",
      "[261/363] Writing tensor blk.28.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  21\n",
      "[262/363] Writing tensor blk.28.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  22\n",
      "[263/363] Writing tensor blk.28.attn_norm.weight                | size   5120           | type F32  | T+  22\n",
      "[264/363] Writing tensor blk.28.ffn_norm.weight                 | size   5120           | type F32  | T+  22\n",
      "[265/363] Writing tensor blk.29.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[266/363] Writing tensor blk.29.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[267/363] Writing tensor blk.29.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[268/363] Writing tensor blk.29.attn_output.weight              | size   5120 x   5120  | type F16  | T+  22\n",
      "[269/363] Writing tensor blk.29.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  22\n",
      "[270/363] Writing tensor blk.29.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  22\n",
      "[271/363] Writing tensor blk.29.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  22\n",
      "[272/363] Writing tensor blk.29.attn_norm.weight                | size   5120           | type F32  | T+  22\n",
      "[273/363] Writing tensor blk.29.ffn_norm.weight                 | size   5120           | type F32  | T+  22\n",
      "[274/363] Writing tensor blk.30.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[275/363] Writing tensor blk.30.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[276/363] Writing tensor blk.30.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[277/363] Writing tensor blk.30.attn_output.weight              | size   5120 x   5120  | type F16  | T+  23\n",
      "[278/363] Writing tensor blk.30.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  23\n",
      "[279/363] Writing tensor blk.30.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  23\n",
      "[280/363] Writing tensor blk.30.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  23\n",
      "[281/363] Writing tensor blk.30.attn_norm.weight                | size   5120           | type F32  | T+  23\n",
      "[282/363] Writing tensor blk.30.ffn_norm.weight                 | size   5120           | type F32  | T+  23\n",
      "[283/363] Writing tensor blk.31.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[284/363] Writing tensor blk.31.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[285/363] Writing tensor blk.31.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[286/363] Writing tensor blk.31.attn_output.weight              | size   5120 x   5120  | type F16  | T+  23\n",
      "[287/363] Writing tensor blk.31.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  24\n",
      "[288/363] Writing tensor blk.31.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  24\n",
      "[289/363] Writing tensor blk.31.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  24\n",
      "[290/363] Writing tensor blk.31.attn_norm.weight                | size   5120           | type F32  | T+  24\n",
      "[291/363] Writing tensor blk.31.ffn_norm.weight                 | size   5120           | type F32  | T+  24\n",
      "[292/363] Writing tensor blk.32.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[293/363] Writing tensor blk.32.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[294/363] Writing tensor blk.32.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[295/363] Writing tensor blk.32.attn_output.weight              | size   5120 x   5120  | type F16  | T+  24\n",
      "[296/363] Writing tensor blk.32.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  24\n",
      "[297/363] Writing tensor blk.32.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  25\n",
      "[298/363] Writing tensor blk.32.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  25\n",
      "[299/363] Writing tensor blk.32.attn_norm.weight                | size   5120           | type F32  | T+  25\n",
      "[300/363] Writing tensor blk.32.ffn_norm.weight                 | size   5120           | type F32  | T+  25\n",
      "[301/363] Writing tensor blk.33.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[302/363] Writing tensor blk.33.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[303/363] Writing tensor blk.33.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[304/363] Writing tensor blk.33.attn_output.weight              | size   5120 x   5120  | type F16  | T+  25\n",
      "[305/363] Writing tensor blk.33.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  25\n",
      "[306/363] Writing tensor blk.33.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  25\n",
      "[307/363] Writing tensor blk.33.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  26\n",
      "[308/363] Writing tensor blk.33.attn_norm.weight                | size   5120           | type F32  | T+  26\n",
      "[309/363] Writing tensor blk.33.ffn_norm.weight                 | size   5120           | type F32  | T+  26\n",
      "[310/363] Writing tensor blk.34.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[311/363] Writing tensor blk.34.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[312/363] Writing tensor blk.34.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[313/363] Writing tensor blk.34.attn_output.weight              | size   5120 x   5120  | type F16  | T+  26\n",
      "[314/363] Writing tensor blk.34.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  26\n",
      "[315/363] Writing tensor blk.34.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  26\n",
      "[316/363] Writing tensor blk.34.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  26\n",
      "[317/363] Writing tensor blk.34.attn_norm.weight                | size   5120           | type F32  | T+  26\n",
      "[318/363] Writing tensor blk.34.ffn_norm.weight                 | size   5120           | type F32  | T+  26\n",
      "[319/363] Writing tensor blk.35.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[320/363] Writing tensor blk.35.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[321/363] Writing tensor blk.35.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[322/363] Writing tensor blk.35.attn_output.weight              | size   5120 x   5120  | type F16  | T+  26\n",
      "[323/363] Writing tensor blk.35.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  27\n",
      "[324/363] Writing tensor blk.35.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  27\n",
      "[325/363] Writing tensor blk.35.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  27\n",
      "[326/363] Writing tensor blk.35.attn_norm.weight                | size   5120           | type F32  | T+  27\n",
      "[327/363] Writing tensor blk.35.ffn_norm.weight                 | size   5120           | type F32  | T+  27\n",
      "[328/363] Writing tensor blk.36.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[329/363] Writing tensor blk.36.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[330/363] Writing tensor blk.36.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[331/363] Writing tensor blk.36.attn_output.weight              | size   5120 x   5120  | type F16  | T+  27\n",
      "[332/363] Writing tensor blk.36.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  27\n",
      "[333/363] Writing tensor blk.36.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  28\n",
      "[334/363] Writing tensor blk.36.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  28\n",
      "[335/363] Writing tensor blk.36.attn_norm.weight                | size   5120           | type F32  | T+  28\n",
      "[336/363] Writing tensor blk.36.ffn_norm.weight                 | size   5120           | type F32  | T+  28\n",
      "[337/363] Writing tensor blk.37.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[338/363] Writing tensor blk.37.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[339/363] Writing tensor blk.37.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[340/363] Writing tensor blk.37.attn_output.weight              | size   5120 x   5120  | type F16  | T+  28\n",
      "[341/363] Writing tensor blk.37.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  28\n",
      "[342/363] Writing tensor blk.37.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  28\n",
      "[343/363] Writing tensor blk.37.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  29\n",
      "[344/363] Writing tensor blk.37.attn_norm.weight                | size   5120           | type F32  | T+  29\n",
      "[345/363] Writing tensor blk.37.ffn_norm.weight                 | size   5120           | type F32  | T+  29\n",
      "[346/363] Writing tensor blk.38.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[347/363] Writing tensor blk.38.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[348/363] Writing tensor blk.38.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[349/363] Writing tensor blk.38.attn_output.weight              | size   5120 x   5120  | type F16  | T+  29\n",
      "[350/363] Writing tensor blk.38.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  29\n",
      "[351/363] Writing tensor blk.38.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  29\n",
      "[352/363] Writing tensor blk.38.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  30\n",
      "[353/363] Writing tensor blk.38.attn_norm.weight                | size   5120           | type F32  | T+  30\n",
      "[354/363] Writing tensor blk.38.ffn_norm.weight                 | size   5120           | type F32  | T+  30\n",
      "[355/363] Writing tensor blk.39.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[356/363] Writing tensor blk.39.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[357/363] Writing tensor blk.39.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[358/363] Writing tensor blk.39.attn_output.weight              | size   5120 x   5120  | type F16  | T+  30\n",
      "[359/363] Writing tensor blk.39.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  30\n",
      "[360/363] Writing tensor blk.39.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  30\n",
      "[361/363] Writing tensor blk.39.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  30\n",
      "[362/363] Writing tensor blk.39.attn_norm.weight                | size   5120           | type F32  | T+  30\n",
      "[363/363] Writing tensor blk.39.ffn_norm.weight                 | size   5120           | type F32  | T+  30\n",
      "Wrote models/13B/ggml-model-f16.gguf\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "params = Params(n_vocab=32000, n_embd=6656, n_layer=60, n_ctx=2048, n_ff=17920, n_head=52, n_head_kv=52, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/30B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 6656]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [6656]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 6656]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [6656]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [6656]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [6656]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [6656]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [6656]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [6656]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [6656]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [6656]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [6656]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [6656]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [6656]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [6656]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [6656]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [6656]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [6656]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [6656]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [6656]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [6656]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [6656]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [6656]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [6656]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [6656]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [6656]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [6656]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [6656]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [6656]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [6656]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [6656]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [6656]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [6656]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [6656]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [6656]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [6656]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [6656]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [6656]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [6656]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [6656]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [6656]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [6656]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [6656]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [6656]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [6656]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [6656]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [6656]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [6656]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [6656]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [6656]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [6656]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [6656]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [6656]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [6656]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [6656]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [6656]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [6656]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [6656]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [6656]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [6656]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [6656]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [6656]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [6656]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [6656]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/30B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/543] Writing tensor token_embd.weight                      | size  32000 x   6656  | type F16  | T+   0\n",
      "[  2/543] Writing tensor output_norm.weight                     | size   6656           | type F32  | T+   1\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type F16  | T+   1\n",
      "[  4/543] Writing tensor blk.0.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   1\n",
      "[  5/543] Writing tensor blk.0.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   1\n",
      "[  6/543] Writing tensor blk.0.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   1\n",
      "[  7/543] Writing tensor blk.0.attn_output.weight               | size   6656 x   6656  | type F16  | T+   1\n",
      "[  8/543] Writing tensor blk.0.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   1\n",
      "[  9/543] Writing tensor blk.0.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   2\n",
      "[ 10/543] Writing tensor blk.0.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   2\n",
      "[ 11/543] Writing tensor blk.0.attn_norm.weight                 | size   6656           | type F32  | T+   2\n",
      "[ 12/543] Writing tensor blk.0.ffn_norm.weight                  | size   6656           | type F32  | T+   2\n",
      "[ 13/543] Writing tensor blk.1.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 14/543] Writing tensor blk.1.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 15/543] Writing tensor blk.1.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 16/543] Writing tensor blk.1.attn_output.weight               | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 17/543] Writing tensor blk.1.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   3\n",
      "[ 18/543] Writing tensor blk.1.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   3\n",
      "[ 19/543] Writing tensor blk.1.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   3\n",
      "[ 20/543] Writing tensor blk.1.attn_norm.weight                 | size   6656           | type F32  | T+   3\n",
      "[ 21/543] Writing tensor blk.1.ffn_norm.weight                  | size   6656           | type F32  | T+   3\n",
      "[ 22/543] Writing tensor blk.2.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 23/543] Writing tensor blk.2.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 24/543] Writing tensor blk.2.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 25/543] Writing tensor blk.2.attn_output.weight               | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 26/543] Writing tensor blk.2.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   4\n",
      "[ 27/543] Writing tensor blk.2.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   4\n",
      "[ 28/543] Writing tensor blk.2.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   4\n",
      "[ 29/543] Writing tensor blk.2.attn_norm.weight                 | size   6656           | type F32  | T+   4\n",
      "[ 30/543] Writing tensor blk.2.ffn_norm.weight                  | size   6656           | type F32  | T+   4\n",
      "[ 31/543] Writing tensor blk.3.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 32/543] Writing tensor blk.3.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 33/543] Writing tensor blk.3.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 34/543] Writing tensor blk.3.attn_output.weight               | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 35/543] Writing tensor blk.3.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   5\n",
      "[ 36/543] Writing tensor blk.3.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   5\n",
      "[ 37/543] Writing tensor blk.3.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   6\n",
      "[ 38/543] Writing tensor blk.3.attn_norm.weight                 | size   6656           | type F32  | T+   6\n",
      "[ 39/543] Writing tensor blk.3.ffn_norm.weight                  | size   6656           | type F32  | T+   6\n",
      "[ 40/543] Writing tensor blk.4.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 41/543] Writing tensor blk.4.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 42/543] Writing tensor blk.4.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 43/543] Writing tensor blk.4.attn_output.weight               | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 44/543] Writing tensor blk.4.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   6\n",
      "[ 45/543] Writing tensor blk.4.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   7\n",
      "[ 46/543] Writing tensor blk.4.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   7\n",
      "[ 47/543] Writing tensor blk.4.attn_norm.weight                 | size   6656           | type F32  | T+   7\n",
      "[ 48/543] Writing tensor blk.4.ffn_norm.weight                  | size   6656           | type F32  | T+   7\n",
      "[ 49/543] Writing tensor blk.5.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 50/543] Writing tensor blk.5.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 51/543] Writing tensor blk.5.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 52/543] Writing tensor blk.5.attn_output.weight               | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 53/543] Writing tensor blk.5.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   8\n",
      "[ 54/543] Writing tensor blk.5.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   8\n",
      "[ 55/543] Writing tensor blk.5.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   8\n",
      "[ 56/543] Writing tensor blk.5.attn_norm.weight                 | size   6656           | type F32  | T+   9\n",
      "[ 57/543] Writing tensor blk.5.ffn_norm.weight                  | size   6656           | type F32  | T+   9\n",
      "[ 58/543] Writing tensor blk.6.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 59/543] Writing tensor blk.6.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 60/543] Writing tensor blk.6.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 61/543] Writing tensor blk.6.attn_output.weight               | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 62/543] Writing tensor blk.6.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   9\n",
      "[ 63/543] Writing tensor blk.6.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  10\n",
      "[ 64/543] Writing tensor blk.6.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  10\n",
      "[ 65/543] Writing tensor blk.6.attn_norm.weight                 | size   6656           | type F32  | T+  10\n",
      "[ 66/543] Writing tensor blk.6.ffn_norm.weight                  | size   6656           | type F32  | T+  10\n",
      "[ 67/543] Writing tensor blk.7.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 68/543] Writing tensor blk.7.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 69/543] Writing tensor blk.7.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 70/543] Writing tensor blk.7.attn_output.weight               | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 71/543] Writing tensor blk.7.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  10\n",
      "[ 72/543] Writing tensor blk.7.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  11\n",
      "[ 73/543] Writing tensor blk.7.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  11\n",
      "[ 74/543] Writing tensor blk.7.attn_norm.weight                 | size   6656           | type F32  | T+  11\n",
      "[ 75/543] Writing tensor blk.7.ffn_norm.weight                  | size   6656           | type F32  | T+  11\n",
      "[ 76/543] Writing tensor blk.8.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 77/543] Writing tensor blk.8.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 78/543] Writing tensor blk.8.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 79/543] Writing tensor blk.8.attn_output.weight               | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 80/543] Writing tensor blk.8.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 81/543] Writing tensor blk.8.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  12\n",
      "[ 82/543] Writing tensor blk.8.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 83/543] Writing tensor blk.8.attn_norm.weight                 | size   6656           | type F32  | T+  12\n",
      "[ 84/543] Writing tensor blk.8.ffn_norm.weight                  | size   6656           | type F32  | T+  12\n",
      "[ 85/543] Writing tensor blk.9.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 86/543] Writing tensor blk.9.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 87/543] Writing tensor blk.9.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 88/543] Writing tensor blk.9.attn_output.weight               | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 89/543] Writing tensor blk.9.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  13\n",
      "[ 90/543] Writing tensor blk.9.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  13\n",
      "[ 91/543] Writing tensor blk.9.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  14\n",
      "[ 92/543] Writing tensor blk.9.attn_norm.weight                 | size   6656           | type F32  | T+  14\n",
      "[ 93/543] Writing tensor blk.9.ffn_norm.weight                  | size   6656           | type F32  | T+  14\n",
      "[ 94/543] Writing tensor blk.10.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 95/543] Writing tensor blk.10.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 96/543] Writing tensor blk.10.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 97/543] Writing tensor blk.10.attn_output.weight              | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 98/543] Writing tensor blk.10.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  14\n",
      "[ 99/543] Writing tensor blk.10.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  15\n",
      "[100/543] Writing tensor blk.10.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  15\n",
      "[101/543] Writing tensor blk.10.attn_norm.weight                | size   6656           | type F32  | T+  15\n",
      "[102/543] Writing tensor blk.10.ffn_norm.weight                 | size   6656           | type F32  | T+  15\n",
      "[103/543] Writing tensor blk.11.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  15\n",
      "[104/543] Writing tensor blk.11.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  15\n",
      "[105/543] Writing tensor blk.11.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  15\n",
      "[106/543] Writing tensor blk.11.attn_output.weight              | size   6656 x   6656  | type F16  | T+  15\n",
      "[107/543] Writing tensor blk.11.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  16\n",
      "[108/543] Writing tensor blk.11.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  16\n",
      "[109/543] Writing tensor blk.11.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  17\n",
      "[110/543] Writing tensor blk.11.attn_norm.weight                | size   6656           | type F32  | T+  17\n",
      "[111/543] Writing tensor blk.11.ffn_norm.weight                 | size   6656           | type F32  | T+  17\n",
      "[112/543] Writing tensor blk.12.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  17\n",
      "[113/543] Writing tensor blk.12.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  17\n",
      "[114/543] Writing tensor blk.12.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  17\n",
      "[115/543] Writing tensor blk.12.attn_output.weight              | size   6656 x   6656  | type F16  | T+  17\n",
      "[116/543] Writing tensor blk.12.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  17\n",
      "[117/543] Writing tensor blk.12.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  18\n",
      "[118/543] Writing tensor blk.12.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  18\n",
      "[119/543] Writing tensor blk.12.attn_norm.weight                | size   6656           | type F32  | T+  18\n",
      "[120/543] Writing tensor blk.12.ffn_norm.weight                 | size   6656           | type F32  | T+  18\n",
      "[121/543] Writing tensor blk.13.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[122/543] Writing tensor blk.13.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[123/543] Writing tensor blk.13.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[124/543] Writing tensor blk.13.attn_output.weight              | size   6656 x   6656  | type F16  | T+  18\n",
      "[125/543] Writing tensor blk.13.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  18\n",
      "[126/543] Writing tensor blk.13.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  19\n",
      "[127/543] Writing tensor blk.13.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  19\n",
      "[128/543] Writing tensor blk.13.attn_norm.weight                | size   6656           | type F32  | T+  19\n",
      "[129/543] Writing tensor blk.13.ffn_norm.weight                 | size   6656           | type F32  | T+  19\n",
      "[130/543] Writing tensor blk.14.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[131/543] Writing tensor blk.14.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[132/543] Writing tensor blk.14.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[133/543] Writing tensor blk.14.attn_output.weight              | size   6656 x   6656  | type F16  | T+  19\n",
      "[134/543] Writing tensor blk.14.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  20\n",
      "[135/543] Writing tensor blk.14.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  20\n",
      "[136/543] Writing tensor blk.14.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  21\n",
      "[137/543] Writing tensor blk.14.attn_norm.weight                | size   6656           | type F32  | T+  21\n",
      "[138/543] Writing tensor blk.14.ffn_norm.weight                 | size   6656           | type F32  | T+  21\n",
      "[139/543] Writing tensor blk.15.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  21\n",
      "[140/543] Writing tensor blk.15.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  21\n",
      "[141/543] Writing tensor blk.15.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  21\n",
      "[142/543] Writing tensor blk.15.attn_output.weight              | size   6656 x   6656  | type F16  | T+  21\n",
      "[143/543] Writing tensor blk.15.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  21\n",
      "[144/543] Writing tensor blk.15.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  22\n",
      "[145/543] Writing tensor blk.15.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  22\n",
      "[146/543] Writing tensor blk.15.attn_norm.weight                | size   6656           | type F32  | T+  22\n",
      "[147/543] Writing tensor blk.15.ffn_norm.weight                 | size   6656           | type F32  | T+  22\n",
      "[148/543] Writing tensor blk.16.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[149/543] Writing tensor blk.16.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[150/543] Writing tensor blk.16.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[151/543] Writing tensor blk.16.attn_output.weight              | size   6656 x   6656  | type F16  | T+  22\n",
      "[152/543] Writing tensor blk.16.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  22\n",
      "[153/543] Writing tensor blk.16.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  23\n",
      "[154/543] Writing tensor blk.16.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  23\n",
      "[155/543] Writing tensor blk.16.attn_norm.weight                | size   6656           | type F32  | T+  23\n",
      "[156/543] Writing tensor blk.16.ffn_norm.weight                 | size   6656           | type F32  | T+  23\n",
      "[157/543] Writing tensor blk.17.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  23\n",
      "[158/543] Writing tensor blk.17.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  23\n",
      "[159/543] Writing tensor blk.17.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  23\n",
      "[160/543] Writing tensor blk.17.attn_output.weight              | size   6656 x   6656  | type F16  | T+  23\n",
      "[161/543] Writing tensor blk.17.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  24\n",
      "[162/543] Writing tensor blk.17.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  24\n",
      "[163/543] Writing tensor blk.17.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  25\n",
      "[164/543] Writing tensor blk.17.attn_norm.weight                | size   6656           | type F32  | T+  25\n",
      "[165/543] Writing tensor blk.17.ffn_norm.weight                 | size   6656           | type F32  | T+  25\n",
      "[166/543] Writing tensor blk.18.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[167/543] Writing tensor blk.18.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[168/543] Writing tensor blk.18.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[169/543] Writing tensor blk.18.attn_output.weight              | size   6656 x   6656  | type F16  | T+  25\n",
      "[170/543] Writing tensor blk.18.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  25\n",
      "[171/543] Writing tensor blk.18.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  26\n",
      "[172/543] Writing tensor blk.18.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  26\n",
      "[173/543] Writing tensor blk.18.attn_norm.weight                | size   6656           | type F32  | T+  26\n",
      "[174/543] Writing tensor blk.18.ffn_norm.weight                 | size   6656           | type F32  | T+  26\n",
      "[175/543] Writing tensor blk.19.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  26\n",
      "[176/543] Writing tensor blk.19.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  26\n",
      "[177/543] Writing tensor blk.19.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  26\n",
      "[178/543] Writing tensor blk.19.attn_output.weight              | size   6656 x   6656  | type F16  | T+  26\n",
      "[179/543] Writing tensor blk.19.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  26\n",
      "[180/543] Writing tensor blk.19.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  27\n",
      "[181/543] Writing tensor blk.19.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  28\n",
      "[182/543] Writing tensor blk.19.attn_norm.weight                | size   6656           | type F32  | T+  28\n",
      "[183/543] Writing tensor blk.19.ffn_norm.weight                 | size   6656           | type F32  | T+  28\n",
      "[184/543] Writing tensor blk.20.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  28\n",
      "[185/543] Writing tensor blk.20.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  28\n",
      "[186/543] Writing tensor blk.20.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  28\n",
      "[187/543] Writing tensor blk.20.attn_output.weight              | size   6656 x   6656  | type F16  | T+  28\n",
      "[188/543] Writing tensor blk.20.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  28\n",
      "[189/543] Writing tensor blk.20.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  29\n",
      "[190/543] Writing tensor blk.20.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  29\n",
      "[191/543] Writing tensor blk.20.attn_norm.weight                | size   6656           | type F32  | T+  29\n",
      "[192/543] Writing tensor blk.20.ffn_norm.weight                 | size   6656           | type F32  | T+  29\n",
      "[193/543] Writing tensor blk.21.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[194/543] Writing tensor blk.21.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[195/543] Writing tensor blk.21.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[196/543] Writing tensor blk.21.attn_output.weight              | size   6656 x   6656  | type F16  | T+  29\n",
      "[197/543] Writing tensor blk.21.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  29\n",
      "[198/543] Writing tensor blk.21.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  30\n",
      "[199/543] Writing tensor blk.21.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  30\n",
      "[200/543] Writing tensor blk.21.attn_norm.weight                | size   6656           | type F32  | T+  30\n",
      "[201/543] Writing tensor blk.21.ffn_norm.weight                 | size   6656           | type F32  | T+  30\n",
      "[202/543] Writing tensor blk.22.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[203/543] Writing tensor blk.22.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[204/543] Writing tensor blk.22.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[205/543] Writing tensor blk.22.attn_output.weight              | size   6656 x   6656  | type F16  | T+  30\n",
      "[206/543] Writing tensor blk.22.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  31\n",
      "[207/543] Writing tensor blk.22.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  31\n",
      "[208/543] Writing tensor blk.22.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  31\n",
      "[209/543] Writing tensor blk.22.attn_norm.weight                | size   6656           | type F32  | T+  32\n",
      "[210/543] Writing tensor blk.22.ffn_norm.weight                 | size   6656           | type F32  | T+  32\n",
      "[211/543] Writing tensor blk.23.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  32\n",
      "[212/543] Writing tensor blk.23.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  32\n",
      "[213/543] Writing tensor blk.23.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  32\n",
      "[214/543] Writing tensor blk.23.attn_output.weight              | size   6656 x   6656  | type F16  | T+  32\n",
      "[215/543] Writing tensor blk.23.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  32\n",
      "[216/543] Writing tensor blk.23.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  33\n",
      "[217/543] Writing tensor blk.23.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  33\n",
      "[218/543] Writing tensor blk.23.attn_norm.weight                | size   6656           | type F32  | T+  33\n",
      "[219/543] Writing tensor blk.23.ffn_norm.weight                 | size   6656           | type F32  | T+  33\n",
      "[220/543] Writing tensor blk.24.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[221/543] Writing tensor blk.24.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[222/543] Writing tensor blk.24.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[223/543] Writing tensor blk.24.attn_output.weight              | size   6656 x   6656  | type F16  | T+  33\n",
      "[224/543] Writing tensor blk.24.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  34\n",
      "[225/543] Writing tensor blk.24.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  34\n",
      "[226/543] Writing tensor blk.24.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  34\n",
      "[227/543] Writing tensor blk.24.attn_norm.weight                | size   6656           | type F32  | T+  34\n",
      "[228/543] Writing tensor blk.24.ffn_norm.weight                 | size   6656           | type F32  | T+  34\n",
      "[229/543] Writing tensor blk.25.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[230/543] Writing tensor blk.25.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[231/543] Writing tensor blk.25.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[232/543] Writing tensor blk.25.attn_output.weight              | size   6656 x   6656  | type F16  | T+  34\n",
      "[233/543] Writing tensor blk.25.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  35\n",
      "[234/543] Writing tensor blk.25.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  35\n",
      "[235/543] Writing tensor blk.25.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  35\n",
      "[236/543] Writing tensor blk.25.attn_norm.weight                | size   6656           | type F32  | T+  36\n",
      "[237/543] Writing tensor blk.25.ffn_norm.weight                 | size   6656           | type F32  | T+  36\n",
      "[238/543] Writing tensor blk.26.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  36\n",
      "[239/543] Writing tensor blk.26.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  36\n",
      "[240/543] Writing tensor blk.26.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  36\n",
      "[241/543] Writing tensor blk.26.attn_output.weight              | size   6656 x   6656  | type F16  | T+  36\n",
      "[242/543] Writing tensor blk.26.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  36\n",
      "[243/543] Writing tensor blk.26.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  37\n",
      "[244/543] Writing tensor blk.26.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  37\n",
      "[245/543] Writing tensor blk.26.attn_norm.weight                | size   6656           | type F32  | T+  37\n",
      "[246/543] Writing tensor blk.26.ffn_norm.weight                 | size   6656           | type F32  | T+  37\n",
      "[247/543] Writing tensor blk.27.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[248/543] Writing tensor blk.27.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[249/543] Writing tensor blk.27.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[250/543] Writing tensor blk.27.attn_output.weight              | size   6656 x   6656  | type F16  | T+  37\n",
      "[251/543] Writing tensor blk.27.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  37\n",
      "[252/543] Writing tensor blk.27.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  38\n",
      "[253/543] Writing tensor blk.27.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  38\n",
      "[254/543] Writing tensor blk.27.attn_norm.weight                | size   6656           | type F32  | T+  38\n",
      "[255/543] Writing tensor blk.27.ffn_norm.weight                 | size   6656           | type F32  | T+  38\n",
      "[256/543] Writing tensor blk.28.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  38\n",
      "[257/543] Writing tensor blk.28.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  38\n",
      "[258/543] Writing tensor blk.28.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  38\n",
      "[259/543] Writing tensor blk.28.attn_output.weight              | size   6656 x   6656  | type F16  | T+  38\n",
      "[260/543] Writing tensor blk.28.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  39\n",
      "[261/543] Writing tensor blk.28.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  39\n",
      "[262/543] Writing tensor blk.28.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  40\n",
      "[263/543] Writing tensor blk.28.attn_norm.weight                | size   6656           | type F32  | T+  40\n",
      "[264/543] Writing tensor blk.28.ffn_norm.weight                 | size   6656           | type F32  | T+  40\n",
      "[265/543] Writing tensor blk.29.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  40\n",
      "[266/543] Writing tensor blk.29.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  40\n",
      "[267/543] Writing tensor blk.29.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  40\n",
      "[268/543] Writing tensor blk.29.attn_output.weight              | size   6656 x   6656  | type F16  | T+  40\n",
      "[269/543] Writing tensor blk.29.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  40\n",
      "[270/543] Writing tensor blk.29.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  41\n",
      "[271/543] Writing tensor blk.29.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  41\n",
      "[272/543] Writing tensor blk.29.attn_norm.weight                | size   6656           | type F32  | T+  41\n",
      "[273/543] Writing tensor blk.29.ffn_norm.weight                 | size   6656           | type F32  | T+  41\n",
      "[274/543] Writing tensor blk.30.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[275/543] Writing tensor blk.30.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[276/543] Writing tensor blk.30.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[277/543] Writing tensor blk.30.attn_output.weight              | size   6656 x   6656  | type F16  | T+  41\n",
      "[278/543] Writing tensor blk.30.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  42\n",
      "[279/543] Writing tensor blk.30.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  42\n",
      "[280/543] Writing tensor blk.30.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  42\n",
      "[281/543] Writing tensor blk.30.attn_norm.weight                | size   6656           | type F32  | T+  42\n",
      "[282/543] Writing tensor blk.30.ffn_norm.weight                 | size   6656           | type F32  | T+  42\n",
      "[283/543] Writing tensor blk.31.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  42\n",
      "[284/543] Writing tensor blk.31.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  42\n",
      "[285/543] Writing tensor blk.31.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  42\n",
      "[286/543] Writing tensor blk.31.attn_output.weight              | size   6656 x   6656  | type F16  | T+  42\n",
      "[287/543] Writing tensor blk.31.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  43\n",
      "[288/543] Writing tensor blk.31.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  43\n",
      "[289/543] Writing tensor blk.31.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  44\n",
      "[290/543] Writing tensor blk.31.attn_norm.weight                | size   6656           | type F32  | T+  44\n",
      "[291/543] Writing tensor blk.31.ffn_norm.weight                 | size   6656           | type F32  | T+  44\n",
      "[292/543] Writing tensor blk.32.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[293/543] Writing tensor blk.32.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[294/543] Writing tensor blk.32.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[295/543] Writing tensor blk.32.attn_output.weight              | size   6656 x   6656  | type F16  | T+  44\n",
      "[296/543] Writing tensor blk.32.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  44\n",
      "[297/543] Writing tensor blk.32.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  45\n",
      "[298/543] Writing tensor blk.32.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  45\n",
      "[299/543] Writing tensor blk.32.attn_norm.weight                | size   6656           | type F32  | T+  45\n",
      "[300/543] Writing tensor blk.32.ffn_norm.weight                 | size   6656           | type F32  | T+  45\n",
      "[301/543] Writing tensor blk.33.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  45\n",
      "[302/543] Writing tensor blk.33.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  45\n",
      "[303/543] Writing tensor blk.33.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  45\n",
      "[304/543] Writing tensor blk.33.attn_output.weight              | size   6656 x   6656  | type F16  | T+  45\n",
      "[305/543] Writing tensor blk.33.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  46\n",
      "[306/543] Writing tensor blk.33.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  46\n",
      "[307/543] Writing tensor blk.33.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  46\n",
      "[308/543] Writing tensor blk.33.attn_norm.weight                | size   6656           | type F32  | T+  46\n",
      "[309/543] Writing tensor blk.33.ffn_norm.weight                 | size   6656           | type F32  | T+  46\n",
      "[310/543] Writing tensor blk.34.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[311/543] Writing tensor blk.34.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[312/543] Writing tensor blk.34.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  47\n",
      "[313/543] Writing tensor blk.34.attn_output.weight              | size   6656 x   6656  | type F16  | T+  47\n",
      "[314/543] Writing tensor blk.34.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  47\n",
      "[315/543] Writing tensor blk.34.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  47\n",
      "[316/543] Writing tensor blk.34.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  48\n",
      "[317/543] Writing tensor blk.34.attn_norm.weight                | size   6656           | type F32  | T+  48\n",
      "[318/543] Writing tensor blk.34.ffn_norm.weight                 | size   6656           | type F32  | T+  48\n",
      "[319/543] Writing tensor blk.35.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[320/543] Writing tensor blk.35.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[321/543] Writing tensor blk.35.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[322/543] Writing tensor blk.35.attn_output.weight              | size   6656 x   6656  | type F16  | T+  48\n",
      "[323/543] Writing tensor blk.35.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  48\n",
      "[324/543] Writing tensor blk.35.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  49\n",
      "[325/543] Writing tensor blk.35.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  49\n",
      "[326/543] Writing tensor blk.35.attn_norm.weight                | size   6656           | type F32  | T+  49\n",
      "[327/543] Writing tensor blk.35.ffn_norm.weight                 | size   6656           | type F32  | T+  49\n",
      "[328/543] Writing tensor blk.36.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[329/543] Writing tensor blk.36.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[330/543] Writing tensor blk.36.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[331/543] Writing tensor blk.36.attn_output.weight              | size   6656 x   6656  | type F16  | T+  49\n",
      "[332/543] Writing tensor blk.36.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  49\n",
      "[333/543] Writing tensor blk.36.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  50\n",
      "[334/543] Writing tensor blk.36.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  50\n",
      "[335/543] Writing tensor blk.36.attn_norm.weight                | size   6656           | type F32  | T+  50\n",
      "[336/543] Writing tensor blk.36.ffn_norm.weight                 | size   6656           | type F32  | T+  50\n",
      "[337/543] Writing tensor blk.37.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  50\n",
      "[338/543] Writing tensor blk.37.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  50\n",
      "[339/543] Writing tensor blk.37.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  50\n",
      "[340/543] Writing tensor blk.37.attn_output.weight              | size   6656 x   6656  | type F16  | T+  50\n",
      "[341/543] Writing tensor blk.37.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  51\n",
      "[342/543] Writing tensor blk.37.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  51\n",
      "[343/543] Writing tensor blk.37.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  52\n",
      "[344/543] Writing tensor blk.37.attn_norm.weight                | size   6656           | type F32  | T+  52\n",
      "[345/543] Writing tensor blk.37.ffn_norm.weight                 | size   6656           | type F32  | T+  52\n",
      "[346/543] Writing tensor blk.38.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  52\n",
      "[347/543] Writing tensor blk.38.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  52\n",
      "[348/543] Writing tensor blk.38.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  52\n",
      "[349/543] Writing tensor blk.38.attn_output.weight              | size   6656 x   6656  | type F16  | T+  52\n",
      "[350/543] Writing tensor blk.38.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  52\n",
      "[351/543] Writing tensor blk.38.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  53\n",
      "[352/543] Writing tensor blk.38.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  53\n",
      "[353/543] Writing tensor blk.38.attn_norm.weight                | size   6656           | type F32  | T+  53\n",
      "[354/543] Writing tensor blk.38.ffn_norm.weight                 | size   6656           | type F32  | T+  53\n",
      "[355/543] Writing tensor blk.39.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  53\n",
      "[356/543] Writing tensor blk.39.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  53\n",
      "[357/543] Writing tensor blk.39.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  53\n",
      "[358/543] Writing tensor blk.39.attn_output.weight              | size   6656 x   6656  | type F16  | T+  53\n",
      "[359/543] Writing tensor blk.39.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  54\n",
      "[360/543] Writing tensor blk.39.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  54\n",
      "[361/543] Writing tensor blk.39.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  54\n",
      "[362/543] Writing tensor blk.39.attn_norm.weight                | size   6656           | type F32  | T+  55\n",
      "[363/543] Writing tensor blk.39.ffn_norm.weight                 | size   6656           | type F32  | T+  55\n",
      "[364/543] Writing tensor blk.40.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  55\n",
      "[365/543] Writing tensor blk.40.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  55\n",
      "[366/543] Writing tensor blk.40.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  55\n",
      "[367/543] Writing tensor blk.40.attn_output.weight              | size   6656 x   6656  | type F16  | T+  55\n",
      "[368/543] Writing tensor blk.40.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  55\n",
      "[369/543] Writing tensor blk.40.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  56\n",
      "[370/543] Writing tensor blk.40.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  56\n",
      "[371/543] Writing tensor blk.40.attn_norm.weight                | size   6656           | type F32  | T+  56\n",
      "[372/543] Writing tensor blk.40.ffn_norm.weight                 | size   6656           | type F32  | T+  56\n",
      "[373/543] Writing tensor blk.41.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[374/543] Writing tensor blk.41.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[375/543] Writing tensor blk.41.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[376/543] Writing tensor blk.41.attn_output.weight              | size   6656 x   6656  | type F16  | T+  56\n",
      "[377/543] Writing tensor blk.41.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  57\n",
      "[378/543] Writing tensor blk.41.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  57\n",
      "[379/543] Writing tensor blk.41.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  57\n",
      "[380/543] Writing tensor blk.41.attn_norm.weight                | size   6656           | type F32  | T+  57\n",
      "[381/543] Writing tensor blk.41.ffn_norm.weight                 | size   6656           | type F32  | T+  57\n",
      "[382/543] Writing tensor blk.42.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[383/543] Writing tensor blk.42.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[384/543] Writing tensor blk.42.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  58\n",
      "[385/543] Writing tensor blk.42.attn_output.weight              | size   6656 x   6656  | type F16  | T+  58\n",
      "[386/543] Writing tensor blk.42.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  58\n",
      "[387/543] Writing tensor blk.42.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  58\n",
      "[388/543] Writing tensor blk.42.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  59\n",
      "[389/543] Writing tensor blk.42.attn_norm.weight                | size   6656           | type F32  | T+  59\n",
      "[390/543] Writing tensor blk.42.ffn_norm.weight                 | size   6656           | type F32  | T+  59\n",
      "[391/543] Writing tensor blk.43.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[392/543] Writing tensor blk.43.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[393/543] Writing tensor blk.43.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[394/543] Writing tensor blk.43.attn_output.weight              | size   6656 x   6656  | type F16  | T+  59\n",
      "[395/543] Writing tensor blk.43.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  59\n",
      "[396/543] Writing tensor blk.43.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  60\n",
      "[397/543] Writing tensor blk.43.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  60\n",
      "[398/543] Writing tensor blk.43.attn_norm.weight                | size   6656           | type F32  | T+  60\n",
      "[399/543] Writing tensor blk.43.ffn_norm.weight                 | size   6656           | type F32  | T+  60\n",
      "[400/543] Writing tensor blk.44.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[401/543] Writing tensor blk.44.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[402/543] Writing tensor blk.44.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[403/543] Writing tensor blk.44.attn_output.weight              | size   6656 x   6656  | type F16  | T+  60\n",
      "[404/543] Writing tensor blk.44.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  60\n",
      "[405/543] Writing tensor blk.44.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  61\n",
      "[406/543] Writing tensor blk.44.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  61\n",
      "[407/543] Writing tensor blk.44.attn_norm.weight                | size   6656           | type F32  | T+  62\n",
      "[408/543] Writing tensor blk.44.ffn_norm.weight                 | size   6656           | type F32  | T+  62\n",
      "[409/543] Writing tensor blk.45.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[410/543] Writing tensor blk.45.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[411/543] Writing tensor blk.45.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[412/543] Writing tensor blk.45.attn_output.weight              | size   6656 x   6656  | type F16  | T+  62\n",
      "[413/543] Writing tensor blk.45.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  62\n",
      "[414/543] Writing tensor blk.45.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  63\n",
      "[415/543] Writing tensor blk.45.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  63\n",
      "[416/543] Writing tensor blk.45.attn_norm.weight                | size   6656           | type F32  | T+  63\n",
      "[417/543] Writing tensor blk.45.ffn_norm.weight                 | size   6656           | type F32  | T+  63\n",
      "[418/543] Writing tensor blk.46.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[419/543] Writing tensor blk.46.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[420/543] Writing tensor blk.46.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[421/543] Writing tensor blk.46.attn_output.weight              | size   6656 x   6656  | type F16  | T+  63\n",
      "[422/543] Writing tensor blk.46.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  63\n",
      "[423/543] Writing tensor blk.46.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  64\n",
      "[424/543] Writing tensor blk.46.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  64\n",
      "[425/543] Writing tensor blk.46.attn_norm.weight                | size   6656           | type F32  | T+  64\n",
      "[426/543] Writing tensor blk.46.ffn_norm.weight                 | size   6656           | type F32  | T+  64\n",
      "[427/543] Writing tensor blk.47.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[428/543] Writing tensor blk.47.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[429/543] Writing tensor blk.47.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[430/543] Writing tensor blk.47.attn_output.weight              | size   6656 x   6656  | type F16  | T+  64\n",
      "[431/543] Writing tensor blk.47.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  65\n",
      "[432/543] Writing tensor blk.47.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  65\n",
      "[433/543] Writing tensor blk.47.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  66\n",
      "[434/543] Writing tensor blk.47.attn_norm.weight                | size   6656           | type F32  | T+  66\n",
      "[435/543] Writing tensor blk.47.ffn_norm.weight                 | size   6656           | type F32  | T+  66\n",
      "[436/543] Writing tensor blk.48.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[437/543] Writing tensor blk.48.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[438/543] Writing tensor blk.48.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[439/543] Writing tensor blk.48.attn_output.weight              | size   6656 x   6656  | type F16  | T+  66\n",
      "[440/543] Writing tensor blk.48.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  66\n",
      "[441/543] Writing tensor blk.48.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  67\n",
      "[442/543] Writing tensor blk.48.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  67\n",
      "[443/543] Writing tensor blk.48.attn_norm.weight                | size   6656           | type F32  | T+  67\n",
      "[444/543] Writing tensor blk.48.ffn_norm.weight                 | size   6656           | type F32  | T+  67\n",
      "[445/543] Writing tensor blk.49.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[446/543] Writing tensor blk.49.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[447/543] Writing tensor blk.49.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[448/543] Writing tensor blk.49.attn_output.weight              | size   6656 x   6656  | type F16  | T+  67\n",
      "[449/543] Writing tensor blk.49.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  68\n",
      "[450/543] Writing tensor blk.49.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  68\n",
      "[451/543] Writing tensor blk.49.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  69\n",
      "[452/543] Writing tensor blk.49.attn_norm.weight                | size   6656           | type F32  | T+  69\n",
      "[453/543] Writing tensor blk.49.ffn_norm.weight                 | size   6656           | type F32  | T+  69\n",
      "[454/543] Writing tensor blk.50.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[455/543] Writing tensor blk.50.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[456/543] Writing tensor blk.50.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[457/543] Writing tensor blk.50.attn_output.weight              | size   6656 x   6656  | type F16  | T+  69\n",
      "[458/543] Writing tensor blk.50.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  69\n",
      "[459/543] Writing tensor blk.50.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  70\n",
      "[460/543] Writing tensor blk.50.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  70\n",
      "[461/543] Writing tensor blk.50.attn_norm.weight                | size   6656           | type F32  | T+  70\n",
      "[462/543] Writing tensor blk.50.ffn_norm.weight                 | size   6656           | type F32  | T+  70\n",
      "[463/543] Writing tensor blk.51.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[464/543] Writing tensor blk.51.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[465/543] Writing tensor blk.51.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[466/543] Writing tensor blk.51.attn_output.weight              | size   6656 x   6656  | type F16  | T+  70\n",
      "[467/543] Writing tensor blk.51.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  71\n",
      "[468/543] Writing tensor blk.51.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  71\n",
      "[469/543] Writing tensor blk.51.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  72\n",
      "[470/543] Writing tensor blk.51.attn_norm.weight                | size   6656           | type F32  | T+  72\n",
      "[471/543] Writing tensor blk.51.ffn_norm.weight                 | size   6656           | type F32  | T+  72\n",
      "[472/543] Writing tensor blk.52.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  72\n",
      "[473/543] Writing tensor blk.52.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  72\n",
      "[474/543] Writing tensor blk.52.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  72\n",
      "[475/543] Writing tensor blk.52.attn_output.weight              | size   6656 x   6656  | type F16  | T+  72\n",
      "[476/543] Writing tensor blk.52.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  72\n",
      "[477/543] Writing tensor blk.52.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  73\n",
      "[478/543] Writing tensor blk.52.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  73\n",
      "[479/543] Writing tensor blk.52.attn_norm.weight                | size   6656           | type F32  | T+  73\n",
      "[480/543] Writing tensor blk.52.ffn_norm.weight                 | size   6656           | type F32  | T+  73\n",
      "[481/543] Writing tensor blk.53.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  73\n",
      "[482/543] Writing tensor blk.53.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  73\n",
      "[483/543] Writing tensor blk.53.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  73\n",
      "[484/543] Writing tensor blk.53.attn_output.weight              | size   6656 x   6656  | type F16  | T+  73\n",
      "[485/543] Writing tensor blk.53.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  74\n",
      "[486/543] Writing tensor blk.53.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  74\n",
      "[487/543] Writing tensor blk.53.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  75\n",
      "[488/543] Writing tensor blk.53.attn_norm.weight                | size   6656           | type F32  | T+  75\n",
      "[489/543] Writing tensor blk.53.ffn_norm.weight                 | size   6656           | type F32  | T+  75\n",
      "[490/543] Writing tensor blk.54.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  75\n",
      "[491/543] Writing tensor blk.54.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  75\n",
      "[492/543] Writing tensor blk.54.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  75\n",
      "[493/543] Writing tensor blk.54.attn_output.weight              | size   6656 x   6656  | type F16  | T+  75\n",
      "[494/543] Writing tensor blk.54.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  75\n",
      "[495/543] Writing tensor blk.54.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  76\n",
      "[496/543] Writing tensor blk.54.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  76\n",
      "[497/543] Writing tensor blk.54.attn_norm.weight                | size   6656           | type F32  | T+  76\n",
      "[498/543] Writing tensor blk.54.ffn_norm.weight                 | size   6656           | type F32  | T+  76\n",
      "[499/543] Writing tensor blk.55.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[500/543] Writing tensor blk.55.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[501/543] Writing tensor blk.55.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[502/543] Writing tensor blk.55.attn_output.weight              | size   6656 x   6656  | type F16  | T+  76\n",
      "[503/543] Writing tensor blk.55.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  77\n",
      "[504/543] Writing tensor blk.55.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  77\n",
      "[505/543] Writing tensor blk.55.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  77\n",
      "[506/543] Writing tensor blk.55.attn_norm.weight                | size   6656           | type F32  | T+  78\n",
      "[507/543] Writing tensor blk.55.ffn_norm.weight                 | size   6656           | type F32  | T+  78\n",
      "[508/543] Writing tensor blk.56.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  78\n",
      "[509/543] Writing tensor blk.56.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  78\n",
      "[510/543] Writing tensor blk.56.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  78\n",
      "[511/543] Writing tensor blk.56.attn_output.weight              | size   6656 x   6656  | type F16  | T+  78\n",
      "[512/543] Writing tensor blk.56.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  78\n",
      "[513/543] Writing tensor blk.56.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  79\n",
      "[514/543] Writing tensor blk.56.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  79\n",
      "[515/543] Writing tensor blk.56.attn_norm.weight                | size   6656           | type F32  | T+  79\n",
      "[516/543] Writing tensor blk.56.ffn_norm.weight                 | size   6656           | type F32  | T+  79\n",
      "[517/543] Writing tensor blk.57.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[518/543] Writing tensor blk.57.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[519/543] Writing tensor blk.57.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[520/543] Writing tensor blk.57.attn_output.weight              | size   6656 x   6656  | type F16  | T+  79\n",
      "[521/543] Writing tensor blk.57.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  80\n",
      "[522/543] Writing tensor blk.57.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  80\n",
      "[523/543] Writing tensor blk.57.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  80\n",
      "[524/543] Writing tensor blk.57.attn_norm.weight                | size   6656           | type F32  | T+  80\n",
      "[525/543] Writing tensor blk.57.ffn_norm.weight                 | size   6656           | type F32  | T+  80\n",
      "[526/543] Writing tensor blk.58.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[527/543] Writing tensor blk.58.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[528/543] Writing tensor blk.58.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[529/543] Writing tensor blk.58.attn_output.weight              | size   6656 x   6656  | type F16  | T+  80\n",
      "[530/543] Writing tensor blk.58.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  80\n",
      "[531/543] Writing tensor blk.58.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  81\n",
      "[532/543] Writing tensor blk.58.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  82\n",
      "[533/543] Writing tensor blk.58.attn_norm.weight                | size   6656           | type F32  | T+  82\n",
      "[534/543] Writing tensor blk.58.ffn_norm.weight                 | size   6656           | type F32  | T+  82\n",
      "[535/543] Writing tensor blk.59.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[536/543] Writing tensor blk.59.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[537/543] Writing tensor blk.59.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[538/543] Writing tensor blk.59.attn_output.weight              | size   6656 x   6656  | type F16  | T+  82\n",
      "[539/543] Writing tensor blk.59.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  82\n",
      "[540/543] Writing tensor blk.59.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  82\n",
      "[541/543] Writing tensor blk.59.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  83\n",
      "[542/543] Writing tensor blk.59.attn_norm.weight                | size   6656           | type F32  | T+  83\n",
      "[543/543] Writing tensor blk.59.ffn_norm.weight                 | size   6656           | type F32  | T+  83\n",
      "Wrote models/30B/ggml-model-f16.gguf\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "params = Params(n_vocab=32000, n_embd=8192, n_layer=80, n_ctx=4096, n_ff=22016, n_head=64, n_head_kv=64, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/65B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 8192]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [8192]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 8192]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [8192]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [8192]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [8192]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [8192]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [8192]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [8192]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [8192]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [8192]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [8192]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [8192]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [8192]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [8192]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [8192]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [8192]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [8192]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [8192]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [8192]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [8192]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [8192]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [8192]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [8192]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [8192]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [8192]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [8192]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [8192]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [8192]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [8192]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [8192]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [8192]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [8192]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [8192]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [8192]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [8192]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [8192]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [8192]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [8192]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [8192]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [8192]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [8192]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [8192]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [8192]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [8192]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [8192]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [8192]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [8192]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [8192]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [8192]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [8192]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [8192]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [8192]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [8192]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [8192]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [8192]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [8192]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [8192]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [8192]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [8192]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [8192]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [8192]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [8192]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.60.attention.wq.weight                    -> blk.60.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wk.weight                    -> blk.60.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wv.weight                    -> blk.60.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wo.weight                    -> blk.60.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.60.feed_forward.w1.weight                 -> blk.60.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.60.feed_forward.w2.weight                 -> blk.60.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.60.feed_forward.w3.weight                 -> blk.60.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.60.attention_norm.weight                  -> blk.60.attn_norm.weight                  | F16    | [8192]\n",
      "layers.60.ffn_norm.weight                        -> blk.60.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.61.attention.wq.weight                    -> blk.61.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wk.weight                    -> blk.61.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wv.weight                    -> blk.61.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wo.weight                    -> blk.61.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.61.feed_forward.w1.weight                 -> blk.61.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.61.feed_forward.w2.weight                 -> blk.61.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.61.feed_forward.w3.weight                 -> blk.61.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.61.attention_norm.weight                  -> blk.61.attn_norm.weight                  | F16    | [8192]\n",
      "layers.61.ffn_norm.weight                        -> blk.61.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.62.attention.wq.weight                    -> blk.62.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wk.weight                    -> blk.62.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wv.weight                    -> blk.62.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wo.weight                    -> blk.62.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.62.feed_forward.w1.weight                 -> blk.62.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.62.feed_forward.w2.weight                 -> blk.62.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.62.feed_forward.w3.weight                 -> blk.62.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.62.attention_norm.weight                  -> blk.62.attn_norm.weight                  | F16    | [8192]\n",
      "layers.62.ffn_norm.weight                        -> blk.62.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.63.attention.wq.weight                    -> blk.63.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wk.weight                    -> blk.63.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wv.weight                    -> blk.63.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wo.weight                    -> blk.63.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.63.feed_forward.w1.weight                 -> blk.63.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.63.feed_forward.w2.weight                 -> blk.63.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.63.feed_forward.w3.weight                 -> blk.63.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.63.attention_norm.weight                  -> blk.63.attn_norm.weight                  | F16    | [8192]\n",
      "layers.63.ffn_norm.weight                        -> blk.63.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.64.attention.wq.weight                    -> blk.64.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wk.weight                    -> blk.64.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wv.weight                    -> blk.64.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wo.weight                    -> blk.64.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.64.feed_forward.w1.weight                 -> blk.64.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.64.feed_forward.w2.weight                 -> blk.64.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.64.feed_forward.w3.weight                 -> blk.64.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.64.attention_norm.weight                  -> blk.64.attn_norm.weight                  | F16    | [8192]\n",
      "layers.64.ffn_norm.weight                        -> blk.64.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.65.attention.wq.weight                    -> blk.65.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wk.weight                    -> blk.65.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wv.weight                    -> blk.65.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wo.weight                    -> blk.65.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.65.feed_forward.w1.weight                 -> blk.65.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.65.feed_forward.w2.weight                 -> blk.65.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.65.feed_forward.w3.weight                 -> blk.65.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.65.attention_norm.weight                  -> blk.65.attn_norm.weight                  | F16    | [8192]\n",
      "layers.65.ffn_norm.weight                        -> blk.65.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.66.attention.wq.weight                    -> blk.66.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wk.weight                    -> blk.66.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wv.weight                    -> blk.66.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wo.weight                    -> blk.66.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.66.feed_forward.w1.weight                 -> blk.66.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.66.feed_forward.w2.weight                 -> blk.66.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.66.feed_forward.w3.weight                 -> blk.66.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.66.attention_norm.weight                  -> blk.66.attn_norm.weight                  | F16    | [8192]\n",
      "layers.66.ffn_norm.weight                        -> blk.66.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.67.attention.wq.weight                    -> blk.67.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wk.weight                    -> blk.67.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wv.weight                    -> blk.67.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wo.weight                    -> blk.67.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.67.feed_forward.w1.weight                 -> blk.67.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.67.feed_forward.w2.weight                 -> blk.67.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.67.feed_forward.w3.weight                 -> blk.67.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.67.attention_norm.weight                  -> blk.67.attn_norm.weight                  | F16    | [8192]\n",
      "layers.67.ffn_norm.weight                        -> blk.67.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.68.attention.wq.weight                    -> blk.68.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wk.weight                    -> blk.68.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wv.weight                    -> blk.68.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wo.weight                    -> blk.68.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.68.feed_forward.w1.weight                 -> blk.68.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.68.feed_forward.w2.weight                 -> blk.68.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.68.feed_forward.w3.weight                 -> blk.68.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.68.attention_norm.weight                  -> blk.68.attn_norm.weight                  | F16    | [8192]\n",
      "layers.68.ffn_norm.weight                        -> blk.68.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.69.attention.wq.weight                    -> blk.69.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wk.weight                    -> blk.69.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wv.weight                    -> blk.69.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wo.weight                    -> blk.69.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.69.feed_forward.w1.weight                 -> blk.69.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.69.feed_forward.w2.weight                 -> blk.69.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.69.feed_forward.w3.weight                 -> blk.69.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.69.attention_norm.weight                  -> blk.69.attn_norm.weight                  | F16    | [8192]\n",
      "layers.69.ffn_norm.weight                        -> blk.69.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.70.attention.wq.weight                    -> blk.70.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wk.weight                    -> blk.70.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wv.weight                    -> blk.70.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wo.weight                    -> blk.70.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.70.feed_forward.w1.weight                 -> blk.70.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.70.feed_forward.w2.weight                 -> blk.70.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.70.feed_forward.w3.weight                 -> blk.70.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.70.attention_norm.weight                  -> blk.70.attn_norm.weight                  | F16    | [8192]\n",
      "layers.70.ffn_norm.weight                        -> blk.70.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.71.attention.wq.weight                    -> blk.71.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wk.weight                    -> blk.71.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wv.weight                    -> blk.71.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wo.weight                    -> blk.71.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.71.feed_forward.w1.weight                 -> blk.71.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.71.feed_forward.w2.weight                 -> blk.71.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.71.feed_forward.w3.weight                 -> blk.71.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.71.attention_norm.weight                  -> blk.71.attn_norm.weight                  | F16    | [8192]\n",
      "layers.71.ffn_norm.weight                        -> blk.71.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.72.attention.wq.weight                    -> blk.72.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wk.weight                    -> blk.72.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wv.weight                    -> blk.72.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wo.weight                    -> blk.72.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.72.feed_forward.w1.weight                 -> blk.72.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.72.feed_forward.w2.weight                 -> blk.72.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.72.feed_forward.w3.weight                 -> blk.72.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.72.attention_norm.weight                  -> blk.72.attn_norm.weight                  | F16    | [8192]\n",
      "layers.72.ffn_norm.weight                        -> blk.72.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.73.attention.wq.weight                    -> blk.73.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wk.weight                    -> blk.73.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wv.weight                    -> blk.73.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wo.weight                    -> blk.73.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.73.feed_forward.w1.weight                 -> blk.73.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.73.feed_forward.w2.weight                 -> blk.73.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.73.feed_forward.w3.weight                 -> blk.73.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.73.attention_norm.weight                  -> blk.73.attn_norm.weight                  | F16    | [8192]\n",
      "layers.73.ffn_norm.weight                        -> blk.73.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.74.attention.wq.weight                    -> blk.74.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wk.weight                    -> blk.74.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wv.weight                    -> blk.74.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wo.weight                    -> blk.74.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.74.feed_forward.w1.weight                 -> blk.74.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.74.feed_forward.w2.weight                 -> blk.74.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.74.feed_forward.w3.weight                 -> blk.74.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.74.attention_norm.weight                  -> blk.74.attn_norm.weight                  | F16    | [8192]\n",
      "layers.74.ffn_norm.weight                        -> blk.74.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.75.attention.wq.weight                    -> blk.75.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wk.weight                    -> blk.75.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wv.weight                    -> blk.75.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wo.weight                    -> blk.75.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.75.feed_forward.w1.weight                 -> blk.75.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.75.feed_forward.w2.weight                 -> blk.75.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.75.feed_forward.w3.weight                 -> blk.75.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.75.attention_norm.weight                  -> blk.75.attn_norm.weight                  | F16    | [8192]\n",
      "layers.75.ffn_norm.weight                        -> blk.75.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.76.attention.wq.weight                    -> blk.76.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wk.weight                    -> blk.76.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wv.weight                    -> blk.76.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wo.weight                    -> blk.76.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.76.feed_forward.w1.weight                 -> blk.76.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.76.feed_forward.w2.weight                 -> blk.76.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.76.feed_forward.w3.weight                 -> blk.76.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.76.attention_norm.weight                  -> blk.76.attn_norm.weight                  | F16    | [8192]\n",
      "layers.76.ffn_norm.weight                        -> blk.76.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.77.attention.wq.weight                    -> blk.77.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wk.weight                    -> blk.77.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wv.weight                    -> blk.77.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wo.weight                    -> blk.77.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.77.feed_forward.w1.weight                 -> blk.77.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.77.feed_forward.w2.weight                 -> blk.77.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.77.feed_forward.w3.weight                 -> blk.77.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.77.attention_norm.weight                  -> blk.77.attn_norm.weight                  | F16    | [8192]\n",
      "layers.77.ffn_norm.weight                        -> blk.77.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.78.attention.wq.weight                    -> blk.78.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wk.weight                    -> blk.78.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wv.weight                    -> blk.78.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wo.weight                    -> blk.78.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.78.feed_forward.w1.weight                 -> blk.78.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.78.feed_forward.w2.weight                 -> blk.78.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.78.feed_forward.w3.weight                 -> blk.78.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.78.attention_norm.weight                  -> blk.78.attn_norm.weight                  | F16    | [8192]\n",
      "layers.78.ffn_norm.weight                        -> blk.78.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.79.attention.wq.weight                    -> blk.79.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wk.weight                    -> blk.79.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wv.weight                    -> blk.79.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wo.weight                    -> blk.79.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.79.feed_forward.w1.weight                 -> blk.79.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.79.feed_forward.w2.weight                 -> blk.79.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.79.feed_forward.w3.weight                 -> blk.79.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.79.attention_norm.weight                  -> blk.79.attn_norm.weight                  | F16    | [8192]\n",
      "layers.79.ffn_norm.weight                        -> blk.79.ffn_norm.weight                   | F16    | [8192]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/65B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/723] Writing tensor token_embd.weight                      | size  32000 x   8192  | type F16  | T+   1\n",
      "[  2/723] Writing tensor output_norm.weight                     | size   8192           | type F32  | T+   2\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type F16  | T+   2\n",
      "[  4/723] Writing tensor blk.0.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[  5/723] Writing tensor blk.0.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[  6/723] Writing tensor blk.0.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[  7/723] Writing tensor blk.0.attn_output.weight               | size   8192 x   8192  | type F16  | T+   2\n",
      "[  8/723] Writing tensor blk.0.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   2\n",
      "[  9/723] Writing tensor blk.0.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   3\n",
      "[ 10/723] Writing tensor blk.0.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   3\n",
      "[ 11/723] Writing tensor blk.0.attn_norm.weight                 | size   8192           | type F32  | T+   3\n",
      "[ 12/723] Writing tensor blk.0.ffn_norm.weight                  | size   8192           | type F32  | T+   3\n",
      "[ 13/723] Writing tensor blk.1.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 14/723] Writing tensor blk.1.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 15/723] Writing tensor blk.1.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 16/723] Writing tensor blk.1.attn_output.weight               | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 17/723] Writing tensor blk.1.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   4\n",
      "[ 18/723] Writing tensor blk.1.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   4\n",
      "[ 19/723] Writing tensor blk.1.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   4\n",
      "[ 20/723] Writing tensor blk.1.attn_norm.weight                 | size   8192           | type F32  | T+   5\n",
      "[ 21/723] Writing tensor blk.1.ffn_norm.weight                  | size   8192           | type F32  | T+   5\n",
      "[ 22/723] Writing tensor blk.2.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 23/723] Writing tensor blk.2.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 24/723] Writing tensor blk.2.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 25/723] Writing tensor blk.2.attn_output.weight               | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 26/723] Writing tensor blk.2.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   5\n",
      "[ 27/723] Writing tensor blk.2.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   6\n",
      "[ 28/723] Writing tensor blk.2.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   6\n",
      "[ 29/723] Writing tensor blk.2.attn_norm.weight                 | size   8192           | type F32  | T+   6\n",
      "[ 30/723] Writing tensor blk.2.ffn_norm.weight                  | size   8192           | type F32  | T+   6\n",
      "[ 31/723] Writing tensor blk.3.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 32/723] Writing tensor blk.3.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 33/723] Writing tensor blk.3.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 34/723] Writing tensor blk.3.attn_output.weight               | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 35/723] Writing tensor blk.3.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   7\n",
      "[ 36/723] Writing tensor blk.3.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   7\n",
      "[ 37/723] Writing tensor blk.3.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   8\n",
      "[ 38/723] Writing tensor blk.3.attn_norm.weight                 | size   8192           | type F32  | T+   8\n",
      "[ 39/723] Writing tensor blk.3.ffn_norm.weight                  | size   8192           | type F32  | T+   8\n",
      "[ 40/723] Writing tensor blk.4.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 41/723] Writing tensor blk.4.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 42/723] Writing tensor blk.4.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 43/723] Writing tensor blk.4.attn_output.weight               | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 44/723] Writing tensor blk.4.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   9\n",
      "[ 45/723] Writing tensor blk.4.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   9\n",
      "[ 46/723] Writing tensor blk.4.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   9\n",
      "[ 47/723] Writing tensor blk.4.attn_norm.weight                 | size   8192           | type F32  | T+   9\n",
      "[ 48/723] Writing tensor blk.4.ffn_norm.weight                  | size   8192           | type F32  | T+   9\n",
      "[ 49/723] Writing tensor blk.5.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 50/723] Writing tensor blk.5.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 51/723] Writing tensor blk.5.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 52/723] Writing tensor blk.5.attn_output.weight               | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 53/723] Writing tensor blk.5.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  10\n",
      "[ 54/723] Writing tensor blk.5.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  11\n",
      "[ 55/723] Writing tensor blk.5.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  11\n",
      "[ 56/723] Writing tensor blk.5.attn_norm.weight                 | size   8192           | type F32  | T+  11\n",
      "[ 57/723] Writing tensor blk.5.ffn_norm.weight                  | size   8192           | type F32  | T+  11\n",
      "[ 58/723] Writing tensor blk.6.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 59/723] Writing tensor blk.6.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 60/723] Writing tensor blk.6.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 61/723] Writing tensor blk.6.attn_output.weight               | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 62/723] Writing tensor blk.6.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 63/723] Writing tensor blk.6.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  12\n",
      "[ 64/723] Writing tensor blk.6.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 65/723] Writing tensor blk.6.attn_norm.weight                 | size   8192           | type F32  | T+  13\n",
      "[ 66/723] Writing tensor blk.6.ffn_norm.weight                  | size   8192           | type F32  | T+  13\n",
      "[ 67/723] Writing tensor blk.7.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 68/723] Writing tensor blk.7.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 69/723] Writing tensor blk.7.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 70/723] Writing tensor blk.7.attn_output.weight               | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 71/723] Writing tensor blk.7.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  14\n",
      "[ 72/723] Writing tensor blk.7.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  14\n",
      "[ 73/723] Writing tensor blk.7.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  14\n",
      "[ 74/723] Writing tensor blk.7.attn_norm.weight                 | size   8192           | type F32  | T+  14\n",
      "[ 75/723] Writing tensor blk.7.ffn_norm.weight                  | size   8192           | type F32  | T+  14\n",
      "[ 76/723] Writing tensor blk.8.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 77/723] Writing tensor blk.8.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 78/723] Writing tensor blk.8.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 79/723] Writing tensor blk.8.attn_output.weight               | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 80/723] Writing tensor blk.8.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  15\n",
      "[ 81/723] Writing tensor blk.8.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  15\n",
      "[ 82/723] Writing tensor blk.8.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  16\n",
      "[ 83/723] Writing tensor blk.8.attn_norm.weight                 | size   8192           | type F32  | T+  16\n",
      "[ 84/723] Writing tensor blk.8.ffn_norm.weight                  | size   8192           | type F32  | T+  16\n",
      "[ 85/723] Writing tensor blk.9.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  16\n",
      "[ 86/723] Writing tensor blk.9.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  16\n",
      "[ 87/723] Writing tensor blk.9.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  16\n",
      "[ 88/723] Writing tensor blk.9.attn_output.weight               | size   8192 x   8192  | type F16  | T+  16\n",
      "[ 89/723] Writing tensor blk.9.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  17\n",
      "[ 90/723] Writing tensor blk.9.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  17\n",
      "[ 91/723] Writing tensor blk.9.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  17\n",
      "[ 92/723] Writing tensor blk.9.attn_norm.weight                 | size   8192           | type F32  | T+  17\n",
      "[ 93/723] Writing tensor blk.9.ffn_norm.weight                  | size   8192           | type F32  | T+  17\n",
      "[ 94/723] Writing tensor blk.10.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 95/723] Writing tensor blk.10.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[ 96/723] Writing tensor blk.10.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[ 97/723] Writing tensor blk.10.attn_output.weight              | size   8192 x   8192  | type F16  | T+  18\n",
      "[ 98/723] Writing tensor blk.10.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  18\n",
      "[ 99/723] Writing tensor blk.10.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  19\n",
      "[100/723] Writing tensor blk.10.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  19\n",
      "[101/723] Writing tensor blk.10.attn_norm.weight                | size   8192           | type F32  | T+  19\n",
      "[102/723] Writing tensor blk.10.ffn_norm.weight                 | size   8192           | type F32  | T+  19\n",
      "[103/723] Writing tensor blk.11.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[104/723] Writing tensor blk.11.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[105/723] Writing tensor blk.11.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[106/723] Writing tensor blk.11.attn_output.weight              | size   8192 x   8192  | type F16  | T+  19\n",
      "[107/723] Writing tensor blk.11.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  20\n",
      "[108/723] Writing tensor blk.11.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  20\n",
      "[109/723] Writing tensor blk.11.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  21\n",
      "[110/723] Writing tensor blk.11.attn_norm.weight                | size   8192           | type F32  | T+  21\n",
      "[111/723] Writing tensor blk.11.ffn_norm.weight                 | size   8192           | type F32  | T+  21\n",
      "[112/723] Writing tensor blk.12.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[113/723] Writing tensor blk.12.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[114/723] Writing tensor blk.12.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[115/723] Writing tensor blk.12.attn_output.weight              | size   8192 x   8192  | type F16  | T+  21\n",
      "[116/723] Writing tensor blk.12.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  21\n",
      "[117/723] Writing tensor blk.12.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  22\n",
      "[118/723] Writing tensor blk.12.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  22\n",
      "[119/723] Writing tensor blk.12.attn_norm.weight                | size   8192           | type F32  | T+  22\n",
      "[120/723] Writing tensor blk.12.ffn_norm.weight                 | size   8192           | type F32  | T+  22\n",
      "[121/723] Writing tensor blk.13.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  22\n",
      "[122/723] Writing tensor blk.13.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  22\n",
      "[123/723] Writing tensor blk.13.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  22\n",
      "[124/723] Writing tensor blk.13.attn_output.weight              | size   8192 x   8192  | type F16  | T+  22\n",
      "[125/723] Writing tensor blk.13.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  23\n",
      "[126/723] Writing tensor blk.13.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  23\n",
      "[127/723] Writing tensor blk.13.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  23\n",
      "[128/723] Writing tensor blk.13.attn_norm.weight                | size   8192           | type F32  | T+  24\n",
      "[129/723] Writing tensor blk.13.ffn_norm.weight                 | size   8192           | type F32  | T+  24\n",
      "[130/723] Writing tensor blk.14.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[131/723] Writing tensor blk.14.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[132/723] Writing tensor blk.14.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[133/723] Writing tensor blk.14.attn_output.weight              | size   8192 x   8192  | type F16  | T+  24\n",
      "[134/723] Writing tensor blk.14.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  25\n",
      "[135/723] Writing tensor blk.14.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  25\n",
      "[136/723] Writing tensor blk.14.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  25\n",
      "[137/723] Writing tensor blk.14.attn_norm.weight                | size   8192           | type F32  | T+  25\n",
      "[138/723] Writing tensor blk.14.ffn_norm.weight                 | size   8192           | type F32  | T+  25\n",
      "[139/723] Writing tensor blk.15.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[140/723] Writing tensor blk.15.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[141/723] Writing tensor blk.15.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[142/723] Writing tensor blk.15.attn_output.weight              | size   8192 x   8192  | type F16  | T+  25\n",
      "[143/723] Writing tensor blk.15.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  26\n",
      "[144/723] Writing tensor blk.15.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  26\n",
      "[145/723] Writing tensor blk.15.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  27\n",
      "[146/723] Writing tensor blk.15.attn_norm.weight                | size   8192           | type F32  | T+  27\n",
      "[147/723] Writing tensor blk.15.ffn_norm.weight                 | size   8192           | type F32  | T+  27\n",
      "[148/723] Writing tensor blk.16.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[149/723] Writing tensor blk.16.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[150/723] Writing tensor blk.16.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[151/723] Writing tensor blk.16.attn_output.weight              | size   8192 x   8192  | type F16  | T+  27\n",
      "[152/723] Writing tensor blk.16.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  27\n",
      "[153/723] Writing tensor blk.16.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  28\n",
      "[154/723] Writing tensor blk.16.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  28\n",
      "[155/723] Writing tensor blk.16.attn_norm.weight                | size   8192           | type F32  | T+  28\n",
      "[156/723] Writing tensor blk.16.ffn_norm.weight                 | size   8192           | type F32  | T+  28\n",
      "[157/723] Writing tensor blk.17.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[158/723] Writing tensor blk.17.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[159/723] Writing tensor blk.17.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[160/723] Writing tensor blk.17.attn_output.weight              | size   8192 x   8192  | type F16  | T+  28\n",
      "[161/723] Writing tensor blk.17.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  29\n",
      "[162/723] Writing tensor blk.17.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  29\n",
      "[163/723] Writing tensor blk.17.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  30\n",
      "[164/723] Writing tensor blk.17.attn_norm.weight                | size   8192           | type F32  | T+  30\n",
      "[165/723] Writing tensor blk.17.ffn_norm.weight                 | size   8192           | type F32  | T+  30\n",
      "[166/723] Writing tensor blk.18.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[167/723] Writing tensor blk.18.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[168/723] Writing tensor blk.18.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[169/723] Writing tensor blk.18.attn_output.weight              | size   8192 x   8192  | type F16  | T+  30\n",
      "[170/723] Writing tensor blk.18.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  31\n",
      "[171/723] Writing tensor blk.18.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  31\n",
      "[172/723] Writing tensor blk.18.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  31\n",
      "[173/723] Writing tensor blk.18.attn_norm.weight                | size   8192           | type F32  | T+  31\n",
      "[174/723] Writing tensor blk.18.ffn_norm.weight                 | size   8192           | type F32  | T+  31\n",
      "[175/723] Writing tensor blk.19.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[176/723] Writing tensor blk.19.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[177/723] Writing tensor blk.19.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[178/723] Writing tensor blk.19.attn_output.weight              | size   8192 x   8192  | type F16  | T+  31\n",
      "[179/723] Writing tensor blk.19.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  32\n",
      "[180/723] Writing tensor blk.19.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  32\n",
      "[181/723] Writing tensor blk.19.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  33\n",
      "[182/723] Writing tensor blk.19.attn_norm.weight                | size   8192           | type F32  | T+  33\n",
      "[183/723] Writing tensor blk.19.ffn_norm.weight                 | size   8192           | type F32  | T+  33\n",
      "[184/723] Writing tensor blk.20.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  33\n",
      "[185/723] Writing tensor blk.20.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  33\n",
      "[186/723] Writing tensor blk.20.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  33\n",
      "[187/723] Writing tensor blk.20.attn_output.weight              | size   8192 x   8192  | type F16  | T+  33\n",
      "[188/723] Writing tensor blk.20.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  34\n",
      "[189/723] Writing tensor blk.20.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  34\n",
      "[190/723] Writing tensor blk.20.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  34\n",
      "[191/723] Writing tensor blk.20.attn_norm.weight                | size   8192           | type F32  | T+  34\n",
      "[192/723] Writing tensor blk.20.ffn_norm.weight                 | size   8192           | type F32  | T+  34\n",
      "[193/723] Writing tensor blk.21.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[194/723] Writing tensor blk.21.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[195/723] Writing tensor blk.21.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[196/723] Writing tensor blk.21.attn_output.weight              | size   8192 x   8192  | type F16  | T+  34\n",
      "[197/723] Writing tensor blk.21.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  35\n",
      "[198/723] Writing tensor blk.21.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  35\n",
      "[199/723] Writing tensor blk.21.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  35\n",
      "[200/723] Writing tensor blk.21.attn_norm.weight                | size   8192           | type F32  | T+  36\n",
      "[201/723] Writing tensor blk.21.ffn_norm.weight                 | size   8192           | type F32  | T+  36\n",
      "[202/723] Writing tensor blk.22.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[203/723] Writing tensor blk.22.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[204/723] Writing tensor blk.22.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[205/723] Writing tensor blk.22.attn_output.weight              | size   8192 x   8192  | type F16  | T+  36\n",
      "[206/723] Writing tensor blk.22.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  37\n",
      "[207/723] Writing tensor blk.22.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  37\n",
      "[208/723] Writing tensor blk.22.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  37\n",
      "[209/723] Writing tensor blk.22.attn_norm.weight                | size   8192           | type F32  | T+  37\n",
      "[210/723] Writing tensor blk.22.ffn_norm.weight                 | size   8192           | type F32  | T+  37\n",
      "[211/723] Writing tensor blk.23.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[212/723] Writing tensor blk.23.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[213/723] Writing tensor blk.23.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[214/723] Writing tensor blk.23.attn_output.weight              | size   8192 x   8192  | type F16  | T+  37\n",
      "[215/723] Writing tensor blk.23.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  38\n",
      "[216/723] Writing tensor blk.23.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  38\n",
      "[217/723] Writing tensor blk.23.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  38\n",
      "[218/723] Writing tensor blk.23.attn_norm.weight                | size   8192           | type F32  | T+  39\n",
      "[219/723] Writing tensor blk.23.ffn_norm.weight                 | size   8192           | type F32  | T+  39\n",
      "[220/723] Writing tensor blk.24.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[221/723] Writing tensor blk.24.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[222/723] Writing tensor blk.24.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[223/723] Writing tensor blk.24.attn_output.weight              | size   8192 x   8192  | type F16  | T+  39\n",
      "[224/723] Writing tensor blk.24.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  40\n",
      "[225/723] Writing tensor blk.24.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  40\n",
      "[226/723] Writing tensor blk.24.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  40\n",
      "[227/723] Writing tensor blk.24.attn_norm.weight                | size   8192           | type F32  | T+  40\n",
      "[228/723] Writing tensor blk.24.ffn_norm.weight                 | size   8192           | type F32  | T+  40\n",
      "[229/723] Writing tensor blk.25.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[230/723] Writing tensor blk.25.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[231/723] Writing tensor blk.25.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[232/723] Writing tensor blk.25.attn_output.weight              | size   8192 x   8192  | type F16  | T+  40\n",
      "[233/723] Writing tensor blk.25.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  42\n",
      "[234/723] Writing tensor blk.25.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  42\n",
      "[235/723] Writing tensor blk.25.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  43\n",
      "[236/723] Writing tensor blk.25.attn_norm.weight                | size   8192           | type F32  | T+  43\n",
      "[237/723] Writing tensor blk.25.ffn_norm.weight                 | size   8192           | type F32  | T+  43\n",
      "[238/723] Writing tensor blk.26.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[239/723] Writing tensor blk.26.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[240/723] Writing tensor blk.26.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[241/723] Writing tensor blk.26.attn_output.weight              | size   8192 x   8192  | type F16  | T+  43\n",
      "[242/723] Writing tensor blk.26.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  44\n",
      "[243/723] Writing tensor blk.26.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  44\n",
      "[244/723] Writing tensor blk.26.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  44\n",
      "[245/723] Writing tensor blk.26.attn_norm.weight                | size   8192           | type F32  | T+  44\n",
      "[246/723] Writing tensor blk.26.ffn_norm.weight                 | size   8192           | type F32  | T+  44\n",
      "[247/723] Writing tensor blk.27.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[248/723] Writing tensor blk.27.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[249/723] Writing tensor blk.27.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[250/723] Writing tensor blk.27.attn_output.weight              | size   8192 x   8192  | type F16  | T+  44\n",
      "[251/723] Writing tensor blk.27.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  45\n",
      "[252/723] Writing tensor blk.27.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  45\n",
      "[253/723] Writing tensor blk.27.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  46\n",
      "[254/723] Writing tensor blk.27.attn_norm.weight                | size   8192           | type F32  | T+  46\n",
      "[255/723] Writing tensor blk.27.ffn_norm.weight                 | size   8192           | type F32  | T+  46\n",
      "[256/723] Writing tensor blk.28.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[257/723] Writing tensor blk.28.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[258/723] Writing tensor blk.28.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[259/723] Writing tensor blk.28.attn_output.weight              | size   8192 x   8192  | type F16  | T+  46\n",
      "[260/723] Writing tensor blk.28.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  48\n",
      "[261/723] Writing tensor blk.28.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  49\n",
      "[262/723] Writing tensor blk.28.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  49\n",
      "[263/723] Writing tensor blk.28.attn_norm.weight                | size   8192           | type F32  | T+  49\n",
      "[264/723] Writing tensor blk.28.ffn_norm.weight                 | size   8192           | type F32  | T+  49\n",
      "[265/723] Writing tensor blk.29.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[266/723] Writing tensor blk.29.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[267/723] Writing tensor blk.29.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[268/723] Writing tensor blk.29.attn_output.weight              | size   8192 x   8192  | type F16  | T+  49\n",
      "[269/723] Writing tensor blk.29.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  50\n",
      "[270/723] Writing tensor blk.29.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  50\n",
      "[271/723] Writing tensor blk.29.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  50\n",
      "[272/723] Writing tensor blk.29.attn_norm.weight                | size   8192           | type F32  | T+  50\n",
      "[273/723] Writing tensor blk.29.ffn_norm.weight                 | size   8192           | type F32  | T+  50\n",
      "[274/723] Writing tensor blk.30.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  50\n",
      "[275/723] Writing tensor blk.30.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  50\n",
      "[276/723] Writing tensor blk.30.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[277/723] Writing tensor blk.30.attn_output.weight              | size   8192 x   8192  | type F16  | T+  51\n",
      "[278/723] Writing tensor blk.30.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  51\n",
      "[279/723] Writing tensor blk.30.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  52\n",
      "[280/723] Writing tensor blk.30.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  52\n",
      "[281/723] Writing tensor blk.30.attn_norm.weight                | size   8192           | type F32  | T+  52\n",
      "[282/723] Writing tensor blk.30.ffn_norm.weight                 | size   8192           | type F32  | T+  52\n",
      "[283/723] Writing tensor blk.31.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[284/723] Writing tensor blk.31.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[285/723] Writing tensor blk.31.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[286/723] Writing tensor blk.31.attn_output.weight              | size   8192 x   8192  | type F16  | T+  52\n",
      "[287/723] Writing tensor blk.31.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  54\n",
      "[288/723] Writing tensor blk.31.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  55\n",
      "[289/723] Writing tensor blk.31.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  55\n",
      "[290/723] Writing tensor blk.31.attn_norm.weight                | size   8192           | type F32  | T+  55\n",
      "[291/723] Writing tensor blk.31.ffn_norm.weight                 | size   8192           | type F32  | T+  55\n",
      "[292/723] Writing tensor blk.32.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[293/723] Writing tensor blk.32.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[294/723] Writing tensor blk.32.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[295/723] Writing tensor blk.32.attn_output.weight              | size   8192 x   8192  | type F16  | T+  55\n",
      "[296/723] Writing tensor blk.32.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  56\n",
      "[297/723] Writing tensor blk.32.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  56\n",
      "[298/723] Writing tensor blk.32.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  57\n",
      "[299/723] Writing tensor blk.32.attn_norm.weight                | size   8192           | type F32  | T+  57\n",
      "[300/723] Writing tensor blk.32.ffn_norm.weight                 | size   8192           | type F32  | T+  57\n",
      "[301/723] Writing tensor blk.33.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  57\n",
      "[302/723] Writing tensor blk.33.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  57\n",
      "[303/723] Writing tensor blk.33.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  57\n",
      "[304/723] Writing tensor blk.33.attn_output.weight              | size   8192 x   8192  | type F16  | T+  57\n",
      "[305/723] Writing tensor blk.33.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  58\n",
      "[306/723] Writing tensor blk.33.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  58\n",
      "[307/723] Writing tensor blk.33.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  58\n",
      "[308/723] Writing tensor blk.33.attn_norm.weight                | size   8192           | type F32  | T+  58\n",
      "[309/723] Writing tensor blk.33.ffn_norm.weight                 | size   8192           | type F32  | T+  58\n",
      "[310/723] Writing tensor blk.34.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[311/723] Writing tensor blk.34.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[312/723] Writing tensor blk.34.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[313/723] Writing tensor blk.34.attn_output.weight              | size   8192 x   8192  | type F16  | T+  59\n",
      "[314/723] Writing tensor blk.34.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  61\n",
      "[315/723] Writing tensor blk.34.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  61\n",
      "[316/723] Writing tensor blk.34.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  62\n",
      "[317/723] Writing tensor blk.34.attn_norm.weight                | size   8192           | type F32  | T+  62\n",
      "[318/723] Writing tensor blk.34.ffn_norm.weight                 | size   8192           | type F32  | T+  62\n",
      "[319/723] Writing tensor blk.35.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[320/723] Writing tensor blk.35.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[321/723] Writing tensor blk.35.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[322/723] Writing tensor blk.35.attn_output.weight              | size   8192 x   8192  | type F16  | T+  62\n",
      "[323/723] Writing tensor blk.35.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  63\n",
      "[324/723] Writing tensor blk.35.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  63\n",
      "[325/723] Writing tensor blk.35.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  63\n",
      "[326/723] Writing tensor blk.35.attn_norm.weight                | size   8192           | type F32  | T+  63\n",
      "[327/723] Writing tensor blk.35.ffn_norm.weight                 | size   8192           | type F32  | T+  63\n",
      "[328/723] Writing tensor blk.36.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  63\n",
      "[329/723] Writing tensor blk.36.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  63\n",
      "[330/723] Writing tensor blk.36.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  63\n",
      "[331/723] Writing tensor blk.36.attn_output.weight              | size   8192 x   8192  | type F16  | T+  63\n",
      "[332/723] Writing tensor blk.36.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  64\n",
      "[333/723] Writing tensor blk.36.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  64\n",
      "[334/723] Writing tensor blk.36.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  64\n",
      "[335/723] Writing tensor blk.36.attn_norm.weight                | size   8192           | type F32  | T+  64\n",
      "[336/723] Writing tensor blk.36.ffn_norm.weight                 | size   8192           | type F32  | T+  64\n",
      "[337/723] Writing tensor blk.37.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[338/723] Writing tensor blk.37.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[339/723] Writing tensor blk.37.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[340/723] Writing tensor blk.37.attn_output.weight              | size   8192 x   8192  | type F16  | T+  65\n",
      "[341/723] Writing tensor blk.37.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  67\n",
      "[342/723] Writing tensor blk.37.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  67\n",
      "[343/723] Writing tensor blk.37.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  68\n",
      "[344/723] Writing tensor blk.37.attn_norm.weight                | size   8192           | type F32  | T+  68\n",
      "[345/723] Writing tensor blk.37.ffn_norm.weight                 | size   8192           | type F32  | T+  68\n",
      "[346/723] Writing tensor blk.38.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[347/723] Writing tensor blk.38.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[348/723] Writing tensor blk.38.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[349/723] Writing tensor blk.38.attn_output.weight              | size   8192 x   8192  | type F16  | T+  68\n",
      "[350/723] Writing tensor blk.38.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  68\n",
      "[351/723] Writing tensor blk.38.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  69\n",
      "[352/723] Writing tensor blk.38.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  69\n",
      "[353/723] Writing tensor blk.38.attn_norm.weight                | size   8192           | type F32  | T+  69\n",
      "[354/723] Writing tensor blk.38.ffn_norm.weight                 | size   8192           | type F32  | T+  69\n",
      "[355/723] Writing tensor blk.39.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  69\n",
      "[356/723] Writing tensor blk.39.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  69\n",
      "[357/723] Writing tensor blk.39.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  69\n",
      "[358/723] Writing tensor blk.39.attn_output.weight              | size   8192 x   8192  | type F16  | T+  69\n",
      "[359/723] Writing tensor blk.39.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  70\n",
      "[360/723] Writing tensor blk.39.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  70\n",
      "[361/723] Writing tensor blk.39.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  70\n",
      "[362/723] Writing tensor blk.39.attn_norm.weight                | size   8192           | type F32  | T+  70\n",
      "[363/723] Writing tensor blk.39.ffn_norm.weight                 | size   8192           | type F32  | T+  70\n",
      "[364/723] Writing tensor blk.40.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[365/723] Writing tensor blk.40.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[366/723] Writing tensor blk.40.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[367/723] Writing tensor blk.40.attn_output.weight              | size   8192 x   8192  | type F16  | T+  71\n",
      "[368/723] Writing tensor blk.40.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  73\n",
      "[369/723] Writing tensor blk.40.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  74\n",
      "[370/723] Writing tensor blk.40.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  74\n",
      "[371/723] Writing tensor blk.40.attn_norm.weight                | size   8192           | type F32  | T+  74\n",
      "[372/723] Writing tensor blk.40.ffn_norm.weight                 | size   8192           | type F32  | T+  74\n",
      "[373/723] Writing tensor blk.41.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[374/723] Writing tensor blk.41.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[375/723] Writing tensor blk.41.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[376/723] Writing tensor blk.41.attn_output.weight              | size   8192 x   8192  | type F16  | T+  74\n",
      "[377/723] Writing tensor blk.41.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  75\n",
      "[378/723] Writing tensor blk.41.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  75\n",
      "[379/723] Writing tensor blk.41.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  75\n",
      "[380/723] Writing tensor blk.41.attn_norm.weight                | size   8192           | type F32  | T+  75\n",
      "[381/723] Writing tensor blk.41.ffn_norm.weight                 | size   8192           | type F32  | T+  75\n",
      "[382/723] Writing tensor blk.42.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[383/723] Writing tensor blk.42.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[384/723] Writing tensor blk.42.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[385/723] Writing tensor blk.42.attn_output.weight              | size   8192 x   8192  | type F16  | T+  76\n",
      "[386/723] Writing tensor blk.42.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  76\n",
      "[387/723] Writing tensor blk.42.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  77\n",
      "[388/723] Writing tensor blk.42.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  77\n",
      "[389/723] Writing tensor blk.42.attn_norm.weight                | size   8192           | type F32  | T+  77\n",
      "[390/723] Writing tensor blk.42.ffn_norm.weight                 | size   8192           | type F32  | T+  77\n",
      "[391/723] Writing tensor blk.43.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[392/723] Writing tensor blk.43.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[393/723] Writing tensor blk.43.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[394/723] Writing tensor blk.43.attn_output.weight              | size   8192 x   8192  | type F16  | T+  77\n",
      "[395/723] Writing tensor blk.43.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  79\n",
      "[396/723] Writing tensor blk.43.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  80\n",
      "[397/723] Writing tensor blk.43.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  80\n",
      "[398/723] Writing tensor blk.43.attn_norm.weight                | size   8192           | type F32  | T+  80\n",
      "[399/723] Writing tensor blk.43.ffn_norm.weight                 | size   8192           | type F32  | T+  80\n",
      "[400/723] Writing tensor blk.44.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[401/723] Writing tensor blk.44.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[402/723] Writing tensor blk.44.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[403/723] Writing tensor blk.44.attn_output.weight              | size   8192 x   8192  | type F16  | T+  80\n",
      "[404/723] Writing tensor blk.44.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  81\n",
      "[405/723] Writing tensor blk.44.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  81\n",
      "[406/723] Writing tensor blk.44.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  82\n",
      "[407/723] Writing tensor blk.44.attn_norm.weight                | size   8192           | type F32  | T+  82\n",
      "[408/723] Writing tensor blk.44.ffn_norm.weight                 | size   8192           | type F32  | T+  82\n",
      "[409/723] Writing tensor blk.45.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[410/723] Writing tensor blk.45.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[411/723] Writing tensor blk.45.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[412/723] Writing tensor blk.45.attn_output.weight              | size   8192 x   8192  | type F16  | T+  82\n",
      "[413/723] Writing tensor blk.45.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  83\n",
      "[414/723] Writing tensor blk.45.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  83\n",
      "[415/723] Writing tensor blk.45.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  83\n",
      "[416/723] Writing tensor blk.45.attn_norm.weight                | size   8192           | type F32  | T+  83\n",
      "[417/723] Writing tensor blk.45.ffn_norm.weight                 | size   8192           | type F32  | T+  83\n",
      "[418/723] Writing tensor blk.46.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[419/723] Writing tensor blk.46.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[420/723] Writing tensor blk.46.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[421/723] Writing tensor blk.46.attn_output.weight              | size   8192 x   8192  | type F16  | T+  84\n",
      "[422/723] Writing tensor blk.46.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  86\n",
      "[423/723] Writing tensor blk.46.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  86\n",
      "[424/723] Writing tensor blk.46.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  87\n",
      "[425/723] Writing tensor blk.46.attn_norm.weight                | size   8192           | type F32  | T+  87\n",
      "[426/723] Writing tensor blk.46.ffn_norm.weight                 | size   8192           | type F32  | T+  87\n",
      "[427/723] Writing tensor blk.47.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[428/723] Writing tensor blk.47.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[429/723] Writing tensor blk.47.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[430/723] Writing tensor blk.47.attn_output.weight              | size   8192 x   8192  | type F16  | T+  87\n",
      "[431/723] Writing tensor blk.47.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  88\n",
      "[432/723] Writing tensor blk.47.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  88\n",
      "[433/723] Writing tensor blk.47.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  88\n",
      "[434/723] Writing tensor blk.47.attn_norm.weight                | size   8192           | type F32  | T+  88\n",
      "[435/723] Writing tensor blk.47.ffn_norm.weight                 | size   8192           | type F32  | T+  88\n",
      "[436/723] Writing tensor blk.48.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[437/723] Writing tensor blk.48.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[438/723] Writing tensor blk.48.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[439/723] Writing tensor blk.48.attn_output.weight              | size   8192 x   8192  | type F16  | T+  88\n",
      "[440/723] Writing tensor blk.48.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  89\n",
      "[441/723] Writing tensor blk.48.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  89\n",
      "[442/723] Writing tensor blk.48.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  89\n",
      "[443/723] Writing tensor blk.48.attn_norm.weight                | size   8192           | type F32  | T+  89\n",
      "[444/723] Writing tensor blk.48.ffn_norm.weight                 | size   8192           | type F32  | T+  89\n",
      "[445/723] Writing tensor blk.49.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[446/723] Writing tensor blk.49.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[447/723] Writing tensor blk.49.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[448/723] Writing tensor blk.49.attn_output.weight              | size   8192 x   8192  | type F16  | T+  90\n",
      "[449/723] Writing tensor blk.49.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  92\n",
      "[450/723] Writing tensor blk.49.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  93\n",
      "[451/723] Writing tensor blk.49.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  93\n",
      "[452/723] Writing tensor blk.49.attn_norm.weight                | size   8192           | type F32  | T+  93\n",
      "[453/723] Writing tensor blk.49.ffn_norm.weight                 | size   8192           | type F32  | T+  93\n",
      "[454/723] Writing tensor blk.50.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[455/723] Writing tensor blk.50.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[456/723] Writing tensor blk.50.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[457/723] Writing tensor blk.50.attn_output.weight              | size   8192 x   8192  | type F16  | T+  93\n",
      "[458/723] Writing tensor blk.50.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  94\n",
      "[459/723] Writing tensor blk.50.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  94\n",
      "[460/723] Writing tensor blk.50.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  94\n",
      "[461/723] Writing tensor blk.50.attn_norm.weight                | size   8192           | type F32  | T+  94\n",
      "[462/723] Writing tensor blk.50.ffn_norm.weight                 | size   8192           | type F32  | T+  94\n",
      "[463/723] Writing tensor blk.51.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[464/723] Writing tensor blk.51.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[465/723] Writing tensor blk.51.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[466/723] Writing tensor blk.51.attn_output.weight              | size   8192 x   8192  | type F16  | T+  95\n",
      "[467/723] Writing tensor blk.51.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  95\n",
      "[468/723] Writing tensor blk.51.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  96\n",
      "[469/723] Writing tensor blk.51.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  96\n",
      "[470/723] Writing tensor blk.51.attn_norm.weight                | size   8192           | type F32  | T+  96\n",
      "[471/723] Writing tensor blk.51.ffn_norm.weight                 | size   8192           | type F32  | T+  96\n",
      "[472/723] Writing tensor blk.52.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[473/723] Writing tensor blk.52.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[474/723] Writing tensor blk.52.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[475/723] Writing tensor blk.52.attn_output.weight              | size   8192 x   8192  | type F16  | T+  96\n",
      "[476/723] Writing tensor blk.52.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  98\n",
      "[477/723] Writing tensor blk.52.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  99\n",
      "[478/723] Writing tensor blk.52.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  99\n",
      "[479/723] Writing tensor blk.52.attn_norm.weight                | size   8192           | type F32  | T+ 100\n",
      "[480/723] Writing tensor blk.52.ffn_norm.weight                 | size   8192           | type F32  | T+ 100\n",
      "[481/723] Writing tensor blk.53.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[482/723] Writing tensor blk.53.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[483/723] Writing tensor blk.53.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[484/723] Writing tensor blk.53.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 100\n",
      "[485/723] Writing tensor blk.53.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 101\n",
      "[486/723] Writing tensor blk.53.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 101\n",
      "[487/723] Writing tensor blk.53.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 101\n",
      "[488/723] Writing tensor blk.53.attn_norm.weight                | size   8192           | type F32  | T+ 101\n",
      "[489/723] Writing tensor blk.53.ffn_norm.weight                 | size   8192           | type F32  | T+ 101\n",
      "[490/723] Writing tensor blk.54.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[491/723] Writing tensor blk.54.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[492/723] Writing tensor blk.54.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[493/723] Writing tensor blk.54.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 101\n",
      "[494/723] Writing tensor blk.54.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 102\n",
      "[495/723] Writing tensor blk.54.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 102\n",
      "[496/723] Writing tensor blk.54.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 102\n",
      "[497/723] Writing tensor blk.54.attn_norm.weight                | size   8192           | type F32  | T+ 102\n",
      "[498/723] Writing tensor blk.54.ffn_norm.weight                 | size   8192           | type F32  | T+ 102\n",
      "[499/723] Writing tensor blk.55.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[500/723] Writing tensor blk.55.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[501/723] Writing tensor blk.55.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[502/723] Writing tensor blk.55.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 103\n",
      "[503/723] Writing tensor blk.55.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 105\n",
      "[504/723] Writing tensor blk.55.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 105\n",
      "[505/723] Writing tensor blk.55.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 106\n",
      "[506/723] Writing tensor blk.55.attn_norm.weight                | size   8192           | type F32  | T+ 106\n",
      "[507/723] Writing tensor blk.55.ffn_norm.weight                 | size   8192           | type F32  | T+ 106\n",
      "[508/723] Writing tensor blk.56.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[509/723] Writing tensor blk.56.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[510/723] Writing tensor blk.56.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[511/723] Writing tensor blk.56.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 106\n",
      "[512/723] Writing tensor blk.56.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 106\n",
      "[513/723] Writing tensor blk.56.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 107\n",
      "[514/723] Writing tensor blk.56.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 107\n",
      "[515/723] Writing tensor blk.56.attn_norm.weight                | size   8192           | type F32  | T+ 107\n",
      "[516/723] Writing tensor blk.56.ffn_norm.weight                 | size   8192           | type F32  | T+ 107\n",
      "[517/723] Writing tensor blk.57.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 107\n",
      "[518/723] Writing tensor blk.57.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 107\n",
      "[519/723] Writing tensor blk.57.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 107\n",
      "[520/723] Writing tensor blk.57.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 107\n",
      "[521/723] Writing tensor blk.57.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 108\n",
      "[522/723] Writing tensor blk.57.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 108\n",
      "[523/723] Writing tensor blk.57.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 109\n",
      "[524/723] Writing tensor blk.57.attn_norm.weight                | size   8192           | type F32  | T+ 109\n",
      "[525/723] Writing tensor blk.57.ffn_norm.weight                 | size   8192           | type F32  | T+ 109\n",
      "[526/723] Writing tensor blk.58.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[527/723] Writing tensor blk.58.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[528/723] Writing tensor blk.58.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[529/723] Writing tensor blk.58.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 109\n",
      "[530/723] Writing tensor blk.58.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 111\n",
      "[531/723] Writing tensor blk.58.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 112\n",
      "[532/723] Writing tensor blk.58.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 112\n",
      "[533/723] Writing tensor blk.58.attn_norm.weight                | size   8192           | type F32  | T+ 112\n",
      "[534/723] Writing tensor blk.58.ffn_norm.weight                 | size   8192           | type F32  | T+ 112\n",
      "[535/723] Writing tensor blk.59.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[536/723] Writing tensor blk.59.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[537/723] Writing tensor blk.59.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[538/723] Writing tensor blk.59.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 112\n",
      "[539/723] Writing tensor blk.59.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 113\n",
      "[540/723] Writing tensor blk.59.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 113\n",
      "[541/723] Writing tensor blk.59.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 114\n",
      "[542/723] Writing tensor blk.59.attn_norm.weight                | size   8192           | type F32  | T+ 114\n",
      "[543/723] Writing tensor blk.59.ffn_norm.weight                 | size   8192           | type F32  | T+ 114\n",
      "[544/723] Writing tensor blk.60.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[545/723] Writing tensor blk.60.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[546/723] Writing tensor blk.60.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[547/723] Writing tensor blk.60.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 114\n",
      "[548/723] Writing tensor blk.60.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 115\n",
      "[549/723] Writing tensor blk.60.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 115\n",
      "[550/723] Writing tensor blk.60.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 115\n",
      "[551/723] Writing tensor blk.60.attn_norm.weight                | size   8192           | type F32  | T+ 115\n",
      "[552/723] Writing tensor blk.60.ffn_norm.weight                 | size   8192           | type F32  | T+ 115\n",
      "[553/723] Writing tensor blk.61.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[554/723] Writing tensor blk.61.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[555/723] Writing tensor blk.61.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[556/723] Writing tensor blk.61.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 116\n",
      "[557/723] Writing tensor blk.61.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 118\n",
      "[558/723] Writing tensor blk.61.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 118\n",
      "[559/723] Writing tensor blk.61.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 118\n",
      "[560/723] Writing tensor blk.61.attn_norm.weight                | size   8192           | type F32  | T+ 119\n",
      "[561/723] Writing tensor blk.61.ffn_norm.weight                 | size   8192           | type F32  | T+ 119\n",
      "[562/723] Writing tensor blk.62.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[563/723] Writing tensor blk.62.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[564/723] Writing tensor blk.62.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[565/723] Writing tensor blk.62.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 119\n",
      "[566/723] Writing tensor blk.62.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 120\n",
      "[567/723] Writing tensor blk.62.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 120\n",
      "[568/723] Writing tensor blk.62.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 120\n",
      "[569/723] Writing tensor blk.62.attn_norm.weight                | size   8192           | type F32  | T+ 120\n",
      "[570/723] Writing tensor blk.62.ffn_norm.weight                 | size   8192           | type F32  | T+ 120\n",
      "[571/723] Writing tensor blk.63.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[572/723] Writing tensor blk.63.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[573/723] Writing tensor blk.63.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[574/723] Writing tensor blk.63.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 120\n",
      "[575/723] Writing tensor blk.63.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 121\n",
      "[576/723] Writing tensor blk.63.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 121\n",
      "[577/723] Writing tensor blk.63.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 122\n",
      "[578/723] Writing tensor blk.63.attn_norm.weight                | size   8192           | type F32  | T+ 122\n",
      "[579/723] Writing tensor blk.63.ffn_norm.weight                 | size   8192           | type F32  | T+ 122\n",
      "[580/723] Writing tensor blk.64.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 122\n",
      "[581/723] Writing tensor blk.64.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 122\n",
      "[582/723] Writing tensor blk.64.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 122\n",
      "[583/723] Writing tensor blk.64.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 122\n",
      "[584/723] Writing tensor blk.64.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 124\n",
      "[585/723] Writing tensor blk.64.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 125\n",
      "[586/723] Writing tensor blk.64.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 125\n",
      "[587/723] Writing tensor blk.64.attn_norm.weight                | size   8192           | type F32  | T+ 125\n",
      "[588/723] Writing tensor blk.64.ffn_norm.weight                 | size   8192           | type F32  | T+ 125\n",
      "[589/723] Writing tensor blk.65.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[590/723] Writing tensor blk.65.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[591/723] Writing tensor blk.65.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[592/723] Writing tensor blk.65.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 125\n",
      "[593/723] Writing tensor blk.65.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 126\n",
      "[594/723] Writing tensor blk.65.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 126\n",
      "[595/723] Writing tensor blk.65.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 126\n",
      "[596/723] Writing tensor blk.65.attn_norm.weight                | size   8192           | type F32  | T+ 126\n",
      "[597/723] Writing tensor blk.65.ffn_norm.weight                 | size   8192           | type F32  | T+ 126\n",
      "[598/723] Writing tensor blk.66.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[599/723] Writing tensor blk.66.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[600/723] Writing tensor blk.66.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[601/723] Writing tensor blk.66.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 126\n",
      "[602/723] Writing tensor blk.66.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 127\n",
      "[603/723] Writing tensor blk.66.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 127\n",
      "[604/723] Writing tensor blk.66.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 128\n",
      "[605/723] Writing tensor blk.66.attn_norm.weight                | size   8192           | type F32  | T+ 128\n",
      "[606/723] Writing tensor blk.66.ffn_norm.weight                 | size   8192           | type F32  | T+ 128\n",
      "[607/723] Writing tensor blk.67.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 128\n",
      "[608/723] Writing tensor blk.67.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 128\n",
      "[609/723] Writing tensor blk.67.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 128\n",
      "[610/723] Writing tensor blk.67.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 128\n",
      "[611/723] Writing tensor blk.67.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 130\n",
      "[612/723] Writing tensor blk.67.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 131\n",
      "[613/723] Writing tensor blk.67.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 131\n",
      "[614/723] Writing tensor blk.67.attn_norm.weight                | size   8192           | type F32  | T+ 131\n",
      "[615/723] Writing tensor blk.67.ffn_norm.weight                 | size   8192           | type F32  | T+ 131\n",
      "[616/723] Writing tensor blk.68.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 131\n",
      "[617/723] Writing tensor blk.68.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 131\n",
      "[618/723] Writing tensor blk.68.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 131\n",
      "[619/723] Writing tensor blk.68.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 131\n",
      "[620/723] Writing tensor blk.68.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 132\n",
      "[621/723] Writing tensor blk.68.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 132\n",
      "[622/723] Writing tensor blk.68.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 133\n",
      "[623/723] Writing tensor blk.68.attn_norm.weight                | size   8192           | type F32  | T+ 133\n",
      "[624/723] Writing tensor blk.68.ffn_norm.weight                 | size   8192           | type F32  | T+ 133\n",
      "[625/723] Writing tensor blk.69.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 133\n",
      "[626/723] Writing tensor blk.69.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 133\n",
      "[627/723] Writing tensor blk.69.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 133\n",
      "[628/723] Writing tensor blk.69.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 133\n",
      "[629/723] Writing tensor blk.69.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 134\n",
      "[630/723] Writing tensor blk.69.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 134\n",
      "[631/723] Writing tensor blk.69.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 134\n",
      "[632/723] Writing tensor blk.69.attn_norm.weight                | size   8192           | type F32  | T+ 134\n",
      "[633/723] Writing tensor blk.69.ffn_norm.weight                 | size   8192           | type F32  | T+ 134\n",
      "[634/723] Writing tensor blk.70.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 134\n",
      "[635/723] Writing tensor blk.70.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 134\n",
      "[636/723] Writing tensor blk.70.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 134\n",
      "[637/723] Writing tensor blk.70.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 135\n",
      "[638/723] Writing tensor blk.70.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 136\n",
      "[639/723] Writing tensor blk.70.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 137\n",
      "[640/723] Writing tensor blk.70.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 137\n",
      "[641/723] Writing tensor blk.70.attn_norm.weight                | size   8192           | type F32  | T+ 138\n",
      "[642/723] Writing tensor blk.70.ffn_norm.weight                 | size   8192           | type F32  | T+ 138\n",
      "[643/723] Writing tensor blk.71.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 138\n",
      "[644/723] Writing tensor blk.71.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 138\n",
      "[645/723] Writing tensor blk.71.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 138\n",
      "[646/723] Writing tensor blk.71.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 138\n",
      "[647/723] Writing tensor blk.71.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 139\n",
      "[648/723] Writing tensor blk.71.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 139\n",
      "[649/723] Writing tensor blk.71.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 139\n",
      "[650/723] Writing tensor blk.71.attn_norm.weight                | size   8192           | type F32  | T+ 139\n",
      "[651/723] Writing tensor blk.71.ffn_norm.weight                 | size   8192           | type F32  | T+ 139\n",
      "[652/723] Writing tensor blk.72.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 139\n",
      "[653/723] Writing tensor blk.72.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 139\n",
      "[654/723] Writing tensor blk.72.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 139\n",
      "[655/723] Writing tensor blk.72.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 139\n",
      "[656/723] Writing tensor blk.72.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 140\n",
      "[657/723] Writing tensor blk.72.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 140\n",
      "[658/723] Writing tensor blk.72.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 140\n",
      "[659/723] Writing tensor blk.72.attn_norm.weight                | size   8192           | type F32  | T+ 140\n",
      "[660/723] Writing tensor blk.72.ffn_norm.weight                 | size   8192           | type F32  | T+ 140\n",
      "[661/723] Writing tensor blk.73.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 140\n",
      "[662/723] Writing tensor blk.73.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 140\n",
      "[663/723] Writing tensor blk.73.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 140\n",
      "[664/723] Writing tensor blk.73.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 141\n",
      "[665/723] Writing tensor blk.73.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 143\n",
      "[666/723] Writing tensor blk.73.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 143\n",
      "[667/723] Writing tensor blk.73.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 144\n",
      "[668/723] Writing tensor blk.73.attn_norm.weight                | size   8192           | type F32  | T+ 144\n",
      "[669/723] Writing tensor blk.73.ffn_norm.weight                 | size   8192           | type F32  | T+ 144\n",
      "[670/723] Writing tensor blk.74.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 144\n",
      "[671/723] Writing tensor blk.74.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 144\n",
      "[672/723] Writing tensor blk.74.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 144\n",
      "[673/723] Writing tensor blk.74.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 144\n",
      "[674/723] Writing tensor blk.74.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 144\n",
      "[675/723] Writing tensor blk.74.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 145\n",
      "[676/723] Writing tensor blk.74.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 145\n",
      "[677/723] Writing tensor blk.74.attn_norm.weight                | size   8192           | type F32  | T+ 145\n",
      "[678/723] Writing tensor blk.74.ffn_norm.weight                 | size   8192           | type F32  | T+ 145\n",
      "[679/723] Writing tensor blk.75.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 145\n",
      "[680/723] Writing tensor blk.75.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 145\n",
      "[681/723] Writing tensor blk.75.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 145\n",
      "[682/723] Writing tensor blk.75.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 145\n",
      "[683/723] Writing tensor blk.75.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 147\n",
      "[684/723] Writing tensor blk.75.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 147\n",
      "[685/723] Writing tensor blk.75.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 147\n",
      "[686/723] Writing tensor blk.75.attn_norm.weight                | size   8192           | type F32  | T+ 147\n",
      "[687/723] Writing tensor blk.75.ffn_norm.weight                 | size   8192           | type F32  | T+ 147\n",
      "[688/723] Writing tensor blk.76.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 147\n",
      "[689/723] Writing tensor blk.76.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 147\n",
      "[690/723] Writing tensor blk.76.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 147\n",
      "[691/723] Writing tensor blk.76.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 147\n",
      "[692/723] Writing tensor blk.76.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 149\n",
      "[693/723] Writing tensor blk.76.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 150\n",
      "[694/723] Writing tensor blk.76.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 150\n",
      "[695/723] Writing tensor blk.76.attn_norm.weight                | size   8192           | type F32  | T+ 150\n",
      "[696/723] Writing tensor blk.76.ffn_norm.weight                 | size   8192           | type F32  | T+ 150\n",
      "[697/723] Writing tensor blk.77.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 150\n",
      "[698/723] Writing tensor blk.77.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 150\n",
      "[699/723] Writing tensor blk.77.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 150\n",
      "[700/723] Writing tensor blk.77.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 150\n",
      "[701/723] Writing tensor blk.77.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 151\n",
      "[702/723] Writing tensor blk.77.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 152\n",
      "[703/723] Writing tensor blk.77.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 152\n",
      "[704/723] Writing tensor blk.77.attn_norm.weight                | size   8192           | type F32  | T+ 152\n",
      "[705/723] Writing tensor blk.77.ffn_norm.weight                 | size   8192           | type F32  | T+ 152\n",
      "[706/723] Writing tensor blk.78.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 152\n",
      "[707/723] Writing tensor blk.78.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 152\n",
      "[708/723] Writing tensor blk.78.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 152\n",
      "[709/723] Writing tensor blk.78.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 152\n",
      "[710/723] Writing tensor blk.78.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 153\n",
      "[711/723] Writing tensor blk.78.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 153\n",
      "[712/723] Writing tensor blk.78.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 153\n",
      "[713/723] Writing tensor blk.78.attn_norm.weight                | size   8192           | type F32  | T+ 153\n",
      "[714/723] Writing tensor blk.78.ffn_norm.weight                 | size   8192           | type F32  | T+ 153\n",
      "[715/723] Writing tensor blk.79.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 153\n",
      "[716/723] Writing tensor blk.79.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 153\n",
      "[717/723] Writing tensor blk.79.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 153\n",
      "[718/723] Writing tensor blk.79.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 154\n",
      "[719/723] Writing tensor blk.79.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 155\n",
      "[720/723] Writing tensor blk.79.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 155\n",
      "[721/723] Writing tensor blk.79.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 156\n",
      "[722/723] Writing tensor blk.79.attn_norm.weight                | size   8192           | type F32  | T+ 156\n",
      "[723/723] Writing tensor blk.79.ffn_norm.weight                 | size   8192           | type F32  | T+ 156\n",
      "Wrote models/65B/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/7B/ggml-model-f16.gguf' to './models/7B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 1714336 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
      "[   4/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   9/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  22/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  82/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 109/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 262/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 271/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 280/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 282/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 290/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time =  8438.04 ms\n",
      "main:    total time =  8438.04 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/13B/ggml-model-f16.gguf' to './models/13B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_quantize_internal: meta size = 1718656 bytes\n",
      "[   1/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   312.50 MiB ->    87.89 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   312.50 MiB ->   128.17 MiB | hist: \n",
      "[   4/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  12/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  13/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  21/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  22/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  30/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  31/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  39/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  40/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  48/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  49/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  57/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  58/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  66/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  67/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  75/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  76/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  84/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  85/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  93/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  94/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 102/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 109/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 111/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 120/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 129/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 138/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 147/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 156/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 165/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 174/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 183/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 192/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 201/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 210/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 219/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 228/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 237/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 246/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 255/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 264/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 273/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 282/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 291/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 300/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 309/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 318/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 327/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 334/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 336/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 343/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 345/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 352/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 354/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 361/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 362/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 363/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 16144.36 ms\n",
      "main:    total time = 16144.36 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/30B/ggml-model-f16.gguf' to './models/30B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llama_model_quantize_internal: meta size = 1729408 bytes\n",
      "[   1/ 543]                    token_embd.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   406.25 MiB ->   114.26 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                   output_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   406.25 MiB ->   166.63 MiB | hist: \n",
      "[   4/ 543]                  blk.0.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]                  blk.0.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]                  blk.0.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]             blk.0.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]                blk.0.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 543]                blk.0.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 543]                  blk.0.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 543]               blk.0.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  12/ 543]                blk.0.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  13/ 543]                  blk.1.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]                  blk.1.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]                  blk.1.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]             blk.1.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]                blk.1.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 543]                blk.1.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]                  blk.1.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]               blk.1.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  21/ 543]                blk.1.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  22/ 543]                  blk.2.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]                  blk.2.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]                  blk.2.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]             blk.2.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]                blk.2.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 543]                blk.2.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]                  blk.2.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]               blk.2.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  30/ 543]                blk.2.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  31/ 543]                  blk.3.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]                  blk.3.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]                  blk.3.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]             blk.3.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]                blk.3.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 543]                blk.3.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  37/ 543]                  blk.3.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 543]               blk.3.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  39/ 543]                blk.3.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  40/ 543]                  blk.4.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]                  blk.4.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]                  blk.4.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]             blk.4.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]                blk.4.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 543]                blk.4.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]                  blk.4.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]               blk.4.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  48/ 543]                blk.4.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  49/ 543]                  blk.5.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]                  blk.5.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]                  blk.5.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]             blk.5.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]                blk.5.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 543]                blk.5.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]                  blk.5.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]               blk.5.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  57/ 543]                blk.5.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  58/ 543]                  blk.6.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]                  blk.6.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]                  blk.6.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]             blk.6.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]                blk.6.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 543]                blk.6.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]                  blk.6.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]               blk.6.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  66/ 543]                blk.6.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  67/ 543]                  blk.7.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]                  blk.7.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]                  blk.7.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]             blk.7.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]                blk.7.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 543]                blk.7.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]                  blk.7.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]               blk.7.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  75/ 543]                blk.7.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  76/ 543]                  blk.8.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]                  blk.8.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]                  blk.8.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]             blk.8.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]                blk.8.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 543]                blk.8.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]                  blk.8.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]               blk.8.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  84/ 543]                blk.8.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  85/ 543]                  blk.9.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]                  blk.9.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]                  blk.9.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]             blk.9.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]                blk.9.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 543]                blk.9.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]                  blk.9.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]               blk.9.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  93/ 543]                blk.9.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  94/ 543]                 blk.10.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]                 blk.10.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]                 blk.10.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]            blk.10.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]               blk.10.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 543]               blk.10.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]                 blk.10.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]              blk.10.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 102/ 543]               blk.10.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]                 blk.11.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]                 blk.11.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]                 blk.11.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]            blk.11.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]               blk.11.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 543]               blk.11.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]                 blk.11.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]              blk.11.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 111/ 543]               blk.11.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]                 blk.12.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]                 blk.12.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]                 blk.12.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]            blk.12.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]               blk.12.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 543]               blk.12.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]                 blk.12.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]              blk.12.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 120/ 543]               blk.12.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]                 blk.13.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]                 blk.13.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]                 blk.13.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]            blk.13.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]               blk.13.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 543]               blk.13.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]                 blk.13.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]              blk.13.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 129/ 543]               blk.13.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]                 blk.14.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]                 blk.14.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]                 blk.14.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]            blk.14.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]               blk.14.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 543]               blk.14.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 543]                 blk.14.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 543]              blk.14.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 138/ 543]               blk.14.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]                 blk.15.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]                 blk.15.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]                 blk.15.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]            blk.15.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]               blk.15.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 543]               blk.15.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]                 blk.15.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]              blk.15.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 147/ 543]               blk.15.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]                 blk.16.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]                 blk.16.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]                 blk.16.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]            blk.16.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]               blk.16.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 543]               blk.16.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]                 blk.16.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]              blk.16.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 156/ 543]               blk.16.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]                 blk.17.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]                 blk.17.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]                 blk.17.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]            blk.17.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]               blk.17.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 543]               blk.17.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]                 blk.17.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]              blk.17.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 165/ 543]               blk.17.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]                 blk.18.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]                 blk.18.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]                 blk.18.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]            blk.18.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]               blk.18.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 543]               blk.18.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]                 blk.18.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]              blk.18.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 174/ 543]               blk.18.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]                 blk.19.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]                 blk.19.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]                 blk.19.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]            blk.19.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]               blk.19.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 543]               blk.19.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 181/ 543]                 blk.19.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 543]              blk.19.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 183/ 543]               blk.19.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]                 blk.20.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]                 blk.20.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]                 blk.20.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]            blk.20.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]               blk.20.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 543]               blk.20.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 190/ 543]                 blk.20.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 543]              blk.20.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 192/ 543]               blk.20.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]                 blk.21.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]                 blk.21.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]                 blk.21.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]            blk.21.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]               blk.21.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 543]               blk.21.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]                 blk.21.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]              blk.21.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 201/ 543]               blk.21.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]                 blk.22.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]                 blk.22.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]                 blk.22.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]            blk.22.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]               blk.22.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 543]               blk.22.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]                 blk.22.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]              blk.22.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 210/ 543]               blk.22.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]                 blk.23.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]                 blk.23.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]                 blk.23.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]            blk.23.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]               blk.23.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 543]               blk.23.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]                 blk.23.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]              blk.23.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 219/ 543]               blk.23.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]                 blk.24.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]                 blk.24.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]                 blk.24.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]            blk.24.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]               blk.24.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 543]               blk.24.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]                 blk.24.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]              blk.24.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 228/ 543]               blk.24.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]                 blk.25.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]                 blk.25.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]                 blk.25.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]            blk.25.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]               blk.25.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 543]               blk.25.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]                 blk.25.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]              blk.25.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 237/ 543]               blk.25.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]                 blk.26.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]                 blk.26.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]                 blk.26.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]            blk.26.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]               blk.26.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 543]               blk.26.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]                 blk.26.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]              blk.26.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 246/ 543]               blk.26.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]                 blk.27.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]                 blk.27.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]                 blk.27.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]            blk.27.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]               blk.27.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 543]               blk.27.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]                 blk.27.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]              blk.27.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 255/ 543]               blk.27.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]                 blk.28.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]                 blk.28.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]                 blk.28.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]            blk.28.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]               blk.28.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 543]               blk.28.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]                 blk.28.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]              blk.28.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 264/ 543]               blk.28.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]                 blk.29.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]                 blk.29.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]                 blk.29.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]            blk.29.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]               blk.29.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 543]               blk.29.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]                 blk.29.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]              blk.29.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 273/ 543]               blk.29.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]                 blk.30.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]                 blk.30.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]                 blk.30.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]            blk.30.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]               blk.30.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 543]               blk.30.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]                 blk.30.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]              blk.30.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 282/ 543]               blk.30.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]                 blk.31.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]                 blk.31.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]                 blk.31.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]            blk.31.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]               blk.31.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 543]               blk.31.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]                 blk.31.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]              blk.31.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 291/ 543]               blk.31.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]                 blk.32.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]                 blk.32.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]                 blk.32.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]            blk.32.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]               blk.32.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 543]               blk.32.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]                 blk.32.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]              blk.32.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 300/ 543]               blk.32.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]                 blk.33.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]                 blk.33.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]                 blk.33.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]            blk.33.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]               blk.33.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 543]               blk.33.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]                 blk.33.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]              blk.33.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 309/ 543]               blk.33.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]                 blk.34.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]                 blk.34.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]                 blk.34.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]            blk.34.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]               blk.34.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 543]               blk.34.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]                 blk.34.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]              blk.34.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 318/ 543]               blk.34.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]                 blk.35.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]                 blk.35.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]                 blk.35.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]            blk.35.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]               blk.35.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 543]               blk.35.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]                 blk.35.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]              blk.35.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 327/ 543]               blk.35.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]                 blk.36.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]                 blk.36.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]                 blk.36.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]            blk.36.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]               blk.36.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 543]               blk.36.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]                 blk.36.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]              blk.36.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 336/ 543]               blk.36.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]                 blk.37.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]                 blk.37.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]                 blk.37.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]            blk.37.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]               blk.37.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 543]               blk.37.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]                 blk.37.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]              blk.37.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 345/ 543]               blk.37.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]                 blk.38.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]                 blk.38.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]                 blk.38.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]            blk.38.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]               blk.38.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 543]               blk.38.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]                 blk.38.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]              blk.38.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 354/ 543]               blk.38.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]                 blk.39.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]                 blk.39.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]                 blk.39.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]            blk.39.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]               blk.39.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 543]               blk.39.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]                 blk.39.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]              blk.39.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 363/ 543]               blk.39.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]                 blk.40.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]                 blk.40.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]                 blk.40.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]            blk.40.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]               blk.40.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 543]               blk.40.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]                 blk.40.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]              blk.40.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 372/ 543]               blk.40.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]                 blk.41.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]                 blk.41.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]                 blk.41.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]            blk.41.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]               blk.41.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 543]               blk.41.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]                 blk.41.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]              blk.41.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 381/ 543]               blk.41.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]                 blk.42.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]                 blk.42.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]                 blk.42.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]            blk.42.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]               blk.42.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 543]               blk.42.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]                 blk.42.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]              blk.42.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 390/ 543]               blk.42.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]                 blk.43.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]                 blk.43.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]                 blk.43.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]            blk.43.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]               blk.43.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 543]               blk.43.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]                 blk.43.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]              blk.43.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 399/ 543]               blk.43.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]                 blk.44.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]                 blk.44.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]                 blk.44.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]            blk.44.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]               blk.44.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 543]               blk.44.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]                 blk.44.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]              blk.44.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 408/ 543]               blk.44.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]                 blk.45.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]                 blk.45.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]                 blk.45.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]            blk.45.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]               blk.45.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 543]               blk.45.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]                 blk.45.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]              blk.45.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 417/ 543]               blk.45.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]                 blk.46.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]                 blk.46.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]                 blk.46.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]            blk.46.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]               blk.46.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 543]               blk.46.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]                 blk.46.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]              blk.46.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 426/ 543]               blk.46.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]                 blk.47.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]                 blk.47.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]                 blk.47.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]            blk.47.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]               blk.47.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 543]               blk.47.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]                 blk.47.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]              blk.47.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 435/ 543]               blk.47.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]                 blk.48.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]                 blk.48.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]                 blk.48.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]            blk.48.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]               blk.48.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 543]               blk.48.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]                 blk.48.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]              blk.48.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 444/ 543]               blk.48.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]                 blk.49.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]                 blk.49.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]                 blk.49.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]            blk.49.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]               blk.49.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 543]               blk.49.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]                 blk.49.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]              blk.49.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 453/ 543]               blk.49.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]                 blk.50.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]                 blk.50.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]                 blk.50.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]            blk.50.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]               blk.50.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 543]               blk.50.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]                 blk.50.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]              blk.50.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 462/ 543]               blk.50.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]                 blk.51.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]                 blk.51.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]                 blk.51.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]            blk.51.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]               blk.51.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 543]               blk.51.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]                 blk.51.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]              blk.51.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 471/ 543]               blk.51.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]                 blk.52.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]                 blk.52.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]                 blk.52.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]            blk.52.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]               blk.52.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 543]               blk.52.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]                 blk.52.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]              blk.52.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 480/ 543]               blk.52.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]                 blk.53.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]                 blk.53.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]                 blk.53.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]            blk.53.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]               blk.53.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 543]               blk.53.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]                 blk.53.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]              blk.53.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 489/ 543]               blk.53.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]                 blk.54.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]                 blk.54.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]                 blk.54.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]            blk.54.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]               blk.54.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 543]               blk.54.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]                 blk.54.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]              blk.54.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 498/ 543]               blk.54.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]                 blk.55.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]                 blk.55.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]                 blk.55.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]            blk.55.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]               blk.55.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 543]               blk.55.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]                 blk.55.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]              blk.55.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 507/ 543]               blk.55.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]                 blk.56.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]                 blk.56.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]                 blk.56.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]            blk.56.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]               blk.56.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 543]               blk.56.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 514/ 543]                 blk.56.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 543]              blk.56.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 516/ 543]               blk.56.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]                 blk.57.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]                 blk.57.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]                 blk.57.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]            blk.57.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]               blk.57.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 543]               blk.57.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 523/ 543]                 blk.57.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 543]              blk.57.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 525/ 543]               blk.57.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]                 blk.58.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]                 blk.58.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]                 blk.58.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]            blk.58.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]               blk.58.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 543]               blk.58.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 532/ 543]                 blk.58.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 543]              blk.58.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 534/ 543]               blk.58.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]                 blk.59.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]                 blk.59.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]                 blk.59.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]            blk.59.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]               blk.59.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 540/ 543]               blk.59.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 541/ 543]                 blk.59.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 542/ 543]              blk.59.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 543/ 543]               blk.59.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 38620.73 ms\n",
      "main:    total time = 38620.73 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/65B/ggml-model-f16.gguf' to './models/65B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llama_model_quantize_internal: meta size = 1740160 bytes\n",
      "[   1/ 723]                    token_embd.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   500.00 MiB ->   140.62 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                   output_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   500.00 MiB ->   205.08 MiB | hist: \n",
      "[   4/ 723]                  blk.0.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]                  blk.0.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]                  blk.0.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]             blk.0.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]                blk.0.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[   9/ 723]                blk.0.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 723]                  blk.0.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  11/ 723]               blk.0.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  12/ 723]                blk.0.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  13/ 723]                  blk.1.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]                  blk.1.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]                  blk.1.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]             blk.1.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]                blk.1.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 723]                blk.1.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]                  blk.1.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]               blk.1.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  21/ 723]                blk.1.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  22/ 723]                  blk.2.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]                  blk.2.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]                  blk.2.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]             blk.2.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]                blk.2.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 723]                blk.2.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]                  blk.2.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]               blk.2.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  30/ 723]                blk.2.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  31/ 723]                  blk.3.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]                  blk.3.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]                  blk.3.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]             blk.3.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]                blk.3.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 723]                blk.3.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]                  blk.3.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]               blk.3.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  39/ 723]                blk.3.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  40/ 723]                  blk.4.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]                  blk.4.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]                  blk.4.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]             blk.4.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]                blk.4.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 723]                blk.4.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]                  blk.4.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]               blk.4.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  48/ 723]                blk.4.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  49/ 723]                  blk.5.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]                  blk.5.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]                  blk.5.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]             blk.5.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]                blk.5.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 723]                blk.5.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]                  blk.5.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]               blk.5.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  57/ 723]                blk.5.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  58/ 723]                  blk.6.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]                  blk.6.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]                  blk.6.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]             blk.6.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]                blk.6.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 723]                blk.6.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]                  blk.6.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]               blk.6.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  66/ 723]                blk.6.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  67/ 723]                  blk.7.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]                  blk.7.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]                  blk.7.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]             blk.7.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]                blk.7.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 723]                blk.7.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]                  blk.7.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]               blk.7.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  75/ 723]                blk.7.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  76/ 723]                  blk.8.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]                  blk.8.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]                  blk.8.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]             blk.8.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]                blk.8.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 723]                blk.8.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]                  blk.8.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]               blk.8.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  84/ 723]                blk.8.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  85/ 723]                  blk.9.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]                  blk.9.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]                  blk.9.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]             blk.9.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]                blk.9.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 723]                blk.9.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 723]                  blk.9.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 723]               blk.9.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  93/ 723]                blk.9.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  94/ 723]                 blk.10.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]                 blk.10.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]                 blk.10.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]            blk.10.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]               blk.10.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 723]               blk.10.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 723]                 blk.10.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 723]              blk.10.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 102/ 723]               blk.10.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]                 blk.11.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]                 blk.11.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]                 blk.11.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]            blk.11.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]               blk.11.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 723]               blk.11.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]                 blk.11.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]              blk.11.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 111/ 723]               blk.11.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]                 blk.12.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]                 blk.12.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]                 blk.12.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]            blk.12.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]               blk.12.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 723]               blk.12.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]                 blk.12.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]              blk.12.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 120/ 723]               blk.12.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]                 blk.13.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]                 blk.13.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]                 blk.13.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]            blk.13.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]               blk.13.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 723]               blk.13.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]                 blk.13.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]              blk.13.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 129/ 723]               blk.13.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]                 blk.14.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]                 blk.14.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]                 blk.14.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]            blk.14.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]               blk.14.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 723]               blk.14.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]                 blk.14.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]              blk.14.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 138/ 723]               blk.14.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]                 blk.15.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]                 blk.15.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]                 blk.15.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]            blk.15.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]               blk.15.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 723]               blk.15.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 723]                 blk.15.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 723]              blk.15.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 147/ 723]               blk.15.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]                 blk.16.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]                 blk.16.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]                 blk.16.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]            blk.16.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]               blk.16.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 723]               blk.16.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]                 blk.16.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]              blk.16.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 156/ 723]               blk.16.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]                 blk.17.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]                 blk.17.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]                 blk.17.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]            blk.17.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]               blk.17.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 723]               blk.17.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]                 blk.17.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]              blk.17.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 165/ 723]               blk.17.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]                 blk.18.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]                 blk.18.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]                 blk.18.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]            blk.18.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]               blk.18.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 723]               blk.18.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]                 blk.18.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]              blk.18.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 174/ 723]               blk.18.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]                 blk.19.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]                 blk.19.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]                 blk.19.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]            blk.19.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]               blk.19.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 723]               blk.19.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]                 blk.19.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]              blk.19.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 183/ 723]               blk.19.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]                 blk.20.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]                 blk.20.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]                 blk.20.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]            blk.20.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]               blk.20.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 723]               blk.20.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]                 blk.20.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]              blk.20.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 192/ 723]               blk.20.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]                 blk.21.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]                 blk.21.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]                 blk.21.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]            blk.21.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]               blk.21.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 723]               blk.21.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]                 blk.21.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]              blk.21.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 201/ 723]               blk.21.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]                 blk.22.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]                 blk.22.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]                 blk.22.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]            blk.22.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]               blk.22.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 723]               blk.22.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]                 blk.22.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]              blk.22.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 210/ 723]               blk.22.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]                 blk.23.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]                 blk.23.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]                 blk.23.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]            blk.23.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]               blk.23.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 723]               blk.23.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]                 blk.23.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]              blk.23.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 219/ 723]               blk.23.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]                 blk.24.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]                 blk.24.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]                 blk.24.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]            blk.24.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]               blk.24.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 723]               blk.24.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]                 blk.24.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]              blk.24.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 228/ 723]               blk.24.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]                 blk.25.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]                 blk.25.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]                 blk.25.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]            blk.25.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]               blk.25.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 723]               blk.25.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]                 blk.25.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]              blk.25.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 237/ 723]               blk.25.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]                 blk.26.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]                 blk.26.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]                 blk.26.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]            blk.26.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]               blk.26.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 723]               blk.26.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]                 blk.26.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]              blk.26.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 246/ 723]               blk.26.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]                 blk.27.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]                 blk.27.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]                 blk.27.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]            blk.27.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]               blk.27.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 723]               blk.27.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]                 blk.27.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]              blk.27.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 255/ 723]               blk.27.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]                 blk.28.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]                 blk.28.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]                 blk.28.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]            blk.28.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]               blk.28.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 723]               blk.28.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]                 blk.28.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]              blk.28.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 264/ 723]               blk.28.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]                 blk.29.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]                 blk.29.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]                 blk.29.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]            blk.29.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]               blk.29.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 723]               blk.29.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]                 blk.29.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]              blk.29.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 273/ 723]               blk.29.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]                 blk.30.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]                 blk.30.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]                 blk.30.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]            blk.30.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]               blk.30.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 723]               blk.30.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]                 blk.30.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]              blk.30.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 282/ 723]               blk.30.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]                 blk.31.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]                 blk.31.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]                 blk.31.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]            blk.31.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]               blk.31.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 723]               blk.31.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]                 blk.31.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]              blk.31.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 291/ 723]               blk.31.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]                 blk.32.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]                 blk.32.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]                 blk.32.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]            blk.32.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]               blk.32.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 723]               blk.32.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]                 blk.32.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]              blk.32.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 300/ 723]               blk.32.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]                 blk.33.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]                 blk.33.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]                 blk.33.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]            blk.33.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]               blk.33.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 723]               blk.33.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]                 blk.33.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]              blk.33.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 309/ 723]               blk.33.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]                 blk.34.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]                 blk.34.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]                 blk.34.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]            blk.34.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]               blk.34.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 723]               blk.34.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]                 blk.34.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]              blk.34.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 318/ 723]               blk.34.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]                 blk.35.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]                 blk.35.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]                 blk.35.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]            blk.35.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]               blk.35.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 723]               blk.35.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]                 blk.35.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]              blk.35.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 327/ 723]               blk.35.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]                 blk.36.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]                 blk.36.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]                 blk.36.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]            blk.36.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]               blk.36.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 723]               blk.36.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]                 blk.36.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]              blk.36.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 336/ 723]               blk.36.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]                 blk.37.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]                 blk.37.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]                 blk.37.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]            blk.37.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]               blk.37.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 723]               blk.37.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]                 blk.37.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]              blk.37.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 345/ 723]               blk.37.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]                 blk.38.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]                 blk.38.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]                 blk.38.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]            blk.38.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]               blk.38.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 723]               blk.38.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]                 blk.38.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]              blk.38.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 354/ 723]               blk.38.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]                 blk.39.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]                 blk.39.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]                 blk.39.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]            blk.39.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]               blk.39.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 723]               blk.39.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]                 blk.39.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]              blk.39.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 363/ 723]               blk.39.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]                 blk.40.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]                 blk.40.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]                 blk.40.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]            blk.40.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]               blk.40.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 723]               blk.40.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]                 blk.40.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]              blk.40.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 372/ 723]               blk.40.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]                 blk.41.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]                 blk.41.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]                 blk.41.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]            blk.41.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]               blk.41.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 723]               blk.41.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]                 blk.41.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]              blk.41.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 381/ 723]               blk.41.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]                 blk.42.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]                 blk.42.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]                 blk.42.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]            blk.42.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]               blk.42.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 723]               blk.42.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]                 blk.42.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]              blk.42.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 390/ 723]               blk.42.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]                 blk.43.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]                 blk.43.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]                 blk.43.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]            blk.43.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]               blk.43.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 723]               blk.43.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]                 blk.43.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]              blk.43.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 399/ 723]               blk.43.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]                 blk.44.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]                 blk.44.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]                 blk.44.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]            blk.44.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]               blk.44.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 723]               blk.44.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]                 blk.44.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]              blk.44.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 408/ 723]               blk.44.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]                 blk.45.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]                 blk.45.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]                 blk.45.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]            blk.45.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]               blk.45.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 723]               blk.45.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]                 blk.45.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]              blk.45.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 417/ 723]               blk.45.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]                 blk.46.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]                 blk.46.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]                 blk.46.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]            blk.46.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]               blk.46.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 723]               blk.46.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]                 blk.46.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]              blk.46.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 426/ 723]               blk.46.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]                 blk.47.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]                 blk.47.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]                 blk.47.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]            blk.47.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]               blk.47.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 723]               blk.47.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]                 blk.47.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]              blk.47.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 435/ 723]               blk.47.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]                 blk.48.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]                 blk.48.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]                 blk.48.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]            blk.48.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]               blk.48.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 723]               blk.48.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]                 blk.48.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]              blk.48.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 444/ 723]               blk.48.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]                 blk.49.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]                 blk.49.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]                 blk.49.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]            blk.49.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]               blk.49.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 723]               blk.49.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]                 blk.49.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]              blk.49.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 453/ 723]               blk.49.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]                 blk.50.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]                 blk.50.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]                 blk.50.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]            blk.50.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]               blk.50.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 723]               blk.50.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]                 blk.50.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]              blk.50.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 462/ 723]               blk.50.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]                 blk.51.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]                 blk.51.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]                 blk.51.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]            blk.51.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]               blk.51.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 723]               blk.51.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]                 blk.51.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]              blk.51.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 471/ 723]               blk.51.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]                 blk.52.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]                 blk.52.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]                 blk.52.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]            blk.52.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]               blk.52.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 723]               blk.52.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]                 blk.52.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]              blk.52.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 480/ 723]               blk.52.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]                 blk.53.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]                 blk.53.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]                 blk.53.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]            blk.53.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]               blk.53.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 723]               blk.53.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]                 blk.53.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]              blk.53.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 489/ 723]               blk.53.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]                 blk.54.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]                 blk.54.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]                 blk.54.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]            blk.54.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]               blk.54.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 723]               blk.54.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]                 blk.54.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]              blk.54.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 498/ 723]               blk.54.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]                 blk.55.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]                 blk.55.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]                 blk.55.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]            blk.55.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]               blk.55.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 723]               blk.55.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]                 blk.55.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]              blk.55.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 507/ 723]               blk.55.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]                 blk.56.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]                 blk.56.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]                 blk.56.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]            blk.56.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]               blk.56.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 723]               blk.56.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]                 blk.56.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]              blk.56.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 516/ 723]               blk.56.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]                 blk.57.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]                 blk.57.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]                 blk.57.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]            blk.57.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]               blk.57.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 723]               blk.57.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]                 blk.57.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]              blk.57.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 525/ 723]               blk.57.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]                 blk.58.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]                 blk.58.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]                 blk.58.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]            blk.58.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]               blk.58.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 723]               blk.58.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]                 blk.58.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]              blk.58.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 534/ 723]               blk.58.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]                 blk.59.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]                 blk.59.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]                 blk.59.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]            blk.59.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]               blk.59.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 540/ 723]               blk.59.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]                 blk.59.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]              blk.59.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 543/ 723]               blk.59.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]                 blk.60.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]                 blk.60.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]                 blk.60.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]            blk.60.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]               blk.60.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 549/ 723]               blk.60.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]                 blk.60.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]              blk.60.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 552/ 723]               blk.60.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]                 blk.61.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]                 blk.61.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]                 blk.61.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]            blk.61.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]               blk.61.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 558/ 723]               blk.61.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]                 blk.61.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]              blk.61.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 561/ 723]               blk.61.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]                 blk.62.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]                 blk.62.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]                 blk.62.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]            blk.62.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]               blk.62.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 567/ 723]               blk.62.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]                 blk.62.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]              blk.62.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 570/ 723]               blk.62.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]                 blk.63.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]                 blk.63.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]                 blk.63.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]            blk.63.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]               blk.63.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 576/ 723]               blk.63.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]                 blk.63.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]              blk.63.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 579/ 723]               blk.63.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]                 blk.64.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]                 blk.64.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]                 blk.64.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]            blk.64.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]               blk.64.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 585/ 723]               blk.64.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]                 blk.64.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]              blk.64.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 588/ 723]               blk.64.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]                 blk.65.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]                 blk.65.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]                 blk.65.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]            blk.65.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]               blk.65.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 594/ 723]               blk.65.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]                 blk.65.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]              blk.65.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 597/ 723]               blk.65.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]                 blk.66.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]                 blk.66.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]                 blk.66.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]            blk.66.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]               blk.66.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 603/ 723]               blk.66.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]                 blk.66.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]              blk.66.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 606/ 723]               blk.66.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]                 blk.67.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]                 blk.67.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]                 blk.67.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]            blk.67.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]               blk.67.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 612/ 723]               blk.67.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]                 blk.67.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]              blk.67.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 615/ 723]               blk.67.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]                 blk.68.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]                 blk.68.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]                 blk.68.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]            blk.68.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]               blk.68.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 621/ 723]               blk.68.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]                 blk.68.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]              blk.68.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 624/ 723]               blk.68.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]                 blk.69.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]                 blk.69.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]                 blk.69.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]            blk.69.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]               blk.69.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 630/ 723]               blk.69.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]                 blk.69.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]              blk.69.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 633/ 723]               blk.69.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]                 blk.70.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]                 blk.70.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]                 blk.70.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]            blk.70.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]               blk.70.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 639/ 723]               blk.70.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]                 blk.70.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]              blk.70.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 642/ 723]               blk.70.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]                 blk.71.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]                 blk.71.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]                 blk.71.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]            blk.71.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]               blk.71.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 648/ 723]               blk.71.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]                 blk.71.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]              blk.71.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 651/ 723]               blk.71.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]                 blk.72.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]                 blk.72.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]                 blk.72.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]            blk.72.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]               blk.72.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 657/ 723]               blk.72.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]                 blk.72.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]              blk.72.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 660/ 723]               blk.72.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]                 blk.73.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]                 blk.73.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]                 blk.73.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]            blk.73.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]               blk.73.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 666/ 723]               blk.73.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]                 blk.73.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]              blk.73.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 669/ 723]               blk.73.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]                 blk.74.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]                 blk.74.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]                 blk.74.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]            blk.74.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]               blk.74.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 675/ 723]               blk.74.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]                 blk.74.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]              blk.74.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 678/ 723]               blk.74.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]                 blk.75.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]                 blk.75.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]                 blk.75.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]            blk.75.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]               blk.75.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 684/ 723]               blk.75.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]                 blk.75.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]              blk.75.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 687/ 723]               blk.75.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]                 blk.76.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]                 blk.76.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]                 blk.76.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]            blk.76.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]               blk.76.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 693/ 723]               blk.76.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 694/ 723]                 blk.76.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 695/ 723]              blk.76.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 696/ 723]               blk.76.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]                 blk.77.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]                 blk.77.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]                 blk.77.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]            blk.77.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]               blk.77.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 702/ 723]               blk.77.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 703/ 723]                 blk.77.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 704/ 723]              blk.77.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 705/ 723]               blk.77.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]                 blk.78.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]                 blk.78.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]                 blk.78.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]            blk.78.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]               blk.78.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 711/ 723]               blk.78.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 712/ 723]                 blk.78.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 713/ 723]              blk.78.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 714/ 723]               blk.78.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]                 blk.79.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]                 blk.79.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]                 blk.79.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]            blk.79.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]               blk.79.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 720/ 723]               blk.79.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 721/ 723]                 blk.79.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 722/ 723]              blk.79.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 723/ 723]               blk.79.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 78932.82 ms\n",
      "main:    total time = 78932.82 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.gguf ./models/13B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.gguf ./models/30B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.gguf ./models/65B/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS:  \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
      "removed 'build-info.o'\n",
      "removed 'common.o'\n",
      "removed 'console.o'\n",
      "removed 'ggml-alloc.o'\n",
      "removed 'ggml-backend.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml-quants.o'\n",
      "removed 'ggml.o'\n",
      "removed 'grammar-parser.o'\n",
      "removed 'llama.o'\n",
      "removed 'sampling.o'\n",
      "removed 'train.o'\n",
      "removed 'tests/test-c.o'\n",
      "removed 'benchmark-matmult'\n",
      "removed 'common/build-info.cpp'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'vdot'\n",
      "removed 'q8dot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'convert-llama2c-to-ggml'\n",
      "removed 'simple'\n",
      "removed 'batched'\n",
      "removed 'batched-bench'\n",
      "removed 'save-load-state'\n",
      "removed 'server'\n",
      "removed 'gguf'\n",
      "removed 'llama-bench'\n",
      "removed 'libllava.a'\n",
      "removed 'llava-cli'\n",
      "removed 'baby-llama'\n",
      "removed 'beam-search'\n",
      "removed 'speculative'\n",
      "removed 'infill'\n",
      "removed 'tokenize'\n",
      "removed 'parallel'\n",
      "removed 'finetune'\n",
      "removed 'export-lora'\n",
      "removed 'lookahead'\n",
      "removed 'lookup'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS: -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
      "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
      "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib   -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib  -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88ef08-68f5-4655-ac49-d0368c11b004",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33724f57-0378-4652-86b1-2b4858d56528",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db5a686-a0d6-4af2-9753-fd4e6b42a706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295455\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 3577.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, like a felicitous young giant, rushed up hill, driving a wedge into the redoubtable flank of arrogance and injustice, and planted there the standard of the people – that is to say, of the industrious classes. \n",
      " France, notwithstanding all the disadvantages resulting from her national peculiarities, was far in front: and, what is still more extraordinary, England, with her defects, was fast overtaking her. \n",
      " On both sides of the channel, and throughout Europe generally, the population was agitating itself into a fever: so that the kettles of the whole family of nations seemed to be in a perpetual state of boiling over. A few years more of this bubbling fermentation will produce results which no human sagacity can foresee. \n",
      " England, at least, was not without her symptoms of approaching disturbance. The press – which is a very loud-mouthed and impudent set of fellows – was becoming unruly: had begun to talk about things not being what they used to be, and to intimate that there would be trouble if nothing were done; and so forth. And it is not to be imagined how little all this has been appreciated at the proper time, and in the proper place. \n",
      " A set of fellows who call themselves Radicals have made noises as if they intended to disturb the public tranquillity: but they are really only a mob of agitators; who, like all such societies, mean nothing by their words, and are merely loudly expressive of indigestion. They are like fire-engines, that come rumbling into your street at four in the morning, to alarm you with a hollow roar: but what is the consequence? Nothing; excepting, perhaps, a good deal of inconvenience to yourself and your neighbours. \n",
      " If England was really a monarchy, she would long ago have sent these fellows to their places of punishment by dint of horse-whipping: but it being a free country (at least nominally), where they can say what they please, they do say it; though no one hears them, or takes any notice of them, till after the event. Then, indeed, you hear the great voice that is in them. \n",
      " It has been well observed by an eminent statesman (but a Radical, and consequently a person to be undervalued), that when all is said, no more can be got out of men's heads than what they have in their minds. The same may be applied to society: the whole value of any association or body of persons being measured by what it has in its mind; not at all by what it pretends to have in its head. \n",
      " A Radical is a person who believes he knows more about his own affairs than other people do; though they are as well acquainted with them as himself – and generally wiser. Therefore, when the Radicals say that the House of Peers will not make good laws for the nation, you may take it for granted that they know better. They have in their minds a clear notion of what laws should be made; but they don't like to express them: they are afraid of being laughed at. Therefore, they would rather sit down quietly with a sour look upon their faces – and think. \n",
      " The Radicals are all in their heads; which, of course, is a good thing. But it would be an excellent thing if we had some Radicals in our heads also: then the other persons who are not so wise as themselves would not have to suffer for what they know nothing about – like those whom they call \"the privileged few.\" \n",
      " The great objection against the House of Peers, is that it does not make laws; but only gives its sanction to them when made by the House of Commons. If this be so, we would rather have no House at all: for then our own representatives would do their duty, and pass good laws for us, without having any interference from men who had no business there, but to sit in a corner with sour faces – and think. \n",
      " The real objection against the House of Lords is this, that it will not make bad laws; because there are so many foolish Radicals among them: therefore we want it altogether removed out of the way, so as not to have any obstacle in the making of good laws for us by our representatives. And when a person tells you he does not like the House of Lords – only because they will make no bad laws; that is, because they have too much sense themselves, and are not sufficiently Radicals, we shall know how to account for them, if there are such people in society.\n",
      "llama_print_timings:        load time =    1607.83 ms\n",
      "llama_print_timings:      sample time =     119.08 ms /  1024 runs   (    0.12 ms per token,  8599.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.45 ms /   494 tokens (    0.33 ms per token,  2985.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9160.49 ms /  1023 runs   (    8.95 ms per token,   111.68 tokens per second)\n",
      "llama_print_timings:       total time =    9613.16 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9324415-e69d-4ed2-8b56-2094995fbe8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295467\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 3577.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was flattered by being told that she had nothing else to do; and England’s kith and kin in the old country were pleased with having something else to do, and something else to think of. \n",
      " Chapter II: The Shadow of a Dream, Section One\n",
      "Whenever I read Charles Dickens, I cannot help but see the influence on Mark Twain (who also had a thing for long sentences), and it is my favorite part about his work – how much he made me laugh! He was a wonderful satirist.\n",
      "I enjoyed this story, and there are several other gems in this collection by Dickens which I look forward to reading.\n",
      "Labels: 2018 Books, 52 Books in 52 Weeks, Author: Charles Dickens, Classics Club, English Lit, Fiction, Review\n",
      "#BookReview: A Tale of Two Cities by Charles D...\n",
      "Review: The Picture of Dorian Gray by Oscar Wil...\n",
      "#BookReview: Bleak House by Charles Dickens #52B...\n",
      "Reading Challenge: 19th Century Classic #52Books ...\n",
      "Review: Dombey and Son by Charles Dickens #52Bo...\n",
      "A Tale of Two Cities Read-Along, Chapters 7-8, ...\n",
      "A Tale of Two Cities Read-Along, Chapter Six, b...\n",
      "#BookReview: Great Expectations by Charles Dick...\n",
      "Review: The Old Curiosity Shop by Charles Dickens...\n",
      "#BookReview: David Copperfield by Charles Dicken...\n",
      "Review: Bleak House by Charles Dickens #52BooksB...\n",
      "A Tale of Two Cities Read-Along, Chapter Five b...\n",
      "A Tale of Two Cities Read-along: Introduction &...\n",
      "#BookReview: Little Women by Louisa May Alcott ...\n",
      "#BookReview: Emma by Jane Austen #52BooksbyWomen ...\n",
      "#BookReview: Vanity Fair by William Thackeray (F...\n",
      "A Tale of Two Cities Read-along, Chapter Four b...\n",
      "A Tale of Two Cities Read-Along, Chapters 1 & 2 ...\n",
      "The Old Curiosity Shop Review #52BooksbyWomen\n",
      "A Tale of Two Cities Read-Along #52BooksbyWomen\n",
      "#BookReview: Northanger Abbey by Jane Austen (F...\n",
      "#BookReview: Middlemarch by George Eliot (Five S...\n",
      "Vanity Fair Review #52BooksbyWomen\n",
      "A Tale of Two Cities Read-Along #52BooksbyWomen\n",
      "#BookReview: Frankenstein by Mary Shelley (Five ...\n",
      "Northanger Abbey Review #52booksbywomen\n",
      "Middlemarch Review #52booksbywomen\n",
      "A Tale of Two Cities Read-Along\n",
      "Vanity Fair Read-along\n",
      "The Old Curiosity Shop Read-Along, Chapters 6 & ...\n",
      "Frankenstein Read-Along, Chp. 4-6\n",
      "Middlemarch: Read-Along, Chapter Three\n",
      "A Tale of Two Cities: Read-Along, Chapter Four (B...\n",
      "Northanger Abbey: Read Along, Chapters One and Tw...\n",
      "#BookReview: Frankenstein by Mary Shelley (Five ...\n",
      "Middlemarch: Read Along, Chapter Three\n",
      "A Tale of Two Cities: Read-Along, Chapter Four (B)\n",
      "A Tale of Two Cities: Read-Along, Chapters One an...\n",
      "#BookReview: Middlemarch by George Eliot (Five S...\n",
      "Vanity Fair: Read Along, Ch. 4-6\n",
      "Northanger Abbey: Read Along, Chps. One and Two\n",
      "The Old Curiosity Shop: Read-Along, Chapter Four ...\n",
      "Middlemarch: Read-along, Chapter Three\n",
      "Vanity Fair: Read-Along, Chapters 1 & 2\n",
      "A Tale of Two Cities: Read-Along, Chapters One a...\n",
      "Northanger Abbey: Read Along, Chps. One and Two (B)\n",
      "Middlemarch: Read along, Chapter Three (Catching ...\n",
      "Vanity Fair Read-along: Chapters 1 & 2\n",
      "A Tale of Two Cities read-along: Chapter Four (B)...\n",
      "Northanger Abbey: Read Along, Chps. One and Two...\n",
      "Middlem\n",
      "llama_print_timings:        load time =    1683.90 ms\n",
      "llama_print_timings:      sample time =     113.24 ms /  1024 runs   (    0.11 ms per token,  9042.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.34 ms /   494 tokens (    0.33 ms per token,  2987.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9183.72 ms /  1023 runs   (    8.98 ms per token,   111.39 tokens per second)\n",
      "llama_print_timings:       total time =    9631.76 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0cf40d6-f822-4ba8-a315-c3819dd2af9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295479\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 3577.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, in spite of Mrs. Southcott's prophesyings (to which the British Lion had responded by snapping up a hundred thousand of the Queen's subjects) sank deeper and deeper, mountain-wise, in the red tape and routine of her own insular policy; till at last the two great nations were divided only by the narrowest possible strip of the water called the Channel—as if that were destined to be some day the only strait in all the circumference of the earth which two such powerful states should never meet across. \n",
      "# CHAPTER XXXVII  \n",
      "CAMBRIDGE\n",
      "IT was a very fine morning, indeed a most beautiful and glorious one, when Mr. Sedley, Mrs. Sedley (who was the Honourable Augusta Bertram), her maid, Miss Jemima, Miss Laura, Mr. Yates, and Mr. Samuel Boythorn set out from town for Cambridge. There had been a great deal of hesitation and consultation whether it would be better to go by coach or by post-chaise: the latter being more expeditious but less comfortable; the former less so but cheaper. In the end they decided for the coach, as more in keeping with their rank—that is, Mr. Boythorn's—and agreeable to Mrs. Sedley's tastes (which were for comfort at any price).\n",
      "Mr. and Mrs. Sedley had gone over these same pleasant doubts, these very questions of expense and comfort, time after time; and the coach was their final answer, as a means of conveyance between London and Cambridge; not to speak of the fact, that in old times when they were undergraduates at Trinity College, this way had always been considered most appropriate for young men and women on visits.\n",
      "Their luggage having arrived on the preceding night from London by coach, Mrs. Sedley was soon comfortably seated in her chaise, with a foot-pinketting to keep the dust off her fine dresses, and a white feather fan held out before her face, whence the air blew over it coolly into her delicate features. Mr. Boythorn, who had a seat next to his pupil, looked through his eyeglasses at all this, approving of every detail, and thinking, \"The young man is very like his mother's family.\" He was an oldish man; tall and spare—in fact, a bony man. In his youth he had been a good deal in India, where the English are always looked upon as very fat people because their bodies have never to struggle through any climate, but always live on cool bread-and-butter; besides which, they are so very fond of eating. Mr. Boythorn had not much bread-and-butter to eat in India (as the saying is), for he was poor: and as for the second part of the clause, \"so very fond of eating,\" it never would have been noticed if there had been a third clause, \"so very fond of drinking.\" When this oldish man first came into England with his pupil, he took care to place them both at an hotel in which it was the custom to give wine with all the meals. As to any bread-and-butter, that was so far from being given with the beef and mutton that I fear Mr. Boythorn's appetite has often been a good deal puzzled what to eat—what to do in fact.\n",
      "There were two other passengers, who, like our young couple, had also been at school; but as they were both going on to Oxford, they took no more notice of one another than the rest of us would have done, if we had only been together for three years instead of seven or eight.\n",
      "One of these people was a little girl—about the same age as my brother Tom; and I suppose her parents thought she could get more amusement out of this young companion of hers at Oxford, than from her elder brother who had just finished school and was going to be entered in Trinity. She was one of those nice, ladylike-looking children, with such a good complexion and such delicate little features; you felt quite sorry for them when they went away—they were so sweet!\n",
      "\"And are your people at Oxford?\" said Mr. Boythorn to Tom. \"What is his name?\" he added, in answer to my brother's question as to who the girl was.\n",
      "Mr. Boythorn had come up to London on purpose to go with us, and not only see Tom into Oxford, but to go himself once more among those dear old booksellers. He would like so much to look over some of his old favourites—to go back to old times\n",
      "llama_print_timings:        load time =     541.50 ms\n",
      "llama_print_timings:      sample time =     119.84 ms /  1024 runs   (    0.12 ms per token,  8545.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.54 ms /   494 tokens (    0.33 ms per token,  3002.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9196.46 ms /  1023 runs   (    8.99 ms per token,   111.24 tokens per second)\n",
      "llama_print_timings:       total time =    9650.43 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a124de-0e22-49fc-8ea3-d324d831a961",
   "metadata": {},
   "source": [
    "### 7B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518024e7-d24d-4701-a788-6445d04b3ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295490\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  250.11 MiB\n",
      "llm_load_tensors: VRAM used           = 12603.02 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m The Government was no longer restrained by any apprehension of a war: in which case, its measures would have been very different. On the contrary, after having for some time past rashly practised economy as a virtue, it was become extravagant to the point of recklessness; and was running headlong into the grossest profligacy of expenditure – not merely in the purchase of foreign supplies of every description, but in its own internal administration. \n",
      " The nation, meanwhile, had grown old and rich, and indifferent: had ceased to attach importance to affairs, and to follow them no longer. Involuntarily giving itself up to enjoyment, it lived without cares or conscientiousness, in a condition of satiation, and even of stupidity. \n",
      " The war being at an end, the French Government began to make arrangements for peace; and in order that they might not be hindered by any considerations of expense, wasted a million francs on making a treaty which had no other object than to save money: while, nevertheless, they went on squandering innumerable millions. The French have always been accused (falsely) of not knowing how to make war; it may now be fairly laid to their charge that they do not know how to make peace – at least for any purpose except the saving of a little money. \n",
      " One thing, however, was still wanting: in order to carry out all these plans and measures with a tranquillity as complete as possible (for peace is always an uncomfortable condition of existence), it was necessary to provide the government with money to keep going until such time as the taxes which were then levied should have been collected. \n",
      " And now, a great difficulty arose: for those very same persons who had squandered millions without stint on making and maintaining peace, would not give money to the government in order to carry it out – or rather, they could not be persuaded that peace ought to cost anything; it was enough, they said, if they paid their taxes. The government, finding itself thus persecuted by the same people whom it had pampered and who were now growing old, decided to sacrifice them in a manner calculated to leave no illusions about its power – as well as its wisdom: for such is always the result of revolutions – namely, to raise the taxes; which was done accordingly. \n",
      " A few months later came the news that peace had been made with England. It was the English who had carried it out and the English who were most exasperated by this piece of folly: they no longer knew whether to believe in the existence of France or not, since it could no longer be denied that the French had shown themselves utterly incapable of understanding any political project which did not consist in plunging Europe into a state of war. \n",
      " As for the French, however, they were satisfied with everything and with nothing; but especially they were very much inclined to believe that if they should pay some taxes from time to time, all would go on swimmingly: it was on this ground that they now proceeded to demand a general reduction of expenditure in order to make room for the payment of debts. \n",
      " This proved impossible: they were soon obliged to yield; but they did not realize what an abyss their finances had fallen into; they even did not know that all their taxes and loans had not brought them a penny; that France had become as deeply in debt as she could be without ceasing to exist, while the government still owed something like a billion of francs to her creditors. \n",
      " This state of things was known only to one man in the world, who had made this discovery before any of the rest and who kept it secret with an unhealthy love; he was not sure how long this would last, since he knew very well that if ever his countrymen should be forced to pay their debts, they would soon begin to accuse him.\n",
      "24th June 1807–22nd November 1809\n",
      "The Revolutionary Idea of Liberty\n",
      "One evening in the month of August 1790 I had just returned from the club and was on my way home when I passed a large public coffee-house whose doors were wide open, as they always are at that hour. There were several groups standing together outside who were listening to someone speaking with great animation from one of those balconies which project from the second floor of these houses; I also stopped for a moment, but only long enough to learn that the man was one of those revolutionary writers whose works had excited so much popular interest at this period. \n",
      " After hearing him for some time without being able to discover whether he was any more intelligent\n",
      "llama_print_timings:        load time =    5838.75 ms\n",
      "llama_print_timings:      sample time =     122.65 ms /  1024 runs   (    0.12 ms per token,  8349.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.97 ms /   494 tokens (    0.27 ms per token,  3743.16 tokens per second)\n",
      "llama_print_timings:        eval time =   23240.89 ms /  1023 runs   (   22.72 ms per token,    44.02 tokens per second)\n",
      "llama_print_timings:       total time =   23669.52 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec4b98bb-a046-4be5-9f98-cc5f64724a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295520\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  250.11 MiB\n",
      "llm_load_tensors: VRAM used           = 12603.02 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England had undertaken to support the French government in its war against the American rebels: so, England set about doing it, as she always did, by borrowing money and selling titles, preferments, pardons, and places of trust in church and state; by altering her mode of coining her currency; and by encouraging private bankers to counterfeit it. In this manner, France was taxed for a time: the taxation was distributed among individuals according to their property: the new taxes were paid in assignats, which were issued at will by the state, or loaned into circulation on bills of exchange, until they were called in and made equal to bank-notes. By degrees, these assignats rose to three hundred per cent.; fell again; rose higher still: then declined, and have since been at various stages of depreciation. \n",
      " In England, a new paper money was issued also, for the same purposes as that of France, except that it was not paid in assignats; but, instead thereof, received its value from a fund established by act of Parliament (to which all classes contributed); and in consideration of which, the holders of the paper money obtained certificates for one-fourth part of their amount: these certificates were paid for in taxes. \n",
      " In France, no distinction was made between the value of assignats, or paper money, and the price of commodities; both were considered as representing the same number of labouring days: the holders of assignats sold them at a premium, as they were issued, until they were worth one hundred per cent. above their nominal value: these, however, being still issued, fell to be discounted below that rate in exchange for hard money, which rose with an increase of taxes and depreciation; so that the holders of assignats at last received back for them only sixty-four per cent. in coin. \n",
      " In England, no distinction was made between the value of paper and commodities; but the nominal value of the fund was not increased, nor did the holder of a certificate sell it to any advantage above its par value. The holders, therefore, of certificates bought them in exchange at their real value in coin. \n",
      " In France, after the assignats rose from one hundred to sixty-four per cent., they were issued without increase of fund; and as they continued to be paid by the government for taxes, the amount of these became greater than could be discharged with a given quantity of commodities: their nominal value then fell below par, because people preferred money in coin. \n",
      " In England, the holders of certificates would have bought them at any rate when they were worth sixty-four per cent., as they represented one hundred; and if the government had paid them for taxes, it must have paid the same value with gold or silver: therefore the fund could not have been increased beyond that proportion. The holders, therefore, of these certificates did not suffer any loss; but on the contrary sold their certificates below their nominal value, because the price of commodities rose more than the rate at which the government would pay them. \n",
      " In France, this increase of money was in consequence of an abatement in taxes. As the assignats, instead of being paid for these taxes, were offered as a part payment, and were received in exchange with less labour and industry, this lowered their value; and those who had money to pay taxes would sell it at the same rate below its nominal value, because the government had no power to force them into an unwilling compliance. \n",
      " The abatement of taxes which caused such an increase in the quantity of paper money, produced all the effect that can be attributed to a reduction; for these assignats were received by those who paid their taxes in silver or gold, in exchange for less value than they had before; but no other effects could follow, unless the government could force them to take this paper in payment when it was offered. \n",
      " The French, however, took great pains to make use of this measure as a fund which might be raised by taxes. They established an assignat bank for that purpose, and paid off all their debts by means of assignats, which were given at par value. The effect was, as the assignats had been made so dear, to increase the rate of interest in France much more than it is known they did elsewhere. When this money was offered, it was preferred to silver and gold, for these latter articles were more expensive; and if any one would accept it, he was obliged to pay an excessive price for them. It was therefore forced upon everybody. In some parts of France, people in the country who had a hundred francs’ worth of property, received nothing but\n",
      "llama_print_timings:        load time =    6061.38 ms\n",
      "llama_print_timings:      sample time =     120.79 ms /  1024 runs   (    0.12 ms per token,  8477.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.72 ms /   494 tokens (    0.27 ms per token,  3694.23 tokens per second)\n",
      "llama_print_timings:        eval time =   23282.30 ms /  1023 runs   (   22.76 ms per token,    43.94 tokens per second)\n",
      "llama_print_timings:       total time =   23709.29 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c07219-6f1a-4e38-9590-bb0fe7cea15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295550\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  250.11 MiB\n",
      "llm_load_tensors: VRAM used           = 12603.02 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, after a brief but brilliant experiment of the same kind, suddenly found itself without money; which led to the arrangement, in 1795, of a fresh loan upon better security than paper: namely, the King himself; an experiment (thank Heaven) since that time discontinued. \n",
      " It was a good year for literary men on both sides of the water; who had a great advantage over all other people, in having a little more money and leisure to spend upon books than their fellow-creatures. I doubt whether there were any poets at present writing, equal either to Pope or Swift, or indeed, with them, except perhaps in one point of view —namely, of the extent to which they are understood. \n",
      " For many years after the death of Johnson, nobody seemed to attempt any thing very ambitious as a poem; unless it were Lord Byron's Childe Harolde (as was said), whose real name never seems to have transpired at all in connection with the production of his work, except to Mrs. ****. \n",
      " The English poets at that time were rather prosaical fellows on the whole —as a body. Wordsworth, who was always an exception to any general rule about poetry, except as it related to himself and his own performances; was nevertheless much more of an old-fashioned moralist in the true sense than Shakspeare ever was. \n",
      " Coleridge had left the church (which he could not have remained in at all, after having made himself known) and devoted himself almost wholly to poetry —with little result as far as respected money or reputation: he had also written a prose work upon religion which was a strange mixture of the most excellent sense, with utter nonsense. \n",
      " Scott and Southey wrote very much together: one of their most successful joint productions being an edition (1798) of the Chronicles of England. These writers have since risen into great reputation; but they had not yet done so in the year I speak of. \n",
      " Campbell was a poet who made a good deal of money by his publications, and whom everybody seemed to praise, when nobody read his poems; which were full of rhetorical fire-works, without much substance beneath them: he wrote, moreover, what we now call novels —and he had invented a kind of heroic metre called Spenserian couplets. \n",
      " Thomson was at this time a name to conjure with in poetry: his Seasons had been very popular; but it was very difficult to conceive how any one should have liked them, for there were very few fine stanzas in the whole poem —and not above two or three passages that were really beautiful. \n",
      " Southey wrote a great deal of miscellaneous poetry —which had also been successful; and he wrote, too, some books upon historical subjects, which had not succeeded. He had written a poem on the Fall of Robespierre which was said to be very fine: I did not read it myself until about ten years after; when I found that it was rather tedious than otherwise —and certainly not good enough to account for all that noise having been made in its favour at the time. \n",
      " Sharon Turner was a very eminent topographical writer, and had written a history of Rutlandshire with great industry and sagacity. He also wrote a History of Greece; but it was a very superficial book. \n",
      " Richard Heber —who is now my father-in-law —had just been published by the subscription for the Temple Poets, and was beginning to be much admired as a poet: his writings were afterwards collected in two volumes —but I cannot say that they are worth reading. He wrote some good things before he had any taste of verse at all; and having formed an opinion of his own merits, he tried his hand on the usual occasions which come round to all writers —and composed verses upon them. The result was very poor indeed —but his friends were so much pleased with his effort that they encouraged him to write more poetry: I never heard whether they paid for it or not; but if they did, it must have cost a great deal of money, for he is not in the least endowed by nature with a poetic genius. He was indeed very modest and reserved —and would hardly say that anything which he had written was worth reading: he has, however, written a considerable amount of poetry; but there is nothing remarkable about it. \n",
      " Henry Cockton was writing at this time, and his poetry was considered very good: but the pieces which I have seen are so little finished in themselves that one cannot help suspecting that he did not take much pains with them —nor perhaps would such a person as he had any taste\n",
      "llama_print_timings:        load time =    2258.56 ms\n",
      "llama_print_timings:      sample time =     118.26 ms /  1024 runs   (    0.12 ms per token,  8659.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.05 ms /   494 tokens (    0.27 ms per token,  3712.89 tokens per second)\n",
      "llama_print_timings:        eval time =   23297.71 ms /  1023 runs   (   22.77 ms per token,    43.91 tokens per second)\n",
      "llama_print_timings:       total time =   23718.73 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b6f4e-6235-47a1-8942-f8d04e6f1b29",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8088f7bc-2a09-43d7-8940-5721b1af6bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295577\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   88.03 MiB\n",
      "llm_load_tensors: VRAM used           = 6936.01 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of these omnicompetent ministers, her annual expenditure increased from sixty millions to eighty, and from eighty to a hundred and twenty. \n",
      "Chapter 4  \n",
      "Concerning the Departure\n",
      "In England, in France, and indeed all over Europe, it was generally felt that there must be some mistake, that things could not be as bad as they were. Everybody expected some great change – everybody imagined that something would turn up – and it did. The French Government had got into a muddle with the Algerians; France would have to leave Algeria: people said so in print, and said it everywhere. Some men who thought about such things believed that this was the reason for M. Thiers' fall from power, though the majority of his own party rejected any such idea. He had done nothing, they argued, to deserve a political assassination. A large number of people believed in the old tradition that no French Government could long hold together unless supported by a strong Opposition; and this, said one man to another man in France, is not a strong Opposition. It seems probable enough that when M. Thiers resigned power on July 24th, 1840, he intended to be Prime Minister again some time or other; but as his enemies were very angry with him, and he had no friends outside his own party who could make things right for him in the National Assembly – unless they voted themselves out of existence altogether – it is likely that his political career was at an end.\n",
      "In England, on July 24th, 1840, Sir Robert Peel resigned power because he did not like the Reform Bill. There is no doubt about this. A very clever man indeed might have persuaded the House of Commons to agree with him; but if Sir Robert had done that he would not have been a statesman – and it was as a statesman that his friends had most reason for pride. In spite of the many things which were said against him, he held his place until 1834: at last, when the Corn Laws were repealed, and the Tory party seemed to be ruined forever, there were only two men in all England who thought as he did; but those two were enough. They believed that they were right because God was with them: and though many people said so, too, it never really mattered what anybody else said or believed – so long as Sir Robert Peel's conscience was clear. He had made a mistake of his own free will in the past; he might make another mistake now – but it would not be a mistake in this particular case; for when God told him that there were only two men who agreed with him, they could say to each other, 'We are right, because Sir Robert Peel is wrong.' And so, without anybody knowing exactly what was going to happen next, the great Liberal party began to break up.\n",
      "As the months went on, however, the great Tory party also began to fall apart. The Duke of Wellington, who had never really wanted a Reform Bill in 1832, because he believed that if you put democracy into an omelette it would be spoilt for ever – did not want another Reform Bill now; and when Mr Gladstone made one, the Tory party split again. In 1846, therefore, there were two parties in England, neither of them strong enough to do anything important.\n",
      "A very clever man might have persuaded the House of Commons to agree with him.\n",
      "Sir Robert Peel's conscience was clear.\n",
      "The Duke of Wellington did not want another Reform Bill now; and when Mr Gladstone made one, the Tory party split again.\n",
      "# _Chapter IV_\n",
      "# REFORM AGAIN\n",
      "_A_ n English gentleman named Sir Robert Peel made a speech to the House of Commons on 27 June 1846. It was not at all an important speech – as you can see for yourself if you look in Mr Gladstone's _Gleanings from the Harvest Field of History._ But it happened to be just one week after Queen Victoria had been crowned, and she had already made a great many changes among her ministers; so perhaps she was thinking more about those people than about Sir Robert Peel. At any rate, on 7 July 1846 the Duke of Wellington resigned and Sir Robert Peel became Prime Minister once again.\n",
      "This time he took with him into his cabinet some men who were not Whigs but Liberals; so we must think of them as the new Liberal party instead of calling it the old Tory party – because, in spite of all their differences, they had more in common than Sir Robert Peel and his old friends. For one thing\n",
      "llama_print_timings:        load time =    3160.70 ms\n",
      "llama_print_timings:      sample time =     118.24 ms /  1024 runs   (    0.12 ms per token,  8660.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.96 ms /   494 tokens (    0.56 ms per token,  1790.10 tokens per second)\n",
      "llama_print_timings:        eval time =   15236.43 ms /  1023 runs   (   14.89 ms per token,    67.14 tokens per second)\n",
      "llama_print_timings:       total time =   15799.34 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2c6185c-75d7-4a75-914e-e198684b3c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295597\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   88.03 MiB\n",
      "llm_load_tensors: VRAM used           = 6936.01 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view; and for no other reason than that his mother had died an heretic. It is likely enough that these processes have been somewhat mollified in the intervening century and a quarter, but one thing at least remains unchanged, – the lot of man.\n",
      "A fairy tale, or, more properly, a wonder story, it can be read as a fable about our fears – the way we feel compelled to blame and shame ourselves for things that happen beyond our control.\n",
      "“… the best fantasy novel I’ve ever read …”\n",
      "“… one of those books that haunt you long after you’ve finished it. It’s terrifying, funny, heartbreaking and true.”\n",
      "— Stephen Fry\n",
      "“A timeless masterpiece that seamlessly blends the real with the imagined, this story will have readers turning page after page in an effort to find out if Little Nell will survive.”\n",
      "The New York Times’ Critics’ Top Books of 2018\n",
      "“Dickensian, it is true; but it is also something else: a story about how we tell stories. It has the epic scale that a classic needs.”\n",
      "“This book isn’t for everyone—but those who like it will love it… It’s not an easy read, yet you may well be tempted to read it more than once…”\n",
      "The 2019 Pulitzer Prize Winner\n",
      "Awarded in 1974. A special citation was awarded in lieu of the usual medal. “The Board desires to recognize Mr. Garner’s unique contribution to American letters. In his novels, short stories and plays he has given us an intimate view of the modern southern scene; the tragic and comedie aspects are revealed with the insight of a great novelist and with a keen understanding of people in all walks of life.”\n",
      "A masterpiece by one of the most acclaimed writers of our time. The American Book Award winner is the story of a young woman, set during the Depression and the early years of World War II, who has to find her way through a world where she has no place…. She will learn what it takes for a good person to survive.”\n",
      "“Few works of literature are as widely read and universally lauded as Harper Lee’s classic 1960 novel To Kill a Mockingbird. Told through the eyes of six-year-old Jean Louise Finch—better known as Scout—To Kill a Mockingbird follows three years in the life of a racially divided Alabama town in the Depression-era 1930s… Harper Lee’s masterful novel provides insight into the many ways that racism and prejudice can be passed from generation to generation.”\n",
      "Awarded in 1980. “For the imaginative, moving and entertaining story of a child’s coming-of-age within a black community in the rural South. The work is marked by clarity of perception and depth of feeling. It reveals both the harsh reality of life at the bottom of society and an abiding faith in the human capacity to prevail.”\n",
      "“As he did in To Kill a Mockingbird, Lee deftly brings together the clash of good and evil in this haunting first novel, which has become a modern American classic… A beautiful story that seems as relevant today as it was when it was written.” —People Magazine\n",
      "Awarded in 2016. “For her body of work, from the groundbreaking reportage in ‘Hell and High Water’ to the New Journalism novelistic style of ‘Fear of Flying,’ and for her brave forays into unspoken personal and cultural truths that have expanded the scope of American literature.”\n",
      "“Covering topics including death and mortality, marriage, travel, sex, and aging, Erica Jong’s work helped change our culture. She was one of the first writers to bring female sexuality to the forefront—and to show how it could be funny, sensual, and unapologetic.”\n",
      "“Written with passionate honesty, the novel established its author as a spokeswoman for her generation … The story of Isadora Wing is full of life, love, and wicked humor, but what makes this work so memorable are the characters… J\n",
      "llama_print_timings:        load time =    1290.18 ms\n",
      "llama_print_timings:      sample time =     122.79 ms /  1024 runs   (    0.12 ms per token,  8339.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.67 ms /   494 tokens (    0.56 ms per token,  1785.51 tokens per second)\n",
      "llama_print_timings:        eval time =   15260.97 ms /  1023 runs   (   14.92 ms per token,    67.03 tokens per second)\n",
      "llama_print_timings:       total time =   15833.00 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0b39cd7-6046-4f03-ab13-50efe29377c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295615\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   88.03 MiB\n",
      "llm_load_tensors: VRAM used           = 6936.01 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m \n",
      " Under the guidance of this exemplary personage, who possessed so intimate a knowledge of the British Constitution, there arose in all parts of the Kingdom, a strange popular movement, which threatened to give to one Englishman the same sort of control over forty-five millions of his fellow countrymen as Robespierre had acquired over twenty-eight millions of Frenchmen. The supreme power was offered to Mr. Cobbett; and he having peremptorily refused to accept it, the refusal was construed into a threat that he would take it. \n",
      "A royal proclamation was issued, appointing commissioners with large powers for putting down 'The Present Rebellion,' as it was called; and the public mind became so worked upon by lying reports and misrepresentations, that at length even Mr. Cobbett's most intimate friends doubted of his veracity. But he had too many virtues not to have some few devoted admirers; and these few were sufficient to give him a decided triumph over the host of those who believed in the popular delusion. \n",
      "On the whole, Mr. Cobbett's life is one which the most experienced politician might study with advantage: it shows how a single man may be the power and the terror of England.\n",
      "_Ipswich Journal_ , 10 October 1831\n",
      "THE MANNERS OF THE PEOPLE IN COURT AND OUT OF IT\n",
      "There are some men who, for particular purposes, must become acquainted with the manners and habits of the great, the good and the wise. For this purpose it is not necessary that they should associate with these classes; but there is a method by which any man may gain an insight into their mode of life, without either knowing or being known by them, if he has only the spirit to observe, and the talent of describing what he observes in others as though it were himself.\n",
      "Of this sort of observation I am now about to give some specimens: the subject is a very popular one; but it has been so much handled that I despair of adding any thing new. Indeed, no one who has not tried this species of description can form an adequate idea how very difficult and delicate it really is: for instance, a man of real talent in painting cannot gain a sufficient insight into the character of his subjects unless he has some knowledge of the arts that lead to success; but if he is destitute of such knowledge, and he attempts to describe the manner in which artists manage their affairs, he will either fail altogether, or make an absurdity of it.\n",
      "When I look back to the beginning of my career as a writer on this subject, I feel almost inclined to despair of ever succeeding in the attempt: but I am encouraged by observing that some persons seem to understand the difficulties which beset such an attempt; and I have good hope that even though I have failed, others may find out means of surmounting the difficulty.\n",
      "It has been said by a clever writer that, 'The manners are the customs and habits which distinguish each rank in life.' But this definition will not do: for a man may be well-bred, yet not well-mannered. I think manners to consist of more than being good-tempered, and giving good dinners; they must extend to dress and behaviour.\n",
      "A good-bred man is one who is accustomed to associate with good society, and to conform his manners to their habits: but this is not being well-mannered. He may be in a company where there is no rule for what to say or do; yet if he observes the mode of doing as other people do, he is considered well-mannered. A gentleman would never think himself well-mannered, merely because he conformed his manners to the habits of good society; and an ill-bred person would not think himself out of place in such a company: but if they should be taken into good society, where every thing is regulated by law, then their manners would either become well-bred or unfit for the world.\n",
      "We can understand why the word _manner_ should have the same meaning as _customs and habits:_ this is what I mean when I speak of manners; but it does not follow that a good-bred person is one who has good customs and habits, merely because he conforms to other people's customs and habits.\n",
      "The gentleman would not like to be taken out of his proper sphere into an uncivilized country; neither would the ill-bred person wish to be confined among gentlemen; but a well-mannered person will make himself at home in either situation, by adapting himself to what is expected from him.\n",
      "llama_print_timings:        load time =    1157.28 ms\n",
      "llama_print_timings:      sample time =     123.22 ms /  1024 runs   (    0.12 ms per token,  8310.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.97 ms /   494 tokens (    0.56 ms per token,  1777.19 tokens per second)\n",
      "llama_print_timings:        eval time =   15274.02 ms /  1023 runs   (   14.93 ms per token,    66.98 tokens per second)\n",
      "llama_print_timings:       total time =   15851.12 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb27f0-9bb9-4ee5-b7d6-617348a5e1d5",
   "metadata": {},
   "source": [
    "### 13B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9de77b-77e4-4b90-b88e-a4d54857a2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295632\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  312.64 MiB\n",
      "llm_load_tensors: VRAM used           = 24514.08 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 24989.09 MiB (model: 24514.08 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his nose torn off, and his body burned alive, for the crime of having plucked a few turnips out of a field on the property of a landlord who was an émigré. The chronicles of France, preserving the memory of these things, are very brief; but it may perhaps be traced in the great dictionary of science, Nature, that her agriculture and her cattle were less destroyed by this visitation than by any former scourge known in her history. \n",
      "Such was the state of public opinion at a period when Paris, under an enlightened government, established periodical fairs of books; where they sold, besides other works on education, many that expounded the method of teaching astronomy as well as geometry; and, indeed, one would have thought that some practical application of mathematical science to trade and traffic was not less desired in France than in any other country. But though these fairs were frequented by the learned in every branch of literature, yet there was at no time a more pernicious superstition than this; which arose partly from the false philosophy introduced into Europe on that eventful 10th of November, 1776. This superstition had crept over all countries with the rapidity of light, and made it an article of faith that whatever was to be discovered in any science was now known, or soon would be: a conclusion equally absurd as it is extravagant; for if there were one branch of knowledge on which we are least liable to fall into this error, that certainly is the one in question.\n",
      "To return, however, from this digression to my subject: it has been said before (and it must be repeated) that a man's first thoughts are always his best; and even though they do not come under the denomination of genius, which is usually applied to them, still they are the most natural and most likely to succeed. It may be further added, that these are not the offspring of study; for they will often occur on a subject, about which we have been long before thoughtful and studious, but in which we have never made any advances: nay, they come, as it were, from heaven, by inspiration. It is no wonder then that I have not arrived at the summit of my design in so short a time; for although my first thoughts on this subject have been many years ago, yet I can only say what was proper to be said and done upon them: it would have required more learning to have pursued the subject further.\n",
      "But if I have made no great progress with my first thoughts, they will nevertheless be useful in some respects, as I hope they will enable a workman, who is not able to think for himself, to execute an instrument which may give him information of what he has observed on any surface that might be employed. This being the case, and many valuable instruments having been made with only one micrometer, it must be granted that, in order to be complete, a double or a triple micrometer is necessary for the use of the observer; which has been the reason why so much trouble has been taken in bringing these tools to perfection. For this purpose I have not scrupled to make and try many instruments of various constructions, and sometimes even those of different powers, though with a very great waste both of my time and money: but I have never thought myself fully rewarded for all the pains which I have taken, till now that the reader shall find what it is in my power to perform.\n",
      "Delegation is an important aspect of the modern workplace and requires effective communication from top management down through every level of the organization. The project manager must clearly understand what she expects each team member to deliver, while simultaneously providing guidelines for completing their tasks in a timely fashion. For more information on delegating effectively and other leadership skills, consult our collection of Free Leadership Templates.\n",
      "For a more thorough look into this subject, we offer many books related to business administration and project management. We suggest these resources: Project Management for Small Business by Shannon Gowen; The PMI Guide to the PMBOK Guide (Third Edition) by Ricardo Vargas; The Power of Project Leadership 2nd Ed. by Robert K. Wysocki Jr.; Essential Skills For the Accidental Project Manager: How To Succeed At Your Job Even When You Weren't Promoted by Louis Efron and Mike Mullany.\n",
      "A project manager has to ensure that the entire workforce understands how to do their jobs and knows how each team member can contribute to the whole. For more\n",
      "llama_print_timings:        load time =   11088.82 ms\n",
      "llama_print_timings:      sample time =     120.95 ms /  1024 runs   (    0.12 ms per token,  8466.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.96 ms /   494 tokens (    0.42 ms per token,  2375.49 tokens per second)\n",
      "llama_print_timings:        eval time =   41496.06 ms /  1023 runs   (   40.56 ms per token,    24.65 tokens per second)\n",
      "llama_print_timings:       total time =   41999.15 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26d0f71c-80d8-411b-8b66-b62016f13ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295686\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  312.64 MiB\n",
      "llm_load_tensors: VRAM used           = 24514.08 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 24989.09 MiB (model: 24514.08 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down and called himself Catholic: at the sight of which barbarity, the whole civilized world shook the other day, with indignant horror, its locks of grey hair. But these were small matters in her eyes compared with the great discovery that it was much easier to make paper money than it was to earn it by labour and by trade: so she gave over, under this denouncement of a higher power, trying to do what had never yet been done, and resolved to employ the new method. \n",
      "Whether this enterprise met with the same success as another project of the French government, about eighteen hundred years ago, I have not learned: that was to cut down all the forests of the country, in order to build a large fleet; but it is still somewhat doubtful whether they were built. However, the fleets were built; and the French navy sailed away, under its own banners, from the harbour where it had so long been blockaded by the English invaders: and the sun shone upon the waves that were agitated but by the breath of native winds: for the first time in eight hundred years.\n",
      "But what was all this to him? He saw his country as one might see a distant island, where there was no rest nor comfort, but rather torment and sorrow; he thought of it only when his heart was moved with pity: or when the thought that they were his people came across his mind unawares.\n",
      "'How is my mother?' he said. 'Does she know that I am dead? She will grieve for me, if she ever learns from any one where I have fallen.'\n",
      "'Do not trouble yourself,' returned the other; 'my father has gone to England already with the tidings: and I was only waiting here until you had rested a little. He came down on his own account, to look after those who were wounded or sick, and he has been here some time. He has taken lodgings, near where my father's house is.'\n",
      "'Thank Heaven for that!' said Richard. 'You have saved me from being alone at such a time; but I should have died without hope, if he had not come to take care of my poor remains.'\n",
      "'Why don't you try?' rejoined the other. 'You look as well now as ever you did in your life.'\n",
      "Richard was astonished, and stared so hard at him that he said:\n",
      "'Do you know me?'\n",
      "'Of course I do!' returned the boy, looking equally surprised; 'what else should I think? You don't mean to say you have forgotten us so soon?'\n",
      "'No. But how did I come here, and why are you with me?'\n",
      "'Why, because I was your comrade in the old days,' he answered, laughing. 'That's my story. It seems a long time since we were boys together at home; but there has been so much to do, that I haven't had many idle moments.'\n",
      "He took hold of his hand again and drew him forward: then he stopped, and looked into Richard's eyes with something like reproach in their expression.\n",
      "'Come! come!' said Richard, as soon as the other moved again; 'let us go to my mother now!'\n",
      "'What is her name?' asked his guide. 'I know you are a Frenchman.'\n",
      "'Marie de Rochefort,' returned Richard, 'and that is where she lives. I have not forgotten.'\n",
      "He put out his hand towards the boy in eager haste and impatience: but he did not move from the spot until they reached the door; when he said:\n",
      "'Don't go too fast for my old legs!'\n",
      "The staircase was steep, and Richard waited with patience at the bottom, while his guide made it, going slowly up. Then he followed him, and went up one step after another. The boy turned round so often on these short flights that, in spite of all Richard's care, he always passed him with such a rush that they nearly tumbled over one another, more than once. When he reached the door above—which was closed—he stopped and knocked three times upon it; then opened it, and went in with his head, leaving Richard to follow.\n",
      "They found themselves on a dark landing, where there were several doors like their own, shut tight, and another staircase leading downwards into a cellar. Their guide, who had pushed the door a little back, but not wide enough for Richard to\n",
      "llama_print_timings:        load time =   10738.06 ms\n",
      "llama_print_timings:      sample time =     122.23 ms /  1024 runs   (    0.12 ms per token,  8377.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.67 ms /   494 tokens (    0.42 ms per token,  2390.33 tokens per second)\n",
      "llama_print_timings:        eval time =   41482.17 ms /  1023 runs   (   40.55 ms per token,    24.66 tokens per second)\n",
      "llama_print_timings:       total time =   41984.58 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "296ddacd-207d-46fc-a508-06cd08d08a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295740\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  312.64 MiB\n",
      "llm_load_tensors: VRAM used           = 24514.08 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 24989.09 MiB (model: 24514.08 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. \n",
      "It is remarkable that, though disused to the practice of taking human life, even for revenge and terror, we should never have had recourse to it for any new or dishonourable purpose: hence our common prudence disapproves of assassination; and thief-taking too, as a convenient mode of unjustly disposing of the subject. In short, murder considered as one of the social duties, has a worse appearance than cock-fighting.\n",
      "The last new thing in Europe was an English King; who was quite as bad as any other man that ever lived, and whose very tomb may be a questionable point in history. But the same England which can produce such a King, can also raise a people capable of doing what no people have done before them; this is the greatness of England's glory, that she has never made war upon earth without carrying along with her a conscientious principle to humanity and liberty: a principle not to be shaken by conquest, or degraded by defeat; but to live for ever in the history of the nation.\n",
      "This people had their own domestic convulsions, and threw away their hard-earned liberties again, as soon as they were recovered; but that was their business and not mine: I cannot be held accountable for other men's folly and wilfulness. It is a fact however, which ought to be recorded in history, and never forgotten by the English themselves—that every attempt made to restore this King has been marked by some new crime, some additional dishonour of that nation; and that all the efforts used by England for the purpose of supporting the usurper on the throne (a task not yet done) have been attended with bloodshed and oppression at home, and a total want of success abroad.\n",
      "The English King, and his minions, are in France at this moment; but it would be unjust to charge all England with those acts which are perpetrated by the few, and disowned by the many. In England itself there is now no legal tyranny. There is still an aristocracy, but their power has been greatly curtailed since they began to lose their popularity; it will continue to decrease, because they cannot help losing it. A people who have fought for liberty cannot love slavery: there are not many of the English who care about what a few call rank and dignity—it is called dignity by those who possess it, but is not dignified in itself. The great majority of England see that they can neither live without labour nor think without liberty; they have the true dignity of labour and the freedom of thought.\n",
      "The French King has lost his people, or else he would long ago have been dethroned. The French King has no aristocracy—that is, he has an army to protect him from his people: a kind of aristocracy very convenient for him; but it will be useless when the people shall be prepared to call their army to account.\n",
      "I think that England can learn much from France—it should not refuse its sympathy in times like these. The French are great in war, and I believe they will become so in peace—in justice, mercy, and honesty; but if England will not help them in these, she cannot expect to be helped when her own turn comes: it is only the idle and useless who imagine that they can live at ease without work.\n",
      "The people of France have suffered greatly from the tyranny of their aristocracy, for which the people now hold the King accountable; but he will not remain a prisoner in this way long, unless his friends keep him there—I wish them all good luck, and that they may have mercy on his enemies.\n",
      "In England the aristocracy has not been tyrannical: it has only oppressed the poor for its own interest, by robbing them of their wages; but at the same time it has given a great part of that money to the rich, by keeping up high rents and large taxes. The English people\n",
      "llama_print_timings:        load time =    4466.09 ms\n",
      "llama_print_timings:      sample time =     121.64 ms /  1024 runs   (    0.12 ms per token,  8418.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.89 ms /   494 tokens (    0.42 ms per token,  2376.30 tokens per second)\n",
      "llama_print_timings:        eval time =   41505.57 ms /  1023 runs   (   40.57 ms per token,    24.65 tokens per second)\n",
      "llama_print_timings:       total time =   42006.92 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab51db-d528-440d-ad00-e9f76d2b4b5d",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c306d621-040a-44e7-bb85-f7e64fcd3785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295787\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.21 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  114.46 MiB\n",
      "llm_load_tensors: VRAM used           = 17390.64 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his lips (thereby inflicting almost instantaneous death upon him), for speaking against a late-enacted dogma of the Church. It is nothing to us that the harmless trifler was hanged at an earlier hour than he otherwise might have been, had these enlightened ecclesiastics waited under their vines and fig trees until there was no getting out of the sentence: it is the eye-salve of a Parisian preacher, that we want. \n",
      " Hardly one year ago, revolutions and battlefields would have startled rather than soothed him in his day of moral disease. \n",
      " Thus, in the seventeenth year of Louis the well-beloved, the doctors prescribed blood-letting, leeches, purges, pukes, powders, and potions; and of emetic-purges, and starving and steaming; and of such like. \n",
      " Few strong men now alive had been born, when the foregoing paragraph was written. \n",
      " For, as to making out the case on Christian evidences, after fifteen hundred years of preaching, believing, burning, hanging, beheading, confiscating, and torturing, it was a mere question whether there was more faith or more infidelity in the world; – heresy was still as rife as ever it had been; and no faith so obdurate or sublime that martyrdom could not turn it into infidelity. \n",
      " As to what the people of England thought, the following scenes, which are sketches from real life, may form some idea: \n",
      " Scene the First. The time, a Sunday morning in London; the place, Westminster Abbey; the personages, Dr Pusey and Mr Beaumont. It is service-time at the Abbey, and a crowd of well-dressed people are flocking through the ancient porches: the band of the Coldstream Guards has just stopped playing in Birdcage Walk while a young officer in uniform hands out of his carriage the last of several girls and women. \n",
      " As they enter the Abbey, one or two observant persons may have noticed that Pusey is in attendance on Mr Beaumont as his private chaplain; but Pusey was so little regarded then that nobody could tell who he was without being told. He walked up the nave, not unconscious of the hundreds of eyes that were turned towards him by their possessors, but with the consciousness rather of what was in him than of what was on him. \n",
      "Mr Beaumont followed at some distance, walking with a stately slowness: his step had in it something more of an intellectual character than is usual with men of fashion. He wore black, and his hair was powdered. As he passed up the Abbey he drew out a prayer-book, and glanced round for the choir. The congregation was now filling rapidly; so rapid was the filling that the nave presented the appearance of having been already filled once: when the service began it was full again. \n",
      "The choristers came streaming through the doors in the screen which separated them from the church, and went to their seats in the choir, the organ having previously given out a prelude to the service. Mr Beaumont took his seat by one of the massive oaken pillars that supported the roof; he placed Pusey on the other side of him. The congregation were now seated, and the service proceeded. \n",
      "It was high mass: the altar had been decorated with crimson draperies which were held in place by numerous bosses of gilt. Two long rows of tall wax tapers stood before the crucifix; a few more at each side of the altar, and several along the rails, lighted up the gloom that hung over the vast building with a softened radiance which had its effect in increasing the general impression produced by the sight. The singing of the service was done by the choir, who, in their red cassocks and lace collars, were now seated in stalls placed along the screen between the pillars of the nave on either side. \n",
      "The congregation did not take part in the service excepting when they joined in a hymn. They listened to the responses of the choir with an air of reverence which showed that they were not insensible to the solemnity of the occasion, and the sublime nature of the ceremony. The whole scene was impressive;\n",
      "llama_print_timings:        load time =    7844.05 ms\n",
      "llama_print_timings:      sample time =     117.70 ms /  1024 runs   (    0.11 ms per token,  8700.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     619.33 ms /   494 tokens (    1.25 ms per token,   797.64 tokens per second)\n",
      "llama_print_timings:        eval time =   34423.19 ms /  1023 runs   (   33.65 ms per token,    29.72 tokens per second)\n",
      "llama_print_timings:       total time =   35330.35 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96cd60f7-7530-446a-8461-a099e73f3b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295831\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.21 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  114.46 MiB\n",
      "llm_load_tensors: VRAM used           = 17390.64 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the protection of her country's guns, though, the finest harvests, the most extravagant wine-crops of all Francoise, were garnered in \"the golden Valley of the Loire.\" \n",
      " CHAPTER III  \n",
      "The Night Shadows\n",
      "\"LOOK! LOOK!\" exclaimed little Lucie Manette, pointing at the rising sun. \"His very self coming up out of the earth, because he could not rest away from us. See how he brings us light! Father, is it a pretty picture?\" \n",
      " The iron door of the large courtyard was opened for the travellers by an uncouth old man with shaggy hair hanging down his shoulder and a bushy beard. Everything within the high stone walls of the prison was dank and dripping as in a day of thunderous weather, though it was a bright sunny morning without a cloud upon the blue sky. The footsteps of Lucie and her father echoed on the stones as if they rubbed against heaps of sulphur laid here and there; and when Mr. Lorry stopped to look round him, he wondered, for an instant, whether the building before which he stood were not a devils' castle. \n",
      " All the gaols in Paris (and they were many) had been made national gaols when the rush of prisoners began in August, and among them was a jail in Paris commonly called La Force. It was a grim strong fortress, the caverned entrance to which was defended by two columns of savage looking soldiers. Earlier in the year, Dr. Manette had been brought to the Tribunal out of La Force; and although he was not then in any personal danger, and was within a few weeks set at liberty, Lucie remembered — and now recalled with horror — that he had been thrown into a cell here. In trembling and terror she repressed a cry: which escaped her father, as they left the courtyard where they had entered: \n",
      " \"My dear daughter! I beg your pardon!\"\n",
      " They passed through an archway in a wall, and found themselves in a miserable corner of a miserable village, among miserable houses, without a single leaf upon a single tree — without a bird singing anywhere — without a glimpse of the sky. The ground was coarse with fragments of pottery and crockery which were flung there indiscriminately out of the neighbouring premises. There were lines of discoloured wash hanging across, and lines of sickly and sorry little children playing in it. There were some ugly features of old abandoned structures, and some misshapen places where heaps of refuse and cinders lay festering under the rays of a hot sun; there were walls and fences covered with placards that seemed as if they had grown upon them, promising wonderful bargains in retail, but never performing; there were rows of wretched tenements exactly alike, made out of triangular bits of seemingly impossible stuccoed stone, fast crumbling away; there were bars to obstruct the way, and gates to block it up like a dungeon; all was squalid with excessive poverty. \n",
      " \"Is it possible,\" said Lucie, \"that they will put him in this place?\" \n",
      " \"My dear, I am afraid it may be. But perhaps his case may not be urgent, and it may be some little time yet.\"\n",
      " They were at a place called the Barriere de la Roquette: where there once had been a roquethe field; but it had long become muddy waste ground. When they arrived here, Lucie was so tired and worn out, that nothing but the power of holding up her head, prevented her from sinking down in the street. \n",
      " They had to wait until some obstruction were overcome by a bullock-cart, moving under a team of a dozen or fourteen yielding horses. The drivers were all drunk, and were quarrelling how drunk they were; but that could not be helped, neither was it possible for any thing to come up to the wheel but post-horses; so there might have been a good deal more drunkenness without causing anybody to feel it. \n",
      " As the great heavy cart moved on again towards Paris, Lucie and Mr. Lorry got into the coach. A seat in the inside had fortunately been reserved for them by the driver under the guidance of Miss Pross, who had mounted up beside him. \n",
      " They went on through the ruined suburbs which so many similar ruined suburbs had had to pass, before they neared the heart of Paris. The wheels of the coach were deep in the reverse of the figure of eight; and\n",
      "llama_print_timings:        load time =    3419.34 ms\n",
      "llama_print_timings:      sample time =     119.20 ms /  1024 runs   (    0.12 ms per token,  8590.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     621.64 ms /   494 tokens (    1.26 ms per token,   794.67 tokens per second)\n",
      "llama_print_timings:        eval time =   34440.25 ms /  1023 runs   (   33.67 ms per token,    29.70 tokens per second)\n",
      "llama_print_timings:       total time =   35351.02 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffca4a61-421e-4071-8f3d-2b3bbff4796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295871\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.21 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  114.46 MiB\n",
      "llm_load_tensors: VRAM used           = 17390.64 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the protection of the country's judicious Government contracts were made for sanguine public edifices, to be erected in the ancient national form, out of the ruins of the Palace of the Tuileries, which a short time previously had been spectacularly burned down by a very large and thoroughly competent body called _La Garde Nationale_ , with a strong auxiliary force of all the French guards, citizen and soldiers, then known. \n",
      " In the year 1780, Mr. Jarvis Lorry, a traveller from Tellson's Bank, then newly arrived in Paris, and stopped in his travels, was waited upon by a person with whom he had transacted business some fifteen years before, and who now did him the honour to keep him company at dinner. As they sat over their wine, this person said: 'I need hardly ask you, M. Lorry, in the cause of your visit to us?' \n",
      " 'How do you do, Doctor Manette?' inquired Mr. Lorry, in his most affable and humble manner. \n",
      " 'I am well,' replied the doctor; 'and you?' \n",
      " 'I am very well,' said Mr. Lorry. \n",
      " `Very well!' repeated the doctor, with a singular emphasis on the word. 'That's well. I hope she is no worse than when I last saw her? There is no cause for uneasiness in that quarter?' \n",
      " `On the contrary,' said Mr. Lorry, 'that is one of our main grounds for satisfaction.'  \n",
      "`I am very glad to hear it. She was always a favourite with me; and I knew her father well: a most deserving man.' \n",
      " `I had heard so much of you,' said Mr. Lorry, in his most seductive manner, `that I took the liberty of writing to you when I was coming here, to assure myself that you were in your old neighbourhood; with some little hope that I might induce you to come over to us.' \n",
      " `I considered it, sir,' said the doctor, `and I am much obliged to you for your courteous remembrance of me. But I have a call upon me _there_ '--he pointed his hand over his shoulder at the street that he had just left.`I have a call there, which I must return in person.' \n",
      " `It is not,' said the doctor, with an air of excessive calmness, `a summons to die?' \n",
      " `You anticipate me,' said the other, with an undisturbed smile. `Yes. It is not a summons to die.'  \n",
      "`What then?' `My friendly neighbour here'--Darnay laid his hand upon the doctor's shoulder, and the doctor crossed his feet, as if he were (as in fact he was) ready for either eventuality:`I said this days ago. Let us have a little talk about it now.' \n",
      " Mr. Lorry took out of his desk two or three papers of old date; opened them; turned them over and read here and there:  `This is stiff work, sir,' said he, looking up from the task, with a curiously intent look upon him. `And when I add that I am not at all prepared for it, and that it comes upon me with an air of utter surprise, you will understand that I stand in need of your assistance.' \n",
      " The doctor sat upright, began to slip his fingers into his waistcoat pockets, and subsequently took them out again. He did not look as the doctor naturally would; but like one who was a little afraid. `You are young,' said Mr. Lorry, looking at him inquisitively;  `how do you come to be so experienced? You have never struck me as being particularly experienced. There has been nothing in your behaviour heretofore to lead one to suppose it. May I ask your age?'  \n",
      "`With all my heart,' answered Darnay, with a pleasant smile, and an air of perfect unconcern.  `I am not old. I may live to be older. Do you wish me to be older?' \n",
      " This question served as the restorer of Mr. Lorry's spirits, which had rather flagged; because it was absurd, and because he knew Charles Darnay to be at a loss for an answer to it. That gentleman, however, rose to the occasion with his usual readiness, though looking, as he did so, uncomfortably like a hanging judge.  `Not at all,' said Mr. Lorry.  `We wish you to be younger. The community is unanimous in presenting its compliments to you on your increasing youth\n",
      "llama_print_timings:        load time =    3093.40 ms\n",
      "llama_print_timings:      sample time =     121.67 ms /  1024 runs   (    0.12 ms per token,  8416.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     622.77 ms /   494 tokens (    1.26 ms per token,   793.23 tokens per second)\n",
      "llama_print_timings:        eval time =   34458.30 ms /  1023 runs   (   33.68 ms per token,    29.69 tokens per second)\n",
      "llama_print_timings:       total time =   35375.60 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b20ad-6a54-458f-a85a-f3cc3002e935",
   "metadata": {},
   "source": [
    "### 30B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dba52581-5544-4b97-85b8-457790a8b342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295910\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.21 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  406.46 MiB\n",
      "llm_load_tensors: VRAM used           = 61639.32 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      ".............................................................................\n",
      "CUDA error 2 at ggml-cuda.cu:9081: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9081: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b24fab-f886-498f-a9f5-d91bf3d5a189",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703295933\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.28 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  140.90 MiB\n",
      "llm_load_tensors: VRAM used           = 34950.11 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 36349.11 MiB (model: 34950.11 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrels of the Revolution.—But that could not be; the Woodman and the Farmer wept. \n",
      " To England through the Forest stole, at one time and another, an outcast here and there, from the oppression under which he groaned: sometimes in irons, sometimes maimed or wounded, sometimes in danger of recapture from those who were only less robbers than himself. With the taste of blood upon his mouth, with the sound of pursuit in his ears, he came—to assassinate a king, and to give away a crown. \n",
      " To England through the Forest stole an outlaw band, with wives and children born and bred in this existence of the open air. More desperate grew they in a country where so many other desperate men had gone before, and where the people pitied and fed them. Ever by unfrequented ways they kept together, under one Jack, whom some time back they had chosen for their leader—though he was but second best in all respects to the man he called his brother; ever among woods, or hiding on heaths, or skulking by hedgerows, making, with what weapons they could contrive to steal from isolated farmhouses which it was their pleasure and pastime to plunder now and then, a shift to live. Their way of life in the Forest brought them at last to that side of it which was nearest London. Near London! Then why should not the great heath they knew so well, where sheep were kept and young trees sheltered, lie as near to London? \n",
      " To England through the Forest stole a company of travelling players—very learned in their calling, very famous on country-stages far and wide,—and there were mummers too among them. The leader of these thespians (who was also the writer of their dramas, the composer of their music, the designer of their scenery, the conductor of their orchestra when they were lucky enough to have one), had two children with him closely bound to his art; a dark-eyed boy of about twelve years old, and a girl some three years younger.\n",
      "It was natural that these three outcast bands should come together in the Forest, not far from London—more natural still, that they should come together at that particular spot which Old Barnes was so careful to avoid. The Outlaws stole food where they could find it. Travelling players and mummers must have food and lodging when and how they can get it. They were sister trades, these two; they fell naturally upon each other's shoulders to cry and be consoled.\n",
      "The day was drawing to a close—the first of the two days appointed for the performance at the Fair. The actors and actresses sat together in their rude tent, eating and drinking, after the labors and disappointments of the morning. Among them was the little girl whom they had discovered on that very day, asleep by her dead father's side. She was the child of a strolling singer and dancer—a woman known to many members of the company, who had now all taken to the girl and adopted her among them; she lay fast asleep at their feet.\n",
      "Suddenly there came a knocking at the tent door: not loud or sudden enough to have startled the child if she had been less weary than she was that night. One of the men looked out, and saw in the gathering twilight Old Barnes—who made some signs entreating him to come without noise to the door. When they stood together outside on the grass, these were the words the\n",
      "llama_print_timings:        load time =   16224.56 ms\n",
      "llama_print_timings:      sample time =     118.66 ms /  1024 runs   (    0.12 ms per token,  8629.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1162.99 ms /   494 tokens (    2.35 ms per token,   424.77 tokens per second)\n",
      "llama_print_timings:        eval time =   65151.78 ms /  1023 runs   (   63.69 ms per token,    15.70 tokens per second)\n",
      "llama_print_timings:       total time =   66604.47 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64c8c8e6-c07d-4854-8206-b2f9321349c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703296017\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.28 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  140.90 MiB\n",
      "llm_load_tensors: VRAM used           = 34950.11 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 36349.11 MiB (model: 34950.11 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrels of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous. In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of “the Captain,” pleasantly excused himself: saying, that he had not been at home for a week, had been to Cheltenham races, must sell out, by G—, for money was terribly scarce, and hoped Mr. So-and-so would forgive him, and drink his health! However, the Stocks went empty, the Gaols were very full, and Newgate was alive with creatures who attained the utmost perfection of their calling, and did their work with an artistic interest in it which perhaps had never been surpassed.\n",
      "In those times there was often no communication between the great and the lowly but at the points of a rough-sword or a sharp-pointed spear; between the knowledge that was stored up beyond the wall, and the wisdom that was daily being enacted on this side of it—no, not so much as the quick clasp of an honest hand, or the light touch of a brother’s in sign of amity.\n",
      "A man who moved in any well-known rank of life had small chance for his bread, if he took credit when he wanted it—at least if he took too much. In this respect London was more backward than Paris, where a fourth-rate clerk with a grisette on one arm, and a bundle of newspapers under the other, had the privileges of the Palais Royal; or Saint Petersburg, where certain drunken coachmen brawled and lived luxuriously, for whom the guardian genius of England would have invented no better provision than the treadmill. Hence it was that a gulf almost impassable by honest credit, divided the well-to-do man from the poor man in London. There were two or three bridges across this gulf—a very shaky one called patronage, upon which certain impecunious youths with small incomes and no particular use for them, earned the popular character of being “friendly” with their dependents; a good stout timber bridge called advertisement, of immense solidity and some cash outlay, on which not a few toiling aspirants crossed over into Easy Street; and lastly, a very long, very narrow, and very insecure one called the Profession of the Law—over which no less than three thousand persons annually fell into the gulf below: while it was ticklish work even for the successful few, who gained a temporary footing now and then by some lucky chance in the political world; and were seen in court or senate shining as members of parliament, crown prosecutors, king’s counsel learned in the law.\n",
      "On this Profession of the Law, nevertheless, early in life Walter Wilkins entered; not with a\n",
      "llama_print_timings:        load time =   16202.40 ms\n",
      "llama_print_timings:      sample time =     122.17 ms /  1024 runs   (    0.12 ms per token,  8381.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1172.02 ms /   494 tokens (    2.37 ms per token,   421.50 tokens per second)\n",
      "llama_print_timings:        eval time =   65232.36 ms /  1023 runs   (   63.77 ms per token,    15.68 tokens per second)\n",
      "llama_print_timings:       total time =   66702.21 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1144341a-84f1-43f2-9688-e2b528401776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703296102\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.28 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  140.90 MiB\n",
      "llm_load_tensors: VRAM used           = 34950.11 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 36349.11 MiB (model: 34950.11 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be laden with far other dust than the dust of barley-ears, and drawn, not by a horse, but by a strange figure, half beast and half man. But that woodman and that tiller, saw nothing of the sort that day; and many a winter and many a summer did many a one of the carts, laden with its homely cargo, trundle out of the rutty yard on the highway, in the forenoon, or come jingling home from market at nightfall, before the Woodman or the Farmer met the other Carrier whom they sought.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the road-agents took pride in dressing gaily, and flaunted on the high-roads in the light of day, when they pleased; even in church, ladies’ pockets were picked by dexterous apes in velvet suits and furred cloaks. So little confidence was felt in these matters that announcements were made from pulpits inviting the co-operation of charitably disposed persons in the apprehension of suspicious characters who frequented places of worship; and rewards, often attaining to fifty pounds sterling were offered by Government itself for information concerning offenders, or for a successful capture. These tended greatly to elucidate such mysteries and to reduce the number of criminals. A few of the knots then active in London, who are still remembered here for organising the plunder of wealthy passengers brought from India by East Indiamen, and dividing their spoil equitably among themselves, were taken, tried, executed, and hung in chains; and some quietness ensued. But the state of the town was a disgrace to civilisation: ancient people tottered into court, from day to day, to prosecute for housebreaking burglars little less decrepit than themselves; families went into mourning for cousins robbed and murdered, posting on the Great North Road; bold highwaymen opened fire on the King’s mail in the very suburbs of the town, and were sometimes hurt or taken; musqueteers were placed at Hampstead to repel foragers from London: and these things came to pass occasionally within a few miles, and not many years ago.\n",
      "\n",
      "In the year 1734 a convicted prisoner sentenced to death was allowed, according to an old custom, to nominate a fellow-prisoner as companion in his dying hour. He selected James Dalton from among several then under sentence at Newgate, and during their confinement they occupied cells on the same floor. On being taken into the condemned pew the doomed pair conversed together with great cheerfulness, but no allusion was made to the subject of religion until a few minutes before execution, when the minister asked them if there were anything in which he could serve them. Dalton answered that they wanted nothing from man; that their sins were forgiven and their names written in heaven; and then, as an assurance of his peace and reconciliation with God, burst into a song of praise so loud as to be heard by the people without. At this time the bell began tolling and the cart was driven out of the prison through the crowd assembled at Newgate Street, where it halted\n",
      "llama_print_timings:        load time =    7814.28 ms\n",
      "llama_print_timings:      sample time =     118.38 ms /  1024 runs   (    0.12 ms per token,  8650.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1181.13 ms /   494 tokens (    2.39 ms per token,   418.24 tokens per second)\n",
      "llama_print_timings:        eval time =   65241.93 ms /  1023 runs   (   63.78 ms per token,    15.68 tokens per second)\n",
      "llama_print_timings:       total time =   66711.82 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7746617-b2b3-4f38-aeb0-8da6ce0b9c87",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a0eddbb-b168-4e9a-a79b-9f257af21cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703296177\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.28 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  500.28 MiB\n",
      "llm_load_tensors: VRAM used           = 124025.03 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      ".......................................\n",
      "CUDA error 2 at ggml-cuda.cu:9081: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9081: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
