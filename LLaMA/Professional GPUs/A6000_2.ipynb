{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 04:38:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:44:00.0 Off |                  Off |\n",
      "| 30%   31C    P8    21W / 300W |      0MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:83:00.0 Off |                  Off |\n",
      "| 30%   31C    P8    31W / 300W |      0MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "model name\t: AMD EPYC 7343 16-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       528228944 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 4916, done.\u001b[K\n",
      "remote: Counting objects: 100% (1709/1709), done.\u001b[K\n",
      "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
      "remote: Total 4916 (delta 1640), reused 1596 (delta 1581), pack-reused 3207\u001b[K\n",
      "Receiving objects: 100% (4916/4916), 4.09 MiB | 6.54 MiB/s, done.\n",
      "Resolving deltas: 100% (3364/3364), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53aa7f97-4a2e-47ab-8c66-ca34489ec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f4126f1-7c42-4d66-8a7d-8e5c29c322da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2eaea4f0-2d91-41ef-9f32-4103f2858796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   8901      0 --:--:-- --:--:-- --:--:--  8899\n",
      "Downloading tokenizer\n",
      "--2023-07-17 04:38:59--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2023-07-17 04:39:00 (103 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-07-17 04:39:00--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:39:00 (85.1 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-07-17 04:39:00--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated.  74%[=============>      ]   9.38G  42.3MB/s    in 5m 3s   \n",
      "\n",
      "2023-07-17 04:44:04 (31.7 MB/s) - Connection closed at byte 10074718208. Retrying.\n",
      "\n",
      "--2023-07-17 04:44:05--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 13476939516 (13G), 3402221308 (3.2G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[++++++++++++++=====>]  12.55G  17.2MB/s    in 2m 11s  \n",
      "\n",
      "2023-07-17 04:46:16 (24.7 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-07-17 04:46:16--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:46:17 (29.3 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 04:46:17--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:46:17 (118 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-07-17 04:46:37--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  45.7MB/s    in 3m 5s   \n",
      "\n",
      "2023-07-17 04:49:42 (67.2 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 04:49:42--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  35.7MB/s    in 7m 51s  \n",
      "\n",
      "2023-07-17 04:57:34 (26.4 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 04:57:34--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:57:34 (7.51 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 04:57:34--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:57:35 (191 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-07-17 04:58:13--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated  17%[==>                 ]   2.69G  15.0MB/s    in 58s     \n",
      "\n",
      "2023-07-17 04:59:11 (47.5 MB/s) - Connection closed at byte 2888105984. Retrying.\n",
      "\n",
      "--2023-07-17 04:59:12--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16265763099 (15G), 13377657115 (12G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[+++================>]  15.15G  38.0MB/s    in 3m 39s  \n",
      "\n",
      "2023-07-17 05:02:52 (58.3 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 05:02:52--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  77.1MB/s    in 3m 47s  \n",
      "\n",
      "2023-07-17 05:06:39 (68.5 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 05:06:39--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  22.5MB/s    in 5m 2s   \n",
      "\n",
      "2023-07-17 05:11:41 (51.4 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 05:11:41--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  80.1MB/s    in 3m 37s  \n",
      "\n",
      "2023-07-17 05:15:18 (71.6 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 05:15:18--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 05:15:19 (36.6 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 05:15:19--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 05:15:19 (320 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-07-17 05:16:50--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  64.7MB/s    in 3m 43s  \n",
      "\n",
      "2023-07-17 05:20:34 (69.7 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:20:34--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  64.5MB/s    in 4m 28s  \n",
      "\n",
      "2023-07-17 05:25:02 (58.2 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:25:02--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  49.3MB/s    in 5m 18s  \n",
      "\n",
      "2023-07-17 05:30:21 (49.0 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:30:21--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  43.0MB/s    in 3m 56s  \n",
      "\n",
      "2023-07-17 05:34:17 (66.1 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:34:17--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  58.4MB/s    in 3m 41s  \n",
      "\n",
      "2023-07-17 05:37:58 (70.3 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:37:58--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  25.4MB/s    in 5m 6s   \n",
      "\n",
      "2023-07-17 05:43:05 (50.8 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:43:05--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  56.8MB/s    in 4m 51s  \n",
      "\n",
      "2023-07-17 05:47:56 (53.6 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:47:56--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated  12%[=>                  ]   1.97G  65.8MB/s    in 34s     \n",
      "\n",
      "2023-07-17 05:48:31 (58.8 MB/s) - Connection closed at byte 2114453504. Retrying.\n",
      "\n",
      "--2023-07-17 05:48:32--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16323959449 (15G), 14209505945 (13G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[++=================>]  15.20G  60.1MB/s    in 4m 28s  \n",
      "\n",
      "2023-07-17 05:53:00 (50.6 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:53:00--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 05:53:00 (37.5 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 05:53:00--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 05:53:01 (608 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed numpy-1.24.0 sentencepiece-0.1.98\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:4096 n_mult:256 n_head:32 n_layer:32\n",
      "Writing vocab...\n",
      "[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/7B/ggml-model-f16.bin\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:5120 n_mult:256 n_head:40 n_layer:40\n",
      "Writing vocab...\n",
      "[  1/363] Writing tensor tok_embeddings.weight                  | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  2/363] Writing tensor norm.weight                            | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  4/363] Writing tensor layers.0.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  5/363] Writing tensor layers.0.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  6/363] Writing tensor layers.0.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  7/363] Writing tensor layers.0.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  8/363] Writing tensor layers.0.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  9/363] Writing tensor layers.0.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 10/363] Writing tensor layers.0.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 11/363] Writing tensor layers.0.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 12/363] Writing tensor layers.0.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 13/363] Writing tensor layers.1.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 14/363] Writing tensor layers.1.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 15/363] Writing tensor layers.1.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 16/363] Writing tensor layers.1.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 17/363] Writing tensor layers.1.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 18/363] Writing tensor layers.1.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 19/363] Writing tensor layers.1.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 20/363] Writing tensor layers.1.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 21/363] Writing tensor layers.1.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 22/363] Writing tensor layers.2.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 23/363] Writing tensor layers.2.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 24/363] Writing tensor layers.2.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 25/363] Writing tensor layers.2.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 26/363] Writing tensor layers.2.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 27/363] Writing tensor layers.2.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 28/363] Writing tensor layers.2.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 29/363] Writing tensor layers.2.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 30/363] Writing tensor layers.2.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 31/363] Writing tensor layers.3.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 32/363] Writing tensor layers.3.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 33/363] Writing tensor layers.3.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 34/363] Writing tensor layers.3.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 35/363] Writing tensor layers.3.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 36/363] Writing tensor layers.3.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 37/363] Writing tensor layers.3.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 38/363] Writing tensor layers.3.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 39/363] Writing tensor layers.3.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 40/363] Writing tensor layers.4.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 41/363] Writing tensor layers.4.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 42/363] Writing tensor layers.4.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 43/363] Writing tensor layers.4.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 44/363] Writing tensor layers.4.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 45/363] Writing tensor layers.4.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 46/363] Writing tensor layers.4.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 47/363] Writing tensor layers.4.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 48/363] Writing tensor layers.4.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 49/363] Writing tensor layers.5.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 50/363] Writing tensor layers.5.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 51/363] Writing tensor layers.5.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 52/363] Writing tensor layers.5.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 53/363] Writing tensor layers.5.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 54/363] Writing tensor layers.5.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 55/363] Writing tensor layers.5.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 56/363] Writing tensor layers.5.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 57/363] Writing tensor layers.5.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 58/363] Writing tensor layers.6.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 59/363] Writing tensor layers.6.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 60/363] Writing tensor layers.6.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 61/363] Writing tensor layers.6.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 62/363] Writing tensor layers.6.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 63/363] Writing tensor layers.6.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 64/363] Writing tensor layers.6.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 65/363] Writing tensor layers.6.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 66/363] Writing tensor layers.6.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 67/363] Writing tensor layers.7.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 68/363] Writing tensor layers.7.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 69/363] Writing tensor layers.7.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 70/363] Writing tensor layers.7.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 71/363] Writing tensor layers.7.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 72/363] Writing tensor layers.7.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 73/363] Writing tensor layers.7.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 74/363] Writing tensor layers.7.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 75/363] Writing tensor layers.7.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 76/363] Writing tensor layers.8.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 77/363] Writing tensor layers.8.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 78/363] Writing tensor layers.8.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 79/363] Writing tensor layers.8.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 80/363] Writing tensor layers.8.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 81/363] Writing tensor layers.8.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 82/363] Writing tensor layers.8.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 83/363] Writing tensor layers.8.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 84/363] Writing tensor layers.8.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 85/363] Writing tensor layers.9.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 86/363] Writing tensor layers.9.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 87/363] Writing tensor layers.9.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 88/363] Writing tensor layers.9.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 89/363] Writing tensor layers.9.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 90/363] Writing tensor layers.9.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 91/363] Writing tensor layers.9.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 92/363] Writing tensor layers.9.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 93/363] Writing tensor layers.9.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 94/363] Writing tensor layers.10.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 95/363] Writing tensor layers.10.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 96/363] Writing tensor layers.10.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 97/363] Writing tensor layers.10.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 98/363] Writing tensor layers.10.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 99/363] Writing tensor layers.10.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[100/363] Writing tensor layers.10.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[101/363] Writing tensor layers.10.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[102/363] Writing tensor layers.10.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[103/363] Writing tensor layers.11.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[104/363] Writing tensor layers.11.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[105/363] Writing tensor layers.11.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[106/363] Writing tensor layers.11.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[107/363] Writing tensor layers.11.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[108/363] Writing tensor layers.11.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[109/363] Writing tensor layers.11.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[110/363] Writing tensor layers.11.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[111/363] Writing tensor layers.11.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[112/363] Writing tensor layers.12.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[113/363] Writing tensor layers.12.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[114/363] Writing tensor layers.12.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[115/363] Writing tensor layers.12.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[116/363] Writing tensor layers.12.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[117/363] Writing tensor layers.12.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[118/363] Writing tensor layers.12.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[119/363] Writing tensor layers.12.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[120/363] Writing tensor layers.12.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[121/363] Writing tensor layers.13.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[122/363] Writing tensor layers.13.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[123/363] Writing tensor layers.13.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[124/363] Writing tensor layers.13.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[125/363] Writing tensor layers.13.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[126/363] Writing tensor layers.13.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[127/363] Writing tensor layers.13.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[128/363] Writing tensor layers.13.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[129/363] Writing tensor layers.13.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[130/363] Writing tensor layers.14.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[131/363] Writing tensor layers.14.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[132/363] Writing tensor layers.14.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[133/363] Writing tensor layers.14.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[134/363] Writing tensor layers.14.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[135/363] Writing tensor layers.14.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[136/363] Writing tensor layers.14.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[137/363] Writing tensor layers.14.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[138/363] Writing tensor layers.14.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[139/363] Writing tensor layers.15.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[140/363] Writing tensor layers.15.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[141/363] Writing tensor layers.15.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[142/363] Writing tensor layers.15.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[143/363] Writing tensor layers.15.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[144/363] Writing tensor layers.15.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[145/363] Writing tensor layers.15.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[146/363] Writing tensor layers.15.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[147/363] Writing tensor layers.15.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[148/363] Writing tensor layers.16.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[149/363] Writing tensor layers.16.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[150/363] Writing tensor layers.16.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[151/363] Writing tensor layers.16.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[152/363] Writing tensor layers.16.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[153/363] Writing tensor layers.16.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[154/363] Writing tensor layers.16.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[155/363] Writing tensor layers.16.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[156/363] Writing tensor layers.16.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[157/363] Writing tensor layers.17.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[158/363] Writing tensor layers.17.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[159/363] Writing tensor layers.17.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[160/363] Writing tensor layers.17.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[161/363] Writing tensor layers.17.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[162/363] Writing tensor layers.17.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[163/363] Writing tensor layers.17.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[164/363] Writing tensor layers.17.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[165/363] Writing tensor layers.17.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[166/363] Writing tensor layers.18.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[167/363] Writing tensor layers.18.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[168/363] Writing tensor layers.18.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[169/363] Writing tensor layers.18.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[170/363] Writing tensor layers.18.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[171/363] Writing tensor layers.18.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[172/363] Writing tensor layers.18.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[173/363] Writing tensor layers.18.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[174/363] Writing tensor layers.18.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[175/363] Writing tensor layers.19.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[176/363] Writing tensor layers.19.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[177/363] Writing tensor layers.19.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[178/363] Writing tensor layers.19.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[179/363] Writing tensor layers.19.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[180/363] Writing tensor layers.19.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[181/363] Writing tensor layers.19.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[182/363] Writing tensor layers.19.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[183/363] Writing tensor layers.19.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[184/363] Writing tensor layers.20.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[185/363] Writing tensor layers.20.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[186/363] Writing tensor layers.20.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[187/363] Writing tensor layers.20.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[188/363] Writing tensor layers.20.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[189/363] Writing tensor layers.20.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[190/363] Writing tensor layers.20.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[191/363] Writing tensor layers.20.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[192/363] Writing tensor layers.20.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[193/363] Writing tensor layers.21.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[194/363] Writing tensor layers.21.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[195/363] Writing tensor layers.21.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[196/363] Writing tensor layers.21.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[197/363] Writing tensor layers.21.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[198/363] Writing tensor layers.21.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[199/363] Writing tensor layers.21.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[200/363] Writing tensor layers.21.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[201/363] Writing tensor layers.21.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[202/363] Writing tensor layers.22.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[203/363] Writing tensor layers.22.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[204/363] Writing tensor layers.22.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[205/363] Writing tensor layers.22.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[206/363] Writing tensor layers.22.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[207/363] Writing tensor layers.22.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[208/363] Writing tensor layers.22.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[209/363] Writing tensor layers.22.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[210/363] Writing tensor layers.22.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[211/363] Writing tensor layers.23.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[212/363] Writing tensor layers.23.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[213/363] Writing tensor layers.23.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[214/363] Writing tensor layers.23.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[215/363] Writing tensor layers.23.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[216/363] Writing tensor layers.23.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[217/363] Writing tensor layers.23.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[218/363] Writing tensor layers.23.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[219/363] Writing tensor layers.23.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[220/363] Writing tensor layers.24.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[221/363] Writing tensor layers.24.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[222/363] Writing tensor layers.24.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[223/363] Writing tensor layers.24.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[224/363] Writing tensor layers.24.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[225/363] Writing tensor layers.24.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[226/363] Writing tensor layers.24.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[227/363] Writing tensor layers.24.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[228/363] Writing tensor layers.24.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[229/363] Writing tensor layers.25.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[230/363] Writing tensor layers.25.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[231/363] Writing tensor layers.25.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[232/363] Writing tensor layers.25.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[233/363] Writing tensor layers.25.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[234/363] Writing tensor layers.25.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[235/363] Writing tensor layers.25.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[236/363] Writing tensor layers.25.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[237/363] Writing tensor layers.25.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[238/363] Writing tensor layers.26.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[239/363] Writing tensor layers.26.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[240/363] Writing tensor layers.26.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[241/363] Writing tensor layers.26.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[242/363] Writing tensor layers.26.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[243/363] Writing tensor layers.26.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[244/363] Writing tensor layers.26.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[245/363] Writing tensor layers.26.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[246/363] Writing tensor layers.26.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[247/363] Writing tensor layers.27.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[248/363] Writing tensor layers.27.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[249/363] Writing tensor layers.27.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[250/363] Writing tensor layers.27.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[251/363] Writing tensor layers.27.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[252/363] Writing tensor layers.27.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[253/363] Writing tensor layers.27.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[254/363] Writing tensor layers.27.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[255/363] Writing tensor layers.27.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[256/363] Writing tensor layers.28.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[257/363] Writing tensor layers.28.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[258/363] Writing tensor layers.28.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[259/363] Writing tensor layers.28.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[260/363] Writing tensor layers.28.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[261/363] Writing tensor layers.28.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[262/363] Writing tensor layers.28.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[263/363] Writing tensor layers.28.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[264/363] Writing tensor layers.28.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[265/363] Writing tensor layers.29.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[266/363] Writing tensor layers.29.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[267/363] Writing tensor layers.29.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[268/363] Writing tensor layers.29.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[269/363] Writing tensor layers.29.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[270/363] Writing tensor layers.29.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[271/363] Writing tensor layers.29.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[272/363] Writing tensor layers.29.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[273/363] Writing tensor layers.29.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[274/363] Writing tensor layers.30.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[275/363] Writing tensor layers.30.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[276/363] Writing tensor layers.30.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[277/363] Writing tensor layers.30.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[278/363] Writing tensor layers.30.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[279/363] Writing tensor layers.30.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[280/363] Writing tensor layers.30.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[281/363] Writing tensor layers.30.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[282/363] Writing tensor layers.30.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[283/363] Writing tensor layers.31.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[284/363] Writing tensor layers.31.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[285/363] Writing tensor layers.31.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[286/363] Writing tensor layers.31.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[287/363] Writing tensor layers.31.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[288/363] Writing tensor layers.31.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[289/363] Writing tensor layers.31.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[290/363] Writing tensor layers.31.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[291/363] Writing tensor layers.31.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[292/363] Writing tensor layers.32.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[293/363] Writing tensor layers.32.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[294/363] Writing tensor layers.32.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[295/363] Writing tensor layers.32.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[296/363] Writing tensor layers.32.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[297/363] Writing tensor layers.32.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[298/363] Writing tensor layers.32.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[299/363] Writing tensor layers.32.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[300/363] Writing tensor layers.32.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[301/363] Writing tensor layers.33.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[302/363] Writing tensor layers.33.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[303/363] Writing tensor layers.33.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[304/363] Writing tensor layers.33.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[305/363] Writing tensor layers.33.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[306/363] Writing tensor layers.33.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[307/363] Writing tensor layers.33.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[308/363] Writing tensor layers.33.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[309/363] Writing tensor layers.33.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[310/363] Writing tensor layers.34.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[311/363] Writing tensor layers.34.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[312/363] Writing tensor layers.34.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[313/363] Writing tensor layers.34.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[314/363] Writing tensor layers.34.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[315/363] Writing tensor layers.34.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[316/363] Writing tensor layers.34.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[317/363] Writing tensor layers.34.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[318/363] Writing tensor layers.34.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[319/363] Writing tensor layers.35.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[320/363] Writing tensor layers.35.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[321/363] Writing tensor layers.35.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[322/363] Writing tensor layers.35.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[323/363] Writing tensor layers.35.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[324/363] Writing tensor layers.35.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[325/363] Writing tensor layers.35.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[326/363] Writing tensor layers.35.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[327/363] Writing tensor layers.35.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[328/363] Writing tensor layers.36.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[329/363] Writing tensor layers.36.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[330/363] Writing tensor layers.36.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[331/363] Writing tensor layers.36.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[332/363] Writing tensor layers.36.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[333/363] Writing tensor layers.36.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[334/363] Writing tensor layers.36.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[335/363] Writing tensor layers.36.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[336/363] Writing tensor layers.36.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[337/363] Writing tensor layers.37.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[338/363] Writing tensor layers.37.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[339/363] Writing tensor layers.37.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[340/363] Writing tensor layers.37.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[341/363] Writing tensor layers.37.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[342/363] Writing tensor layers.37.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[343/363] Writing tensor layers.37.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[344/363] Writing tensor layers.37.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[345/363] Writing tensor layers.37.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[346/363] Writing tensor layers.38.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[347/363] Writing tensor layers.38.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[348/363] Writing tensor layers.38.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[349/363] Writing tensor layers.38.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[350/363] Writing tensor layers.38.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[351/363] Writing tensor layers.38.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[352/363] Writing tensor layers.38.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[353/363] Writing tensor layers.38.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[354/363] Writing tensor layers.38.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[355/363] Writing tensor layers.39.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[356/363] Writing tensor layers.39.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[357/363] Writing tensor layers.39.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[358/363] Writing tensor layers.39.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[359/363] Writing tensor layers.39.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[360/363] Writing tensor layers.39.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[361/363] Writing tensor layers.39.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[362/363] Writing tensor layers.39.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[363/363] Writing tensor layers.39.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/13B/ggml-model-f16.bin\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:6656 n_mult:256 n_head:52 n_layer:60\n",
      "Writing vocab...\n",
      "[  1/543] Writing tensor tok_embeddings.weight                  | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  2/543] Writing tensor norm.weight                            | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  4/543] Writing tensor layers.0.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  5/543] Writing tensor layers.0.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  6/543] Writing tensor layers.0.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  7/543] Writing tensor layers.0.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  8/543] Writing tensor layers.0.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  9/543] Writing tensor layers.0.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 10/543] Writing tensor layers.0.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 11/543] Writing tensor layers.0.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 12/543] Writing tensor layers.0.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 13/543] Writing tensor layers.1.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 14/543] Writing tensor layers.1.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 15/543] Writing tensor layers.1.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 16/543] Writing tensor layers.1.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 17/543] Writing tensor layers.1.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 18/543] Writing tensor layers.1.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 19/543] Writing tensor layers.1.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 20/543] Writing tensor layers.1.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 21/543] Writing tensor layers.1.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 22/543] Writing tensor layers.2.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 23/543] Writing tensor layers.2.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 24/543] Writing tensor layers.2.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 25/543] Writing tensor layers.2.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 26/543] Writing tensor layers.2.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 27/543] Writing tensor layers.2.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 28/543] Writing tensor layers.2.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 29/543] Writing tensor layers.2.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 30/543] Writing tensor layers.2.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 31/543] Writing tensor layers.3.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 32/543] Writing tensor layers.3.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 33/543] Writing tensor layers.3.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 34/543] Writing tensor layers.3.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 35/543] Writing tensor layers.3.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 36/543] Writing tensor layers.3.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 37/543] Writing tensor layers.3.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 38/543] Writing tensor layers.3.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 39/543] Writing tensor layers.3.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 40/543] Writing tensor layers.4.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 41/543] Writing tensor layers.4.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 42/543] Writing tensor layers.4.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 43/543] Writing tensor layers.4.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 44/543] Writing tensor layers.4.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 45/543] Writing tensor layers.4.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 46/543] Writing tensor layers.4.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 47/543] Writing tensor layers.4.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 48/543] Writing tensor layers.4.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 49/543] Writing tensor layers.5.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 50/543] Writing tensor layers.5.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 51/543] Writing tensor layers.5.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 52/543] Writing tensor layers.5.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 53/543] Writing tensor layers.5.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 54/543] Writing tensor layers.5.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 55/543] Writing tensor layers.5.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 56/543] Writing tensor layers.5.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 57/543] Writing tensor layers.5.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 58/543] Writing tensor layers.6.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 59/543] Writing tensor layers.6.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 60/543] Writing tensor layers.6.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 61/543] Writing tensor layers.6.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 62/543] Writing tensor layers.6.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 63/543] Writing tensor layers.6.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 64/543] Writing tensor layers.6.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 65/543] Writing tensor layers.6.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 66/543] Writing tensor layers.6.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 67/543] Writing tensor layers.7.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 68/543] Writing tensor layers.7.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 69/543] Writing tensor layers.7.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 70/543] Writing tensor layers.7.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 71/543] Writing tensor layers.7.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 72/543] Writing tensor layers.7.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 73/543] Writing tensor layers.7.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 74/543] Writing tensor layers.7.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 75/543] Writing tensor layers.7.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 76/543] Writing tensor layers.8.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 77/543] Writing tensor layers.8.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 78/543] Writing tensor layers.8.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 79/543] Writing tensor layers.8.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 80/543] Writing tensor layers.8.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 81/543] Writing tensor layers.8.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 82/543] Writing tensor layers.8.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 83/543] Writing tensor layers.8.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 84/543] Writing tensor layers.8.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 85/543] Writing tensor layers.9.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 86/543] Writing tensor layers.9.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 87/543] Writing tensor layers.9.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 88/543] Writing tensor layers.9.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 89/543] Writing tensor layers.9.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 90/543] Writing tensor layers.9.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 91/543] Writing tensor layers.9.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 92/543] Writing tensor layers.9.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 93/543] Writing tensor layers.9.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 94/543] Writing tensor layers.10.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 95/543] Writing tensor layers.10.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 96/543] Writing tensor layers.10.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 97/543] Writing tensor layers.10.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 98/543] Writing tensor layers.10.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 99/543] Writing tensor layers.10.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[100/543] Writing tensor layers.10.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[101/543] Writing tensor layers.10.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[102/543] Writing tensor layers.10.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[103/543] Writing tensor layers.11.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[104/543] Writing tensor layers.11.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[105/543] Writing tensor layers.11.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[106/543] Writing tensor layers.11.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[107/543] Writing tensor layers.11.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[108/543] Writing tensor layers.11.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[109/543] Writing tensor layers.11.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[110/543] Writing tensor layers.11.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[111/543] Writing tensor layers.11.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[112/543] Writing tensor layers.12.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[113/543] Writing tensor layers.12.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[114/543] Writing tensor layers.12.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[115/543] Writing tensor layers.12.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[116/543] Writing tensor layers.12.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[117/543] Writing tensor layers.12.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[118/543] Writing tensor layers.12.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[119/543] Writing tensor layers.12.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[120/543] Writing tensor layers.12.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[121/543] Writing tensor layers.13.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[122/543] Writing tensor layers.13.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[123/543] Writing tensor layers.13.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[124/543] Writing tensor layers.13.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[125/543] Writing tensor layers.13.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[126/543] Writing tensor layers.13.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[127/543] Writing tensor layers.13.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[128/543] Writing tensor layers.13.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[129/543] Writing tensor layers.13.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[130/543] Writing tensor layers.14.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[131/543] Writing tensor layers.14.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[132/543] Writing tensor layers.14.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[133/543] Writing tensor layers.14.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[134/543] Writing tensor layers.14.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[135/543] Writing tensor layers.14.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[136/543] Writing tensor layers.14.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[137/543] Writing tensor layers.14.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[138/543] Writing tensor layers.14.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[139/543] Writing tensor layers.15.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[140/543] Writing tensor layers.15.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[141/543] Writing tensor layers.15.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[142/543] Writing tensor layers.15.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[143/543] Writing tensor layers.15.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[144/543] Writing tensor layers.15.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[145/543] Writing tensor layers.15.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[146/543] Writing tensor layers.15.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[147/543] Writing tensor layers.15.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[148/543] Writing tensor layers.16.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[149/543] Writing tensor layers.16.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[150/543] Writing tensor layers.16.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[151/543] Writing tensor layers.16.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[152/543] Writing tensor layers.16.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[153/543] Writing tensor layers.16.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[154/543] Writing tensor layers.16.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[155/543] Writing tensor layers.16.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[156/543] Writing tensor layers.16.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[157/543] Writing tensor layers.17.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[158/543] Writing tensor layers.17.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[159/543] Writing tensor layers.17.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[160/543] Writing tensor layers.17.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[161/543] Writing tensor layers.17.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[162/543] Writing tensor layers.17.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[163/543] Writing tensor layers.17.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[164/543] Writing tensor layers.17.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[165/543] Writing tensor layers.17.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[166/543] Writing tensor layers.18.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[167/543] Writing tensor layers.18.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[168/543] Writing tensor layers.18.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[169/543] Writing tensor layers.18.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[170/543] Writing tensor layers.18.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[171/543] Writing tensor layers.18.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[172/543] Writing tensor layers.18.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[173/543] Writing tensor layers.18.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[174/543] Writing tensor layers.18.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[175/543] Writing tensor layers.19.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[176/543] Writing tensor layers.19.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[177/543] Writing tensor layers.19.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[178/543] Writing tensor layers.19.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[179/543] Writing tensor layers.19.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[180/543] Writing tensor layers.19.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[181/543] Writing tensor layers.19.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[182/543] Writing tensor layers.19.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[183/543] Writing tensor layers.19.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[184/543] Writing tensor layers.20.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[185/543] Writing tensor layers.20.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[186/543] Writing tensor layers.20.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[187/543] Writing tensor layers.20.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[188/543] Writing tensor layers.20.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[189/543] Writing tensor layers.20.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[190/543] Writing tensor layers.20.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[191/543] Writing tensor layers.20.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[192/543] Writing tensor layers.20.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[193/543] Writing tensor layers.21.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[194/543] Writing tensor layers.21.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[195/543] Writing tensor layers.21.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[196/543] Writing tensor layers.21.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[197/543] Writing tensor layers.21.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[198/543] Writing tensor layers.21.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[199/543] Writing tensor layers.21.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[200/543] Writing tensor layers.21.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[201/543] Writing tensor layers.21.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[202/543] Writing tensor layers.22.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[203/543] Writing tensor layers.22.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[204/543] Writing tensor layers.22.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[205/543] Writing tensor layers.22.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[206/543] Writing tensor layers.22.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[207/543] Writing tensor layers.22.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[208/543] Writing tensor layers.22.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[209/543] Writing tensor layers.22.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[210/543] Writing tensor layers.22.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[211/543] Writing tensor layers.23.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[212/543] Writing tensor layers.23.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[213/543] Writing tensor layers.23.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[214/543] Writing tensor layers.23.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[215/543] Writing tensor layers.23.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[216/543] Writing tensor layers.23.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[217/543] Writing tensor layers.23.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[218/543] Writing tensor layers.23.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[219/543] Writing tensor layers.23.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[220/543] Writing tensor layers.24.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[221/543] Writing tensor layers.24.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[222/543] Writing tensor layers.24.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[223/543] Writing tensor layers.24.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[224/543] Writing tensor layers.24.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[225/543] Writing tensor layers.24.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[226/543] Writing tensor layers.24.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[227/543] Writing tensor layers.24.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[228/543] Writing tensor layers.24.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[229/543] Writing tensor layers.25.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[230/543] Writing tensor layers.25.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[231/543] Writing tensor layers.25.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[232/543] Writing tensor layers.25.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[233/543] Writing tensor layers.25.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[234/543] Writing tensor layers.25.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[235/543] Writing tensor layers.25.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[236/543] Writing tensor layers.25.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[237/543] Writing tensor layers.25.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[238/543] Writing tensor layers.26.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[239/543] Writing tensor layers.26.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[240/543] Writing tensor layers.26.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[241/543] Writing tensor layers.26.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[242/543] Writing tensor layers.26.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[243/543] Writing tensor layers.26.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[244/543] Writing tensor layers.26.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[245/543] Writing tensor layers.26.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[246/543] Writing tensor layers.26.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[247/543] Writing tensor layers.27.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[248/543] Writing tensor layers.27.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[249/543] Writing tensor layers.27.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[250/543] Writing tensor layers.27.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[251/543] Writing tensor layers.27.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[252/543] Writing tensor layers.27.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[253/543] Writing tensor layers.27.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[254/543] Writing tensor layers.27.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[255/543] Writing tensor layers.27.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[256/543] Writing tensor layers.28.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[257/543] Writing tensor layers.28.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[258/543] Writing tensor layers.28.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[259/543] Writing tensor layers.28.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[260/543] Writing tensor layers.28.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[261/543] Writing tensor layers.28.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[262/543] Writing tensor layers.28.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[263/543] Writing tensor layers.28.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[264/543] Writing tensor layers.28.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[265/543] Writing tensor layers.29.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[266/543] Writing tensor layers.29.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[267/543] Writing tensor layers.29.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[268/543] Writing tensor layers.29.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[269/543] Writing tensor layers.29.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[270/543] Writing tensor layers.29.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[271/543] Writing tensor layers.29.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[272/543] Writing tensor layers.29.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[273/543] Writing tensor layers.29.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[274/543] Writing tensor layers.30.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[275/543] Writing tensor layers.30.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[276/543] Writing tensor layers.30.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[277/543] Writing tensor layers.30.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[278/543] Writing tensor layers.30.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[279/543] Writing tensor layers.30.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[280/543] Writing tensor layers.30.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[281/543] Writing tensor layers.30.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[282/543] Writing tensor layers.30.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[283/543] Writing tensor layers.31.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[284/543] Writing tensor layers.31.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[285/543] Writing tensor layers.31.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[286/543] Writing tensor layers.31.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[287/543] Writing tensor layers.31.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[288/543] Writing tensor layers.31.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[289/543] Writing tensor layers.31.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[290/543] Writing tensor layers.31.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[291/543] Writing tensor layers.31.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[292/543] Writing tensor layers.32.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[293/543] Writing tensor layers.32.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[294/543] Writing tensor layers.32.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[295/543] Writing tensor layers.32.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[296/543] Writing tensor layers.32.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[297/543] Writing tensor layers.32.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[298/543] Writing tensor layers.32.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[299/543] Writing tensor layers.32.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[300/543] Writing tensor layers.32.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[301/543] Writing tensor layers.33.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[302/543] Writing tensor layers.33.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[303/543] Writing tensor layers.33.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[304/543] Writing tensor layers.33.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[305/543] Writing tensor layers.33.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[306/543] Writing tensor layers.33.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[307/543] Writing tensor layers.33.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[308/543] Writing tensor layers.33.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[309/543] Writing tensor layers.33.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[310/543] Writing tensor layers.34.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[311/543] Writing tensor layers.34.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[312/543] Writing tensor layers.34.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[313/543] Writing tensor layers.34.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[314/543] Writing tensor layers.34.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[315/543] Writing tensor layers.34.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[316/543] Writing tensor layers.34.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[317/543] Writing tensor layers.34.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[318/543] Writing tensor layers.34.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[319/543] Writing tensor layers.35.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[320/543] Writing tensor layers.35.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[321/543] Writing tensor layers.35.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[322/543] Writing tensor layers.35.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[323/543] Writing tensor layers.35.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[324/543] Writing tensor layers.35.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[325/543] Writing tensor layers.35.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[326/543] Writing tensor layers.35.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[327/543] Writing tensor layers.35.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[328/543] Writing tensor layers.36.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[329/543] Writing tensor layers.36.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[330/543] Writing tensor layers.36.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[331/543] Writing tensor layers.36.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[332/543] Writing tensor layers.36.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[333/543] Writing tensor layers.36.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[334/543] Writing tensor layers.36.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[335/543] Writing tensor layers.36.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[336/543] Writing tensor layers.36.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[337/543] Writing tensor layers.37.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[338/543] Writing tensor layers.37.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[339/543] Writing tensor layers.37.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[340/543] Writing tensor layers.37.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[341/543] Writing tensor layers.37.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[342/543] Writing tensor layers.37.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[343/543] Writing tensor layers.37.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[344/543] Writing tensor layers.37.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[345/543] Writing tensor layers.37.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[346/543] Writing tensor layers.38.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[347/543] Writing tensor layers.38.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[348/543] Writing tensor layers.38.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[349/543] Writing tensor layers.38.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[350/543] Writing tensor layers.38.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[351/543] Writing tensor layers.38.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[352/543] Writing tensor layers.38.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[353/543] Writing tensor layers.38.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[354/543] Writing tensor layers.38.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[355/543] Writing tensor layers.39.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[356/543] Writing tensor layers.39.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[357/543] Writing tensor layers.39.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[358/543] Writing tensor layers.39.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[359/543] Writing tensor layers.39.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[360/543] Writing tensor layers.39.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[361/543] Writing tensor layers.39.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[362/543] Writing tensor layers.39.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[363/543] Writing tensor layers.39.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[364/543] Writing tensor layers.40.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[365/543] Writing tensor layers.40.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[366/543] Writing tensor layers.40.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[367/543] Writing tensor layers.40.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[368/543] Writing tensor layers.40.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[369/543] Writing tensor layers.40.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[370/543] Writing tensor layers.40.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[371/543] Writing tensor layers.40.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[372/543] Writing tensor layers.40.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[373/543] Writing tensor layers.41.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[374/543] Writing tensor layers.41.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[375/543] Writing tensor layers.41.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[376/543] Writing tensor layers.41.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[377/543] Writing tensor layers.41.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[378/543] Writing tensor layers.41.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[379/543] Writing tensor layers.41.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[380/543] Writing tensor layers.41.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[381/543] Writing tensor layers.41.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[382/543] Writing tensor layers.42.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[383/543] Writing tensor layers.42.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[384/543] Writing tensor layers.42.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[385/543] Writing tensor layers.42.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[386/543] Writing tensor layers.42.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[387/543] Writing tensor layers.42.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[388/543] Writing tensor layers.42.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[389/543] Writing tensor layers.42.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[390/543] Writing tensor layers.42.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[391/543] Writing tensor layers.43.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[392/543] Writing tensor layers.43.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[393/543] Writing tensor layers.43.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[394/543] Writing tensor layers.43.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[395/543] Writing tensor layers.43.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[396/543] Writing tensor layers.43.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[397/543] Writing tensor layers.43.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[398/543] Writing tensor layers.43.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[399/543] Writing tensor layers.43.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[400/543] Writing tensor layers.44.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[401/543] Writing tensor layers.44.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[402/543] Writing tensor layers.44.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[403/543] Writing tensor layers.44.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[404/543] Writing tensor layers.44.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[405/543] Writing tensor layers.44.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[406/543] Writing tensor layers.44.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[407/543] Writing tensor layers.44.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[408/543] Writing tensor layers.44.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[409/543] Writing tensor layers.45.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[410/543] Writing tensor layers.45.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[411/543] Writing tensor layers.45.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[412/543] Writing tensor layers.45.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[413/543] Writing tensor layers.45.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[414/543] Writing tensor layers.45.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[415/543] Writing tensor layers.45.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[416/543] Writing tensor layers.45.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[417/543] Writing tensor layers.45.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[418/543] Writing tensor layers.46.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[419/543] Writing tensor layers.46.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[420/543] Writing tensor layers.46.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[421/543] Writing tensor layers.46.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[422/543] Writing tensor layers.46.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[423/543] Writing tensor layers.46.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[424/543] Writing tensor layers.46.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[425/543] Writing tensor layers.46.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[426/543] Writing tensor layers.46.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[427/543] Writing tensor layers.47.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[428/543] Writing tensor layers.47.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[429/543] Writing tensor layers.47.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[430/543] Writing tensor layers.47.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[431/543] Writing tensor layers.47.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[432/543] Writing tensor layers.47.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[433/543] Writing tensor layers.47.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[434/543] Writing tensor layers.47.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[435/543] Writing tensor layers.47.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[436/543] Writing tensor layers.48.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[437/543] Writing tensor layers.48.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[438/543] Writing tensor layers.48.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[439/543] Writing tensor layers.48.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[440/543] Writing tensor layers.48.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[441/543] Writing tensor layers.48.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[442/543] Writing tensor layers.48.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[443/543] Writing tensor layers.48.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[444/543] Writing tensor layers.48.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[445/543] Writing tensor layers.49.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[446/543] Writing tensor layers.49.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[447/543] Writing tensor layers.49.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[448/543] Writing tensor layers.49.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[449/543] Writing tensor layers.49.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[450/543] Writing tensor layers.49.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[451/543] Writing tensor layers.49.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[452/543] Writing tensor layers.49.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[453/543] Writing tensor layers.49.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[454/543] Writing tensor layers.50.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[455/543] Writing tensor layers.50.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[456/543] Writing tensor layers.50.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[457/543] Writing tensor layers.50.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[458/543] Writing tensor layers.50.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[459/543] Writing tensor layers.50.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[460/543] Writing tensor layers.50.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[461/543] Writing tensor layers.50.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[462/543] Writing tensor layers.50.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[463/543] Writing tensor layers.51.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[464/543] Writing tensor layers.51.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[465/543] Writing tensor layers.51.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[466/543] Writing tensor layers.51.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[467/543] Writing tensor layers.51.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[468/543] Writing tensor layers.51.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[469/543] Writing tensor layers.51.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[470/543] Writing tensor layers.51.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[471/543] Writing tensor layers.51.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[472/543] Writing tensor layers.52.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[473/543] Writing tensor layers.52.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[474/543] Writing tensor layers.52.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[475/543] Writing tensor layers.52.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[476/543] Writing tensor layers.52.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[477/543] Writing tensor layers.52.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[478/543] Writing tensor layers.52.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[479/543] Writing tensor layers.52.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[480/543] Writing tensor layers.52.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[481/543] Writing tensor layers.53.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[482/543] Writing tensor layers.53.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[483/543] Writing tensor layers.53.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[484/543] Writing tensor layers.53.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[485/543] Writing tensor layers.53.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[486/543] Writing tensor layers.53.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[487/543] Writing tensor layers.53.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[488/543] Writing tensor layers.53.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[489/543] Writing tensor layers.53.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[490/543] Writing tensor layers.54.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[491/543] Writing tensor layers.54.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[492/543] Writing tensor layers.54.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[493/543] Writing tensor layers.54.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[494/543] Writing tensor layers.54.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[495/543] Writing tensor layers.54.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[496/543] Writing tensor layers.54.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[497/543] Writing tensor layers.54.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[498/543] Writing tensor layers.54.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[499/543] Writing tensor layers.55.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[500/543] Writing tensor layers.55.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[501/543] Writing tensor layers.55.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[502/543] Writing tensor layers.55.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[503/543] Writing tensor layers.55.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[504/543] Writing tensor layers.55.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[505/543] Writing tensor layers.55.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[506/543] Writing tensor layers.55.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[507/543] Writing tensor layers.55.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[508/543] Writing tensor layers.56.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[509/543] Writing tensor layers.56.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[510/543] Writing tensor layers.56.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[511/543] Writing tensor layers.56.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[512/543] Writing tensor layers.56.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[513/543] Writing tensor layers.56.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[514/543] Writing tensor layers.56.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[515/543] Writing tensor layers.56.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[516/543] Writing tensor layers.56.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[517/543] Writing tensor layers.57.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[518/543] Writing tensor layers.57.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[519/543] Writing tensor layers.57.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[520/543] Writing tensor layers.57.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[521/543] Writing tensor layers.57.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[522/543] Writing tensor layers.57.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[523/543] Writing tensor layers.57.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[524/543] Writing tensor layers.57.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[525/543] Writing tensor layers.57.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[526/543] Writing tensor layers.58.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[527/543] Writing tensor layers.58.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[528/543] Writing tensor layers.58.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[529/543] Writing tensor layers.58.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[530/543] Writing tensor layers.58.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[531/543] Writing tensor layers.58.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[532/543] Writing tensor layers.58.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[533/543] Writing tensor layers.58.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[534/543] Writing tensor layers.58.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[535/543] Writing tensor layers.59.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[536/543] Writing tensor layers.59.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[537/543] Writing tensor layers.59.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[538/543] Writing tensor layers.59.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[539/543] Writing tensor layers.59.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[540/543] Writing tensor layers.59.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[541/543] Writing tensor layers.59.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[542/543] Writing tensor layers.59.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[543/543] Writing tensor layers.59.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/30B/ggml-model-f16.bin\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:8192 n_mult:256 n_head:64 n_layer:80\n",
      "Writing vocab...\n",
      "[  1/723] Writing tensor tok_embeddings.weight                  | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  2/723] Writing tensor norm.weight                            | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  4/723] Writing tensor layers.0.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  5/723] Writing tensor layers.0.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  6/723] Writing tensor layers.0.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  7/723] Writing tensor layers.0.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  8/723] Writing tensor layers.0.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  9/723] Writing tensor layers.0.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 10/723] Writing tensor layers.0.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 11/723] Writing tensor layers.0.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 12/723] Writing tensor layers.0.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 13/723] Writing tensor layers.1.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 14/723] Writing tensor layers.1.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 15/723] Writing tensor layers.1.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 16/723] Writing tensor layers.1.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 17/723] Writing tensor layers.1.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 18/723] Writing tensor layers.1.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 19/723] Writing tensor layers.1.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 20/723] Writing tensor layers.1.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 21/723] Writing tensor layers.1.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 22/723] Writing tensor layers.2.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 23/723] Writing tensor layers.2.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 24/723] Writing tensor layers.2.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 25/723] Writing tensor layers.2.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 26/723] Writing tensor layers.2.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 27/723] Writing tensor layers.2.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 28/723] Writing tensor layers.2.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 29/723] Writing tensor layers.2.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 30/723] Writing tensor layers.2.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 31/723] Writing tensor layers.3.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 32/723] Writing tensor layers.3.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 33/723] Writing tensor layers.3.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 34/723] Writing tensor layers.3.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 35/723] Writing tensor layers.3.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 36/723] Writing tensor layers.3.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 37/723] Writing tensor layers.3.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 38/723] Writing tensor layers.3.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 39/723] Writing tensor layers.3.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 40/723] Writing tensor layers.4.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 41/723] Writing tensor layers.4.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 42/723] Writing tensor layers.4.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 43/723] Writing tensor layers.4.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 44/723] Writing tensor layers.4.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 45/723] Writing tensor layers.4.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 46/723] Writing tensor layers.4.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 47/723] Writing tensor layers.4.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 48/723] Writing tensor layers.4.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 49/723] Writing tensor layers.5.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 50/723] Writing tensor layers.5.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 51/723] Writing tensor layers.5.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 52/723] Writing tensor layers.5.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 53/723] Writing tensor layers.5.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 54/723] Writing tensor layers.5.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 55/723] Writing tensor layers.5.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 56/723] Writing tensor layers.5.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 57/723] Writing tensor layers.5.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 58/723] Writing tensor layers.6.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 59/723] Writing tensor layers.6.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 60/723] Writing tensor layers.6.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 61/723] Writing tensor layers.6.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 62/723] Writing tensor layers.6.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 63/723] Writing tensor layers.6.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 64/723] Writing tensor layers.6.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 65/723] Writing tensor layers.6.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 66/723] Writing tensor layers.6.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 67/723] Writing tensor layers.7.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 68/723] Writing tensor layers.7.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 69/723] Writing tensor layers.7.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 70/723] Writing tensor layers.7.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 71/723] Writing tensor layers.7.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 72/723] Writing tensor layers.7.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 73/723] Writing tensor layers.7.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 74/723] Writing tensor layers.7.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 75/723] Writing tensor layers.7.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 76/723] Writing tensor layers.8.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 77/723] Writing tensor layers.8.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 78/723] Writing tensor layers.8.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 79/723] Writing tensor layers.8.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 80/723] Writing tensor layers.8.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 81/723] Writing tensor layers.8.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 82/723] Writing tensor layers.8.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 83/723] Writing tensor layers.8.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 84/723] Writing tensor layers.8.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 85/723] Writing tensor layers.9.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 86/723] Writing tensor layers.9.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 87/723] Writing tensor layers.9.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 88/723] Writing tensor layers.9.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 89/723] Writing tensor layers.9.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 90/723] Writing tensor layers.9.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 91/723] Writing tensor layers.9.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 92/723] Writing tensor layers.9.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 93/723] Writing tensor layers.9.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 94/723] Writing tensor layers.10.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 95/723] Writing tensor layers.10.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 96/723] Writing tensor layers.10.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 97/723] Writing tensor layers.10.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 98/723] Writing tensor layers.10.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 99/723] Writing tensor layers.10.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[100/723] Writing tensor layers.10.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[101/723] Writing tensor layers.10.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[102/723] Writing tensor layers.10.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[103/723] Writing tensor layers.11.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[104/723] Writing tensor layers.11.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[105/723] Writing tensor layers.11.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[106/723] Writing tensor layers.11.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[107/723] Writing tensor layers.11.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[108/723] Writing tensor layers.11.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[109/723] Writing tensor layers.11.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[110/723] Writing tensor layers.11.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[111/723] Writing tensor layers.11.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[112/723] Writing tensor layers.12.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[113/723] Writing tensor layers.12.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[114/723] Writing tensor layers.12.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[115/723] Writing tensor layers.12.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[116/723] Writing tensor layers.12.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[117/723] Writing tensor layers.12.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[118/723] Writing tensor layers.12.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[119/723] Writing tensor layers.12.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[120/723] Writing tensor layers.12.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[121/723] Writing tensor layers.13.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[122/723] Writing tensor layers.13.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[123/723] Writing tensor layers.13.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[124/723] Writing tensor layers.13.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[125/723] Writing tensor layers.13.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[126/723] Writing tensor layers.13.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[127/723] Writing tensor layers.13.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[128/723] Writing tensor layers.13.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[129/723] Writing tensor layers.13.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[130/723] Writing tensor layers.14.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[131/723] Writing tensor layers.14.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[132/723] Writing tensor layers.14.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[133/723] Writing tensor layers.14.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[134/723] Writing tensor layers.14.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[135/723] Writing tensor layers.14.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[136/723] Writing tensor layers.14.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[137/723] Writing tensor layers.14.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[138/723] Writing tensor layers.14.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[139/723] Writing tensor layers.15.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[140/723] Writing tensor layers.15.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[141/723] Writing tensor layers.15.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[142/723] Writing tensor layers.15.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[143/723] Writing tensor layers.15.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[144/723] Writing tensor layers.15.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[145/723] Writing tensor layers.15.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[146/723] Writing tensor layers.15.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[147/723] Writing tensor layers.15.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[148/723] Writing tensor layers.16.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[149/723] Writing tensor layers.16.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[150/723] Writing tensor layers.16.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[151/723] Writing tensor layers.16.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[152/723] Writing tensor layers.16.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[153/723] Writing tensor layers.16.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[154/723] Writing tensor layers.16.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[155/723] Writing tensor layers.16.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[156/723] Writing tensor layers.16.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[157/723] Writing tensor layers.17.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[158/723] Writing tensor layers.17.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[159/723] Writing tensor layers.17.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[160/723] Writing tensor layers.17.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[161/723] Writing tensor layers.17.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[162/723] Writing tensor layers.17.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[163/723] Writing tensor layers.17.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[164/723] Writing tensor layers.17.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[165/723] Writing tensor layers.17.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[166/723] Writing tensor layers.18.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[167/723] Writing tensor layers.18.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[168/723] Writing tensor layers.18.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[169/723] Writing tensor layers.18.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[170/723] Writing tensor layers.18.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[171/723] Writing tensor layers.18.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[172/723] Writing tensor layers.18.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[173/723] Writing tensor layers.18.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[174/723] Writing tensor layers.18.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[175/723] Writing tensor layers.19.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[176/723] Writing tensor layers.19.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[177/723] Writing tensor layers.19.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[178/723] Writing tensor layers.19.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[179/723] Writing tensor layers.19.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[180/723] Writing tensor layers.19.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[181/723] Writing tensor layers.19.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[182/723] Writing tensor layers.19.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[183/723] Writing tensor layers.19.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[184/723] Writing tensor layers.20.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[185/723] Writing tensor layers.20.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[186/723] Writing tensor layers.20.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[187/723] Writing tensor layers.20.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[188/723] Writing tensor layers.20.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[189/723] Writing tensor layers.20.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[190/723] Writing tensor layers.20.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[191/723] Writing tensor layers.20.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[192/723] Writing tensor layers.20.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[193/723] Writing tensor layers.21.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[194/723] Writing tensor layers.21.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[195/723] Writing tensor layers.21.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[196/723] Writing tensor layers.21.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[197/723] Writing tensor layers.21.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[198/723] Writing tensor layers.21.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[199/723] Writing tensor layers.21.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[200/723] Writing tensor layers.21.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[201/723] Writing tensor layers.21.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[202/723] Writing tensor layers.22.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[203/723] Writing tensor layers.22.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[204/723] Writing tensor layers.22.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[205/723] Writing tensor layers.22.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[206/723] Writing tensor layers.22.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[207/723] Writing tensor layers.22.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[208/723] Writing tensor layers.22.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[209/723] Writing tensor layers.22.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[210/723] Writing tensor layers.22.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[211/723] Writing tensor layers.23.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[212/723] Writing tensor layers.23.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[213/723] Writing tensor layers.23.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[214/723] Writing tensor layers.23.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[215/723] Writing tensor layers.23.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[216/723] Writing tensor layers.23.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[217/723] Writing tensor layers.23.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[218/723] Writing tensor layers.23.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[219/723] Writing tensor layers.23.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[220/723] Writing tensor layers.24.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[221/723] Writing tensor layers.24.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[222/723] Writing tensor layers.24.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[223/723] Writing tensor layers.24.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[224/723] Writing tensor layers.24.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[225/723] Writing tensor layers.24.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[226/723] Writing tensor layers.24.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[227/723] Writing tensor layers.24.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[228/723] Writing tensor layers.24.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[229/723] Writing tensor layers.25.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[230/723] Writing tensor layers.25.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[231/723] Writing tensor layers.25.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[232/723] Writing tensor layers.25.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[233/723] Writing tensor layers.25.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[234/723] Writing tensor layers.25.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[235/723] Writing tensor layers.25.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[236/723] Writing tensor layers.25.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[237/723] Writing tensor layers.25.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[238/723] Writing tensor layers.26.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[239/723] Writing tensor layers.26.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[240/723] Writing tensor layers.26.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[241/723] Writing tensor layers.26.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[242/723] Writing tensor layers.26.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[243/723] Writing tensor layers.26.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[244/723] Writing tensor layers.26.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[245/723] Writing tensor layers.26.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[246/723] Writing tensor layers.26.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[247/723] Writing tensor layers.27.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[248/723] Writing tensor layers.27.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[249/723] Writing tensor layers.27.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[250/723] Writing tensor layers.27.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[251/723] Writing tensor layers.27.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[252/723] Writing tensor layers.27.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[253/723] Writing tensor layers.27.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[254/723] Writing tensor layers.27.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[255/723] Writing tensor layers.27.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[256/723] Writing tensor layers.28.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[257/723] Writing tensor layers.28.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[258/723] Writing tensor layers.28.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[259/723] Writing tensor layers.28.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[260/723] Writing tensor layers.28.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[261/723] Writing tensor layers.28.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[262/723] Writing tensor layers.28.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[263/723] Writing tensor layers.28.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[264/723] Writing tensor layers.28.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[265/723] Writing tensor layers.29.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[266/723] Writing tensor layers.29.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[267/723] Writing tensor layers.29.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[268/723] Writing tensor layers.29.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[269/723] Writing tensor layers.29.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[270/723] Writing tensor layers.29.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[271/723] Writing tensor layers.29.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[272/723] Writing tensor layers.29.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[273/723] Writing tensor layers.29.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[274/723] Writing tensor layers.30.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[275/723] Writing tensor layers.30.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[276/723] Writing tensor layers.30.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[277/723] Writing tensor layers.30.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[278/723] Writing tensor layers.30.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[279/723] Writing tensor layers.30.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[280/723] Writing tensor layers.30.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[281/723] Writing tensor layers.30.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[282/723] Writing tensor layers.30.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[283/723] Writing tensor layers.31.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[284/723] Writing tensor layers.31.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[285/723] Writing tensor layers.31.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[286/723] Writing tensor layers.31.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[287/723] Writing tensor layers.31.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[288/723] Writing tensor layers.31.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[289/723] Writing tensor layers.31.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[290/723] Writing tensor layers.31.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[291/723] Writing tensor layers.31.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[292/723] Writing tensor layers.32.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[293/723] Writing tensor layers.32.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[294/723] Writing tensor layers.32.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[295/723] Writing tensor layers.32.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[296/723] Writing tensor layers.32.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[297/723] Writing tensor layers.32.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[298/723] Writing tensor layers.32.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[299/723] Writing tensor layers.32.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[300/723] Writing tensor layers.32.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[301/723] Writing tensor layers.33.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[302/723] Writing tensor layers.33.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[303/723] Writing tensor layers.33.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[304/723] Writing tensor layers.33.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[305/723] Writing tensor layers.33.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[306/723] Writing tensor layers.33.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[307/723] Writing tensor layers.33.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[308/723] Writing tensor layers.33.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[309/723] Writing tensor layers.33.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[310/723] Writing tensor layers.34.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[311/723] Writing tensor layers.34.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[312/723] Writing tensor layers.34.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[313/723] Writing tensor layers.34.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[314/723] Writing tensor layers.34.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[315/723] Writing tensor layers.34.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[316/723] Writing tensor layers.34.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[317/723] Writing tensor layers.34.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[318/723] Writing tensor layers.34.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[319/723] Writing tensor layers.35.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[320/723] Writing tensor layers.35.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[321/723] Writing tensor layers.35.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[322/723] Writing tensor layers.35.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[323/723] Writing tensor layers.35.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[324/723] Writing tensor layers.35.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[325/723] Writing tensor layers.35.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[326/723] Writing tensor layers.35.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[327/723] Writing tensor layers.35.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[328/723] Writing tensor layers.36.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[329/723] Writing tensor layers.36.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[330/723] Writing tensor layers.36.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[331/723] Writing tensor layers.36.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[332/723] Writing tensor layers.36.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[333/723] Writing tensor layers.36.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[334/723] Writing tensor layers.36.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[335/723] Writing tensor layers.36.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[336/723] Writing tensor layers.36.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[337/723] Writing tensor layers.37.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[338/723] Writing tensor layers.37.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[339/723] Writing tensor layers.37.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[340/723] Writing tensor layers.37.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[341/723] Writing tensor layers.37.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[342/723] Writing tensor layers.37.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[343/723] Writing tensor layers.37.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[344/723] Writing tensor layers.37.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[345/723] Writing tensor layers.37.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[346/723] Writing tensor layers.38.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[347/723] Writing tensor layers.38.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[348/723] Writing tensor layers.38.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[349/723] Writing tensor layers.38.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[350/723] Writing tensor layers.38.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[351/723] Writing tensor layers.38.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[352/723] Writing tensor layers.38.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[353/723] Writing tensor layers.38.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[354/723] Writing tensor layers.38.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[355/723] Writing tensor layers.39.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[356/723] Writing tensor layers.39.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[357/723] Writing tensor layers.39.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[358/723] Writing tensor layers.39.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[359/723] Writing tensor layers.39.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[360/723] Writing tensor layers.39.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[361/723] Writing tensor layers.39.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[362/723] Writing tensor layers.39.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[363/723] Writing tensor layers.39.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[364/723] Writing tensor layers.40.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[365/723] Writing tensor layers.40.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[366/723] Writing tensor layers.40.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[367/723] Writing tensor layers.40.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[368/723] Writing tensor layers.40.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[369/723] Writing tensor layers.40.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[370/723] Writing tensor layers.40.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[371/723] Writing tensor layers.40.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[372/723] Writing tensor layers.40.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[373/723] Writing tensor layers.41.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[374/723] Writing tensor layers.41.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[375/723] Writing tensor layers.41.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[376/723] Writing tensor layers.41.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[377/723] Writing tensor layers.41.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[378/723] Writing tensor layers.41.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[379/723] Writing tensor layers.41.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[380/723] Writing tensor layers.41.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[381/723] Writing tensor layers.41.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[382/723] Writing tensor layers.42.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[383/723] Writing tensor layers.42.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[384/723] Writing tensor layers.42.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[385/723] Writing tensor layers.42.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[386/723] Writing tensor layers.42.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[387/723] Writing tensor layers.42.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[388/723] Writing tensor layers.42.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[389/723] Writing tensor layers.42.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[390/723] Writing tensor layers.42.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[391/723] Writing tensor layers.43.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[392/723] Writing tensor layers.43.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[393/723] Writing tensor layers.43.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[394/723] Writing tensor layers.43.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[395/723] Writing tensor layers.43.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[396/723] Writing tensor layers.43.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[397/723] Writing tensor layers.43.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[398/723] Writing tensor layers.43.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[399/723] Writing tensor layers.43.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[400/723] Writing tensor layers.44.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[401/723] Writing tensor layers.44.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[402/723] Writing tensor layers.44.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[403/723] Writing tensor layers.44.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[404/723] Writing tensor layers.44.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[405/723] Writing tensor layers.44.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[406/723] Writing tensor layers.44.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[407/723] Writing tensor layers.44.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[408/723] Writing tensor layers.44.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[409/723] Writing tensor layers.45.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[410/723] Writing tensor layers.45.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[411/723] Writing tensor layers.45.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[412/723] Writing tensor layers.45.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[413/723] Writing tensor layers.45.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[414/723] Writing tensor layers.45.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[415/723] Writing tensor layers.45.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[416/723] Writing tensor layers.45.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[417/723] Writing tensor layers.45.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[418/723] Writing tensor layers.46.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[419/723] Writing tensor layers.46.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[420/723] Writing tensor layers.46.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[421/723] Writing tensor layers.46.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[422/723] Writing tensor layers.46.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[423/723] Writing tensor layers.46.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[424/723] Writing tensor layers.46.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[425/723] Writing tensor layers.46.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[426/723] Writing tensor layers.46.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[427/723] Writing tensor layers.47.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[428/723] Writing tensor layers.47.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[429/723] Writing tensor layers.47.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[430/723] Writing tensor layers.47.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[431/723] Writing tensor layers.47.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[432/723] Writing tensor layers.47.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[433/723] Writing tensor layers.47.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[434/723] Writing tensor layers.47.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[435/723] Writing tensor layers.47.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[436/723] Writing tensor layers.48.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[437/723] Writing tensor layers.48.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[438/723] Writing tensor layers.48.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[439/723] Writing tensor layers.48.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[440/723] Writing tensor layers.48.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[441/723] Writing tensor layers.48.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[442/723] Writing tensor layers.48.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[443/723] Writing tensor layers.48.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[444/723] Writing tensor layers.48.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[445/723] Writing tensor layers.49.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[446/723] Writing tensor layers.49.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[447/723] Writing tensor layers.49.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[448/723] Writing tensor layers.49.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[449/723] Writing tensor layers.49.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[450/723] Writing tensor layers.49.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[451/723] Writing tensor layers.49.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[452/723] Writing tensor layers.49.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[453/723] Writing tensor layers.49.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[454/723] Writing tensor layers.50.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[455/723] Writing tensor layers.50.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[456/723] Writing tensor layers.50.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[457/723] Writing tensor layers.50.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[458/723] Writing tensor layers.50.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[459/723] Writing tensor layers.50.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[460/723] Writing tensor layers.50.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[461/723] Writing tensor layers.50.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[462/723] Writing tensor layers.50.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[463/723] Writing tensor layers.51.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[464/723] Writing tensor layers.51.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[465/723] Writing tensor layers.51.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[466/723] Writing tensor layers.51.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[467/723] Writing tensor layers.51.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[468/723] Writing tensor layers.51.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[469/723] Writing tensor layers.51.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[470/723] Writing tensor layers.51.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[471/723] Writing tensor layers.51.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[472/723] Writing tensor layers.52.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[473/723] Writing tensor layers.52.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[474/723] Writing tensor layers.52.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[475/723] Writing tensor layers.52.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[476/723] Writing tensor layers.52.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[477/723] Writing tensor layers.52.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[478/723] Writing tensor layers.52.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[479/723] Writing tensor layers.52.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[480/723] Writing tensor layers.52.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[481/723] Writing tensor layers.53.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[482/723] Writing tensor layers.53.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[483/723] Writing tensor layers.53.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[484/723] Writing tensor layers.53.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[485/723] Writing tensor layers.53.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[486/723] Writing tensor layers.53.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[487/723] Writing tensor layers.53.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[488/723] Writing tensor layers.53.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[489/723] Writing tensor layers.53.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[490/723] Writing tensor layers.54.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[491/723] Writing tensor layers.54.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[492/723] Writing tensor layers.54.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[493/723] Writing tensor layers.54.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[494/723] Writing tensor layers.54.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[495/723] Writing tensor layers.54.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[496/723] Writing tensor layers.54.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[497/723] Writing tensor layers.54.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[498/723] Writing tensor layers.54.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[499/723] Writing tensor layers.55.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[500/723] Writing tensor layers.55.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[501/723] Writing tensor layers.55.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[502/723] Writing tensor layers.55.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[503/723] Writing tensor layers.55.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[504/723] Writing tensor layers.55.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[505/723] Writing tensor layers.55.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[506/723] Writing tensor layers.55.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[507/723] Writing tensor layers.55.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[508/723] Writing tensor layers.56.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[509/723] Writing tensor layers.56.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[510/723] Writing tensor layers.56.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[511/723] Writing tensor layers.56.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[512/723] Writing tensor layers.56.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[513/723] Writing tensor layers.56.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[514/723] Writing tensor layers.56.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[515/723] Writing tensor layers.56.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[516/723] Writing tensor layers.56.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[517/723] Writing tensor layers.57.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[518/723] Writing tensor layers.57.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[519/723] Writing tensor layers.57.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[520/723] Writing tensor layers.57.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[521/723] Writing tensor layers.57.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[522/723] Writing tensor layers.57.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[523/723] Writing tensor layers.57.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[524/723] Writing tensor layers.57.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[525/723] Writing tensor layers.57.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[526/723] Writing tensor layers.58.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[527/723] Writing tensor layers.58.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[528/723] Writing tensor layers.58.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[529/723] Writing tensor layers.58.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[530/723] Writing tensor layers.58.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[531/723] Writing tensor layers.58.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[532/723] Writing tensor layers.58.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[533/723] Writing tensor layers.58.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[534/723] Writing tensor layers.58.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[535/723] Writing tensor layers.59.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[536/723] Writing tensor layers.59.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[537/723] Writing tensor layers.59.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[538/723] Writing tensor layers.59.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[539/723] Writing tensor layers.59.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[540/723] Writing tensor layers.59.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[541/723] Writing tensor layers.59.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[542/723] Writing tensor layers.59.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[543/723] Writing tensor layers.59.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[544/723] Writing tensor layers.60.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[545/723] Writing tensor layers.60.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[546/723] Writing tensor layers.60.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[547/723] Writing tensor layers.60.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[548/723] Writing tensor layers.60.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[549/723] Writing tensor layers.60.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[550/723] Writing tensor layers.60.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[551/723] Writing tensor layers.60.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[552/723] Writing tensor layers.60.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[553/723] Writing tensor layers.61.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[554/723] Writing tensor layers.61.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[555/723] Writing tensor layers.61.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[556/723] Writing tensor layers.61.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[557/723] Writing tensor layers.61.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[558/723] Writing tensor layers.61.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[559/723] Writing tensor layers.61.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[560/723] Writing tensor layers.61.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[561/723] Writing tensor layers.61.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[562/723] Writing tensor layers.62.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[563/723] Writing tensor layers.62.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[564/723] Writing tensor layers.62.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[565/723] Writing tensor layers.62.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[566/723] Writing tensor layers.62.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[567/723] Writing tensor layers.62.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[568/723] Writing tensor layers.62.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[569/723] Writing tensor layers.62.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[570/723] Writing tensor layers.62.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[571/723] Writing tensor layers.63.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[572/723] Writing tensor layers.63.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[573/723] Writing tensor layers.63.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[574/723] Writing tensor layers.63.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[575/723] Writing tensor layers.63.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[576/723] Writing tensor layers.63.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[577/723] Writing tensor layers.63.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[578/723] Writing tensor layers.63.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[579/723] Writing tensor layers.63.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[580/723] Writing tensor layers.64.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[581/723] Writing tensor layers.64.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[582/723] Writing tensor layers.64.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[583/723] Writing tensor layers.64.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[584/723] Writing tensor layers.64.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[585/723] Writing tensor layers.64.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[586/723] Writing tensor layers.64.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[587/723] Writing tensor layers.64.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[588/723] Writing tensor layers.64.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[589/723] Writing tensor layers.65.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[590/723] Writing tensor layers.65.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[591/723] Writing tensor layers.65.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[592/723] Writing tensor layers.65.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[593/723] Writing tensor layers.65.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[594/723] Writing tensor layers.65.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[595/723] Writing tensor layers.65.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[596/723] Writing tensor layers.65.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[597/723] Writing tensor layers.65.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[598/723] Writing tensor layers.66.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[599/723] Writing tensor layers.66.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[600/723] Writing tensor layers.66.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[601/723] Writing tensor layers.66.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[602/723] Writing tensor layers.66.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[603/723] Writing tensor layers.66.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[604/723] Writing tensor layers.66.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[605/723] Writing tensor layers.66.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[606/723] Writing tensor layers.66.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[607/723] Writing tensor layers.67.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[608/723] Writing tensor layers.67.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[609/723] Writing tensor layers.67.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[610/723] Writing tensor layers.67.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[611/723] Writing tensor layers.67.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[612/723] Writing tensor layers.67.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[613/723] Writing tensor layers.67.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[614/723] Writing tensor layers.67.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[615/723] Writing tensor layers.67.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[616/723] Writing tensor layers.68.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[617/723] Writing tensor layers.68.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[618/723] Writing tensor layers.68.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[619/723] Writing tensor layers.68.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[620/723] Writing tensor layers.68.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[621/723] Writing tensor layers.68.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[622/723] Writing tensor layers.68.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[623/723] Writing tensor layers.68.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[624/723] Writing tensor layers.68.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[625/723] Writing tensor layers.69.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[626/723] Writing tensor layers.69.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[627/723] Writing tensor layers.69.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[628/723] Writing tensor layers.69.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[629/723] Writing tensor layers.69.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[630/723] Writing tensor layers.69.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[631/723] Writing tensor layers.69.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[632/723] Writing tensor layers.69.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[633/723] Writing tensor layers.69.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[634/723] Writing tensor layers.70.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[635/723] Writing tensor layers.70.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[636/723] Writing tensor layers.70.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[637/723] Writing tensor layers.70.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[638/723] Writing tensor layers.70.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[639/723] Writing tensor layers.70.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[640/723] Writing tensor layers.70.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[641/723] Writing tensor layers.70.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[642/723] Writing tensor layers.70.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[643/723] Writing tensor layers.71.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[644/723] Writing tensor layers.71.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[645/723] Writing tensor layers.71.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[646/723] Writing tensor layers.71.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[647/723] Writing tensor layers.71.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[648/723] Writing tensor layers.71.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[649/723] Writing tensor layers.71.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[650/723] Writing tensor layers.71.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[651/723] Writing tensor layers.71.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[652/723] Writing tensor layers.72.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[653/723] Writing tensor layers.72.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[654/723] Writing tensor layers.72.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[655/723] Writing tensor layers.72.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[656/723] Writing tensor layers.72.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[657/723] Writing tensor layers.72.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[658/723] Writing tensor layers.72.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[659/723] Writing tensor layers.72.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[660/723] Writing tensor layers.72.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[661/723] Writing tensor layers.73.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[662/723] Writing tensor layers.73.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[663/723] Writing tensor layers.73.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[664/723] Writing tensor layers.73.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[665/723] Writing tensor layers.73.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[666/723] Writing tensor layers.73.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[667/723] Writing tensor layers.73.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[668/723] Writing tensor layers.73.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[669/723] Writing tensor layers.73.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[670/723] Writing tensor layers.74.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[671/723] Writing tensor layers.74.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[672/723] Writing tensor layers.74.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[673/723] Writing tensor layers.74.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[674/723] Writing tensor layers.74.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[675/723] Writing tensor layers.74.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[676/723] Writing tensor layers.74.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[677/723] Writing tensor layers.74.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[678/723] Writing tensor layers.74.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[679/723] Writing tensor layers.75.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[680/723] Writing tensor layers.75.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[681/723] Writing tensor layers.75.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[682/723] Writing tensor layers.75.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[683/723] Writing tensor layers.75.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[684/723] Writing tensor layers.75.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[685/723] Writing tensor layers.75.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[686/723] Writing tensor layers.75.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[687/723] Writing tensor layers.75.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[688/723] Writing tensor layers.76.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[689/723] Writing tensor layers.76.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[690/723] Writing tensor layers.76.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[691/723] Writing tensor layers.76.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[692/723] Writing tensor layers.76.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[693/723] Writing tensor layers.76.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[694/723] Writing tensor layers.76.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[695/723] Writing tensor layers.76.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[696/723] Writing tensor layers.76.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[697/723] Writing tensor layers.77.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[698/723] Writing tensor layers.77.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[699/723] Writing tensor layers.77.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[700/723] Writing tensor layers.77.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[701/723] Writing tensor layers.77.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[702/723] Writing tensor layers.77.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[703/723] Writing tensor layers.77.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[704/723] Writing tensor layers.77.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[705/723] Writing tensor layers.77.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[706/723] Writing tensor layers.78.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[707/723] Writing tensor layers.78.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[708/723] Writing tensor layers.78.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[709/723] Writing tensor layers.78.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[710/723] Writing tensor layers.78.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[711/723] Writing tensor layers.78.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[712/723] Writing tensor layers.78.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[713/723] Writing tensor layers.78.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[714/723] Writing tensor layers.78.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[715/723] Writing tensor layers.79.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[716/723] Writing tensor layers.79.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[717/723] Writing tensor layers.79.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[718/723] Writing tensor layers.79.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[719/723] Writing tensor layers.79.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[720/723] Writing tensor layers.79.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[721/723] Writing tensor layers.79.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[722/723] Writing tensor layers.79.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[723/723] Writing tensor layers.79.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/65B/ggml-model-f16.bin\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/7B/ggml-model-f16.bin' to './models/7B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/7B/ggml-model-q4_0.bin\n",
      "[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->    70.31 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->   102.54 MB | hist: \n",
      "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 20679.35 ms\n",
      "main:    total time = 20679.35 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/13B/ggml-model-f16.bin' to './models/13B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/13B/ggml-model-q4_0.bin\n",
      "[   1/ 363]                tok_embeddings.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->    87.89 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                          norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->   128.17 MB | hist: \n",
      "[   4/ 363]         layers.0.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]         layers.0.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]         layers.0.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]         layers.0.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]       layers.0.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   9/ 363]      layers.0.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 363]      layers.0.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 363]      layers.0.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 363]             layers.0.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  13/ 363]         layers.1.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]         layers.1.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]         layers.1.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]         layers.1.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]       layers.1.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  18/ 363]      layers.1.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]      layers.1.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]      layers.1.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 363]             layers.1.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  22/ 363]         layers.2.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]         layers.2.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]         layers.2.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]         layers.2.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]       layers.2.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  27/ 363]      layers.2.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]      layers.2.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]      layers.2.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 363]             layers.2.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  31/ 363]         layers.3.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]         layers.3.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]         layers.3.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]         layers.3.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]       layers.3.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  36/ 363]      layers.3.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]      layers.3.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]      layers.3.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 363]             layers.3.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  40/ 363]         layers.4.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]         layers.4.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]         layers.4.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]         layers.4.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]       layers.4.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  45/ 363]      layers.4.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]      layers.4.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]      layers.4.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 363]             layers.4.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  49/ 363]         layers.5.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]         layers.5.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]         layers.5.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]         layers.5.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]       layers.5.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  54/ 363]      layers.5.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]      layers.5.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]      layers.5.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 363]             layers.5.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  58/ 363]         layers.6.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]         layers.6.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]         layers.6.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]         layers.6.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]       layers.6.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  63/ 363]      layers.6.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 363]      layers.6.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 363]      layers.6.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 363]             layers.6.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  67/ 363]         layers.7.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]         layers.7.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]         layers.7.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]         layers.7.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]       layers.7.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  72/ 363]      layers.7.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 363]      layers.7.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 363]      layers.7.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 363]             layers.7.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  76/ 363]         layers.8.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]         layers.8.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]         layers.8.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]         layers.8.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]       layers.8.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  81/ 363]      layers.8.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]      layers.8.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]      layers.8.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 363]             layers.8.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  85/ 363]         layers.9.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]         layers.9.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]         layers.9.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]         layers.9.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]       layers.9.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  90/ 363]      layers.9.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 363]      layers.9.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 363]      layers.9.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 363]             layers.9.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  94/ 363]        layers.10.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]        layers.10.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]        layers.10.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]        layers.10.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]      layers.10.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  99/ 363]     layers.10.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 363]     layers.10.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 363]     layers.10.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 363]            layers.10.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]        layers.11.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]        layers.11.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]        layers.11.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]        layers.11.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]      layers.11.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 108/ 363]     layers.11.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 363]     layers.11.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 110/ 363]     layers.11.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 363]            layers.11.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]        layers.12.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]        layers.12.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]        layers.12.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]        layers.12.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]      layers.12.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 117/ 363]     layers.12.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 363]     layers.12.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 363]     layers.12.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 363]            layers.12.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]        layers.13.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]        layers.13.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]        layers.13.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]        layers.13.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]      layers.13.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 126/ 363]     layers.13.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 363]     layers.13.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 363]     layers.13.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 363]            layers.13.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]        layers.14.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]        layers.14.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]        layers.14.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]        layers.14.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]      layers.14.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 135/ 363]     layers.14.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 363]     layers.14.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 363]     layers.14.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 363]            layers.14.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]        layers.15.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]        layers.15.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]        layers.15.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]        layers.15.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]      layers.15.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 144/ 363]     layers.15.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]     layers.15.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]     layers.15.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 363]            layers.15.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]        layers.16.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]        layers.16.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]        layers.16.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]        layers.16.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]      layers.16.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 153/ 363]     layers.16.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 363]     layers.16.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 363]     layers.16.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 363]            layers.16.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]        layers.17.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]        layers.17.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]        layers.17.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]        layers.17.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]      layers.17.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 162/ 363]     layers.17.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]     layers.17.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]     layers.17.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 363]            layers.17.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]        layers.18.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]        layers.18.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]        layers.18.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]        layers.18.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]      layers.18.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 171/ 363]     layers.18.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]     layers.18.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]     layers.18.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 363]            layers.18.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]        layers.19.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]        layers.19.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]        layers.19.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]        layers.19.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]      layers.19.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 180/ 363]     layers.19.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]     layers.19.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]     layers.19.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 363]            layers.19.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]        layers.20.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]        layers.20.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]        layers.20.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]        layers.20.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]      layers.20.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 189/ 363]     layers.20.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]     layers.20.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]     layers.20.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 363]            layers.20.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]        layers.21.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]        layers.21.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]        layers.21.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]        layers.21.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]      layers.21.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 198/ 363]     layers.21.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]     layers.21.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]     layers.21.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 363]            layers.21.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]        layers.22.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]        layers.22.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]        layers.22.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]        layers.22.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]      layers.22.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 207/ 363]     layers.22.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]     layers.22.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]     layers.22.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 363]            layers.22.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]        layers.23.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]        layers.23.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]        layers.23.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]        layers.23.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]      layers.23.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 216/ 363]     layers.23.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]     layers.23.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]     layers.23.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 363]            layers.23.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]        layers.24.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]        layers.24.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]        layers.24.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]        layers.24.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]      layers.24.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 225/ 363]     layers.24.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]     layers.24.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]     layers.24.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 363]            layers.24.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]        layers.25.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]        layers.25.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]        layers.25.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]        layers.25.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]      layers.25.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 234/ 363]     layers.25.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]     layers.25.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]     layers.25.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 363]            layers.25.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]        layers.26.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]        layers.26.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]        layers.26.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]        layers.26.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]      layers.26.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 243/ 363]     layers.26.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]     layers.26.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]     layers.26.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 363]            layers.26.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]        layers.27.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]        layers.27.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]        layers.27.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]        layers.27.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]      layers.27.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 252/ 363]     layers.27.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]     layers.27.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]     layers.27.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 363]            layers.27.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]        layers.28.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]        layers.28.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]        layers.28.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]        layers.28.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]      layers.28.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 261/ 363]     layers.28.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]     layers.28.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]     layers.28.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 363]            layers.28.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]        layers.29.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]        layers.29.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]        layers.29.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]        layers.29.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]      layers.29.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 270/ 363]     layers.29.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]     layers.29.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]     layers.29.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 363]            layers.29.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]        layers.30.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]        layers.30.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]        layers.30.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]        layers.30.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]      layers.30.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 279/ 363]     layers.30.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]     layers.30.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]     layers.30.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 363]            layers.30.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]        layers.31.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]        layers.31.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]        layers.31.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]        layers.31.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]      layers.31.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 288/ 363]     layers.31.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]     layers.31.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]     layers.31.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 363]            layers.31.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]        layers.32.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]        layers.32.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]        layers.32.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]        layers.32.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]      layers.32.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 297/ 363]     layers.32.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]     layers.32.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]     layers.32.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 363]            layers.32.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]        layers.33.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]        layers.33.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]        layers.33.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]        layers.33.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]      layers.33.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 306/ 363]     layers.33.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]     layers.33.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]     layers.33.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 363]            layers.33.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]        layers.34.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]        layers.34.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]        layers.34.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]        layers.34.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]      layers.34.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 315/ 363]     layers.34.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]     layers.34.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]     layers.34.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 363]            layers.34.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]        layers.35.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]        layers.35.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]        layers.35.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]        layers.35.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]      layers.35.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 324/ 363]     layers.35.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]     layers.35.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]     layers.35.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 363]            layers.35.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]        layers.36.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]        layers.36.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]        layers.36.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]        layers.36.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]      layers.36.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 333/ 363]     layers.36.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 363]     layers.36.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 335/ 363]     layers.36.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 363]            layers.36.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]        layers.37.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]        layers.37.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]        layers.37.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]        layers.37.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]      layers.37.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 342/ 363]     layers.37.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 363]     layers.37.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 344/ 363]     layers.37.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 363]            layers.37.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]        layers.38.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]        layers.38.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]        layers.38.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]        layers.38.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]      layers.38.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 351/ 363]     layers.38.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 363]     layers.38.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 353/ 363]     layers.38.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 363]            layers.38.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]        layers.39.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]        layers.39.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]        layers.39.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 38772.46 ms\n",
      "main:    total time = 38772.46 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/30B/ggml-model-f16.bin' to './models/30B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/30B/ggml-model-q4_0.bin\n",
      "[   1/ 543]                tok_embeddings.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   114.26 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                          norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   166.63 MB | hist: \n",
      "[   4/ 543]         layers.0.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]         layers.0.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]         layers.0.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]         layers.0.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]       layers.0.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   9/ 543]      layers.0.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 543]      layers.0.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 543]      layers.0.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 543]             layers.0.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  13/ 543]         layers.1.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]         layers.1.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]         layers.1.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]         layers.1.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]       layers.1.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  18/ 543]      layers.1.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]      layers.1.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]      layers.1.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 543]             layers.1.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  22/ 543]         layers.2.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]         layers.2.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]         layers.2.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]         layers.2.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]       layers.2.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  27/ 543]      layers.2.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]      layers.2.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]      layers.2.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 543]             layers.2.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  31/ 543]         layers.3.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]         layers.3.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]         layers.3.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]         layers.3.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]       layers.3.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  36/ 543]      layers.3.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 543]      layers.3.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  38/ 543]      layers.3.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 543]             layers.3.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  40/ 543]         layers.4.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]         layers.4.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]         layers.4.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]         layers.4.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]       layers.4.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  45/ 543]      layers.4.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]      layers.4.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]      layers.4.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 543]             layers.4.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  49/ 543]         layers.5.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]         layers.5.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]         layers.5.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]         layers.5.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]       layers.5.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  54/ 543]      layers.5.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]      layers.5.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]      layers.5.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 543]             layers.5.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  58/ 543]         layers.6.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]         layers.6.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]         layers.6.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]         layers.6.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]       layers.6.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  63/ 543]      layers.6.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]      layers.6.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]      layers.6.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 543]             layers.6.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  67/ 543]         layers.7.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]         layers.7.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]         layers.7.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]         layers.7.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]       layers.7.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  72/ 543]      layers.7.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]      layers.7.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]      layers.7.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 543]             layers.7.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  76/ 543]         layers.8.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]         layers.8.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]         layers.8.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]         layers.8.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]       layers.8.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  81/ 543]      layers.8.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]      layers.8.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]      layers.8.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 543]             layers.8.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  85/ 543]         layers.9.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]         layers.9.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]         layers.9.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]         layers.9.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]       layers.9.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  90/ 543]      layers.9.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]      layers.9.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]      layers.9.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 543]             layers.9.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  94/ 543]        layers.10.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]        layers.10.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]        layers.10.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]        layers.10.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]      layers.10.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  99/ 543]     layers.10.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]     layers.10.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]     layers.10.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 543]            layers.10.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]        layers.11.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]        layers.11.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]        layers.11.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]        layers.11.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]      layers.11.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 108/ 543]     layers.11.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]     layers.11.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]     layers.11.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 543]            layers.11.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]        layers.12.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]        layers.12.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]        layers.12.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]        layers.12.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]      layers.12.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 117/ 543]     layers.12.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]     layers.12.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]     layers.12.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 543]            layers.12.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]        layers.13.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]        layers.13.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]        layers.13.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]        layers.13.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]      layers.13.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 126/ 543]     layers.13.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]     layers.13.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]     layers.13.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 543]            layers.13.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]        layers.14.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]        layers.14.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]        layers.14.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]        layers.14.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]      layers.14.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 135/ 543]     layers.14.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 543]     layers.14.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 543]     layers.14.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 543]            layers.14.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]        layers.15.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]        layers.15.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]        layers.15.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]        layers.15.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]      layers.15.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 144/ 543]     layers.15.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]     layers.15.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]     layers.15.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 543]            layers.15.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]        layers.16.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]        layers.16.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]        layers.16.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]        layers.16.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]      layers.16.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 153/ 543]     layers.16.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]     layers.16.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]     layers.16.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 543]            layers.16.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]        layers.17.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]        layers.17.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]        layers.17.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]        layers.17.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]      layers.17.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 162/ 543]     layers.17.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]     layers.17.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]     layers.17.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 543]            layers.17.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]        layers.18.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]        layers.18.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]        layers.18.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]        layers.18.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]      layers.18.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 171/ 543]     layers.18.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]     layers.18.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]     layers.18.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 543]            layers.18.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]        layers.19.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]        layers.19.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]        layers.19.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]        layers.19.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]      layers.19.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 180/ 543]     layers.19.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 543]     layers.19.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 182/ 543]     layers.19.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 543]            layers.19.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]        layers.20.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]        layers.20.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]        layers.20.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]        layers.20.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]      layers.20.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 189/ 543]     layers.20.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 543]     layers.20.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 191/ 543]     layers.20.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 543]            layers.20.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]        layers.21.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]        layers.21.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]        layers.21.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]        layers.21.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]      layers.21.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 198/ 543]     layers.21.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]     layers.21.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]     layers.21.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 543]            layers.21.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]        layers.22.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]        layers.22.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]        layers.22.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]        layers.22.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]      layers.22.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 207/ 543]     layers.22.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]     layers.22.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]     layers.22.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 543]            layers.22.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]        layers.23.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]        layers.23.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]        layers.23.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]        layers.23.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]      layers.23.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 216/ 543]     layers.23.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]     layers.23.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]     layers.23.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 543]            layers.23.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]        layers.24.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]        layers.24.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]        layers.24.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]        layers.24.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]      layers.24.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 225/ 543]     layers.24.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]     layers.24.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]     layers.24.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 543]            layers.24.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]        layers.25.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]        layers.25.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]        layers.25.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]        layers.25.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]      layers.25.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 234/ 543]     layers.25.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]     layers.25.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]     layers.25.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 543]            layers.25.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]        layers.26.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]        layers.26.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]        layers.26.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]        layers.26.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]      layers.26.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 243/ 543]     layers.26.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]     layers.26.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]     layers.26.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 543]            layers.26.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]        layers.27.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]        layers.27.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]        layers.27.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]        layers.27.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]      layers.27.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 252/ 543]     layers.27.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]     layers.27.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]     layers.27.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 543]            layers.27.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]        layers.28.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]        layers.28.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]        layers.28.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]        layers.28.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]      layers.28.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 261/ 543]     layers.28.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]     layers.28.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]     layers.28.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 543]            layers.28.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]        layers.29.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]        layers.29.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]        layers.29.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]        layers.29.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]      layers.29.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 270/ 543]     layers.29.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]     layers.29.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]     layers.29.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 543]            layers.29.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]        layers.30.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]        layers.30.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]        layers.30.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]        layers.30.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]      layers.30.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 279/ 543]     layers.30.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]     layers.30.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]     layers.30.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 543]            layers.30.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]        layers.31.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]        layers.31.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]        layers.31.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]        layers.31.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]      layers.31.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 288/ 543]     layers.31.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]     layers.31.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]     layers.31.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 543]            layers.31.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]        layers.32.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]        layers.32.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]        layers.32.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]        layers.32.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]      layers.32.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 297/ 543]     layers.32.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]     layers.32.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]     layers.32.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 543]            layers.32.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]        layers.33.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]        layers.33.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]        layers.33.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]        layers.33.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]      layers.33.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 306/ 543]     layers.33.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]     layers.33.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]     layers.33.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 543]            layers.33.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]        layers.34.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]        layers.34.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]        layers.34.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]        layers.34.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]      layers.34.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 315/ 543]     layers.34.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]     layers.34.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]     layers.34.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 543]            layers.34.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]        layers.35.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]        layers.35.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]        layers.35.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]        layers.35.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]      layers.35.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 324/ 543]     layers.35.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]     layers.35.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]     layers.35.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 543]            layers.35.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]        layers.36.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]        layers.36.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]        layers.36.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]        layers.36.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]      layers.36.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 333/ 543]     layers.36.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]     layers.36.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]     layers.36.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 543]            layers.36.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]        layers.37.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]        layers.37.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]        layers.37.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]        layers.37.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]      layers.37.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 342/ 543]     layers.37.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]     layers.37.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]     layers.37.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 543]            layers.37.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]        layers.38.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]        layers.38.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]        layers.38.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]        layers.38.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]      layers.38.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 351/ 543]     layers.38.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]     layers.38.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]     layers.38.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 543]            layers.38.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]        layers.39.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]        layers.39.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]        layers.39.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]        layers.39.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]      layers.39.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 360/ 543]     layers.39.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]     layers.39.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]     layers.39.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 543]            layers.39.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]        layers.40.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]        layers.40.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]        layers.40.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]        layers.40.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]      layers.40.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 369/ 543]     layers.40.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]     layers.40.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]     layers.40.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 543]            layers.40.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]        layers.41.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]        layers.41.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]        layers.41.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]        layers.41.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]      layers.41.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 378/ 543]     layers.41.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]     layers.41.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]     layers.41.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 543]            layers.41.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]        layers.42.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]        layers.42.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]        layers.42.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]        layers.42.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]      layers.42.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 387/ 543]     layers.42.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]     layers.42.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]     layers.42.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 543]            layers.42.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]        layers.43.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]        layers.43.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]        layers.43.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]        layers.43.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]      layers.43.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 396/ 543]     layers.43.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]     layers.43.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]     layers.43.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 543]            layers.43.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]        layers.44.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]        layers.44.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]        layers.44.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]        layers.44.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]      layers.44.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 405/ 543]     layers.44.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]     layers.44.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]     layers.44.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 543]            layers.44.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]        layers.45.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]        layers.45.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]        layers.45.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]        layers.45.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]      layers.45.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 414/ 543]     layers.45.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]     layers.45.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]     layers.45.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 543]            layers.45.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]        layers.46.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]        layers.46.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]        layers.46.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]        layers.46.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]      layers.46.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 423/ 543]     layers.46.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]     layers.46.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]     layers.46.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 543]            layers.46.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]        layers.47.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]        layers.47.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]        layers.47.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]        layers.47.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]      layers.47.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 432/ 543]     layers.47.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]     layers.47.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]     layers.47.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 543]            layers.47.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]        layers.48.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]        layers.48.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]        layers.48.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]        layers.48.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]      layers.48.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 441/ 543]     layers.48.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]     layers.48.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]     layers.48.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 543]            layers.48.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]        layers.49.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]        layers.49.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]        layers.49.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]        layers.49.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]      layers.49.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 450/ 543]     layers.49.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]     layers.49.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]     layers.49.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 543]            layers.49.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]        layers.50.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]        layers.50.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]        layers.50.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]        layers.50.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]      layers.50.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 459/ 543]     layers.50.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]     layers.50.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]     layers.50.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 543]            layers.50.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]        layers.51.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]        layers.51.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]        layers.51.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]        layers.51.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]      layers.51.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 468/ 543]     layers.51.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]     layers.51.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]     layers.51.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 543]            layers.51.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]        layers.52.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]        layers.52.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]        layers.52.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]        layers.52.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]      layers.52.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 477/ 543]     layers.52.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]     layers.52.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]     layers.52.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 543]            layers.52.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]        layers.53.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]        layers.53.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]        layers.53.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]        layers.53.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]      layers.53.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 486/ 543]     layers.53.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]     layers.53.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]     layers.53.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 543]            layers.53.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]        layers.54.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]        layers.54.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]        layers.54.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]        layers.54.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]      layers.54.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 495/ 543]     layers.54.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]     layers.54.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]     layers.54.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 543]            layers.54.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]        layers.55.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]        layers.55.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]        layers.55.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]        layers.55.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]      layers.55.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 504/ 543]     layers.55.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]     layers.55.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]     layers.55.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 543]            layers.55.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]        layers.56.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]        layers.56.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]        layers.56.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]        layers.56.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]      layers.56.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 513/ 543]     layers.56.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 543]     layers.56.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 515/ 543]     layers.56.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 543]            layers.56.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]        layers.57.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]        layers.57.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]        layers.57.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]        layers.57.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]      layers.57.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 522/ 543]     layers.57.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 543]     layers.57.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 524/ 543]     layers.57.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 543]            layers.57.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]        layers.58.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]        layers.58.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]        layers.58.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]        layers.58.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]      layers.58.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 531/ 543]     layers.58.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 543]     layers.58.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 533/ 543]     layers.58.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 543]            layers.58.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]        layers.59.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]        layers.59.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]        layers.59.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]        layers.59.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]      layers.59.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 540/ 543]     layers.59.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 541/ 543]     layers.59.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 542/ 543]     layers.59.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 543/ 543]            layers.59.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 100841.48 ms\n",
      "main:    total time = 100841.48 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/65B/ggml-model-f16.bin' to './models/65B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/65B/ggml-model-q4_0.bin\n",
      "[   1/ 723]                tok_embeddings.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   140.62 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                          norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   205.08 MB | hist: \n",
      "[   4/ 723]         layers.0.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]         layers.0.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]         layers.0.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]         layers.0.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]       layers.0.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   9/ 723]      layers.0.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[  10/ 723]      layers.0.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  11/ 723]      layers.0.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  12/ 723]             layers.0.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  13/ 723]         layers.1.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]         layers.1.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]         layers.1.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]         layers.1.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]       layers.1.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  18/ 723]      layers.1.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]      layers.1.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]      layers.1.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 723]             layers.1.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  22/ 723]         layers.2.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]         layers.2.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]         layers.2.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]         layers.2.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]       layers.2.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  27/ 723]      layers.2.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]      layers.2.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]      layers.2.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 723]             layers.2.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  31/ 723]         layers.3.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]         layers.3.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]         layers.3.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]         layers.3.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]       layers.3.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  36/ 723]      layers.3.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]      layers.3.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]      layers.3.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 723]             layers.3.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  40/ 723]         layers.4.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]         layers.4.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]         layers.4.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]         layers.4.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]       layers.4.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  45/ 723]      layers.4.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]      layers.4.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]      layers.4.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 723]             layers.4.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  49/ 723]         layers.5.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]         layers.5.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]         layers.5.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]         layers.5.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]       layers.5.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  54/ 723]      layers.5.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]      layers.5.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]      layers.5.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 723]             layers.5.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  58/ 723]         layers.6.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]         layers.6.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]         layers.6.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]         layers.6.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]       layers.6.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  63/ 723]      layers.6.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]      layers.6.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]      layers.6.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 723]             layers.6.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  67/ 723]         layers.7.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]         layers.7.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]         layers.7.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]         layers.7.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]       layers.7.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  72/ 723]      layers.7.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]      layers.7.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]      layers.7.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 723]             layers.7.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  76/ 723]         layers.8.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]         layers.8.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]         layers.8.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]         layers.8.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]       layers.8.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  81/ 723]      layers.8.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]      layers.8.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]      layers.8.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 723]             layers.8.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  85/ 723]         layers.9.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]         layers.9.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]         layers.9.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]         layers.9.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]       layers.9.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  90/ 723]      layers.9.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 723]      layers.9.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 723]      layers.9.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 723]             layers.9.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  94/ 723]        layers.10.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]        layers.10.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]        layers.10.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]        layers.10.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]      layers.10.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  99/ 723]     layers.10.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 723]     layers.10.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 723]     layers.10.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 723]            layers.10.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]        layers.11.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]        layers.11.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]        layers.11.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]        layers.11.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]      layers.11.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 108/ 723]     layers.11.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]     layers.11.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]     layers.11.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 723]            layers.11.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]        layers.12.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]        layers.12.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]        layers.12.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]        layers.12.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]      layers.12.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 117/ 723]     layers.12.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]     layers.12.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]     layers.12.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 723]            layers.12.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]        layers.13.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]        layers.13.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]        layers.13.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]        layers.13.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]      layers.13.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 126/ 723]     layers.13.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]     layers.13.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]     layers.13.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 723]            layers.13.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]        layers.14.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]        layers.14.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]        layers.14.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]        layers.14.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]      layers.14.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 135/ 723]     layers.14.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]     layers.14.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]     layers.14.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 723]            layers.14.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]        layers.15.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]        layers.15.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]        layers.15.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]        layers.15.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]      layers.15.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 144/ 723]     layers.15.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 723]     layers.15.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 723]     layers.15.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 723]            layers.15.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]        layers.16.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]        layers.16.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]        layers.16.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]        layers.16.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]      layers.16.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 153/ 723]     layers.16.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]     layers.16.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]     layers.16.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 723]            layers.16.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]        layers.17.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]        layers.17.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]        layers.17.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]        layers.17.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]      layers.17.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 162/ 723]     layers.17.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]     layers.17.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]     layers.17.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 723]            layers.17.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]        layers.18.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]        layers.18.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]        layers.18.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]        layers.18.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]      layers.18.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 171/ 723]     layers.18.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]     layers.18.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]     layers.18.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 723]            layers.18.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]        layers.19.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]        layers.19.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]        layers.19.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]        layers.19.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]      layers.19.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 180/ 723]     layers.19.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]     layers.19.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]     layers.19.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 723]            layers.19.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]        layers.20.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]        layers.20.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]        layers.20.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]        layers.20.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]      layers.20.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 189/ 723]     layers.20.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]     layers.20.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]     layers.20.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 723]            layers.20.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]        layers.21.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]        layers.21.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]        layers.21.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]        layers.21.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]      layers.21.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 198/ 723]     layers.21.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]     layers.21.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]     layers.21.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 723]            layers.21.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]        layers.22.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]        layers.22.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]        layers.22.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]        layers.22.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]      layers.22.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 207/ 723]     layers.22.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]     layers.22.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]     layers.22.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 723]            layers.22.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]        layers.23.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]        layers.23.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]        layers.23.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]        layers.23.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]      layers.23.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 216/ 723]     layers.23.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]     layers.23.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]     layers.23.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 723]            layers.23.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]        layers.24.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]        layers.24.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]        layers.24.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]        layers.24.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]      layers.24.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 225/ 723]     layers.24.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]     layers.24.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]     layers.24.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 723]            layers.24.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]        layers.25.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]        layers.25.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]        layers.25.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]        layers.25.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]      layers.25.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 234/ 723]     layers.25.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]     layers.25.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]     layers.25.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 723]            layers.25.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]        layers.26.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]        layers.26.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]        layers.26.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]        layers.26.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]      layers.26.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 243/ 723]     layers.26.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]     layers.26.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]     layers.26.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 723]            layers.26.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]        layers.27.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]        layers.27.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]        layers.27.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]        layers.27.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]      layers.27.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 252/ 723]     layers.27.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]     layers.27.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]     layers.27.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 723]            layers.27.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]        layers.28.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]        layers.28.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]        layers.28.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]        layers.28.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]      layers.28.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 261/ 723]     layers.28.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]     layers.28.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]     layers.28.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 723]            layers.28.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]        layers.29.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]        layers.29.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]        layers.29.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]        layers.29.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]      layers.29.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 270/ 723]     layers.29.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]     layers.29.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]     layers.29.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 723]            layers.29.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]        layers.30.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]        layers.30.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]        layers.30.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]        layers.30.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]      layers.30.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 279/ 723]     layers.30.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]     layers.30.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]     layers.30.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 723]            layers.30.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]        layers.31.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]        layers.31.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]        layers.31.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]        layers.31.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]      layers.31.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 288/ 723]     layers.31.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]     layers.31.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]     layers.31.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 723]            layers.31.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]        layers.32.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]        layers.32.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]        layers.32.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]        layers.32.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]      layers.32.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 297/ 723]     layers.32.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]     layers.32.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]     layers.32.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 723]            layers.32.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]        layers.33.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]        layers.33.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]        layers.33.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]        layers.33.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]      layers.33.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 306/ 723]     layers.33.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]     layers.33.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]     layers.33.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 723]            layers.33.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]        layers.34.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]        layers.34.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]        layers.34.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]        layers.34.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]      layers.34.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 315/ 723]     layers.34.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]     layers.34.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]     layers.34.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 723]            layers.34.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]        layers.35.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]        layers.35.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]        layers.35.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]        layers.35.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]      layers.35.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 324/ 723]     layers.35.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]     layers.35.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]     layers.35.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 723]            layers.35.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]        layers.36.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]        layers.36.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]        layers.36.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]        layers.36.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]      layers.36.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 333/ 723]     layers.36.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]     layers.36.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]     layers.36.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 723]            layers.36.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]        layers.37.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]        layers.37.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]        layers.37.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]        layers.37.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]      layers.37.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 342/ 723]     layers.37.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]     layers.37.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]     layers.37.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 723]            layers.37.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]        layers.38.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]        layers.38.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]        layers.38.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]        layers.38.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]      layers.38.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 351/ 723]     layers.38.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]     layers.38.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]     layers.38.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 723]            layers.38.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]        layers.39.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]        layers.39.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]        layers.39.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]        layers.39.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]      layers.39.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 360/ 723]     layers.39.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]     layers.39.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]     layers.39.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 723]            layers.39.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]        layers.40.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]        layers.40.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]        layers.40.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]        layers.40.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]      layers.40.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 369/ 723]     layers.40.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]     layers.40.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]     layers.40.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 723]            layers.40.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]        layers.41.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]        layers.41.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]        layers.41.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]        layers.41.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]      layers.41.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 378/ 723]     layers.41.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]     layers.41.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]     layers.41.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 723]            layers.41.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]        layers.42.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]        layers.42.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]        layers.42.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]        layers.42.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]      layers.42.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 387/ 723]     layers.42.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]     layers.42.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]     layers.42.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 723]            layers.42.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]        layers.43.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]        layers.43.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]        layers.43.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]        layers.43.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]      layers.43.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 396/ 723]     layers.43.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]     layers.43.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]     layers.43.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 723]            layers.43.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]        layers.44.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]        layers.44.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]        layers.44.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]        layers.44.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]      layers.44.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 405/ 723]     layers.44.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]     layers.44.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]     layers.44.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 723]            layers.44.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]        layers.45.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]        layers.45.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]        layers.45.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]        layers.45.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]      layers.45.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 414/ 723]     layers.45.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]     layers.45.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]     layers.45.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 723]            layers.45.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]        layers.46.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]        layers.46.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]        layers.46.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]        layers.46.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]      layers.46.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 423/ 723]     layers.46.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]     layers.46.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]     layers.46.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 723]            layers.46.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]        layers.47.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]        layers.47.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]        layers.47.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]        layers.47.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]      layers.47.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 432/ 723]     layers.47.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]     layers.47.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]     layers.47.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 723]            layers.47.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]        layers.48.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]        layers.48.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]        layers.48.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]        layers.48.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]      layers.48.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 441/ 723]     layers.48.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]     layers.48.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]     layers.48.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 723]            layers.48.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]        layers.49.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]        layers.49.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]        layers.49.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]        layers.49.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]      layers.49.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 450/ 723]     layers.49.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]     layers.49.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]     layers.49.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 723]            layers.49.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]        layers.50.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]        layers.50.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]        layers.50.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]        layers.50.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]      layers.50.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 459/ 723]     layers.50.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]     layers.50.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]     layers.50.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 723]            layers.50.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]        layers.51.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]        layers.51.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]        layers.51.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]        layers.51.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]      layers.51.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 468/ 723]     layers.51.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]     layers.51.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]     layers.51.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 723]            layers.51.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]        layers.52.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]        layers.52.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]        layers.52.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]        layers.52.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]      layers.52.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 477/ 723]     layers.52.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]     layers.52.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]     layers.52.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 723]            layers.52.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]        layers.53.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]        layers.53.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]        layers.53.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]        layers.53.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]      layers.53.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 486/ 723]     layers.53.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]     layers.53.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]     layers.53.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 723]            layers.53.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]        layers.54.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]        layers.54.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]        layers.54.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]        layers.54.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]      layers.54.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 495/ 723]     layers.54.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]     layers.54.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]     layers.54.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 723]            layers.54.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]        layers.55.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]        layers.55.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]        layers.55.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]        layers.55.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]      layers.55.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 504/ 723]     layers.55.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]     layers.55.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]     layers.55.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 723]            layers.55.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]        layers.56.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]        layers.56.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]        layers.56.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]        layers.56.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]      layers.56.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 513/ 723]     layers.56.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]     layers.56.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]     layers.56.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 723]            layers.56.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]        layers.57.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]        layers.57.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]        layers.57.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]        layers.57.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]      layers.57.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 522/ 723]     layers.57.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]     layers.57.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]     layers.57.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 723]            layers.57.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]        layers.58.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]        layers.58.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]        layers.58.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]        layers.58.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]      layers.58.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 531/ 723]     layers.58.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]     layers.58.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]     layers.58.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 723]            layers.58.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]        layers.59.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]        layers.59.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]        layers.59.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]        layers.59.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]      layers.59.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 540/ 723]     layers.59.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]     layers.59.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]     layers.59.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 543/ 723]            layers.59.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]        layers.60.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]        layers.60.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]        layers.60.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]        layers.60.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]      layers.60.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 549/ 723]     layers.60.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]     layers.60.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]     layers.60.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 552/ 723]            layers.60.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]        layers.61.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]        layers.61.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]        layers.61.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]        layers.61.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]      layers.61.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 558/ 723]     layers.61.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]     layers.61.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]     layers.61.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 561/ 723]            layers.61.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]        layers.62.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]        layers.62.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]        layers.62.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]        layers.62.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]      layers.62.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 567/ 723]     layers.62.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]     layers.62.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]     layers.62.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 570/ 723]            layers.62.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]        layers.63.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]        layers.63.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]        layers.63.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]        layers.63.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]      layers.63.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 576/ 723]     layers.63.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]     layers.63.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]     layers.63.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 579/ 723]            layers.63.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]        layers.64.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]        layers.64.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]        layers.64.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]        layers.64.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]      layers.64.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 585/ 723]     layers.64.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]     layers.64.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]     layers.64.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 588/ 723]            layers.64.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]        layers.65.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]        layers.65.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]        layers.65.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]        layers.65.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]      layers.65.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 594/ 723]     layers.65.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]     layers.65.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]     layers.65.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 597/ 723]            layers.65.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]        layers.66.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]        layers.66.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]        layers.66.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]        layers.66.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]      layers.66.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 603/ 723]     layers.66.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]     layers.66.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]     layers.66.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 606/ 723]            layers.66.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]        layers.67.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]        layers.67.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]        layers.67.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]        layers.67.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]      layers.67.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 612/ 723]     layers.67.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]     layers.67.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]     layers.67.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 615/ 723]            layers.67.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]        layers.68.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]        layers.68.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]        layers.68.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]        layers.68.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]      layers.68.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 621/ 723]     layers.68.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]     layers.68.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]     layers.68.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 624/ 723]            layers.68.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]        layers.69.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]        layers.69.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]        layers.69.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]        layers.69.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]      layers.69.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 630/ 723]     layers.69.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]     layers.69.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]     layers.69.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 633/ 723]            layers.69.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]        layers.70.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]        layers.70.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]        layers.70.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]        layers.70.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]      layers.70.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 639/ 723]     layers.70.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]     layers.70.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]     layers.70.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 642/ 723]            layers.70.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]        layers.71.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]        layers.71.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]        layers.71.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]        layers.71.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]      layers.71.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 648/ 723]     layers.71.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]     layers.71.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]     layers.71.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 651/ 723]            layers.71.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]        layers.72.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]        layers.72.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]        layers.72.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]        layers.72.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]      layers.72.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 657/ 723]     layers.72.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]     layers.72.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]     layers.72.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 660/ 723]            layers.72.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]        layers.73.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]        layers.73.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]        layers.73.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]        layers.73.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]      layers.73.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 666/ 723]     layers.73.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]     layers.73.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]     layers.73.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 669/ 723]            layers.73.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]        layers.74.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]        layers.74.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]        layers.74.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]        layers.74.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]      layers.74.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 675/ 723]     layers.74.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]     layers.74.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]     layers.74.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 678/ 723]            layers.74.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]        layers.75.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]        layers.75.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]        layers.75.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]        layers.75.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]      layers.75.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 684/ 723]     layers.75.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]     layers.75.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]     layers.75.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 687/ 723]            layers.75.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]        layers.76.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]        layers.76.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]        layers.76.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]        layers.76.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]      layers.76.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 693/ 723]     layers.76.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 694/ 723]     layers.76.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 695/ 723]     layers.76.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 696/ 723]            layers.76.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]        layers.77.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]        layers.77.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]        layers.77.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]        layers.77.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]      layers.77.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 702/ 723]     layers.77.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 703/ 723]     layers.77.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 704/ 723]     layers.77.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 705/ 723]            layers.77.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]        layers.78.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]        layers.78.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]        layers.78.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]        layers.78.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]      layers.78.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 711/ 723]     layers.78.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 712/ 723]     layers.78.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 713/ 723]     layers.78.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 714/ 723]            layers.78.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]        layers.79.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]        layers.79.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]        layers.79.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]        layers.79.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]      layers.79.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 720/ 723]     layers.79.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 721/ 723]     layers.79.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 722/ 723]     layers.79.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 723/ 723]            layers.79.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 227401.85 ms\n",
      "main:    total time = 227401.85 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.bin ./models/13B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.bin ./models/30B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.bin ./models/65B/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574034\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. To be happy and feel good while doing that, you must have a purpose in your life. And if you don’t know what your purpose is, then you are probably not living your full potential. If we all lived our full potential, the world would be a much better place.\n",
      "Here are some of my suggestions for finding your purpose:\n",
      "1) Start doing stuff! Don’t just sit around and think about life… Do something! Learn how to play an instrument or learn to speak another language, create art, get involved in political debates, go camping, do volunteer work, start a business, etc. Just start doing things that will help you find your purpose in life.\n",
      "2) Find the right people who can help you find your purpose. Your parents and family are not going to be able to help you find your purpose. You need people who have lived their full potential, and have something to say about it. A mentor or a guru will probably be able to give you advice on how to start living your full potential.\n",
      "3) Be courageous enough to try out new things. Try out different things until you find the right thing that makes you feel good inside. If you just sit around and think, then you are not going to be able to find your purpose in life.\n",
      "4) Think about what you want out of life. What kind of lifestyle do you want? Do you want a family? Have your own business? Be an artist? Whatever it is that you want, find the way to get there. Find the steps that you need to take and go for it!\n",
      "5) Look at what other people are doing in life. Don’t copy them, but learn from their experience. You might be able to discover a new purpose in your life by seeing how others live their lives.\n",
      "6) Listen to your heart and soul. Ask yourself questions like: “What do I want out of my life? What makes me happy?” If you are not sure what these questions mean, then ask someone who can help you find your purpose in life.\n",
      "7) Think about the things that make you feel good inside. Doing stuff that makes you happy is a great way to start finding your purpose in life. If you don’t know what kind of stuff brings you happiness, then ask yourself questions like: “What kinds of activities do I really enjoy doing?” or “What kind of music do I listen\n",
      "llama_print_timings:        load time =  6893.00 ms\n",
      "llama_print_timings:      sample time =   251.81 ms /   512 runs   (    0.49 ms per token,  2033.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1338.23 ms /   265 tokens (    5.05 ms per token,   198.02 tokens per second)\n",
      "llama_print_timings:        eval time = 11052.38 ms /   510 runs   (   21.67 ms per token,    46.14 tokens per second)\n",
      "llama_print_timings:       total time = 12730.35 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f7a0912-2746-4e7d-adaf-92b60cb2ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574057\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live this day in the most purposeful way you can.\n",
      "I believe that as long as there are people who need help, there will always be a place for you to give it.\n",
      "As long as there are people who have dreams, there will always be someone to help them realize those dreams.\n",
      "You’re never too old or young to make a difference in this world, and the time to do so is now.\n",
      "You’re never too young or old to start making your mark on the world, no matter how big or small that mark may be.\n",
      "You can never be successful if you don’t find joy in what you are doing.\n",
      "While there will always be a place for you to give it, there is also a need for you to receive it.\n",
      "And while you’re at it, remember that helping others should not just be limited to the people who are close to you. There are millions of strangers out there who could use your help as well.\n",
      "You can make a difference in this world by being kind and generous to other people even if they don’t deserve it because it is a sin to turn away from another person in need.\n",
      "No matter what your age, you will never be too old or young to give your time, talent and treasure to others in need.\n",
      "Life is like the weather. It can change on a dime so learn to enjoy every day and make the most out of it.\n",
      "If you’re not doing something that makes you happy, then do something else because life is too short to spend it doing things that will only bring you misery or regrets.\n",
      "There are millions of people in this world who need someone to listen to them so don’t be afraid to speak up and let your voice be heard. The people around us can always use a helping hand, not just financially but emotionally as well.\n",
      "Don’t ever stop learning because life is meant to be lived with a purpose, and this can never happen unless you become the best person that you can possibly be.\n",
      "You should know that every day of your life has been pre-ordained by God so enjoy it while you’re still alive. The moment you stop living, that’s when you die.\n",
      "So many people around us are experiencing hardships and misfortunes but the one thing they can always count on is for someone to show them love and kindness.\n",
      "Whenever possible\n",
      "llama_print_timings:        load time =  1945.95 ms\n",
      "llama_print_timings:      sample time =   246.66 ms /   512 runs   (    0.48 ms per token,  2075.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1353.18 ms /   265 tokens (    5.11 ms per token,   195.83 tokens per second)\n",
      "llama_print_timings:        eval time = 10941.43 ms /   510 runs   (   21.45 ms per token,    46.61 tokens per second)\n",
      "llama_print_timings:       total time = 12629.64 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3324a21-7a43-4c68-83eb-52417e487c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574074\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live in a way that will make you happy.\n",
      "I would like to do it differently but my inner fears, my insecurities are too big and I need to conquer those before I can feel satisfied with myself.\n",
      "As long as I am alive, I have a chance to change and become the best version of me.\n",
      "The meaning in life is to be happy!\n",
      "Previous Post Previous post:What’s Your Favourite?\n",
      "Next Post What’s your favourite?\n",
      "Lovely post and so true!\n",
      "Thank you for visiting my blog, and following it ��� I am also a newbie in the world of blogging.\n",
      "I would like to ask you, is there any problem with the name of this site? Wouldn’t it be better to change it to something more meaningful?\n",
      "Hi, no there isn’t any problem with the name but thanks for asking. I don’t have a specific theme and I am still figuring out what my blog will be about.\n",
      "You are not alone. We all go through life, at times feeling lost or directionless. I too feel that way, sometimes. But we can do something about it. We can start by finding our purpose in life; our reason for being here on earth.\n",
      "I would like to share with you a link where you will find some great advice from an experienced blogger who also struggled for quite some time before she was able to find her voice and purpose online. I hope this article will be of help to you as it was to me.\n",
      "Thank you for your words, you’re right we can do something about it.\n",
      "Hi, thank you so much for your kind words!\n",
      "Nice blog here! Also your site a lot up fast! What host are you using? Can I get your affiliate link to your host? I want to say that this is an excellent read. Thanks for sharing….\n",
      "Thank you! I use hosting with Bluehost and WordPress. Thanks for reading ���� Have a great day.\n",
      "It’s a great answer!!! Thank you so much!!!! You’re awesome.\n",
      "Love the positive outlook on life, that is what we all need.\n",
      "I was searching for your email address and I found this post. I just wanted to say thank you for following my blog!\n",
      "Your posts are really inspiring. I look forward to reading more of\n",
      "llama_print_timings:        load time =  1596.28 ms\n",
      "llama_print_timings:      sample time =   252.94 ms /   512 runs   (    0.49 ms per token,  2024.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1254.15 ms /   265 tokens (    4.73 ms per token,   211.30 tokens per second)\n",
      "llama_print_timings:        eval time = 10886.30 ms /   510 runs   (   21.35 ms per token,    46.85 tokens per second)\n",
      "llama_print_timings:       total time = 12480.80 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a98a39e0-c339-4234-b6d2-49d642ffb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574090\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living it.\n",
      "We all have a responsibility for our actions, and we also must take responsibility for our own happiness. We cannot rely on others to make us happy; that would be like asking someone else to scratch an itch you can't reach yourself. We are responsible for our own lives, and that means we must find out what makes us whole or healthy or happy and then go after the things that bring those qualities.\n",
      "I have always believed in the importance of looking to the future with hope and confidence. I believe it is possible to be a realist without being negative, to be an optimist without denying the reality around you. It is possible to take life seriously while still managing to enjoy it.\n",
      "The first rule of leadership: everything is your fault.\n",
      "I don't want to make decisions based on fear or potential problems; I deal with those as they arise, but I don't anticipate them beforehand. That way, if something goes wrong there will be no doubt in my mind that it was the right decision.\n",
      "The first step is to accept responsibility for everything you do and everything you feel.\n",
      "There are all kinds of ways to be successful—you can take shortcuts or be lazy; you can lie or cheat your way along, or cut corners and just look out for yourself. But in my opinion this kind of success doesn't mean anything. It is not the real thing. When you do things right, when you get a job done well, that's real success.\n",
      "It's hard to accept that people will love us only as long as they agree with us—as if we were gods and had some special knowledge no one else has. We tend to forget that our minds are fallible and that it is possible for someone else to see things in a different way than we do, or even have information we don't know about.\n",
      "It was an honor for me to be given the opportunity to play with the great musicians I worked with at Blue Note Records; they were some of the best jazz artists alive and I learned so much from them, not just about music but about life as well.\n",
      "I'm a very spiritual person who believes that we are all connected to each other in everything we do. It is not our differences that separate us, it is how we perceive those differences.\n",
      "Too often we allow other people and their ideas to affect the way we think and\n",
      "llama_print_timings:        load time = 12047.28 ms\n",
      "llama_print_timings:      sample time =   249.13 ms /   512 runs   (    0.49 ms per token,  2055.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1649.12 ms /   265 tokens (    6.22 ms per token,   160.69 tokens per second)\n",
      "llama_print_timings:        eval time = 15031.03 ms /   510 runs   (   29.47 ms per token,    33.93 tokens per second)\n",
      "llama_print_timings:       total time = 17019.04 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0296e978-7c63-4d87-a1a9-dc41b49dc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574121\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "But what's in it? The beauty that you see in it,\n",
      "the joy that you feel from it, that's the meaning.\n",
      "The first step in a journey of a thousand miles is to take a hundred steps...\n",
      "And you have to do it one at a time.\n",
      "Love is the condition in which the happiness of another person is essential to your own.\n",
      "I will always be grateful for what I learned and continue to learn about love, life and living from my father. And though he was not perfect (as we all know no one is), I could not have asked for a better role model or teacher. As you read this post, as you think about his legacy - and the lives of others who have touched your own in meaningful ways - keep in mind that each of us has a chance to be someone else's 'Fred Katz.'\n",
      "And that is what life is all about, isn't it?\n",
      "This weekend my family gathered in Philadelphia for a memorial service. It was a beautiful and meaningful occasion filled with love and respect for the memory of a man who lived his life well - and who left us far too soon!\n",
      "Labels: Family, Life Lessons, Love, Respect\n",
      "The world will never run out of people like you who can inspire others to be better human beings. And your dad would be proud that he had such a daughter as you.\n",
      "That is the most beautiful tribute I've ever seen on someone's blog. Thank you for sharing!\n",
      "Wow, what an amazing legacy Fred Katz left behind. What a very lovely and touching piece of writing. The world needs more people like him.\n",
      "What a wonderful tribute. Your father sounds like he was an amazing man... I can see why you were so proud to call him yours.\n",
      "You're right, this IS what life is all about!\n",
      "I am so sorry for your family's loss. It sounds as if Fred Katz lived fully and loved well. Thank you for sharing his legacy with the world.\n",
      "What a beautiful tribute. I hope that when my time comes my children can say the same thing about me.\n",
      "This is beautiful, Nancy!\n",
      "Hey, just read your blog, and it's great. I'm sorry for your loss though.\n",
      "I've been going through similar stuff lately as well. My uncle\n",
      "llama_print_timings:        load time =  1458.48 ms\n",
      "llama_print_timings:      sample time =   251.43 ms /   512 runs   (    0.49 ms per token,  2036.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1636.12 ms /   265 tokens (    6.17 ms per token,   161.97 tokens per second)\n",
      "llama_print_timings:        eval time = 15681.37 ms /   510 runs   (   30.75 ms per token,    32.52 tokens per second)\n",
      "llama_print_timings:       total time = 17657.62 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebca2498-774d-4f51-ac56-a9da8f359523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574143\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m helping others discover their own potential.\n",
      "I’m a 20-something year old girl from Denmark and I have been dreaming about this for as long as I can remember. And it’s not just the money that motivates me, but to change lives and make the world a better place!\n",
      "But what really inspired me was when I read the book “Rich Dad Poor Dad” by Robert Kiyosaki (I highly recommend you to check this out if you haven’t yet). One of the chapters is about financial education in schools. And I think that it would be so amazing, if we could learn more about finances, how money works and stuff like that, in school.\n",
      "And when I saw all the amazing people here on Patreon who are sharing their knowledge with others – I was truly inspired to do the same!\n",
      "I’m not gonna tell you how this is going to work. Because I really want to keep it a surprise.. But let me just say that I have been working hard behind the scene for months and preparing everything in advance. It’s all ready, so from now on it will be easy to implement. And the first episode will air in late March.\n",
      "If you wanna get the latest updates about this project – go ahead and sign up below!\n",
      "I hope that I can help you make your dream come true, just as many other amazing people have done for me so far!!\n",
      "Get access to my Patreon-only feed with exclusive content and sneak peeks.\n",
      "Get a special thank you in the end of every episode!\n",
      "You'll get all the rewards from earlier pledges PLUS an invite to a Facebook group where we can talk about the project, the episodes etc.\n",
      "You'll get all the previous rewards PLUS a 30% discount on my online course \"How To Become A Successful Blogger\"!\n",
      "Wow, thank you so much for supporting me!! I'm gonna do everything in my power to make sure this project will be worth your support. And I can promise you that it’s going to be an amazing ride!\n",
      "If we reach $500 per month I will be able to create more content and spend more time on the project, without having to take a full-time job in order to pay for it.. This means that I'll be able to spend\n",
      "llama_print_timings:        load time =  1492.44 ms\n",
      "llama_print_timings:      sample time =   247.09 ms /   512 runs   (    0.48 ms per token,  2072.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1870.63 ms /   265 tokens (    7.06 ms per token,   141.66 tokens per second)\n",
      "llama_print_timings:        eval time = 14903.76 ms /   510 runs   (   29.22 ms per token,    34.22 tokens per second)\n",
      "llama_print_timings:       total time = 17112.18 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6d246e2-1ed4-46f7-aa3e-0d4787cc3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574164\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love.\n",
      "I’m a writer, but I don’t think that means anything. If you write, it doesn’t make you special or better than anyone else; it just makes you human. As humans we all have stories inside us. The ones that need sharing are the ones that can help others. It’s why I write what I do. I want to be a light in the darkness, a voice for those who can’t speak out yet and an example of survival and strength.\n",
      "I was born in 1987 with cerebral palsy. When I was born my father walked out on us. He couldn’t handle it. He left and never came back. My mother, however, was a super-hero. She stayed with me and took care of me. She gave up her dreams so that she could help me achieve mine.\n",
      "My childhood was normal. I went to school, played with friends, had sleepovers and birthday parties like everyone else. I got made fun of for being different, but I learned at a young age that if you can’t beat them join them. My best friend at the time and I would laugh about the names we were called because it didn’t hurt us. We became inseparable, until she moved away. After that my life changed drastically.\n",
      "I was bullied by classmates for being different. It started out small with notes and pranks, but as the years went on the bullying got worse and I wasn’t able to do anything about it. The school wouldn’t help me, my mother couldn’t do much either because she was at work, and the police didn’t care unless someone died or got seriously injured.\n",
      "After high school I moved to a new town in hopes of starting over, but I was wrong. After moving, I met a man online who convinced me he cared about me. That wasn’t true though; he just wanted to use and abuse me. For four years I suffered physical, mental, emotional, verbal, sexual and financial abuse. I lost myself completely.\n",
      "On January 1st, 2015 the man who was supposed to love me raped me and beat me so badly that I ended up in the hospital. I called my mother from the hospital bed and asked her to come get me. She did. The next day she went with me to\n",
      "llama_print_timings:        load time = 29052.42 ms\n",
      "llama_print_timings:      sample time =   251.29 ms /   512 runs   (    0.49 ms per token,  2037.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3026.99 ms /   265 tokens (   11.42 ms per token,    87.55 tokens per second)\n",
      "llama_print_timings:        eval time = 28164.34 ms /   510 runs   (   55.22 ms per token,    18.11 tokens per second)\n",
      "llama_print_timings:       total time = 31531.39 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce27ab44-b446-4126-8972-7472c658a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574227\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in understanding what we are and how we came to be.\n",
      "I believe the meaning of life is to be found in understanding what we are and how we came to be, but I don’t believe we will ever understand ourselves completely. We are so infinitely complex as individuals that there will always be a mysterious element to our lives. I think this is wonderful because it gives us something to live for and keeps the excitement going.\n",
      "I also don’t believe that if you were brought up in different circumstances, you would necessarily have been a different person. I believe people are born with certain genes and they will develop as those genes dictate. But I do feel we are all capable of becoming more than our genes allow us to be initially.\n",
      "I don’t think any aspect of life is overrated, but I do think some things are misunderstood because the majority of people lack self-awareness. Most humans are in a state of denial about what they want and need from life. In general, we all like to believe that our lives are different from everyone else’s, while at the same time assuming that everyone wants the same things as us: money, success, a happy family life, good health. The world is full of people who chase these goals because they think they will make them happy, but I don’t believe they actually lead to happiness. We are all looking for something different and we need to remember that when dealing with others.\n",
      "I would like to be remembered as a person who was curious about the world and who had a good sense of humour.\n",
      "I don’t think you can ever be prepared for death, but I do believe it is important to have done all you can before you die. To me this means that you have not wasted time or energy on things that don’t matter: you have lived in the moment and appreciated what life has given you.\n",
      "I think happiness comes from understanding yourself and living your life accordingly, but I do believe there are things we will never understand about ourselves. So we must accept that fact and make our peace with it. That is how we find some kind of contentment. If someone is unhappy, the first step to making a change is to recognise this. Then you have to take responsibility for your life. You cannot blame anyone else if you are not happy. No one has the right to make you unhappy. Your family or friends can help and give advice\n",
      "llama_print_timings:        load time =  2692.34 ms\n",
      "llama_print_timings:      sample time =   255.81 ms /   512 runs   (    0.50 ms per token,  2001.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3067.92 ms /   265 tokens (   11.58 ms per token,    86.38 tokens per second)\n",
      "llama_print_timings:        eval time = 28737.10 ms /   510 runs   (   56.35 ms per token,    17.75 tokens per second)\n",
      "llama_print_timings:       total time = 32149.90 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eec90d1-7b7b-4071-af17-849bcb505108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574265\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live, and to live fully. To me, that means living in a manner that is true to your soul; finding your path and walking it with joy; exploring what is meaningful to you; giving back generously to those around you; appreciating every moment and allowing yourself to feel all the emotions of life – from exhilaration to despair. The meaning of life is not a singular, static state but a dynamic process that evolves as we do – it is what allows us to grow into who we are meant to be.\n",
      "I believe the purpose of life is to love and to learn; that we are here to experience everything this world offers so that we can become more fully ourselves. The purpose of life is not the same for everyone, but rather unique for each individual – it is what allows us to grow into who we are meant to be.\n",
      "I believe that our soul exists forever and that there is a higher power. I don’t have all the answers (or even most) about how this works; I just know that my own experiences, as well as those of friends, family, and even strangers I’ve heard about or met, have shown me time and again that we are more than our bodies – we are energy in a physical form. This higher power is one with us, a part of us, but also greater than us; it is what allows us to grow into who we are meant to be.\n",
      "I believe there is no right or wrong way to live life. We are all on this earth to explore and experience the world around us in our own way and at our own pace, and to learn from each other’s unique experiences. There may be a “best” way for you as an individual, but it may not work for someone else. What is right for one person is not necessarily right for another – no two people are the same, after all! We all have something in common with others, but we also each have our own unique perspective that can’t be learned from anyone else. It is what allows us to grow into who we are meant to be.\n",
      "I believe we should treat everyone with respect and kindness because we are all connected; we are all made of the same energy, the same spirit. We should live in a manner that brings peace and joy not only to ourselves but also to others. We should do unto others as we would have them do unto us. It is what allows us to grow into who we are\n",
      "llama_print_timings:        load time =  2656.16 ms\n",
      "llama_print_timings:      sample time =   255.13 ms /   512 runs   (    0.50 ms per token,  2006.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2920.96 ms /   265 tokens (   11.02 ms per token,    90.72 tokens per second)\n",
      "llama_print_timings:        eval time = 28784.83 ms /   510 runs   (   56.44 ms per token,    17.72 tokens per second)\n",
      "llama_print_timings:       total time = 32052.59 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04719be2-a272-4fc9-aed8-193e4bfe9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574302\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I have always been fascinated by people who are able to use their creativity and imagination to turn their ideas into art, be it a beautiful painting or an amazing piece of music.\n",
      "To me there is nothing more satisfying than to create something beautiful that has never existed before…or to take something old and broken and make it new again. I think that is what life is all about – creating beauty in this world that we live in!\n",
      "For many years, I have been doing just that through my work as a graphic designer at my company, K2 Designs. Now that my two children are older (and need me less), I am excited to explore other creative outlets and share some of the things I love with you.\n",
      "So please join me in this journey of discovery where we will take old things that have been forgotten or ignored by the world, and together bring them back to life! We will show how they can be transformed into something special and beautiful once again.\n",
      "And if you want, let’s talk about our lives – what makes us happy, what makes us sad, what gives us hope – and share those stories with each other and the rest of the world. Let’s be honest, but not judgmental or hurtful; let’s laugh, cry and celebrate together!\n",
      "I look forward to meeting you and getting to know who you are!\n",
      "Hey Karen, I love your idea about what life is all about.. “creating beauty in this world that we live in”. That is so true. I will be looking forward to seeing more of your work!\n",
      "Thank you for sharing the beautiful photos of the wedding. I am blessed to have been there and witnessed the joy, love, laughter, fun, smiles… I could go on and on ���� Thank you for inviting us into your lives!\n",
      "Karen, It was nice meeting you last week at the LCMS church. I’m glad we got to talk about our blogs and share ideas. I like what you are doing here. Keep up the good work and keep in touch.\n",
      "Thank you so much for stopping by my blog! I look forward to reading more of your posts as well.\n",
      "Beautiful site Karen – can’t wait to see where this journey takes us!!\n",
      "Thanks, Kathy! I appreciate it! We\n",
      "llama_print_timings:        load time = 41233.70 ms\n",
      "llama_print_timings:      sample time =   250.85 ms /   512 runs   (    0.49 ms per token,  2041.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5114.32 ms /   265 tokens (   19.30 ms per token,    51.82 tokens per second)\n",
      "llama_print_timings:        eval time = 44076.32 ms /   510 runs   (   86.42 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time = 49532.79 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1730091a-3146-476d-ab57-49f8722c351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574397\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "If you feel a little out of sorts today, don’t worry! A full moon lunar eclipse is happening this evening, so you might want to keep your feelings in check until tomorrow. During these eclipses, emotions can run high and it may be difficult to see things clearly.\n",
      "While the world around us seems to be spiraling out of control, we have a chance to take control of our own lives by making small but important changes. One such change is in the way we interact with each other. The words we speak are powerful. With them, we can build someone up and make their day better or tear someone down and make their life worse.\n",
      "It’s so easy to get caught up in our own lives that we forget there are people around us who may need encouragement. I know there have been times when I thought my world was collapsing, only to find out that the person next to me is going through something much worse than what I am experiencing.\n",
      "Today, let’s make a pact with each other to be kind and speak positive words to those we come into contact with. It won’t cost us anything but it can change someone’s life forever!\n",
      "I have a new book coming out in May, called “The 30-Day Sobriety Solution.” The book is about how to break the cycle of addiction and help you reclaim your mind, body, and spirit. It’s available for preorder on Amazon.com now, so make sure you check it out!\n",
      "Learn more about my new book here: https://www.amazon.com/30-Day-Sobriety-Solution-Crave-Alcohol/dp/1628735946?ie=UTF8&qid=&ref_=tmm_hrd_swatch_0&sr=\n",
      "Tags: 30-day sobriety solution, addiction, amazon.com, be kind, change, cycle of addiction, daily message, eclipse, emotions, feelings, full moon, give it away, gift, interaction, Jack Canfield, life, lunar eclipse, May, meaning of life, pact, powerful words, purpose of life, spiraling out of control, speak positive, spiraling upward, spoken word, the\n",
      "llama_print_timings:        load time =  5646.66 ms\n",
      "llama_print_timings:      sample time =   248.72 ms /   512 runs   (    0.49 ms per token,  2058.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4928.56 ms /   265 tokens (   18.60 ms per token,    53.77 tokens per second)\n",
      "llama_print_timings:        eval time = 43344.66 ms /   510 runs   (   84.99 ms per token,    11.77 tokens per second)\n",
      "llama_print_timings:       total time = 48612.03 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "351608bd-64ae-4f26-9fb5-ccf3ee5fb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574455\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to follow your passion, and mine is travel. That’s why 13 years ago, I founded Elevate Destinations with my husband, Russ. Our goal was to create a company that provides opportunities for travelers who want to combine their passion for seeing the world with helping others in need. Since then, we have been privileged to design and organize thousands of trips for clients who share our vision.\n",
      "I believe that travel can be a powerful force in making our lives – and the world – better. At Elevate Destinations, we are proud to offer customized travel itineraries that focus on responsible tourism, authentic culture, wildlife conservation and community development. We work closely with each of our clients to create unique trips that reflect their specific interests and needs.\n",
      "I’ve been a global traveler my whole life, and am lucky enough to have visited 120+ countries. I feel very fortunate to be able to apply my skills in the service of others, and share my love for responsible tourism with our clients.\n",
      "Most memorable travel moment? The day that my husband Russ and I spent the night in a Maasai village in Kenya. What an incredible experience! We had never been so warmly welcomed by a community – it was truly touching. It’s experiences like these that make me feel more connected to the world we live in.\n",
      "Favorite airplane movie? I don’t watch movies on planes, but I do read a lot! Whenever possible, my reading material includes books about the country I am visiting so that I can learn even more about the place before arriving.\n",
      "What are you reading right now? “We Were The Lucky Ones” by Georgia Hunter. It’s a true story of one family’s survival during World War II.\n",
      "How do people greet each other in your native country? In South Africa, we don’t say much – we simply shake hands!\n",
      "What is the best thing about traveling with children? I love seeing the world through their eyes. When you are a child, everything seems so big and amazing! Children also tend to bring out the best in people around them – they seem to be able to connect with anyone, no matter where they come from.\n",
      "Favorite foreign word or phrase: “Lekker” is an Afrikaans word that means\n",
      "llama_print_timings:        load time =  5440.16 ms\n",
      "llama_print_timings:      sample time =   252.23 ms /   512 runs   (    0.49 ms per token,  2029.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4790.22 ms /   265 tokens (   18.08 ms per token,    55.32 tokens per second)\n",
      "llama_print_timings:        eval time = 45435.50 ms /   510 runs   (   89.09 ms per token,    11.22 tokens per second)\n",
      "llama_print_timings:       total time = 50568.12 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff34085-ceab-48a8-96cb-f47d3f8cc5b3",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "303e2727-09d0-47be-818f-2a6519aa5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574515\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. As long as you are happy, you will be able to do your best in everything that comes along with it.\n",
      "The hardest thing about being happy is that it’s very easy to feel unhappy even when things are going extremely well for us, which can make us question the meaning of life and our existence. It’s something I struggle with every day, but I know that if I can be happy in the present moment (which can be difficult), then I will always find meaning in my life because I will have the ability to do anything that comes along with it.\n",
      "I think the meaning of life is happiness.\n",
      "The meaning of life is to be happy and to help others, even if you don’t know them.\n",
      "To live your dreams without hesitation. To love yourself as much as you love those around you. The meaning of life is never ending. It’s like a journey that takes you on an infinite number of paths in which you will learn and grow. It’s about the people you meet along the way, the challenges they help you overcome, the choices you make to get closer to your dreams. Every moment counts so live life to the fullest.\n",
      "The meaning of life is happiness.\n",
      "I think that my meaning of life is living in the now and enjoying each day as it comes because you never know what’ll happen tomorrow. I do this by spending time with family and friends, going outdoors, staying active, being happy and enjoying all the small things in life.\n",
      "The meaning of my life is to be happy and spread love.\n",
      "I think that happiness is a prerequisite for living a fulfilling life. Happiness makes us feel contented with ourselves and those around us, it allows us to appreciate what we have as well as those who are important in our lives. I believe that the meaning of my life is to be happy and make other people happy too.\n",
      "I think that being happy is a big part of living an authentic life. If you’re not happy then something isn’t right, so you need to figure out what it is and how to fix it before you can move on with your life. I try to stay positive every day because it affects the lives around me and makes it better for them.\n",
      "The meaning of my life is to be authentic, real and true to myself in all situations.\n",
      "I think that being authentic means being\n",
      "llama_print_timings:        load time = 20159.94 ms\n",
      "llama_print_timings:      sample time =   247.92 ms /   512 runs   (    0.48 ms per token,  2065.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1316.42 ms /   265 tokens (    4.97 ms per token,   201.30 tokens per second)\n",
      "llama_print_timings:        eval time = 14494.17 ms /   510 runs   (   28.42 ms per token,    35.19 tokens per second)\n",
      "llama_print_timings:       total time = 16146.53 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "061c4fdf-de31-47b9-a6f7-55873c2b690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574554\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make a difference in someone else’s life.\n",
      "I have had the opportunity to work with children and families for over 20 years through school counseling, parent coaching, and family therapy services. My goal is to support clients on their journey of healing and growth so that they may live their lives fully connected, inspired, and content.\n",
      "My clinical experience includes working in a private practice setting; providing mental health counseling services for children, adolescents, adults, couples, and families. I have provided case management services for individuals of all ages with mental illnesses, developmental disabilities, traumatic brain injuries, and substance abuse addictions.\n",
      "I have also worked as a clinical supervisor overseeing the work of therapists in training. Additionally, I have supported families of children with special needs through parent coaching services; enhancing communication, building self-esteem, and improving family relationships.\n",
      "Through my work with children, I developed an interest in supporting parents of young children to strengthen their relationship with their child. This inspired me to pursue the field of Family Therapy which has led me to work with parents and families in a therapeutic manner; helping them to develop positive patterns of communication, problem solving skills, and learn to better understand one another.\n",
      "In addition to working with children and families, I have provided individual counseling services for adults on the autism spectrum, individuals with traumatic brain injuries, and those suffering from depression, anxiety, relationship problems, and other emotional concerns.\n",
      "I value my work as a therapist because of the impact it has had on others and myself. The clients who have trusted me enough to share their deepest thoughts, feelings, and experiences truly inspire me. Having the opportunity to see them overcome challenges that they once thought were impossible has been humbling and rewarding. I am grateful for this work because it has helped me grow as a person and given me greater insight into what is possible when we are able to tap into our innate wisdom, strengths, and resiliency.\n",
      "My approach to therapy is eclectic; drawing upon various theories of psychology to best support individual needs. I use an empathetic, warm, and caring style in my work with clients. In addition, I incorporate a variety of techniques from cognitive behavioral\n",
      "llama_print_timings:        load time =  2209.30 ms\n",
      "llama_print_timings:      sample time =   250.55 ms /   512 runs   (    0.49 ms per token,  2043.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1347.47 ms /   265 tokens (    5.08 ms per token,   196.67 tokens per second)\n",
      "llama_print_timings:        eval time = 14621.70 ms /   510 runs   (   28.67 ms per token,    34.88 tokens per second)\n",
      "llama_print_timings:       total time = 16307.73 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb37cbe3-16ab-4f63-9903-2e7951ce751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574575\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love others.\n",
      "Loving others is a process of understanding, appreciating and respecting others. It's like being in a long distance relationship...I may not always be able to see you, but that doesn't mean I don't love you.\n",
      "We are all unique individuals with different circumstances, experiences and perspectives. The way we perceive people and the world is affected by our past and present experiences (including beliefs, perceptions, feelings, etc.). That's why loving others is a process...we need to continuously learn about them in order to love them better.\n",
      "The more we know about other people, the easier it will be for us to understand them and respect their differences. This way, we can have healthier relationships with each other. :)\n",
      "Loving others is also like eating a stew...the ingredients are similar, but you need enough time in order to mix everything well before serving them on your plate. It's the same with people...it takes a lot of time and effort to understand their needs and wants. We have different personalities, background, opinions and beliefs which make it difficult for us to relate to each other.\n",
      "Hence, loving others requires patience, understanding and compassion. The more we learn about others, the better our relationships will be...it'll become easier for us to communicate with them because we know what they want/need from us. We will also understand their actions/words better as well as how to respond accordingly.\n",
      "I believe that loving others is a process that I am still learning (a lifetime journey)...It takes time, energy and effort on my part to love others. But in the end, it's all worth it because of the wonderful relationships that are created. :)\n",
      "Loving others is like eating stew...\n",
      "We are unique individuals with different backgrounds\n",
      "but we should always learn about each other before loving them! ♥\n",
      "Labels: Inspirational, Life Philosophy, Love\n",
      "\"I'm not here to be liked. I'm here to make a difference.\" -Unknown\n",
      "A few days ago, someone posted this quote on Facebook and it really struck me because I think it is very true...\n",
      "If we are living our lives for other people then we will never truly live life to the fullest. We have to do what makes us happy in order to get the most out of everything\n",
      "llama_print_timings:        load time =  2236.37 ms\n",
      "llama_print_timings:      sample time =   249.66 ms /   512 runs   (    0.49 ms per token,  2050.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1197.54 ms /   265 tokens (    4.52 ms per token,   221.29 tokens per second)\n",
      "llama_print_timings:        eval time = 14049.82 ms /   510 runs   (   27.55 ms per token,    36.30 tokens per second)\n",
      "llama_print_timings:       total time = 15585.31 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33cb1a63-f863-4a7f-b627-6c76e534f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574595\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I have learned that every day you should reach out and touch someone. People love a warm hug, or just a friendly pat on the back.\n",
      "I have learned that people will forget what you said, people will forget what you did, but people will never forget how you made them feel.\n",
      "I have learned that money doesn’t buy happiness.\n",
      "I have learned to ask for help when I need it.\n",
      "I have learned not to judge a book by its cover.\n",
      "I have learned that you should take a moment and stop and smell the roses. Take time to look around and see all the beauty in the world.\n",
      "I have learned that we are put on this earth to become the best person we can be – so, live life in a way that will make you happy.\n",
      "When I am sad or down, these are some of the things I do:\n",
      "I call my best friend and tell her how much she means to me, or I talk with another girlfriend or family member about what is bothering me, but it usually makes me feel better because I know that there are people who love me.\n",
      "I go for a walk in nature and enjoy the beautiful scenery – or I just sit and look at the sky and think of all the good things in my life (there are many).\n",
      "I listen to music or read something that inspires me and makes me feel strong.\n",
      "I remind myself of how fortunate I am, because there is always someone who has it worse than you do – so even on your worst day, remember there’s a reason why it can be better.\n",
      "I put my headphones on and dance around my room to the music I love. It makes me feel happy and puts a smile on my face. Dance like no one is watching!\n",
      "I go for a jog or workout – even if it just means going up and down the stairs several times, because when you’re done you will always feel better about yourself.\n",
      "I talk to people that I love and tell them how much they mean to me. They bring out the best in my life!\n",
      "I read a book that inspires me, like one of my favorite books, The Secret, or just a good novel. It helps clear my mind and makes it easier for me to think straight.\n",
      "I remember all the people who have done nice things for me – I try to do something for them in return so they feel appreciated\n",
      "llama_print_timings:        load time = 40415.59 ms\n",
      "llama_print_timings:      sample time =   266.98 ms /   512 runs   (    0.52 ms per token,  1917.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1649.69 ms /   265 tokens (    6.23 ms per token,   160.64 tokens per second)\n",
      "llama_print_timings:        eval time = 22149.35 ms /   510 runs   (   43.43 ms per token,    23.03 tokens per second)\n",
      "llama_print_timings:       total time = 24154.56 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "250c511c-864d-4b20-8d87-86f2409b277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574663\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m happiness, not pleasure. The difference between the two is that happiness comes from within, and you can achieve it in a life where every day is filled with painful disappointments. Pleasure on the other hand comes from outside and lasts only as long as the stimulant does or until you are overwhelmed by the realization of what you have lost.\n",
      "As I walked along the street, I saw a man sitting in front of his store, reading a paper. A moment later I noticed that he had dozed off. The wind was blowing gently, so there was no danger to him. All around him people were scurrying by; they knew where they were going and what they had to do but this man didn't. He wasn't a homeless person because the street was full of shops and he was reading a paper. I wondered if someone had put him there, perhaps an old friend who wanted to see how long it would take before somebody noticed that something was wrong with his friend. Or maybe he had been sitting there for hours and nobody gave a damn.\n",
      "I passed him and stopped when I saw two other men talking together at the corner of the next block. They were both dressed like homeless people, and they looked tired. I asked them if they knew where an organization called Wayside was located that offered help to the street people in New York City, but neither one could give me a direction because he hadn't heard of it.\n",
      "One of the men suggested I go to the Salvation Army on West 46th Street and ask for directions there. I thanked him and walked away. As I reached the corner, I saw that they were still standing there. The homeless man who had given me no direction was crying; tears were running down his weather-beaten cheeks. It was a beautiful day but he wasn't seeing it because he was blind. The other two men were trying to comfort him. They patted him on the back and tried to tell him that everything would be all right, but their words seemed meaningless because the man in front of them was crying.\n",
      "It struck me then how life is like a road. There are many people who don't know where they are going or what they want to do with it. They just keep on walking forward until something happens and they lose direction. Many other people have no reason to go on because they are blind; they can'\n",
      "llama_print_timings:        load time =  3648.59 ms\n",
      "llama_print_timings:      sample time =   251.02 ms /   512 runs   (    0.49 ms per token,  2039.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1667.49 ms /   265 tokens (    6.29 ms per token,   158.92 tokens per second)\n",
      "llama_print_timings:        eval time = 22144.57 ms /   510 runs   (   43.42 ms per token,    23.03 tokens per second)\n",
      "llama_print_timings:       total time = 24151.30 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03f792b2-0f7e-4b93-9d3e-69d4479f6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574694\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a good life.\n",
      "I feel like I have lived a good life. I was born into a loving family with parents who are still together and in love 45 years later. I was raised by two loving parents who supported me in all my endeavors. My dad taught me how to be a man, how to work hard, and how to take care of myself. My mom taught me how to be kind, how to love, and most importantly she showed me what true unconditional love was like.\n",
      "I was raised in the LDS church (Mormon) which I have been a member for all my life. This faith has provided me with guidance and support through many tough times as well as wonderful moments that make up my life.\n",
      "My wife and children are the reason why I live. I love them so much and will continue to love them forever. We work together, we laugh together, and we cry together. They teach me more than they know about myself and life in general. I have learned more from my children then all of my schooling put together!\n",
      "I am very lucky to be a part of an amazing family that has been put together through the grace of God. I hope someday to see them again, as I believe we will reunite with our loved ones after this life is over.\n",
      "This entry was posted in Life and tagged beliefs, faith, family, love, meaning of life, Religion. Bookmark the permalink.\n",
      "← The Gifts We Give To Others\n",
      "Dave Ramsey’s “Total Money Makeover” →\n",
      "4 thoughts on “The Meaning Of Life?”\n",
      "Pingback: My Blog » The Meaning Of Life? « Ramblings of a Redneck\n",
      "Von | April 21, 2010 at 7:52 pm\n",
      "I agree with the statement that we will reunite again after this life. I can feel it every time my mom or dad passes away and I know they are still living on in another world…and I know I’ll see them when I die. It is a very comforting thought.\n",
      "Thanks for stopping by…I hope you come back!\n",
      "Pingback: The Meaning Of Life? « Ramblings of a Redneck | Financial Peace University\n",
      "Rosie | April 23, 2010 at 8:\n",
      "llama_print_timings:        load time =  3566.17 ms\n",
      "llama_print_timings:      sample time =   248.27 ms /   512 runs   (    0.48 ms per token,  2062.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1659.93 ms /   265 tokens (    6.26 ms per token,   159.65 tokens per second)\n",
      "llama_print_timings:        eval time = 22305.19 ms /   510 runs   (   43.74 ms per token,    22.86 tokens per second)\n",
      "llama_print_timings:       total time = 24302.12 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd03d5a9-978f-454f-9dde-da67377225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574725\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to enjoy it and have fun.\n",
      "I'm a single man looking for romance.\n",
      "I'm an energetic, positive, optimistic, romantic, passionate, friendly, loyal person. I do not smoke, drink, or gamble and I am financially stable. I love sports and watching movies and TV shows.\n",
      "My future wife is a beautiful woman inside and out. She has a sweet smile and a good heart. She's kind, loving, honest, faithful, respectful to others, and she believes in God. She must not smoke or drink alcohol. Her personality traits are very important to me. It's better we meet and know each other before I marry her.\n",
      "I look forward to meeting a woman who is interested in building a serious long term relationship with me. I am faithful, loyal, honest, and looking for my future wife and soulmate. Please read my profile carefully before contacting me. I hope she can be the one for me. Thanks!\n",
      "I want to meet someone that is real and honest and not playing games. I am a very romantic person. I don't believe in cheating on anyone. If you are not going to be faithful then why get into any relationship? I need a woman who can communicate with me all the time. I need someone who can make me happy and laugh everyday because I love to see people smile and feel good about themselves.\n",
      "I am a simple man, but hard working. I want to enjoy life in every aspect; travel, food, etc. I like to have fun, be romantic, affectionate, and passionate with the right person. I consider myself a gentleman.\n",
      "I'm looking for someone who is serious about a relationship, wants a family, loves children, and is ready to settle down. I want a woman that has a good heart, can communicate, and knows how to be faithful in a relationship. She must not smoke or drink alcohol.\n",
      "I would like to find love on this site because many people have had success with it. I hope she will be my future wife. I want to find a woman who is looking for the same things as me; someone who can communicate, and we can live together in happiness forever.\n",
      "I hope we can build good communication, get along well, and see each other before I would travel to meet her.\n",
      "I like many kinds of food, but my favorite d\n",
      "llama_print_timings:        load time = 108066.04 ms\n",
      "llama_print_timings:      sample time =   256.59 ms /   512 runs   (    0.50 ms per token,  1995.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3188.24 ms /   265 tokens (   12.03 ms per token,    83.12 tokens per second)\n",
      "llama_print_timings:        eval time = 43005.72 ms /   510 runs   (   84.32 ms per token,    11.86 tokens per second)\n",
      "llama_print_timings:       total time = 46541.25 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26bfaa12-c184-4be7-9d83-27cacb9ab31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574885\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a purpose.\n",
      "Everyone has their own way of finding this meaning, but many people are stuck in the cycle of doing what they think they should do instead of looking for what they truly want to do with their lives.\n",
      "What if the true answer was just waiting for you?\n",
      "I'm here to help you uncover your purpose and achieve it.\n",
      "Learn More About My Methodology\n",
      "Sign up for a free 60 minute session to see how I can help you!\n",
      "Schedule my free 60-minute consultation now!\n",
      "\"Craig was able to give me the tools, tips and tricks to make my dream of launching my own business a reality. Without his coaching, encouragement and advice... I’d still be sitting on the sidelines thinking about starting instead of actually DOING IT!\"\n",
      "-Brian D.\n",
      "Craig was able to give me the tools, tips and tricks to make my dream of launching my own business a reality. Without his coaching, encouragement and advice... I’d still be sitting on the sidelines thinking about starting instead of actually DOING IT!\n",
      "http://craigsterncoach.com/testimonials/brian-dempsey/\n",
      "\"Craig was instrumental in helping me to discover what I really wanted and needed out of my career. And that is an understatement.\n",
      "I have a background in marketing communications, but when I found myself at a crossroads in my life (as well as in my career), I decided that it was time to make a change. Craig helped me figure out what that looked like and how to get there.\"\n",
      "-Kristin S.\n",
      "Craig was instrumental in helping me to discover what I really wanted and needed out of my career. And that is an understatement.\n",
      "http://craigsterncoach.com/testimonials/kristin-shea/\n",
      "\"I am very grateful for the opportunity I had to work with Craig Stern. He has been instrumental in helping me achieve many goals and dreams.\"\n",
      "-Eric M.\n",
      "I am very grateful for the opportunity I had to work with Craig Stern. He has been instrumental in helping me achieve many goals and dreams.\n",
      "http://craigsterncoach.com/testimonials/eric-martin/\n",
      "llama_print_timings:        load time =  8966.11 ms\n",
      "llama_print_timings:      sample time =   271.76 ms /   512 runs   (    0.53 ms per token,  1883.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3299.66 ms /   265 tokens (   12.45 ms per token,    80.31 tokens per second)\n",
      "llama_print_timings:        eval time = 44511.58 ms /   510 runs   (   87.28 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time = 48176.88 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3108ab5c-6c71-446c-a9bd-b362e18dddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689574948\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to enjoy it.\n",
      "Life can be full of sadness, but we must not forget happiness.\n",
      "I am a romantic person who loves and enjoys every moment of my life.\n",
      "I like spending time on the beach, watching sunset, walking with friends or family in a forest, reading books and listening to music. I really enjoy travelling and discovering new places.\n",
      "I hope you share same interests as me! Maybe we can spend some time together?\n",
      "Education: University of Technology, Faculty of Chemistry, Pharmaceutical Chemistry, Bachelor’s Degree.\n",
      "A few years ago I went to a concert with my friends in a big club in the center of Moscow. It was so crowded that day, because one of the most popular musicians played there. At the beginning of the concert we saw an attractive guy who was standing alone. He looked so sad and lonely. We decided to cheer him up and invited him to join our group. It turned out he didn’t know anyone in this club. So we became friends that day! His name was Dmitry, I fell in love with him at first sight!\n",
      "The most important thing for me is the feeling of harmony between two people. I see my man as a caring and loving person, who knows how to take care about himself and his loved ones. He should have good sense of humor and be able to laugh at the funny things that happen in life. But no matter what, he always remains honest and sincere with me.\n",
      "I want to meet a man who will love me for who I am!\n",
      "I was born in Moscow, Russia on July 19th, 1988.\n",
      "My parents have been divorced since I was little. I have very good relationships with both of them and they are still friends. They often spend time together.\n",
      "It is interesting to know that my mom had a twin sister who died when she was only 3 years old. Her name was Irina too, so it’s the same as mine!\n",
      "I like travelling very much. I have already visited several countries – Austria, Italy, Spain, Greece and Turkey. My dream is to travel around the world one day.\n",
      "My favorite color is green because it reminds me of nature.\n",
      "I love animals, especially cats! I have a cat named Lolita.\n",
      "I like going out\n",
      "llama_print_timings:        load time =  9145.64 ms\n",
      "llama_print_timings:      sample time =   252.52 ms /   512 runs   (    0.49 ms per token,  2027.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3172.17 ms /   265 tokens (   11.97 ms per token,    83.54 tokens per second)\n",
      "llama_print_timings:        eval time = 44578.16 ms /   510 runs   (   87.41 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time = 48094.57 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dab82951-aac5-47a6-a4fc-c3febd980109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689575010\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A6000) as main device\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ad93e61-f733-46ae-a08f-1043fe19dd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============OS================\n",
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.2 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n",
      "5.4.0-148-generic\n"
     ]
    }
   ],
   "source": [
    "# operating system\n",
    "print(\"============OS================\")\n",
    "!lsb_release -a\n",
    "!uname -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee33c3-82c0-485b-9fea-7a705bfcc475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
