{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 04:11:17 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gener...    On | 00000000:41:00.0 Off |                  Off |\n",
      "| 39%   67C    P8               42W / 300W|      1MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 5965WX 24-Cores\n",
      "============Memory================\n",
      "MemTotal:       206014632 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 4916, done.\u001b[K\n",
      "remote: Counting objects: 100% (1713/1713), done.\u001b[K\n",
      "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
      "remote: Total 4916 (delta 1641), reused 1597 (delta 1580), pack-reused 3203\u001b[K\n",
      "Receiving objects: 100% (4916/4916), 4.07 MiB | 4.53 MiB/s, done.\n",
      "Resolving deltas: 100% (3365/3365), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53aa7f97-4a2e-47ab-8c66-ca34489ec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4126f1-7c42-4d66-8a7d-8e5c29c322da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eaea4f0-2d91-41ef-9f32-4103f2858796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   5384      0 --:--:-- --:--:-- --:--:--  5398\n",
      "Downloading tokenizer\n",
      "--2023-07-17 04:11:50--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K   386KB/s    in 1.3s    \n",
      "\n",
      "2023-07-17 04:11:52 (386 KB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-07-17 04:11:52--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:11:52 (19.0 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-07-17 04:11:52--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated.  36%[======>             ]   4.57G  23.8MB/s    in 4m 12s  \n",
      "\n",
      "2023-07-17 04:16:05 (18.6 MB/s) - Connection closed at byte 4902092800. Retrying.\n",
      "\n",
      "--2023-07-17 04:16:06--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 13476939516 (13G), 8574846716 (8.0G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[+++++++============>]  12.55G  16.1MB/s    in 7m 15s  \n",
      "\n",
      "2023-07-17 04:23:23 (18.8 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-07-17 04:23:23--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:23:23 (23.0 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 04:23:24--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:23:24 (34.1 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-07-17 04:23:41--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  55.4MB/s    in 3m 51s  \n",
      "\n",
      "2023-07-17 04:27:32 (53.8 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 04:27:32--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  12.6MB/s    in 12m 47s \n",
      "\n",
      "2023-07-17 04:40:20 (16.2 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 04:40:20--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:40:21 (5.56 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 04:40:21--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:40:22 (52.6 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-07-17 04:40:54--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  25.7MB/s    in 11m 43s \n",
      "\n",
      "2023-07-17 04:52:37 (22.1 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 04:52:37--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  55.5MB/s    in 4m 48s  \n",
      "\n",
      "2023-07-17 04:57:26 (53.8 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 04:57:26--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  19.7MB/s    in 10m 47s \n",
      "\n",
      "2023-07-17 05:08:14 (24.0 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 05:08:14--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  26.6MB/s    in 9m 44s  \n",
      "\n",
      "2023-07-17 05:17:58 (26.6 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 05:17:58--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 05:17:59 (6.16 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 05:17:59--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 05:18:00 (89.4 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-07-17 05:19:20--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated  93%[=================>  ]  14.23G  53.9MB/s    in 4m 32s  \n",
      "\n",
      "2023-07-17 05:23:52 (53.6 MB/s) - Read error at byte 15279915008/16323959449 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2023-07-17 05:23:53--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16323959449 (15G), 1044044441 (996M) remaining [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[++++++++++++++++++=>]  15.20G  55.2MB/s    in 21s     \n",
      "\n",
      "2023-07-17 05:24:15 (46.8 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:24:15--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  55.4MB/s    in 4m 51s  \n",
      "\n",
      "2023-07-17 05:29:07 (53.4 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:29:07--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  26.1MB/s    in 11m 5s  \n",
      "\n",
      "2023-07-17 05:40:13 (23.4 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:40:13--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  25.8MB/s    in 9m 52s  \n",
      "\n",
      "2023-07-17 05:50:06 (26.3 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:50:06--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  27.6MB/s    in 9m 47s  \n",
      "\n",
      "2023-07-17 05:59:54 (26.5 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 05:59:54--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  26.6MB/s    in 10m 46s \n",
      "\n",
      "2023-07-17 06:10:40 (24.1 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 06:10:40--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  26.0MB/s    in 10m 56s \n",
      "\n",
      "2023-07-17 06:21:37 (23.7 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 06:21:37--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated  66%[============>       ]  10.11G  24.0MB/s    in 7m 21s  \n",
      "\n",
      "2023-07-17 06:28:59 (23.5 MB/s) - Connection closed at byte 10851581952. Retrying.\n",
      "\n",
      "--2023-07-17 06:29:00--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16323959449 (15G), 5472377497 (5.1G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[+++++++++++++======>]  15.20G  51.6MB/s    in 1m 54s  \n",
      "\n",
      "2023-07-17 06:30:55 (45.7 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 06:30:55--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 06:30:56 (20.3 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 06:30:56--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 06:30:57 (126 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed numpy-1.24.0 sentencepiece-0.1.98\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:4096 n_mult:256 n_head:32 n_layer:32\n",
      "Writing vocab...\n",
      "[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/7B/ggml-model-f16.bin\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:5120 n_mult:256 n_head:40 n_layer:40\n",
      "Writing vocab...\n",
      "[  1/363] Writing tensor tok_embeddings.weight                  | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  2/363] Writing tensor norm.weight                            | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  4/363] Writing tensor layers.0.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  5/363] Writing tensor layers.0.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  6/363] Writing tensor layers.0.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  7/363] Writing tensor layers.0.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  8/363] Writing tensor layers.0.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  9/363] Writing tensor layers.0.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 10/363] Writing tensor layers.0.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 11/363] Writing tensor layers.0.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 12/363] Writing tensor layers.0.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 13/363] Writing tensor layers.1.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 14/363] Writing tensor layers.1.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 15/363] Writing tensor layers.1.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 16/363] Writing tensor layers.1.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 17/363] Writing tensor layers.1.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 18/363] Writing tensor layers.1.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 19/363] Writing tensor layers.1.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 20/363] Writing tensor layers.1.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 21/363] Writing tensor layers.1.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 22/363] Writing tensor layers.2.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 23/363] Writing tensor layers.2.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 24/363] Writing tensor layers.2.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 25/363] Writing tensor layers.2.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 26/363] Writing tensor layers.2.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 27/363] Writing tensor layers.2.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 28/363] Writing tensor layers.2.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 29/363] Writing tensor layers.2.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 30/363] Writing tensor layers.2.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 31/363] Writing tensor layers.3.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 32/363] Writing tensor layers.3.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 33/363] Writing tensor layers.3.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 34/363] Writing tensor layers.3.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 35/363] Writing tensor layers.3.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 36/363] Writing tensor layers.3.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 37/363] Writing tensor layers.3.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 38/363] Writing tensor layers.3.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 39/363] Writing tensor layers.3.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 40/363] Writing tensor layers.4.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 41/363] Writing tensor layers.4.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 42/363] Writing tensor layers.4.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 43/363] Writing tensor layers.4.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 44/363] Writing tensor layers.4.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 45/363] Writing tensor layers.4.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 46/363] Writing tensor layers.4.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 47/363] Writing tensor layers.4.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 48/363] Writing tensor layers.4.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 49/363] Writing tensor layers.5.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 50/363] Writing tensor layers.5.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 51/363] Writing tensor layers.5.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 52/363] Writing tensor layers.5.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 53/363] Writing tensor layers.5.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 54/363] Writing tensor layers.5.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 55/363] Writing tensor layers.5.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 56/363] Writing tensor layers.5.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 57/363] Writing tensor layers.5.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 58/363] Writing tensor layers.6.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 59/363] Writing tensor layers.6.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 60/363] Writing tensor layers.6.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 61/363] Writing tensor layers.6.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 62/363] Writing tensor layers.6.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 63/363] Writing tensor layers.6.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 64/363] Writing tensor layers.6.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 65/363] Writing tensor layers.6.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 66/363] Writing tensor layers.6.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 67/363] Writing tensor layers.7.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 68/363] Writing tensor layers.7.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 69/363] Writing tensor layers.7.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 70/363] Writing tensor layers.7.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 71/363] Writing tensor layers.7.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 72/363] Writing tensor layers.7.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 73/363] Writing tensor layers.7.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 74/363] Writing tensor layers.7.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 75/363] Writing tensor layers.7.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 76/363] Writing tensor layers.8.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 77/363] Writing tensor layers.8.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 78/363] Writing tensor layers.8.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 79/363] Writing tensor layers.8.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 80/363] Writing tensor layers.8.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 81/363] Writing tensor layers.8.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 82/363] Writing tensor layers.8.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 83/363] Writing tensor layers.8.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 84/363] Writing tensor layers.8.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 85/363] Writing tensor layers.9.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 86/363] Writing tensor layers.9.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 87/363] Writing tensor layers.9.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 88/363] Writing tensor layers.9.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 89/363] Writing tensor layers.9.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 90/363] Writing tensor layers.9.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 91/363] Writing tensor layers.9.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 92/363] Writing tensor layers.9.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 93/363] Writing tensor layers.9.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 94/363] Writing tensor layers.10.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 95/363] Writing tensor layers.10.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 96/363] Writing tensor layers.10.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 97/363] Writing tensor layers.10.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 98/363] Writing tensor layers.10.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 99/363] Writing tensor layers.10.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[100/363] Writing tensor layers.10.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[101/363] Writing tensor layers.10.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[102/363] Writing tensor layers.10.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[103/363] Writing tensor layers.11.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[104/363] Writing tensor layers.11.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[105/363] Writing tensor layers.11.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[106/363] Writing tensor layers.11.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[107/363] Writing tensor layers.11.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[108/363] Writing tensor layers.11.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[109/363] Writing tensor layers.11.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[110/363] Writing tensor layers.11.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[111/363] Writing tensor layers.11.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[112/363] Writing tensor layers.12.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[113/363] Writing tensor layers.12.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[114/363] Writing tensor layers.12.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[115/363] Writing tensor layers.12.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[116/363] Writing tensor layers.12.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[117/363] Writing tensor layers.12.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[118/363] Writing tensor layers.12.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[119/363] Writing tensor layers.12.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[120/363] Writing tensor layers.12.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[121/363] Writing tensor layers.13.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[122/363] Writing tensor layers.13.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[123/363] Writing tensor layers.13.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[124/363] Writing tensor layers.13.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[125/363] Writing tensor layers.13.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[126/363] Writing tensor layers.13.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[127/363] Writing tensor layers.13.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[128/363] Writing tensor layers.13.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[129/363] Writing tensor layers.13.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[130/363] Writing tensor layers.14.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[131/363] Writing tensor layers.14.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[132/363] Writing tensor layers.14.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[133/363] Writing tensor layers.14.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[134/363] Writing tensor layers.14.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[135/363] Writing tensor layers.14.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[136/363] Writing tensor layers.14.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[137/363] Writing tensor layers.14.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[138/363] Writing tensor layers.14.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[139/363] Writing tensor layers.15.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[140/363] Writing tensor layers.15.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[141/363] Writing tensor layers.15.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[142/363] Writing tensor layers.15.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[143/363] Writing tensor layers.15.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[144/363] Writing tensor layers.15.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[145/363] Writing tensor layers.15.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[146/363] Writing tensor layers.15.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[147/363] Writing tensor layers.15.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[148/363] Writing tensor layers.16.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[149/363] Writing tensor layers.16.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[150/363] Writing tensor layers.16.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[151/363] Writing tensor layers.16.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[152/363] Writing tensor layers.16.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[153/363] Writing tensor layers.16.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[154/363] Writing tensor layers.16.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[155/363] Writing tensor layers.16.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[156/363] Writing tensor layers.16.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[157/363] Writing tensor layers.17.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[158/363] Writing tensor layers.17.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[159/363] Writing tensor layers.17.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[160/363] Writing tensor layers.17.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[161/363] Writing tensor layers.17.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[162/363] Writing tensor layers.17.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[163/363] Writing tensor layers.17.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[164/363] Writing tensor layers.17.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[165/363] Writing tensor layers.17.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[166/363] Writing tensor layers.18.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[167/363] Writing tensor layers.18.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[168/363] Writing tensor layers.18.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[169/363] Writing tensor layers.18.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[170/363] Writing tensor layers.18.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[171/363] Writing tensor layers.18.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[172/363] Writing tensor layers.18.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[173/363] Writing tensor layers.18.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[174/363] Writing tensor layers.18.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[175/363] Writing tensor layers.19.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[176/363] Writing tensor layers.19.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[177/363] Writing tensor layers.19.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[178/363] Writing tensor layers.19.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[179/363] Writing tensor layers.19.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[180/363] Writing tensor layers.19.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[181/363] Writing tensor layers.19.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[182/363] Writing tensor layers.19.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[183/363] Writing tensor layers.19.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[184/363] Writing tensor layers.20.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[185/363] Writing tensor layers.20.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[186/363] Writing tensor layers.20.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[187/363] Writing tensor layers.20.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[188/363] Writing tensor layers.20.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[189/363] Writing tensor layers.20.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[190/363] Writing tensor layers.20.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[191/363] Writing tensor layers.20.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[192/363] Writing tensor layers.20.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[193/363] Writing tensor layers.21.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[194/363] Writing tensor layers.21.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[195/363] Writing tensor layers.21.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[196/363] Writing tensor layers.21.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[197/363] Writing tensor layers.21.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[198/363] Writing tensor layers.21.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[199/363] Writing tensor layers.21.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[200/363] Writing tensor layers.21.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[201/363] Writing tensor layers.21.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[202/363] Writing tensor layers.22.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[203/363] Writing tensor layers.22.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[204/363] Writing tensor layers.22.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[205/363] Writing tensor layers.22.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[206/363] Writing tensor layers.22.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[207/363] Writing tensor layers.22.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[208/363] Writing tensor layers.22.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[209/363] Writing tensor layers.22.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[210/363] Writing tensor layers.22.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[211/363] Writing tensor layers.23.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[212/363] Writing tensor layers.23.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[213/363] Writing tensor layers.23.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[214/363] Writing tensor layers.23.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[215/363] Writing tensor layers.23.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[216/363] Writing tensor layers.23.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[217/363] Writing tensor layers.23.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[218/363] Writing tensor layers.23.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[219/363] Writing tensor layers.23.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[220/363] Writing tensor layers.24.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[221/363] Writing tensor layers.24.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[222/363] Writing tensor layers.24.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[223/363] Writing tensor layers.24.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[224/363] Writing tensor layers.24.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[225/363] Writing tensor layers.24.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[226/363] Writing tensor layers.24.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[227/363] Writing tensor layers.24.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[228/363] Writing tensor layers.24.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[229/363] Writing tensor layers.25.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[230/363] Writing tensor layers.25.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[231/363] Writing tensor layers.25.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[232/363] Writing tensor layers.25.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[233/363] Writing tensor layers.25.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[234/363] Writing tensor layers.25.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[235/363] Writing tensor layers.25.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[236/363] Writing tensor layers.25.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[237/363] Writing tensor layers.25.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[238/363] Writing tensor layers.26.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[239/363] Writing tensor layers.26.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[240/363] Writing tensor layers.26.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[241/363] Writing tensor layers.26.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[242/363] Writing tensor layers.26.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[243/363] Writing tensor layers.26.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[244/363] Writing tensor layers.26.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[245/363] Writing tensor layers.26.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[246/363] Writing tensor layers.26.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[247/363] Writing tensor layers.27.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[248/363] Writing tensor layers.27.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[249/363] Writing tensor layers.27.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[250/363] Writing tensor layers.27.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[251/363] Writing tensor layers.27.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[252/363] Writing tensor layers.27.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[253/363] Writing tensor layers.27.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[254/363] Writing tensor layers.27.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[255/363] Writing tensor layers.27.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[256/363] Writing tensor layers.28.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[257/363] Writing tensor layers.28.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[258/363] Writing tensor layers.28.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[259/363] Writing tensor layers.28.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[260/363] Writing tensor layers.28.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[261/363] Writing tensor layers.28.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[262/363] Writing tensor layers.28.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[263/363] Writing tensor layers.28.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[264/363] Writing tensor layers.28.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[265/363] Writing tensor layers.29.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[266/363] Writing tensor layers.29.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[267/363] Writing tensor layers.29.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[268/363] Writing tensor layers.29.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[269/363] Writing tensor layers.29.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[270/363] Writing tensor layers.29.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[271/363] Writing tensor layers.29.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[272/363] Writing tensor layers.29.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[273/363] Writing tensor layers.29.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[274/363] Writing tensor layers.30.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[275/363] Writing tensor layers.30.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[276/363] Writing tensor layers.30.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[277/363] Writing tensor layers.30.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[278/363] Writing tensor layers.30.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[279/363] Writing tensor layers.30.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[280/363] Writing tensor layers.30.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[281/363] Writing tensor layers.30.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[282/363] Writing tensor layers.30.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[283/363] Writing tensor layers.31.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[284/363] Writing tensor layers.31.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[285/363] Writing tensor layers.31.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[286/363] Writing tensor layers.31.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[287/363] Writing tensor layers.31.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[288/363] Writing tensor layers.31.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[289/363] Writing tensor layers.31.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[290/363] Writing tensor layers.31.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[291/363] Writing tensor layers.31.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[292/363] Writing tensor layers.32.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[293/363] Writing tensor layers.32.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[294/363] Writing tensor layers.32.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[295/363] Writing tensor layers.32.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[296/363] Writing tensor layers.32.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[297/363] Writing tensor layers.32.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[298/363] Writing tensor layers.32.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[299/363] Writing tensor layers.32.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[300/363] Writing tensor layers.32.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[301/363] Writing tensor layers.33.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[302/363] Writing tensor layers.33.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[303/363] Writing tensor layers.33.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[304/363] Writing tensor layers.33.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[305/363] Writing tensor layers.33.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[306/363] Writing tensor layers.33.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[307/363] Writing tensor layers.33.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[308/363] Writing tensor layers.33.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[309/363] Writing tensor layers.33.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[310/363] Writing tensor layers.34.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[311/363] Writing tensor layers.34.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[312/363] Writing tensor layers.34.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[313/363] Writing tensor layers.34.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[314/363] Writing tensor layers.34.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[315/363] Writing tensor layers.34.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[316/363] Writing tensor layers.34.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[317/363] Writing tensor layers.34.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[318/363] Writing tensor layers.34.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[319/363] Writing tensor layers.35.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[320/363] Writing tensor layers.35.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[321/363] Writing tensor layers.35.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[322/363] Writing tensor layers.35.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[323/363] Writing tensor layers.35.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[324/363] Writing tensor layers.35.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[325/363] Writing tensor layers.35.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[326/363] Writing tensor layers.35.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[327/363] Writing tensor layers.35.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[328/363] Writing tensor layers.36.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[329/363] Writing tensor layers.36.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[330/363] Writing tensor layers.36.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[331/363] Writing tensor layers.36.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[332/363] Writing tensor layers.36.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[333/363] Writing tensor layers.36.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[334/363] Writing tensor layers.36.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[335/363] Writing tensor layers.36.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[336/363] Writing tensor layers.36.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[337/363] Writing tensor layers.37.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[338/363] Writing tensor layers.37.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[339/363] Writing tensor layers.37.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[340/363] Writing tensor layers.37.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[341/363] Writing tensor layers.37.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[342/363] Writing tensor layers.37.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[343/363] Writing tensor layers.37.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[344/363] Writing tensor layers.37.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[345/363] Writing tensor layers.37.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[346/363] Writing tensor layers.38.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[347/363] Writing tensor layers.38.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[348/363] Writing tensor layers.38.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[349/363] Writing tensor layers.38.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[350/363] Writing tensor layers.38.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[351/363] Writing tensor layers.38.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[352/363] Writing tensor layers.38.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[353/363] Writing tensor layers.38.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[354/363] Writing tensor layers.38.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[355/363] Writing tensor layers.39.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[356/363] Writing tensor layers.39.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[357/363] Writing tensor layers.39.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[358/363] Writing tensor layers.39.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[359/363] Writing tensor layers.39.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[360/363] Writing tensor layers.39.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[361/363] Writing tensor layers.39.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[362/363] Writing tensor layers.39.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[363/363] Writing tensor layers.39.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/13B/ggml-model-f16.bin\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:6656 n_mult:256 n_head:52 n_layer:60\n",
      "Writing vocab...\n",
      "[  1/543] Writing tensor tok_embeddings.weight                  | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  2/543] Writing tensor norm.weight                            | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  4/543] Writing tensor layers.0.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  5/543] Writing tensor layers.0.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  6/543] Writing tensor layers.0.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  7/543] Writing tensor layers.0.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  8/543] Writing tensor layers.0.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  9/543] Writing tensor layers.0.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 10/543] Writing tensor layers.0.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 11/543] Writing tensor layers.0.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 12/543] Writing tensor layers.0.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 13/543] Writing tensor layers.1.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 14/543] Writing tensor layers.1.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 15/543] Writing tensor layers.1.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 16/543] Writing tensor layers.1.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 17/543] Writing tensor layers.1.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 18/543] Writing tensor layers.1.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 19/543] Writing tensor layers.1.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 20/543] Writing tensor layers.1.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 21/543] Writing tensor layers.1.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 22/543] Writing tensor layers.2.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 23/543] Writing tensor layers.2.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 24/543] Writing tensor layers.2.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 25/543] Writing tensor layers.2.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 26/543] Writing tensor layers.2.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 27/543] Writing tensor layers.2.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 28/543] Writing tensor layers.2.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 29/543] Writing tensor layers.2.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 30/543] Writing tensor layers.2.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 31/543] Writing tensor layers.3.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 32/543] Writing tensor layers.3.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 33/543] Writing tensor layers.3.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 34/543] Writing tensor layers.3.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 35/543] Writing tensor layers.3.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 36/543] Writing tensor layers.3.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 37/543] Writing tensor layers.3.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 38/543] Writing tensor layers.3.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 39/543] Writing tensor layers.3.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 40/543] Writing tensor layers.4.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 41/543] Writing tensor layers.4.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 42/543] Writing tensor layers.4.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 43/543] Writing tensor layers.4.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 44/543] Writing tensor layers.4.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 45/543] Writing tensor layers.4.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 46/543] Writing tensor layers.4.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 47/543] Writing tensor layers.4.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 48/543] Writing tensor layers.4.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 49/543] Writing tensor layers.5.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 50/543] Writing tensor layers.5.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 51/543] Writing tensor layers.5.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 52/543] Writing tensor layers.5.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 53/543] Writing tensor layers.5.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 54/543] Writing tensor layers.5.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 55/543] Writing tensor layers.5.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 56/543] Writing tensor layers.5.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 57/543] Writing tensor layers.5.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 58/543] Writing tensor layers.6.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 59/543] Writing tensor layers.6.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 60/543] Writing tensor layers.6.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 61/543] Writing tensor layers.6.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 62/543] Writing tensor layers.6.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 63/543] Writing tensor layers.6.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 64/543] Writing tensor layers.6.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 65/543] Writing tensor layers.6.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 66/543] Writing tensor layers.6.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 67/543] Writing tensor layers.7.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 68/543] Writing tensor layers.7.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 69/543] Writing tensor layers.7.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 70/543] Writing tensor layers.7.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 71/543] Writing tensor layers.7.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 72/543] Writing tensor layers.7.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 73/543] Writing tensor layers.7.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 74/543] Writing tensor layers.7.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 75/543] Writing tensor layers.7.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 76/543] Writing tensor layers.8.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 77/543] Writing tensor layers.8.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 78/543] Writing tensor layers.8.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 79/543] Writing tensor layers.8.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 80/543] Writing tensor layers.8.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 81/543] Writing tensor layers.8.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 82/543] Writing tensor layers.8.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 83/543] Writing tensor layers.8.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 84/543] Writing tensor layers.8.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 85/543] Writing tensor layers.9.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 86/543] Writing tensor layers.9.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 87/543] Writing tensor layers.9.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 88/543] Writing tensor layers.9.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 89/543] Writing tensor layers.9.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 90/543] Writing tensor layers.9.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 91/543] Writing tensor layers.9.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 92/543] Writing tensor layers.9.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 93/543] Writing tensor layers.9.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 94/543] Writing tensor layers.10.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 95/543] Writing tensor layers.10.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 96/543] Writing tensor layers.10.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 97/543] Writing tensor layers.10.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 98/543] Writing tensor layers.10.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 99/543] Writing tensor layers.10.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[100/543] Writing tensor layers.10.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[101/543] Writing tensor layers.10.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[102/543] Writing tensor layers.10.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[103/543] Writing tensor layers.11.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[104/543] Writing tensor layers.11.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[105/543] Writing tensor layers.11.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[106/543] Writing tensor layers.11.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[107/543] Writing tensor layers.11.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[108/543] Writing tensor layers.11.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[109/543] Writing tensor layers.11.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[110/543] Writing tensor layers.11.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[111/543] Writing tensor layers.11.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[112/543] Writing tensor layers.12.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[113/543] Writing tensor layers.12.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[114/543] Writing tensor layers.12.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[115/543] Writing tensor layers.12.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[116/543] Writing tensor layers.12.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[117/543] Writing tensor layers.12.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[118/543] Writing tensor layers.12.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[119/543] Writing tensor layers.12.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[120/543] Writing tensor layers.12.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[121/543] Writing tensor layers.13.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[122/543] Writing tensor layers.13.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[123/543] Writing tensor layers.13.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[124/543] Writing tensor layers.13.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[125/543] Writing tensor layers.13.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[126/543] Writing tensor layers.13.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[127/543] Writing tensor layers.13.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[128/543] Writing tensor layers.13.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[129/543] Writing tensor layers.13.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[130/543] Writing tensor layers.14.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[131/543] Writing tensor layers.14.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[132/543] Writing tensor layers.14.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[133/543] Writing tensor layers.14.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[134/543] Writing tensor layers.14.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[135/543] Writing tensor layers.14.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[136/543] Writing tensor layers.14.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[137/543] Writing tensor layers.14.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[138/543] Writing tensor layers.14.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[139/543] Writing tensor layers.15.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[140/543] Writing tensor layers.15.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[141/543] Writing tensor layers.15.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[142/543] Writing tensor layers.15.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[143/543] Writing tensor layers.15.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[144/543] Writing tensor layers.15.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[145/543] Writing tensor layers.15.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[146/543] Writing tensor layers.15.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[147/543] Writing tensor layers.15.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[148/543] Writing tensor layers.16.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[149/543] Writing tensor layers.16.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[150/543] Writing tensor layers.16.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[151/543] Writing tensor layers.16.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[152/543] Writing tensor layers.16.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[153/543] Writing tensor layers.16.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[154/543] Writing tensor layers.16.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[155/543] Writing tensor layers.16.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[156/543] Writing tensor layers.16.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[157/543] Writing tensor layers.17.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[158/543] Writing tensor layers.17.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[159/543] Writing tensor layers.17.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[160/543] Writing tensor layers.17.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[161/543] Writing tensor layers.17.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[162/543] Writing tensor layers.17.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[163/543] Writing tensor layers.17.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[164/543] Writing tensor layers.17.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[165/543] Writing tensor layers.17.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[166/543] Writing tensor layers.18.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[167/543] Writing tensor layers.18.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[168/543] Writing tensor layers.18.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[169/543] Writing tensor layers.18.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[170/543] Writing tensor layers.18.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[171/543] Writing tensor layers.18.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[172/543] Writing tensor layers.18.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[173/543] Writing tensor layers.18.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[174/543] Writing tensor layers.18.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[175/543] Writing tensor layers.19.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[176/543] Writing tensor layers.19.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[177/543] Writing tensor layers.19.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[178/543] Writing tensor layers.19.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[179/543] Writing tensor layers.19.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[180/543] Writing tensor layers.19.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[181/543] Writing tensor layers.19.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[182/543] Writing tensor layers.19.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[183/543] Writing tensor layers.19.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[184/543] Writing tensor layers.20.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[185/543] Writing tensor layers.20.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[186/543] Writing tensor layers.20.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[187/543] Writing tensor layers.20.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[188/543] Writing tensor layers.20.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[189/543] Writing tensor layers.20.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[190/543] Writing tensor layers.20.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[191/543] Writing tensor layers.20.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[192/543] Writing tensor layers.20.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[193/543] Writing tensor layers.21.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[194/543] Writing tensor layers.21.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[195/543] Writing tensor layers.21.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[196/543] Writing tensor layers.21.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[197/543] Writing tensor layers.21.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[198/543] Writing tensor layers.21.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[199/543] Writing tensor layers.21.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[200/543] Writing tensor layers.21.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[201/543] Writing tensor layers.21.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[202/543] Writing tensor layers.22.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[203/543] Writing tensor layers.22.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[204/543] Writing tensor layers.22.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[205/543] Writing tensor layers.22.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[206/543] Writing tensor layers.22.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[207/543] Writing tensor layers.22.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[208/543] Writing tensor layers.22.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[209/543] Writing tensor layers.22.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[210/543] Writing tensor layers.22.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[211/543] Writing tensor layers.23.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[212/543] Writing tensor layers.23.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[213/543] Writing tensor layers.23.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[214/543] Writing tensor layers.23.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[215/543] Writing tensor layers.23.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[216/543] Writing tensor layers.23.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[217/543] Writing tensor layers.23.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[218/543] Writing tensor layers.23.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[219/543] Writing tensor layers.23.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[220/543] Writing tensor layers.24.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[221/543] Writing tensor layers.24.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[222/543] Writing tensor layers.24.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[223/543] Writing tensor layers.24.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[224/543] Writing tensor layers.24.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[225/543] Writing tensor layers.24.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[226/543] Writing tensor layers.24.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[227/543] Writing tensor layers.24.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[228/543] Writing tensor layers.24.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[229/543] Writing tensor layers.25.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[230/543] Writing tensor layers.25.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[231/543] Writing tensor layers.25.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[232/543] Writing tensor layers.25.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[233/543] Writing tensor layers.25.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[234/543] Writing tensor layers.25.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[235/543] Writing tensor layers.25.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[236/543] Writing tensor layers.25.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[237/543] Writing tensor layers.25.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[238/543] Writing tensor layers.26.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[239/543] Writing tensor layers.26.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[240/543] Writing tensor layers.26.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[241/543] Writing tensor layers.26.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[242/543] Writing tensor layers.26.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[243/543] Writing tensor layers.26.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[244/543] Writing tensor layers.26.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[245/543] Writing tensor layers.26.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[246/543] Writing tensor layers.26.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[247/543] Writing tensor layers.27.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[248/543] Writing tensor layers.27.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[249/543] Writing tensor layers.27.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[250/543] Writing tensor layers.27.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[251/543] Writing tensor layers.27.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[252/543] Writing tensor layers.27.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[253/543] Writing tensor layers.27.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[254/543] Writing tensor layers.27.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[255/543] Writing tensor layers.27.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[256/543] Writing tensor layers.28.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[257/543] Writing tensor layers.28.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[258/543] Writing tensor layers.28.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[259/543] Writing tensor layers.28.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[260/543] Writing tensor layers.28.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[261/543] Writing tensor layers.28.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[262/543] Writing tensor layers.28.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[263/543] Writing tensor layers.28.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[264/543] Writing tensor layers.28.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[265/543] Writing tensor layers.29.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[266/543] Writing tensor layers.29.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[267/543] Writing tensor layers.29.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[268/543] Writing tensor layers.29.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[269/543] Writing tensor layers.29.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[270/543] Writing tensor layers.29.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[271/543] Writing tensor layers.29.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[272/543] Writing tensor layers.29.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[273/543] Writing tensor layers.29.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[274/543] Writing tensor layers.30.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[275/543] Writing tensor layers.30.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[276/543] Writing tensor layers.30.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[277/543] Writing tensor layers.30.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[278/543] Writing tensor layers.30.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[279/543] Writing tensor layers.30.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[280/543] Writing tensor layers.30.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[281/543] Writing tensor layers.30.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[282/543] Writing tensor layers.30.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[283/543] Writing tensor layers.31.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[284/543] Writing tensor layers.31.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[285/543] Writing tensor layers.31.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[286/543] Writing tensor layers.31.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[287/543] Writing tensor layers.31.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[288/543] Writing tensor layers.31.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[289/543] Writing tensor layers.31.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[290/543] Writing tensor layers.31.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[291/543] Writing tensor layers.31.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[292/543] Writing tensor layers.32.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[293/543] Writing tensor layers.32.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[294/543] Writing tensor layers.32.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[295/543] Writing tensor layers.32.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[296/543] Writing tensor layers.32.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[297/543] Writing tensor layers.32.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[298/543] Writing tensor layers.32.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[299/543] Writing tensor layers.32.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[300/543] Writing tensor layers.32.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[301/543] Writing tensor layers.33.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[302/543] Writing tensor layers.33.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[303/543] Writing tensor layers.33.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[304/543] Writing tensor layers.33.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[305/543] Writing tensor layers.33.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[306/543] Writing tensor layers.33.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[307/543] Writing tensor layers.33.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[308/543] Writing tensor layers.33.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[309/543] Writing tensor layers.33.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[310/543] Writing tensor layers.34.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[311/543] Writing tensor layers.34.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[312/543] Writing tensor layers.34.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[313/543] Writing tensor layers.34.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[314/543] Writing tensor layers.34.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[315/543] Writing tensor layers.34.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[316/543] Writing tensor layers.34.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[317/543] Writing tensor layers.34.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[318/543] Writing tensor layers.34.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[319/543] Writing tensor layers.35.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[320/543] Writing tensor layers.35.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[321/543] Writing tensor layers.35.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[322/543] Writing tensor layers.35.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[323/543] Writing tensor layers.35.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[324/543] Writing tensor layers.35.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[325/543] Writing tensor layers.35.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[326/543] Writing tensor layers.35.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[327/543] Writing tensor layers.35.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[328/543] Writing tensor layers.36.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[329/543] Writing tensor layers.36.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[330/543] Writing tensor layers.36.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[331/543] Writing tensor layers.36.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[332/543] Writing tensor layers.36.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[333/543] Writing tensor layers.36.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[334/543] Writing tensor layers.36.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[335/543] Writing tensor layers.36.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[336/543] Writing tensor layers.36.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[337/543] Writing tensor layers.37.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[338/543] Writing tensor layers.37.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[339/543] Writing tensor layers.37.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[340/543] Writing tensor layers.37.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[341/543] Writing tensor layers.37.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[342/543] Writing tensor layers.37.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[343/543] Writing tensor layers.37.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[344/543] Writing tensor layers.37.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[345/543] Writing tensor layers.37.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[346/543] Writing tensor layers.38.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[347/543] Writing tensor layers.38.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[348/543] Writing tensor layers.38.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[349/543] Writing tensor layers.38.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[350/543] Writing tensor layers.38.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[351/543] Writing tensor layers.38.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[352/543] Writing tensor layers.38.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[353/543] Writing tensor layers.38.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[354/543] Writing tensor layers.38.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[355/543] Writing tensor layers.39.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[356/543] Writing tensor layers.39.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[357/543] Writing tensor layers.39.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[358/543] Writing tensor layers.39.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[359/543] Writing tensor layers.39.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[360/543] Writing tensor layers.39.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[361/543] Writing tensor layers.39.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[362/543] Writing tensor layers.39.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[363/543] Writing tensor layers.39.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[364/543] Writing tensor layers.40.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[365/543] Writing tensor layers.40.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[366/543] Writing tensor layers.40.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[367/543] Writing tensor layers.40.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[368/543] Writing tensor layers.40.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[369/543] Writing tensor layers.40.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[370/543] Writing tensor layers.40.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[371/543] Writing tensor layers.40.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[372/543] Writing tensor layers.40.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[373/543] Writing tensor layers.41.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[374/543] Writing tensor layers.41.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[375/543] Writing tensor layers.41.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[376/543] Writing tensor layers.41.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[377/543] Writing tensor layers.41.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[378/543] Writing tensor layers.41.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[379/543] Writing tensor layers.41.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[380/543] Writing tensor layers.41.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[381/543] Writing tensor layers.41.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[382/543] Writing tensor layers.42.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[383/543] Writing tensor layers.42.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[384/543] Writing tensor layers.42.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[385/543] Writing tensor layers.42.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[386/543] Writing tensor layers.42.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[387/543] Writing tensor layers.42.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[388/543] Writing tensor layers.42.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[389/543] Writing tensor layers.42.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[390/543] Writing tensor layers.42.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[391/543] Writing tensor layers.43.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[392/543] Writing tensor layers.43.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[393/543] Writing tensor layers.43.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[394/543] Writing tensor layers.43.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[395/543] Writing tensor layers.43.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[396/543] Writing tensor layers.43.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[397/543] Writing tensor layers.43.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[398/543] Writing tensor layers.43.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[399/543] Writing tensor layers.43.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[400/543] Writing tensor layers.44.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[401/543] Writing tensor layers.44.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[402/543] Writing tensor layers.44.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[403/543] Writing tensor layers.44.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[404/543] Writing tensor layers.44.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[405/543] Writing tensor layers.44.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[406/543] Writing tensor layers.44.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[407/543] Writing tensor layers.44.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[408/543] Writing tensor layers.44.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[409/543] Writing tensor layers.45.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[410/543] Writing tensor layers.45.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[411/543] Writing tensor layers.45.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[412/543] Writing tensor layers.45.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[413/543] Writing tensor layers.45.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[414/543] Writing tensor layers.45.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[415/543] Writing tensor layers.45.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[416/543] Writing tensor layers.45.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[417/543] Writing tensor layers.45.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[418/543] Writing tensor layers.46.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[419/543] Writing tensor layers.46.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[420/543] Writing tensor layers.46.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[421/543] Writing tensor layers.46.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[422/543] Writing tensor layers.46.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[423/543] Writing tensor layers.46.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[424/543] Writing tensor layers.46.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[425/543] Writing tensor layers.46.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[426/543] Writing tensor layers.46.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[427/543] Writing tensor layers.47.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[428/543] Writing tensor layers.47.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[429/543] Writing tensor layers.47.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[430/543] Writing tensor layers.47.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[431/543] Writing tensor layers.47.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[432/543] Writing tensor layers.47.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[433/543] Writing tensor layers.47.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[434/543] Writing tensor layers.47.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[435/543] Writing tensor layers.47.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[436/543] Writing tensor layers.48.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[437/543] Writing tensor layers.48.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[438/543] Writing tensor layers.48.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[439/543] Writing tensor layers.48.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[440/543] Writing tensor layers.48.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[441/543] Writing tensor layers.48.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[442/543] Writing tensor layers.48.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[443/543] Writing tensor layers.48.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[444/543] Writing tensor layers.48.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[445/543] Writing tensor layers.49.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[446/543] Writing tensor layers.49.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[447/543] Writing tensor layers.49.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[448/543] Writing tensor layers.49.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[449/543] Writing tensor layers.49.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[450/543] Writing tensor layers.49.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[451/543] Writing tensor layers.49.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[452/543] Writing tensor layers.49.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[453/543] Writing tensor layers.49.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[454/543] Writing tensor layers.50.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[455/543] Writing tensor layers.50.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[456/543] Writing tensor layers.50.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[457/543] Writing tensor layers.50.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[458/543] Writing tensor layers.50.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[459/543] Writing tensor layers.50.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[460/543] Writing tensor layers.50.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[461/543] Writing tensor layers.50.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[462/543] Writing tensor layers.50.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[463/543] Writing tensor layers.51.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[464/543] Writing tensor layers.51.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[465/543] Writing tensor layers.51.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[466/543] Writing tensor layers.51.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[467/543] Writing tensor layers.51.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[468/543] Writing tensor layers.51.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[469/543] Writing tensor layers.51.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[470/543] Writing tensor layers.51.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[471/543] Writing tensor layers.51.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[472/543] Writing tensor layers.52.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[473/543] Writing tensor layers.52.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[474/543] Writing tensor layers.52.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[475/543] Writing tensor layers.52.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[476/543] Writing tensor layers.52.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[477/543] Writing tensor layers.52.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[478/543] Writing tensor layers.52.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[479/543] Writing tensor layers.52.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[480/543] Writing tensor layers.52.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[481/543] Writing tensor layers.53.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[482/543] Writing tensor layers.53.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[483/543] Writing tensor layers.53.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[484/543] Writing tensor layers.53.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[485/543] Writing tensor layers.53.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[486/543] Writing tensor layers.53.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[487/543] Writing tensor layers.53.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[488/543] Writing tensor layers.53.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[489/543] Writing tensor layers.53.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[490/543] Writing tensor layers.54.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[491/543] Writing tensor layers.54.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[492/543] Writing tensor layers.54.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[493/543] Writing tensor layers.54.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[494/543] Writing tensor layers.54.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[495/543] Writing tensor layers.54.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[496/543] Writing tensor layers.54.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[497/543] Writing tensor layers.54.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[498/543] Writing tensor layers.54.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[499/543] Writing tensor layers.55.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[500/543] Writing tensor layers.55.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[501/543] Writing tensor layers.55.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[502/543] Writing tensor layers.55.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[503/543] Writing tensor layers.55.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[504/543] Writing tensor layers.55.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[505/543] Writing tensor layers.55.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[506/543] Writing tensor layers.55.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[507/543] Writing tensor layers.55.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[508/543] Writing tensor layers.56.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[509/543] Writing tensor layers.56.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[510/543] Writing tensor layers.56.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[511/543] Writing tensor layers.56.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[512/543] Writing tensor layers.56.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[513/543] Writing tensor layers.56.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[514/543] Writing tensor layers.56.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[515/543] Writing tensor layers.56.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[516/543] Writing tensor layers.56.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[517/543] Writing tensor layers.57.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[518/543] Writing tensor layers.57.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[519/543] Writing tensor layers.57.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[520/543] Writing tensor layers.57.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[521/543] Writing tensor layers.57.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[522/543] Writing tensor layers.57.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[523/543] Writing tensor layers.57.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[524/543] Writing tensor layers.57.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[525/543] Writing tensor layers.57.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[526/543] Writing tensor layers.58.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[527/543] Writing tensor layers.58.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[528/543] Writing tensor layers.58.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[529/543] Writing tensor layers.58.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[530/543] Writing tensor layers.58.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[531/543] Writing tensor layers.58.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[532/543] Writing tensor layers.58.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[533/543] Writing tensor layers.58.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[534/543] Writing tensor layers.58.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[535/543] Writing tensor layers.59.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[536/543] Writing tensor layers.59.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[537/543] Writing tensor layers.59.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[538/543] Writing tensor layers.59.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[539/543] Writing tensor layers.59.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[540/543] Writing tensor layers.59.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[541/543] Writing tensor layers.59.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[542/543] Writing tensor layers.59.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[543/543] Writing tensor layers.59.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/30B/ggml-model-f16.bin\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:8192 n_mult:256 n_head:64 n_layer:80\n",
      "Writing vocab...\n",
      "[  1/723] Writing tensor tok_embeddings.weight                  | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  2/723] Writing tensor norm.weight                            | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  4/723] Writing tensor layers.0.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  5/723] Writing tensor layers.0.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  6/723] Writing tensor layers.0.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  7/723] Writing tensor layers.0.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  8/723] Writing tensor layers.0.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  9/723] Writing tensor layers.0.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 10/723] Writing tensor layers.0.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 11/723] Writing tensor layers.0.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 12/723] Writing tensor layers.0.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 13/723] Writing tensor layers.1.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 14/723] Writing tensor layers.1.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 15/723] Writing tensor layers.1.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 16/723] Writing tensor layers.1.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 17/723] Writing tensor layers.1.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 18/723] Writing tensor layers.1.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 19/723] Writing tensor layers.1.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 20/723] Writing tensor layers.1.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 21/723] Writing tensor layers.1.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 22/723] Writing tensor layers.2.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 23/723] Writing tensor layers.2.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 24/723] Writing tensor layers.2.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 25/723] Writing tensor layers.2.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 26/723] Writing tensor layers.2.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 27/723] Writing tensor layers.2.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 28/723] Writing tensor layers.2.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 29/723] Writing tensor layers.2.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 30/723] Writing tensor layers.2.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 31/723] Writing tensor layers.3.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 32/723] Writing tensor layers.3.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 33/723] Writing tensor layers.3.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 34/723] Writing tensor layers.3.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 35/723] Writing tensor layers.3.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 36/723] Writing tensor layers.3.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 37/723] Writing tensor layers.3.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 38/723] Writing tensor layers.3.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 39/723] Writing tensor layers.3.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 40/723] Writing tensor layers.4.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 41/723] Writing tensor layers.4.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 42/723] Writing tensor layers.4.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 43/723] Writing tensor layers.4.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 44/723] Writing tensor layers.4.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 45/723] Writing tensor layers.4.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 46/723] Writing tensor layers.4.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 47/723] Writing tensor layers.4.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 48/723] Writing tensor layers.4.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 49/723] Writing tensor layers.5.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 50/723] Writing tensor layers.5.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 51/723] Writing tensor layers.5.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 52/723] Writing tensor layers.5.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 53/723] Writing tensor layers.5.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 54/723] Writing tensor layers.5.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 55/723] Writing tensor layers.5.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 56/723] Writing tensor layers.5.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 57/723] Writing tensor layers.5.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 58/723] Writing tensor layers.6.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 59/723] Writing tensor layers.6.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 60/723] Writing tensor layers.6.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 61/723] Writing tensor layers.6.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 62/723] Writing tensor layers.6.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 63/723] Writing tensor layers.6.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 64/723] Writing tensor layers.6.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 65/723] Writing tensor layers.6.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 66/723] Writing tensor layers.6.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 67/723] Writing tensor layers.7.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 68/723] Writing tensor layers.7.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 69/723] Writing tensor layers.7.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 70/723] Writing tensor layers.7.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 71/723] Writing tensor layers.7.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 72/723] Writing tensor layers.7.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 73/723] Writing tensor layers.7.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 74/723] Writing tensor layers.7.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 75/723] Writing tensor layers.7.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 76/723] Writing tensor layers.8.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 77/723] Writing tensor layers.8.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 78/723] Writing tensor layers.8.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 79/723] Writing tensor layers.8.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 80/723] Writing tensor layers.8.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 81/723] Writing tensor layers.8.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 82/723] Writing tensor layers.8.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 83/723] Writing tensor layers.8.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 84/723] Writing tensor layers.8.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 85/723] Writing tensor layers.9.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 86/723] Writing tensor layers.9.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 87/723] Writing tensor layers.9.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 88/723] Writing tensor layers.9.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 89/723] Writing tensor layers.9.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 90/723] Writing tensor layers.9.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 91/723] Writing tensor layers.9.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 92/723] Writing tensor layers.9.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 93/723] Writing tensor layers.9.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 94/723] Writing tensor layers.10.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 95/723] Writing tensor layers.10.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 96/723] Writing tensor layers.10.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 97/723] Writing tensor layers.10.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 98/723] Writing tensor layers.10.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 99/723] Writing tensor layers.10.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[100/723] Writing tensor layers.10.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[101/723] Writing tensor layers.10.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[102/723] Writing tensor layers.10.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[103/723] Writing tensor layers.11.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[104/723] Writing tensor layers.11.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[105/723] Writing tensor layers.11.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[106/723] Writing tensor layers.11.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[107/723] Writing tensor layers.11.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[108/723] Writing tensor layers.11.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[109/723] Writing tensor layers.11.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[110/723] Writing tensor layers.11.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[111/723] Writing tensor layers.11.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[112/723] Writing tensor layers.12.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[113/723] Writing tensor layers.12.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[114/723] Writing tensor layers.12.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[115/723] Writing tensor layers.12.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[116/723] Writing tensor layers.12.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[117/723] Writing tensor layers.12.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[118/723] Writing tensor layers.12.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[119/723] Writing tensor layers.12.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[120/723] Writing tensor layers.12.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[121/723] Writing tensor layers.13.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[122/723] Writing tensor layers.13.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[123/723] Writing tensor layers.13.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[124/723] Writing tensor layers.13.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[125/723] Writing tensor layers.13.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[126/723] Writing tensor layers.13.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[127/723] Writing tensor layers.13.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[128/723] Writing tensor layers.13.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[129/723] Writing tensor layers.13.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[130/723] Writing tensor layers.14.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[131/723] Writing tensor layers.14.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[132/723] Writing tensor layers.14.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[133/723] Writing tensor layers.14.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[134/723] Writing tensor layers.14.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[135/723] Writing tensor layers.14.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[136/723] Writing tensor layers.14.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[137/723] Writing tensor layers.14.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[138/723] Writing tensor layers.14.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[139/723] Writing tensor layers.15.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[140/723] Writing tensor layers.15.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[141/723] Writing tensor layers.15.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[142/723] Writing tensor layers.15.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[143/723] Writing tensor layers.15.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[144/723] Writing tensor layers.15.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[145/723] Writing tensor layers.15.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[146/723] Writing tensor layers.15.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[147/723] Writing tensor layers.15.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[148/723] Writing tensor layers.16.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[149/723] Writing tensor layers.16.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[150/723] Writing tensor layers.16.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[151/723] Writing tensor layers.16.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[152/723] Writing tensor layers.16.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[153/723] Writing tensor layers.16.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[154/723] Writing tensor layers.16.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[155/723] Writing tensor layers.16.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[156/723] Writing tensor layers.16.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[157/723] Writing tensor layers.17.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[158/723] Writing tensor layers.17.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[159/723] Writing tensor layers.17.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[160/723] Writing tensor layers.17.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[161/723] Writing tensor layers.17.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[162/723] Writing tensor layers.17.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[163/723] Writing tensor layers.17.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[164/723] Writing tensor layers.17.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[165/723] Writing tensor layers.17.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[166/723] Writing tensor layers.18.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[167/723] Writing tensor layers.18.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[168/723] Writing tensor layers.18.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[169/723] Writing tensor layers.18.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[170/723] Writing tensor layers.18.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[171/723] Writing tensor layers.18.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[172/723] Writing tensor layers.18.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[173/723] Writing tensor layers.18.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[174/723] Writing tensor layers.18.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[175/723] Writing tensor layers.19.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[176/723] Writing tensor layers.19.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[177/723] Writing tensor layers.19.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[178/723] Writing tensor layers.19.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[179/723] Writing tensor layers.19.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[180/723] Writing tensor layers.19.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[181/723] Writing tensor layers.19.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[182/723] Writing tensor layers.19.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[183/723] Writing tensor layers.19.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[184/723] Writing tensor layers.20.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[185/723] Writing tensor layers.20.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[186/723] Writing tensor layers.20.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[187/723] Writing tensor layers.20.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[188/723] Writing tensor layers.20.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[189/723] Writing tensor layers.20.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[190/723] Writing tensor layers.20.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[191/723] Writing tensor layers.20.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[192/723] Writing tensor layers.20.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[193/723] Writing tensor layers.21.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[194/723] Writing tensor layers.21.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[195/723] Writing tensor layers.21.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[196/723] Writing tensor layers.21.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[197/723] Writing tensor layers.21.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[198/723] Writing tensor layers.21.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[199/723] Writing tensor layers.21.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[200/723] Writing tensor layers.21.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[201/723] Writing tensor layers.21.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[202/723] Writing tensor layers.22.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[203/723] Writing tensor layers.22.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[204/723] Writing tensor layers.22.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[205/723] Writing tensor layers.22.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[206/723] Writing tensor layers.22.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[207/723] Writing tensor layers.22.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[208/723] Writing tensor layers.22.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[209/723] Writing tensor layers.22.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[210/723] Writing tensor layers.22.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[211/723] Writing tensor layers.23.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[212/723] Writing tensor layers.23.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[213/723] Writing tensor layers.23.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[214/723] Writing tensor layers.23.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[215/723] Writing tensor layers.23.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[216/723] Writing tensor layers.23.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[217/723] Writing tensor layers.23.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[218/723] Writing tensor layers.23.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[219/723] Writing tensor layers.23.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[220/723] Writing tensor layers.24.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[221/723] Writing tensor layers.24.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[222/723] Writing tensor layers.24.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[223/723] Writing tensor layers.24.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[224/723] Writing tensor layers.24.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[225/723] Writing tensor layers.24.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[226/723] Writing tensor layers.24.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[227/723] Writing tensor layers.24.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[228/723] Writing tensor layers.24.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[229/723] Writing tensor layers.25.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[230/723] Writing tensor layers.25.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[231/723] Writing tensor layers.25.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[232/723] Writing tensor layers.25.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[233/723] Writing tensor layers.25.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[234/723] Writing tensor layers.25.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[235/723] Writing tensor layers.25.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[236/723] Writing tensor layers.25.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[237/723] Writing tensor layers.25.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[238/723] Writing tensor layers.26.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[239/723] Writing tensor layers.26.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[240/723] Writing tensor layers.26.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[241/723] Writing tensor layers.26.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[242/723] Writing tensor layers.26.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[243/723] Writing tensor layers.26.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[244/723] Writing tensor layers.26.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[245/723] Writing tensor layers.26.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[246/723] Writing tensor layers.26.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[247/723] Writing tensor layers.27.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[248/723] Writing tensor layers.27.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[249/723] Writing tensor layers.27.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[250/723] Writing tensor layers.27.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[251/723] Writing tensor layers.27.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[252/723] Writing tensor layers.27.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[253/723] Writing tensor layers.27.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[254/723] Writing tensor layers.27.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[255/723] Writing tensor layers.27.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[256/723] Writing tensor layers.28.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[257/723] Writing tensor layers.28.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[258/723] Writing tensor layers.28.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[259/723] Writing tensor layers.28.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[260/723] Writing tensor layers.28.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[261/723] Writing tensor layers.28.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[262/723] Writing tensor layers.28.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[263/723] Writing tensor layers.28.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[264/723] Writing tensor layers.28.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[265/723] Writing tensor layers.29.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[266/723] Writing tensor layers.29.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[267/723] Writing tensor layers.29.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[268/723] Writing tensor layers.29.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[269/723] Writing tensor layers.29.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[270/723] Writing tensor layers.29.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[271/723] Writing tensor layers.29.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[272/723] Writing tensor layers.29.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[273/723] Writing tensor layers.29.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[274/723] Writing tensor layers.30.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[275/723] Writing tensor layers.30.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[276/723] Writing tensor layers.30.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[277/723] Writing tensor layers.30.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[278/723] Writing tensor layers.30.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[279/723] Writing tensor layers.30.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[280/723] Writing tensor layers.30.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[281/723] Writing tensor layers.30.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[282/723] Writing tensor layers.30.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[283/723] Writing tensor layers.31.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[284/723] Writing tensor layers.31.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[285/723] Writing tensor layers.31.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[286/723] Writing tensor layers.31.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[287/723] Writing tensor layers.31.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[288/723] Writing tensor layers.31.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[289/723] Writing tensor layers.31.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[290/723] Writing tensor layers.31.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[291/723] Writing tensor layers.31.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[292/723] Writing tensor layers.32.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[293/723] Writing tensor layers.32.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[294/723] Writing tensor layers.32.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[295/723] Writing tensor layers.32.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[296/723] Writing tensor layers.32.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[297/723] Writing tensor layers.32.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[298/723] Writing tensor layers.32.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[299/723] Writing tensor layers.32.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[300/723] Writing tensor layers.32.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[301/723] Writing tensor layers.33.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[302/723] Writing tensor layers.33.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[303/723] Writing tensor layers.33.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[304/723] Writing tensor layers.33.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[305/723] Writing tensor layers.33.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[306/723] Writing tensor layers.33.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[307/723] Writing tensor layers.33.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[308/723] Writing tensor layers.33.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[309/723] Writing tensor layers.33.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[310/723] Writing tensor layers.34.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[311/723] Writing tensor layers.34.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[312/723] Writing tensor layers.34.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[313/723] Writing tensor layers.34.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[314/723] Writing tensor layers.34.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[315/723] Writing tensor layers.34.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[316/723] Writing tensor layers.34.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[317/723] Writing tensor layers.34.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[318/723] Writing tensor layers.34.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[319/723] Writing tensor layers.35.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[320/723] Writing tensor layers.35.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[321/723] Writing tensor layers.35.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[322/723] Writing tensor layers.35.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[323/723] Writing tensor layers.35.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[324/723] Writing tensor layers.35.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[325/723] Writing tensor layers.35.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[326/723] Writing tensor layers.35.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[327/723] Writing tensor layers.35.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[328/723] Writing tensor layers.36.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[329/723] Writing tensor layers.36.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[330/723] Writing tensor layers.36.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[331/723] Writing tensor layers.36.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[332/723] Writing tensor layers.36.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[333/723] Writing tensor layers.36.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[334/723] Writing tensor layers.36.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[335/723] Writing tensor layers.36.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[336/723] Writing tensor layers.36.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[337/723] Writing tensor layers.37.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[338/723] Writing tensor layers.37.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[339/723] Writing tensor layers.37.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[340/723] Writing tensor layers.37.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[341/723] Writing tensor layers.37.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[342/723] Writing tensor layers.37.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[343/723] Writing tensor layers.37.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[344/723] Writing tensor layers.37.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[345/723] Writing tensor layers.37.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[346/723] Writing tensor layers.38.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[347/723] Writing tensor layers.38.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[348/723] Writing tensor layers.38.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[349/723] Writing tensor layers.38.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[350/723] Writing tensor layers.38.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[351/723] Writing tensor layers.38.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[352/723] Writing tensor layers.38.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[353/723] Writing tensor layers.38.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[354/723] Writing tensor layers.38.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[355/723] Writing tensor layers.39.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[356/723] Writing tensor layers.39.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[357/723] Writing tensor layers.39.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[358/723] Writing tensor layers.39.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[359/723] Writing tensor layers.39.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[360/723] Writing tensor layers.39.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[361/723] Writing tensor layers.39.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[362/723] Writing tensor layers.39.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[363/723] Writing tensor layers.39.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[364/723] Writing tensor layers.40.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[365/723] Writing tensor layers.40.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[366/723] Writing tensor layers.40.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[367/723] Writing tensor layers.40.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[368/723] Writing tensor layers.40.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[369/723] Writing tensor layers.40.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[370/723] Writing tensor layers.40.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[371/723] Writing tensor layers.40.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[372/723] Writing tensor layers.40.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[373/723] Writing tensor layers.41.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[374/723] Writing tensor layers.41.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[375/723] Writing tensor layers.41.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[376/723] Writing tensor layers.41.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[377/723] Writing tensor layers.41.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[378/723] Writing tensor layers.41.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[379/723] Writing tensor layers.41.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[380/723] Writing tensor layers.41.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[381/723] Writing tensor layers.41.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[382/723] Writing tensor layers.42.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[383/723] Writing tensor layers.42.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[384/723] Writing tensor layers.42.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[385/723] Writing tensor layers.42.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[386/723] Writing tensor layers.42.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[387/723] Writing tensor layers.42.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[388/723] Writing tensor layers.42.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[389/723] Writing tensor layers.42.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[390/723] Writing tensor layers.42.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[391/723] Writing tensor layers.43.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[392/723] Writing tensor layers.43.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[393/723] Writing tensor layers.43.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[394/723] Writing tensor layers.43.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[395/723] Writing tensor layers.43.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[396/723] Writing tensor layers.43.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[397/723] Writing tensor layers.43.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[398/723] Writing tensor layers.43.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[399/723] Writing tensor layers.43.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[400/723] Writing tensor layers.44.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[401/723] Writing tensor layers.44.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[402/723] Writing tensor layers.44.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[403/723] Writing tensor layers.44.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[404/723] Writing tensor layers.44.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[405/723] Writing tensor layers.44.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[406/723] Writing tensor layers.44.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[407/723] Writing tensor layers.44.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[408/723] Writing tensor layers.44.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[409/723] Writing tensor layers.45.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[410/723] Writing tensor layers.45.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[411/723] Writing tensor layers.45.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[412/723] Writing tensor layers.45.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[413/723] Writing tensor layers.45.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[414/723] Writing tensor layers.45.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[415/723] Writing tensor layers.45.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[416/723] Writing tensor layers.45.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[417/723] Writing tensor layers.45.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[418/723] Writing tensor layers.46.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[419/723] Writing tensor layers.46.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[420/723] Writing tensor layers.46.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[421/723] Writing tensor layers.46.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[422/723] Writing tensor layers.46.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[423/723] Writing tensor layers.46.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[424/723] Writing tensor layers.46.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[425/723] Writing tensor layers.46.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[426/723] Writing tensor layers.46.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[427/723] Writing tensor layers.47.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[428/723] Writing tensor layers.47.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[429/723] Writing tensor layers.47.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[430/723] Writing tensor layers.47.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[431/723] Writing tensor layers.47.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[432/723] Writing tensor layers.47.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[433/723] Writing tensor layers.47.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[434/723] Writing tensor layers.47.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[435/723] Writing tensor layers.47.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[436/723] Writing tensor layers.48.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[437/723] Writing tensor layers.48.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[438/723] Writing tensor layers.48.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[439/723] Writing tensor layers.48.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[440/723] Writing tensor layers.48.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[441/723] Writing tensor layers.48.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[442/723] Writing tensor layers.48.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[443/723] Writing tensor layers.48.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[444/723] Writing tensor layers.48.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[445/723] Writing tensor layers.49.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[446/723] Writing tensor layers.49.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[447/723] Writing tensor layers.49.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[448/723] Writing tensor layers.49.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[449/723] Writing tensor layers.49.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[450/723] Writing tensor layers.49.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[451/723] Writing tensor layers.49.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[452/723] Writing tensor layers.49.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[453/723] Writing tensor layers.49.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[454/723] Writing tensor layers.50.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[455/723] Writing tensor layers.50.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[456/723] Writing tensor layers.50.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[457/723] Writing tensor layers.50.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[458/723] Writing tensor layers.50.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[459/723] Writing tensor layers.50.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[460/723] Writing tensor layers.50.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[461/723] Writing tensor layers.50.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[462/723] Writing tensor layers.50.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[463/723] Writing tensor layers.51.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[464/723] Writing tensor layers.51.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[465/723] Writing tensor layers.51.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[466/723] Writing tensor layers.51.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[467/723] Writing tensor layers.51.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[468/723] Writing tensor layers.51.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[469/723] Writing tensor layers.51.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[470/723] Writing tensor layers.51.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[471/723] Writing tensor layers.51.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[472/723] Writing tensor layers.52.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[473/723] Writing tensor layers.52.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[474/723] Writing tensor layers.52.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[475/723] Writing tensor layers.52.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[476/723] Writing tensor layers.52.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[477/723] Writing tensor layers.52.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[478/723] Writing tensor layers.52.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[479/723] Writing tensor layers.52.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[480/723] Writing tensor layers.52.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[481/723] Writing tensor layers.53.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[482/723] Writing tensor layers.53.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[483/723] Writing tensor layers.53.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[484/723] Writing tensor layers.53.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[485/723] Writing tensor layers.53.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[486/723] Writing tensor layers.53.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[487/723] Writing tensor layers.53.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[488/723] Writing tensor layers.53.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[489/723] Writing tensor layers.53.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[490/723] Writing tensor layers.54.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[491/723] Writing tensor layers.54.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[492/723] Writing tensor layers.54.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[493/723] Writing tensor layers.54.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[494/723] Writing tensor layers.54.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[495/723] Writing tensor layers.54.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[496/723] Writing tensor layers.54.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[497/723] Writing tensor layers.54.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[498/723] Writing tensor layers.54.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[499/723] Writing tensor layers.55.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[500/723] Writing tensor layers.55.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[501/723] Writing tensor layers.55.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[502/723] Writing tensor layers.55.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[503/723] Writing tensor layers.55.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[504/723] Writing tensor layers.55.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[505/723] Writing tensor layers.55.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[506/723] Writing tensor layers.55.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[507/723] Writing tensor layers.55.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[508/723] Writing tensor layers.56.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[509/723] Writing tensor layers.56.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[510/723] Writing tensor layers.56.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[511/723] Writing tensor layers.56.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[512/723] Writing tensor layers.56.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[513/723] Writing tensor layers.56.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[514/723] Writing tensor layers.56.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[515/723] Writing tensor layers.56.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[516/723] Writing tensor layers.56.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[517/723] Writing tensor layers.57.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[518/723] Writing tensor layers.57.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[519/723] Writing tensor layers.57.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[520/723] Writing tensor layers.57.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[521/723] Writing tensor layers.57.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[522/723] Writing tensor layers.57.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[523/723] Writing tensor layers.57.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[524/723] Writing tensor layers.57.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[525/723] Writing tensor layers.57.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[526/723] Writing tensor layers.58.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[527/723] Writing tensor layers.58.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[528/723] Writing tensor layers.58.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[529/723] Writing tensor layers.58.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[530/723] Writing tensor layers.58.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[531/723] Writing tensor layers.58.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[532/723] Writing tensor layers.58.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[533/723] Writing tensor layers.58.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[534/723] Writing tensor layers.58.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[535/723] Writing tensor layers.59.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[536/723] Writing tensor layers.59.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[537/723] Writing tensor layers.59.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[538/723] Writing tensor layers.59.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[539/723] Writing tensor layers.59.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[540/723] Writing tensor layers.59.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[541/723] Writing tensor layers.59.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[542/723] Writing tensor layers.59.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[543/723] Writing tensor layers.59.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[544/723] Writing tensor layers.60.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[545/723] Writing tensor layers.60.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[546/723] Writing tensor layers.60.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[547/723] Writing tensor layers.60.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[548/723] Writing tensor layers.60.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[549/723] Writing tensor layers.60.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[550/723] Writing tensor layers.60.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[551/723] Writing tensor layers.60.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[552/723] Writing tensor layers.60.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[553/723] Writing tensor layers.61.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[554/723] Writing tensor layers.61.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[555/723] Writing tensor layers.61.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[556/723] Writing tensor layers.61.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[557/723] Writing tensor layers.61.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[558/723] Writing tensor layers.61.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[559/723] Writing tensor layers.61.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[560/723] Writing tensor layers.61.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[561/723] Writing tensor layers.61.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[562/723] Writing tensor layers.62.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[563/723] Writing tensor layers.62.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[564/723] Writing tensor layers.62.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[565/723] Writing tensor layers.62.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[566/723] Writing tensor layers.62.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[567/723] Writing tensor layers.62.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[568/723] Writing tensor layers.62.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[569/723] Writing tensor layers.62.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[570/723] Writing tensor layers.62.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[571/723] Writing tensor layers.63.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[572/723] Writing tensor layers.63.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[573/723] Writing tensor layers.63.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[574/723] Writing tensor layers.63.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[575/723] Writing tensor layers.63.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[576/723] Writing tensor layers.63.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[577/723] Writing tensor layers.63.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[578/723] Writing tensor layers.63.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[579/723] Writing tensor layers.63.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[580/723] Writing tensor layers.64.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[581/723] Writing tensor layers.64.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[582/723] Writing tensor layers.64.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[583/723] Writing tensor layers.64.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[584/723] Writing tensor layers.64.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[585/723] Writing tensor layers.64.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[586/723] Writing tensor layers.64.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[587/723] Writing tensor layers.64.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[588/723] Writing tensor layers.64.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[589/723] Writing tensor layers.65.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[590/723] Writing tensor layers.65.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[591/723] Writing tensor layers.65.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[592/723] Writing tensor layers.65.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[593/723] Writing tensor layers.65.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[594/723] Writing tensor layers.65.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[595/723] Writing tensor layers.65.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[596/723] Writing tensor layers.65.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[597/723] Writing tensor layers.65.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[598/723] Writing tensor layers.66.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[599/723] Writing tensor layers.66.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[600/723] Writing tensor layers.66.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[601/723] Writing tensor layers.66.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[602/723] Writing tensor layers.66.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[603/723] Writing tensor layers.66.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[604/723] Writing tensor layers.66.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[605/723] Writing tensor layers.66.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[606/723] Writing tensor layers.66.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[607/723] Writing tensor layers.67.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[608/723] Writing tensor layers.67.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[609/723] Writing tensor layers.67.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[610/723] Writing tensor layers.67.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[611/723] Writing tensor layers.67.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[612/723] Writing tensor layers.67.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[613/723] Writing tensor layers.67.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[614/723] Writing tensor layers.67.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[615/723] Writing tensor layers.67.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[616/723] Writing tensor layers.68.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[617/723] Writing tensor layers.68.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[618/723] Writing tensor layers.68.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[619/723] Writing tensor layers.68.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[620/723] Writing tensor layers.68.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[621/723] Writing tensor layers.68.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[622/723] Writing tensor layers.68.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[623/723] Writing tensor layers.68.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[624/723] Writing tensor layers.68.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[625/723] Writing tensor layers.69.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[626/723] Writing tensor layers.69.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[627/723] Writing tensor layers.69.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[628/723] Writing tensor layers.69.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[629/723] Writing tensor layers.69.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[630/723] Writing tensor layers.69.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[631/723] Writing tensor layers.69.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[632/723] Writing tensor layers.69.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[633/723] Writing tensor layers.69.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[634/723] Writing tensor layers.70.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[635/723] Writing tensor layers.70.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[636/723] Writing tensor layers.70.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[637/723] Writing tensor layers.70.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[638/723] Writing tensor layers.70.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[639/723] Writing tensor layers.70.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[640/723] Writing tensor layers.70.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[641/723] Writing tensor layers.70.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[642/723] Writing tensor layers.70.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[643/723] Writing tensor layers.71.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[644/723] Writing tensor layers.71.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[645/723] Writing tensor layers.71.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[646/723] Writing tensor layers.71.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[647/723] Writing tensor layers.71.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[648/723] Writing tensor layers.71.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[649/723] Writing tensor layers.71.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[650/723] Writing tensor layers.71.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[651/723] Writing tensor layers.71.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[652/723] Writing tensor layers.72.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[653/723] Writing tensor layers.72.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[654/723] Writing tensor layers.72.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[655/723] Writing tensor layers.72.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[656/723] Writing tensor layers.72.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[657/723] Writing tensor layers.72.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[658/723] Writing tensor layers.72.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[659/723] Writing tensor layers.72.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[660/723] Writing tensor layers.72.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[661/723] Writing tensor layers.73.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[662/723] Writing tensor layers.73.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[663/723] Writing tensor layers.73.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[664/723] Writing tensor layers.73.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[665/723] Writing tensor layers.73.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[666/723] Writing tensor layers.73.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[667/723] Writing tensor layers.73.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[668/723] Writing tensor layers.73.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[669/723] Writing tensor layers.73.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[670/723] Writing tensor layers.74.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[671/723] Writing tensor layers.74.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[672/723] Writing tensor layers.74.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[673/723] Writing tensor layers.74.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[674/723] Writing tensor layers.74.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[675/723] Writing tensor layers.74.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[676/723] Writing tensor layers.74.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[677/723] Writing tensor layers.74.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[678/723] Writing tensor layers.74.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[679/723] Writing tensor layers.75.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[680/723] Writing tensor layers.75.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[681/723] Writing tensor layers.75.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[682/723] Writing tensor layers.75.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[683/723] Writing tensor layers.75.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[684/723] Writing tensor layers.75.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[685/723] Writing tensor layers.75.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[686/723] Writing tensor layers.75.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[687/723] Writing tensor layers.75.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[688/723] Writing tensor layers.76.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[689/723] Writing tensor layers.76.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[690/723] Writing tensor layers.76.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[691/723] Writing tensor layers.76.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[692/723] Writing tensor layers.76.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[693/723] Writing tensor layers.76.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[694/723] Writing tensor layers.76.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[695/723] Writing tensor layers.76.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[696/723] Writing tensor layers.76.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[697/723] Writing tensor layers.77.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[698/723] Writing tensor layers.77.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[699/723] Writing tensor layers.77.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[700/723] Writing tensor layers.77.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[701/723] Writing tensor layers.77.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[702/723] Writing tensor layers.77.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[703/723] Writing tensor layers.77.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[704/723] Writing tensor layers.77.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[705/723] Writing tensor layers.77.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[706/723] Writing tensor layers.78.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[707/723] Writing tensor layers.78.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[708/723] Writing tensor layers.78.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[709/723] Writing tensor layers.78.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[710/723] Writing tensor layers.78.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[711/723] Writing tensor layers.78.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[712/723] Writing tensor layers.78.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[713/723] Writing tensor layers.78.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[714/723] Writing tensor layers.78.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[715/723] Writing tensor layers.79.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[716/723] Writing tensor layers.79.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[717/723] Writing tensor layers.79.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[718/723] Writing tensor layers.79.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[719/723] Writing tensor layers.79.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[720/723] Writing tensor layers.79.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[721/723] Writing tensor layers.79.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[722/723] Writing tensor layers.79.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[723/723] Writing tensor layers.79.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/65B/ggml-model-f16.bin\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/7B/ggml-model-f16.bin' to './models/7B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/7B/ggml-model-q4_0.bin\n",
      "[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->    70.31 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->   102.54 MB | hist: \n",
      "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 22588.49 ms\n",
      "main:    total time = 22588.49 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/13B/ggml-model-f16.bin' to './models/13B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/13B/ggml-model-q4_0.bin\n",
      "[   1/ 363]                tok_embeddings.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->    87.89 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                          norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->   128.17 MB | hist: \n",
      "[   4/ 363]         layers.0.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]         layers.0.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]         layers.0.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]         layers.0.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]       layers.0.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   9/ 363]      layers.0.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 363]      layers.0.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 363]      layers.0.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 363]             layers.0.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  13/ 363]         layers.1.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]         layers.1.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]         layers.1.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]         layers.1.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]       layers.1.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  18/ 363]      layers.1.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]      layers.1.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]      layers.1.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 363]             layers.1.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  22/ 363]         layers.2.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]         layers.2.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]         layers.2.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]         layers.2.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]       layers.2.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  27/ 363]      layers.2.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]      layers.2.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]      layers.2.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 363]             layers.2.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  31/ 363]         layers.3.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]         layers.3.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]         layers.3.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]         layers.3.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]       layers.3.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  36/ 363]      layers.3.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]      layers.3.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]      layers.3.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 363]             layers.3.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  40/ 363]         layers.4.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]         layers.4.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]         layers.4.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]         layers.4.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]       layers.4.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  45/ 363]      layers.4.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]      layers.4.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]      layers.4.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 363]             layers.4.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  49/ 363]         layers.5.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]         layers.5.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]         layers.5.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]         layers.5.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]       layers.5.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  54/ 363]      layers.5.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]      layers.5.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]      layers.5.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 363]             layers.5.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  58/ 363]         layers.6.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]         layers.6.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]         layers.6.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]         layers.6.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]       layers.6.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  63/ 363]      layers.6.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 363]      layers.6.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 363]      layers.6.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 363]             layers.6.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  67/ 363]         layers.7.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]         layers.7.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]         layers.7.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]         layers.7.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]       layers.7.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  72/ 363]      layers.7.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 363]      layers.7.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 363]      layers.7.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 363]             layers.7.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  76/ 363]         layers.8.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]         layers.8.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]         layers.8.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]         layers.8.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]       layers.8.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  81/ 363]      layers.8.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]      layers.8.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]      layers.8.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 363]             layers.8.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  85/ 363]         layers.9.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]         layers.9.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]         layers.9.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]         layers.9.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]       layers.9.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  90/ 363]      layers.9.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 363]      layers.9.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 363]      layers.9.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 363]             layers.9.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  94/ 363]        layers.10.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]        layers.10.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]        layers.10.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]        layers.10.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]      layers.10.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  99/ 363]     layers.10.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 363]     layers.10.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 363]     layers.10.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 363]            layers.10.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]        layers.11.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]        layers.11.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]        layers.11.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]        layers.11.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]      layers.11.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 108/ 363]     layers.11.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 363]     layers.11.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 110/ 363]     layers.11.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 363]            layers.11.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]        layers.12.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]        layers.12.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]        layers.12.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]        layers.12.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]      layers.12.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 117/ 363]     layers.12.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 363]     layers.12.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 363]     layers.12.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 363]            layers.12.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]        layers.13.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]        layers.13.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]        layers.13.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]        layers.13.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]      layers.13.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 126/ 363]     layers.13.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 363]     layers.13.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 363]     layers.13.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 363]            layers.13.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]        layers.14.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]        layers.14.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]        layers.14.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]        layers.14.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]      layers.14.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 135/ 363]     layers.14.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 363]     layers.14.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 363]     layers.14.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 363]            layers.14.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]        layers.15.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]        layers.15.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]        layers.15.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]        layers.15.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]      layers.15.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 144/ 363]     layers.15.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]     layers.15.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]     layers.15.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 363]            layers.15.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]        layers.16.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]        layers.16.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]        layers.16.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]        layers.16.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]      layers.16.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 153/ 363]     layers.16.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 363]     layers.16.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 363]     layers.16.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 363]            layers.16.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]        layers.17.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]        layers.17.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]        layers.17.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]        layers.17.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]      layers.17.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 162/ 363]     layers.17.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]     layers.17.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]     layers.17.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 363]            layers.17.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]        layers.18.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]        layers.18.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]        layers.18.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]        layers.18.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]      layers.18.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 171/ 363]     layers.18.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]     layers.18.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]     layers.18.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 363]            layers.18.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]        layers.19.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]        layers.19.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]        layers.19.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]        layers.19.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]      layers.19.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 180/ 363]     layers.19.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]     layers.19.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]     layers.19.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 363]            layers.19.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]        layers.20.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]        layers.20.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]        layers.20.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]        layers.20.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]      layers.20.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 189/ 363]     layers.20.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]     layers.20.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]     layers.20.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 363]            layers.20.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]        layers.21.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]        layers.21.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]        layers.21.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]        layers.21.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]      layers.21.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 198/ 363]     layers.21.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]     layers.21.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]     layers.21.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 363]            layers.21.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]        layers.22.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]        layers.22.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]        layers.22.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]        layers.22.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]      layers.22.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 207/ 363]     layers.22.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]     layers.22.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]     layers.22.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 363]            layers.22.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]        layers.23.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]        layers.23.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]        layers.23.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]        layers.23.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]      layers.23.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 216/ 363]     layers.23.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]     layers.23.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]     layers.23.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 363]            layers.23.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]        layers.24.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]        layers.24.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]        layers.24.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]        layers.24.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]      layers.24.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 225/ 363]     layers.24.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]     layers.24.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]     layers.24.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 363]            layers.24.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]        layers.25.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]        layers.25.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]        layers.25.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]        layers.25.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]      layers.25.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 234/ 363]     layers.25.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]     layers.25.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]     layers.25.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 363]            layers.25.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]        layers.26.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]        layers.26.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]        layers.26.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]        layers.26.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]      layers.26.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 243/ 363]     layers.26.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]     layers.26.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]     layers.26.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 363]            layers.26.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]        layers.27.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]        layers.27.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]        layers.27.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]        layers.27.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]      layers.27.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 252/ 363]     layers.27.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]     layers.27.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]     layers.27.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 363]            layers.27.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]        layers.28.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]        layers.28.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]        layers.28.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]        layers.28.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]      layers.28.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 261/ 363]     layers.28.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]     layers.28.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]     layers.28.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 363]            layers.28.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]        layers.29.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]        layers.29.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]        layers.29.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]        layers.29.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]      layers.29.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 270/ 363]     layers.29.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]     layers.29.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]     layers.29.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 363]            layers.29.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]        layers.30.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]        layers.30.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]        layers.30.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]        layers.30.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]      layers.30.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 279/ 363]     layers.30.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]     layers.30.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]     layers.30.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 363]            layers.30.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]        layers.31.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]        layers.31.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]        layers.31.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]        layers.31.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]      layers.31.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 288/ 363]     layers.31.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]     layers.31.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]     layers.31.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 363]            layers.31.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]        layers.32.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]        layers.32.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]        layers.32.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]        layers.32.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]      layers.32.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 297/ 363]     layers.32.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]     layers.32.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]     layers.32.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 363]            layers.32.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]        layers.33.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]        layers.33.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]        layers.33.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]        layers.33.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]      layers.33.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 306/ 363]     layers.33.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]     layers.33.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]     layers.33.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 363]            layers.33.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]        layers.34.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]        layers.34.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]        layers.34.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]        layers.34.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]      layers.34.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 315/ 363]     layers.34.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]     layers.34.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]     layers.34.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 363]            layers.34.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]        layers.35.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]        layers.35.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]        layers.35.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]        layers.35.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]      layers.35.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 324/ 363]     layers.35.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]     layers.35.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]     layers.35.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 363]            layers.35.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]        layers.36.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]        layers.36.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]        layers.36.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]        layers.36.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]      layers.36.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 333/ 363]     layers.36.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 363]     layers.36.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 335/ 363]     layers.36.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 363]            layers.36.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]        layers.37.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]        layers.37.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]        layers.37.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]        layers.37.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]      layers.37.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 342/ 363]     layers.37.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 363]     layers.37.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 344/ 363]     layers.37.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 363]            layers.37.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]        layers.38.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]        layers.38.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]        layers.38.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]        layers.38.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]      layers.38.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 351/ 363]     layers.38.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 363]     layers.38.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 353/ 363]     layers.38.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 363]            layers.38.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]        layers.39.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]        layers.39.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]        layers.39.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 42267.73 ms\n",
      "main:    total time = 42267.74 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/30B/ggml-model-f16.bin' to './models/30B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/30B/ggml-model-q4_0.bin\n",
      "[   1/ 543]                tok_embeddings.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   114.26 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                          norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   166.63 MB | hist: \n",
      "[   4/ 543]         layers.0.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]         layers.0.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]         layers.0.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]         layers.0.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]       layers.0.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   9/ 543]      layers.0.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 543]      layers.0.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 543]      layers.0.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 543]             layers.0.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  13/ 543]         layers.1.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]         layers.1.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]         layers.1.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]         layers.1.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]       layers.1.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  18/ 543]      layers.1.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]      layers.1.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]      layers.1.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 543]             layers.1.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  22/ 543]         layers.2.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]         layers.2.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]         layers.2.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]         layers.2.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]       layers.2.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  27/ 543]      layers.2.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]      layers.2.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]      layers.2.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 543]             layers.2.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  31/ 543]         layers.3.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]         layers.3.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]         layers.3.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]         layers.3.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]       layers.3.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  36/ 543]      layers.3.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 543]      layers.3.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  38/ 543]      layers.3.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 543]             layers.3.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  40/ 543]         layers.4.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]         layers.4.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]         layers.4.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]         layers.4.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]       layers.4.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  45/ 543]      layers.4.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]      layers.4.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]      layers.4.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 543]             layers.4.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  49/ 543]         layers.5.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]         layers.5.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]         layers.5.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]         layers.5.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]       layers.5.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  54/ 543]      layers.5.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]      layers.5.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]      layers.5.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 543]             layers.5.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  58/ 543]         layers.6.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]         layers.6.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]         layers.6.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]         layers.6.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]       layers.6.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  63/ 543]      layers.6.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]      layers.6.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]      layers.6.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 543]             layers.6.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  67/ 543]         layers.7.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]         layers.7.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]         layers.7.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]         layers.7.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]       layers.7.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  72/ 543]      layers.7.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]      layers.7.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]      layers.7.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 543]             layers.7.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  76/ 543]         layers.8.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]         layers.8.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]         layers.8.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]         layers.8.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]       layers.8.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  81/ 543]      layers.8.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]      layers.8.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]      layers.8.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 543]             layers.8.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  85/ 543]         layers.9.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]         layers.9.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]         layers.9.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]         layers.9.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]       layers.9.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  90/ 543]      layers.9.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]      layers.9.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]      layers.9.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 543]             layers.9.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  94/ 543]        layers.10.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]        layers.10.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]        layers.10.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]        layers.10.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]      layers.10.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  99/ 543]     layers.10.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]     layers.10.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]     layers.10.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 543]            layers.10.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]        layers.11.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]        layers.11.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]        layers.11.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]        layers.11.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]      layers.11.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 108/ 543]     layers.11.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]     layers.11.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]     layers.11.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 543]            layers.11.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]        layers.12.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]        layers.12.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]        layers.12.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]        layers.12.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]      layers.12.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 117/ 543]     layers.12.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]     layers.12.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]     layers.12.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 543]            layers.12.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]        layers.13.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]        layers.13.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]        layers.13.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]        layers.13.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]      layers.13.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 126/ 543]     layers.13.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]     layers.13.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]     layers.13.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 543]            layers.13.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]        layers.14.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]        layers.14.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]        layers.14.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]        layers.14.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]      layers.14.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 135/ 543]     layers.14.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 543]     layers.14.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 543]     layers.14.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 543]            layers.14.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]        layers.15.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]        layers.15.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]        layers.15.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]        layers.15.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]      layers.15.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 144/ 543]     layers.15.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]     layers.15.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]     layers.15.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 543]            layers.15.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]        layers.16.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]        layers.16.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]        layers.16.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]        layers.16.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]      layers.16.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 153/ 543]     layers.16.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]     layers.16.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]     layers.16.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 543]            layers.16.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]        layers.17.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]        layers.17.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]        layers.17.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]        layers.17.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]      layers.17.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 162/ 543]     layers.17.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]     layers.17.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]     layers.17.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 543]            layers.17.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]        layers.18.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]        layers.18.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]        layers.18.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]        layers.18.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]      layers.18.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 171/ 543]     layers.18.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]     layers.18.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]     layers.18.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 543]            layers.18.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]        layers.19.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]        layers.19.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]        layers.19.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]        layers.19.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]      layers.19.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 180/ 543]     layers.19.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 543]     layers.19.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 182/ 543]     layers.19.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 543]            layers.19.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]        layers.20.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]        layers.20.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]        layers.20.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]        layers.20.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]      layers.20.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 189/ 543]     layers.20.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 543]     layers.20.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 191/ 543]     layers.20.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 543]            layers.20.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]        layers.21.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]        layers.21.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]        layers.21.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]        layers.21.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]      layers.21.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 198/ 543]     layers.21.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]     layers.21.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]     layers.21.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 543]            layers.21.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]        layers.22.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]        layers.22.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]        layers.22.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]        layers.22.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]      layers.22.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 207/ 543]     layers.22.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]     layers.22.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]     layers.22.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 543]            layers.22.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]        layers.23.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]        layers.23.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]        layers.23.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]        layers.23.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]      layers.23.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 216/ 543]     layers.23.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]     layers.23.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]     layers.23.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 543]            layers.23.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]        layers.24.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]        layers.24.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]        layers.24.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]        layers.24.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]      layers.24.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 225/ 543]     layers.24.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]     layers.24.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]     layers.24.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 543]            layers.24.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]        layers.25.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]        layers.25.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]        layers.25.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]        layers.25.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]      layers.25.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 234/ 543]     layers.25.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]     layers.25.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]     layers.25.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 543]            layers.25.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]        layers.26.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]        layers.26.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]        layers.26.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]        layers.26.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]      layers.26.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 243/ 543]     layers.26.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]     layers.26.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]     layers.26.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 543]            layers.26.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]        layers.27.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]        layers.27.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]        layers.27.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]        layers.27.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]      layers.27.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 252/ 543]     layers.27.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]     layers.27.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]     layers.27.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 543]            layers.27.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]        layers.28.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]        layers.28.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]        layers.28.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]        layers.28.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]      layers.28.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 261/ 543]     layers.28.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]     layers.28.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]     layers.28.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 543]            layers.28.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]        layers.29.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]        layers.29.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]        layers.29.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]        layers.29.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]      layers.29.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 270/ 543]     layers.29.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]     layers.29.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]     layers.29.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 543]            layers.29.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]        layers.30.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]        layers.30.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]        layers.30.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]        layers.30.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]      layers.30.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 279/ 543]     layers.30.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]     layers.30.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]     layers.30.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 543]            layers.30.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]        layers.31.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]        layers.31.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]        layers.31.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]        layers.31.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]      layers.31.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 288/ 543]     layers.31.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]     layers.31.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]     layers.31.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 543]            layers.31.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]        layers.32.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]        layers.32.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]        layers.32.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]        layers.32.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]      layers.32.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 297/ 543]     layers.32.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]     layers.32.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]     layers.32.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 543]            layers.32.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]        layers.33.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]        layers.33.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]        layers.33.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]        layers.33.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]      layers.33.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 306/ 543]     layers.33.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]     layers.33.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]     layers.33.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 543]            layers.33.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]        layers.34.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]        layers.34.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]        layers.34.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]        layers.34.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]      layers.34.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 315/ 543]     layers.34.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]     layers.34.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]     layers.34.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 543]            layers.34.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]        layers.35.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]        layers.35.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]        layers.35.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]        layers.35.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]      layers.35.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 324/ 543]     layers.35.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]     layers.35.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]     layers.35.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 543]            layers.35.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]        layers.36.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]        layers.36.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]        layers.36.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]        layers.36.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]      layers.36.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 333/ 543]     layers.36.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]     layers.36.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]     layers.36.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 543]            layers.36.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]        layers.37.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]        layers.37.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]        layers.37.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]        layers.37.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]      layers.37.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 342/ 543]     layers.37.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]     layers.37.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]     layers.37.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 543]            layers.37.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]        layers.38.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]        layers.38.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]        layers.38.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]        layers.38.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]      layers.38.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 351/ 543]     layers.38.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]     layers.38.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]     layers.38.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 543]            layers.38.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]        layers.39.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]        layers.39.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]        layers.39.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]        layers.39.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]      layers.39.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 360/ 543]     layers.39.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]     layers.39.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]     layers.39.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 543]            layers.39.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]        layers.40.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]        layers.40.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]        layers.40.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]        layers.40.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]      layers.40.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 369/ 543]     layers.40.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]     layers.40.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]     layers.40.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 543]            layers.40.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]        layers.41.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]        layers.41.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]        layers.41.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]        layers.41.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]      layers.41.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 378/ 543]     layers.41.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]     layers.41.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]     layers.41.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 543]            layers.41.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]        layers.42.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]        layers.42.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]        layers.42.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]        layers.42.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]      layers.42.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 387/ 543]     layers.42.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]     layers.42.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]     layers.42.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 543]            layers.42.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]        layers.43.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]        layers.43.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]        layers.43.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]        layers.43.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]      layers.43.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 396/ 543]     layers.43.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]     layers.43.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]     layers.43.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 543]            layers.43.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]        layers.44.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]        layers.44.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]        layers.44.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]        layers.44.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]      layers.44.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 405/ 543]     layers.44.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]     layers.44.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]     layers.44.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 543]            layers.44.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]        layers.45.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]        layers.45.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]        layers.45.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]        layers.45.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]      layers.45.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 414/ 543]     layers.45.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]     layers.45.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]     layers.45.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 543]            layers.45.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]        layers.46.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]        layers.46.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]        layers.46.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]        layers.46.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]      layers.46.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 423/ 543]     layers.46.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]     layers.46.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]     layers.46.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 543]            layers.46.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]        layers.47.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]        layers.47.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]        layers.47.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]        layers.47.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]      layers.47.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 432/ 543]     layers.47.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]     layers.47.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]     layers.47.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 543]            layers.47.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]        layers.48.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]        layers.48.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]        layers.48.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]        layers.48.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]      layers.48.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 441/ 543]     layers.48.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]     layers.48.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]     layers.48.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 543]            layers.48.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]        layers.49.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]        layers.49.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]        layers.49.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]        layers.49.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]      layers.49.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 450/ 543]     layers.49.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]     layers.49.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]     layers.49.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 543]            layers.49.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]        layers.50.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]        layers.50.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]        layers.50.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]        layers.50.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]      layers.50.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 459/ 543]     layers.50.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]     layers.50.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]     layers.50.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 543]            layers.50.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]        layers.51.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]        layers.51.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]        layers.51.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]        layers.51.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]      layers.51.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 468/ 543]     layers.51.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]     layers.51.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]     layers.51.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 543]            layers.51.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]        layers.52.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]        layers.52.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]        layers.52.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]        layers.52.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]      layers.52.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 477/ 543]     layers.52.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]     layers.52.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]     layers.52.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 543]            layers.52.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]        layers.53.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]        layers.53.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]        layers.53.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]        layers.53.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]      layers.53.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 486/ 543]     layers.53.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]     layers.53.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]     layers.53.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 543]            layers.53.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]        layers.54.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]        layers.54.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]        layers.54.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]        layers.54.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]      layers.54.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 495/ 543]     layers.54.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]     layers.54.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]     layers.54.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 543]            layers.54.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]        layers.55.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]        layers.55.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]        layers.55.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]        layers.55.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]      layers.55.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 504/ 543]     layers.55.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]     layers.55.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]     layers.55.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 543]            layers.55.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]        layers.56.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]        layers.56.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]        layers.56.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]        layers.56.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]      layers.56.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 513/ 543]     layers.56.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 543]     layers.56.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 515/ 543]     layers.56.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 543]            layers.56.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]        layers.57.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]        layers.57.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]        layers.57.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]        layers.57.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]      layers.57.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 522/ 543]     layers.57.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 543]     layers.57.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 524/ 543]     layers.57.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 543]            layers.57.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]        layers.58.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]        layers.58.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]        layers.58.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]        layers.58.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]      layers.58.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 531/ 543]     layers.58.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 543]     layers.58.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 533/ 543]     layers.58.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 543]            layers.58.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]        layers.59.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]        layers.59.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]        layers.59.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]        layers.59.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]      layers.59.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 540/ 543]     layers.59.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 541/ 543]     layers.59.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 542/ 543]     layers.59.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 543/ 543]            layers.59.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 121535.30 ms\n",
      "main:    total time = 121535.31 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/65B/ggml-model-f16.bin' to './models/65B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/65B/ggml-model-q4_0.bin\n",
      "[   1/ 723]                tok_embeddings.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   140.62 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                          norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   205.08 MB | hist: \n",
      "[   4/ 723]         layers.0.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]         layers.0.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]         layers.0.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]         layers.0.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]       layers.0.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   9/ 723]      layers.0.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[  10/ 723]      layers.0.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  11/ 723]      layers.0.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  12/ 723]             layers.0.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  13/ 723]         layers.1.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]         layers.1.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]         layers.1.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]         layers.1.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]       layers.1.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  18/ 723]      layers.1.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]      layers.1.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]      layers.1.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 723]             layers.1.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  22/ 723]         layers.2.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]         layers.2.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]         layers.2.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]         layers.2.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]       layers.2.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  27/ 723]      layers.2.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]      layers.2.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]      layers.2.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 723]             layers.2.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  31/ 723]         layers.3.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]         layers.3.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]         layers.3.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]         layers.3.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]       layers.3.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  36/ 723]      layers.3.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]      layers.3.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]      layers.3.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 723]             layers.3.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  40/ 723]         layers.4.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]         layers.4.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]         layers.4.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]         layers.4.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]       layers.4.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  45/ 723]      layers.4.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]      layers.4.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]      layers.4.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 723]             layers.4.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  49/ 723]         layers.5.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]         layers.5.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]         layers.5.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]         layers.5.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]       layers.5.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  54/ 723]      layers.5.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]      layers.5.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]      layers.5.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 723]             layers.5.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  58/ 723]         layers.6.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]         layers.6.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]         layers.6.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]         layers.6.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]       layers.6.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  63/ 723]      layers.6.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]      layers.6.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]      layers.6.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 723]             layers.6.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  67/ 723]         layers.7.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]         layers.7.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]         layers.7.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]         layers.7.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]       layers.7.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  72/ 723]      layers.7.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]      layers.7.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]      layers.7.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 723]             layers.7.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  76/ 723]         layers.8.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]         layers.8.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]         layers.8.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]         layers.8.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]       layers.8.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  81/ 723]      layers.8.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]      layers.8.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]      layers.8.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 723]             layers.8.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  85/ 723]         layers.9.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]         layers.9.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]         layers.9.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]         layers.9.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]       layers.9.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  90/ 723]      layers.9.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 723]      layers.9.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 723]      layers.9.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 723]             layers.9.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  94/ 723]        layers.10.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]        layers.10.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]        layers.10.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]        layers.10.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]      layers.10.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  99/ 723]     layers.10.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 723]     layers.10.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 723]     layers.10.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 723]            layers.10.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]        layers.11.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]        layers.11.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]        layers.11.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]        layers.11.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]      layers.11.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 108/ 723]     layers.11.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]     layers.11.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]     layers.11.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 723]            layers.11.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]        layers.12.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]        layers.12.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]        layers.12.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]        layers.12.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]      layers.12.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 117/ 723]     layers.12.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]     layers.12.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]     layers.12.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 723]            layers.12.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]        layers.13.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]        layers.13.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]        layers.13.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]        layers.13.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]      layers.13.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 126/ 723]     layers.13.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]     layers.13.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]     layers.13.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 723]            layers.13.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]        layers.14.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]        layers.14.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]        layers.14.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]        layers.14.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]      layers.14.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 135/ 723]     layers.14.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]     layers.14.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]     layers.14.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 723]            layers.14.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]        layers.15.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]        layers.15.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]        layers.15.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]        layers.15.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]      layers.15.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 144/ 723]     layers.15.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 723]     layers.15.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 723]     layers.15.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 723]            layers.15.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]        layers.16.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]        layers.16.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]        layers.16.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]        layers.16.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]      layers.16.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 153/ 723]     layers.16.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]     layers.16.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]     layers.16.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 723]            layers.16.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]        layers.17.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]        layers.17.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]        layers.17.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]        layers.17.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]      layers.17.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 162/ 723]     layers.17.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]     layers.17.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]     layers.17.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 723]            layers.17.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]        layers.18.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]        layers.18.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]        layers.18.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]        layers.18.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]      layers.18.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 171/ 723]     layers.18.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]     layers.18.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]     layers.18.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 723]            layers.18.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]        layers.19.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]        layers.19.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]        layers.19.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]        layers.19.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]      layers.19.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 180/ 723]     layers.19.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]     layers.19.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]     layers.19.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 723]            layers.19.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]        layers.20.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]        layers.20.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]        layers.20.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]        layers.20.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]      layers.20.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 189/ 723]     layers.20.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]     layers.20.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]     layers.20.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 723]            layers.20.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]        layers.21.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]        layers.21.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]        layers.21.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]        layers.21.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]      layers.21.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 198/ 723]     layers.21.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]     layers.21.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]     layers.21.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 723]            layers.21.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]        layers.22.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]        layers.22.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]        layers.22.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]        layers.22.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]      layers.22.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 207/ 723]     layers.22.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]     layers.22.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]     layers.22.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 723]            layers.22.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]        layers.23.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]        layers.23.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]        layers.23.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]        layers.23.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]      layers.23.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 216/ 723]     layers.23.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]     layers.23.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]     layers.23.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 723]            layers.23.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]        layers.24.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]        layers.24.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]        layers.24.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]        layers.24.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]      layers.24.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 225/ 723]     layers.24.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]     layers.24.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]     layers.24.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 723]            layers.24.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]        layers.25.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]        layers.25.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]        layers.25.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]        layers.25.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]      layers.25.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 234/ 723]     layers.25.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]     layers.25.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]     layers.25.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 723]            layers.25.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]        layers.26.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]        layers.26.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]        layers.26.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]        layers.26.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]      layers.26.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 243/ 723]     layers.26.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]     layers.26.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]     layers.26.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 723]            layers.26.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]        layers.27.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]        layers.27.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]        layers.27.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]        layers.27.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]      layers.27.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 252/ 723]     layers.27.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]     layers.27.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]     layers.27.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 723]            layers.27.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]        layers.28.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]        layers.28.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]        layers.28.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]        layers.28.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]      layers.28.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 261/ 723]     layers.28.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]     layers.28.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]     layers.28.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 723]            layers.28.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]        layers.29.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]        layers.29.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]        layers.29.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]        layers.29.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]      layers.29.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 270/ 723]     layers.29.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]     layers.29.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]     layers.29.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 723]            layers.29.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]        layers.30.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]        layers.30.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]        layers.30.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]        layers.30.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]      layers.30.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 279/ 723]     layers.30.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]     layers.30.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]     layers.30.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 723]            layers.30.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]        layers.31.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]        layers.31.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]        layers.31.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]        layers.31.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]      layers.31.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 288/ 723]     layers.31.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]     layers.31.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]     layers.31.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 723]            layers.31.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]        layers.32.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]        layers.32.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]        layers.32.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]        layers.32.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]      layers.32.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 297/ 723]     layers.32.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]     layers.32.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]     layers.32.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 723]            layers.32.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]        layers.33.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]        layers.33.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]        layers.33.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]        layers.33.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]      layers.33.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 306/ 723]     layers.33.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]     layers.33.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]     layers.33.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 723]            layers.33.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]        layers.34.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]        layers.34.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]        layers.34.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]        layers.34.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]      layers.34.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 315/ 723]     layers.34.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]     layers.34.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]     layers.34.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 723]            layers.34.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]        layers.35.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]        layers.35.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]        layers.35.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]        layers.35.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]      layers.35.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 324/ 723]     layers.35.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]     layers.35.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]     layers.35.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 723]            layers.35.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]        layers.36.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]        layers.36.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]        layers.36.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]        layers.36.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]      layers.36.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 333/ 723]     layers.36.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]     layers.36.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]     layers.36.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 723]            layers.36.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]        layers.37.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]        layers.37.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]        layers.37.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]        layers.37.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]      layers.37.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 342/ 723]     layers.37.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]     layers.37.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]     layers.37.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 723]            layers.37.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]        layers.38.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]        layers.38.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]        layers.38.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]        layers.38.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]      layers.38.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 351/ 723]     layers.38.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]     layers.38.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]     layers.38.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 723]            layers.38.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]        layers.39.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]        layers.39.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]        layers.39.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]        layers.39.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]      layers.39.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 360/ 723]     layers.39.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]     layers.39.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]     layers.39.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 723]            layers.39.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]        layers.40.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]        layers.40.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]        layers.40.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]        layers.40.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]      layers.40.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 369/ 723]     layers.40.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]     layers.40.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]     layers.40.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 723]            layers.40.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]        layers.41.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]        layers.41.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]        layers.41.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]        layers.41.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]      layers.41.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 378/ 723]     layers.41.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]     layers.41.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]     layers.41.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 723]            layers.41.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]        layers.42.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]        layers.42.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]        layers.42.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]        layers.42.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]      layers.42.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 387/ 723]     layers.42.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]     layers.42.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]     layers.42.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 723]            layers.42.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]        layers.43.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]        layers.43.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]        layers.43.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]        layers.43.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]      layers.43.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 396/ 723]     layers.43.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]     layers.43.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]     layers.43.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 723]            layers.43.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]        layers.44.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]        layers.44.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]        layers.44.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]        layers.44.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]      layers.44.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 405/ 723]     layers.44.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]     layers.44.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]     layers.44.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 723]            layers.44.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]        layers.45.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]        layers.45.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]        layers.45.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]        layers.45.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]      layers.45.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 414/ 723]     layers.45.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]     layers.45.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]     layers.45.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 723]            layers.45.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]        layers.46.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]        layers.46.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]        layers.46.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]        layers.46.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]      layers.46.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 423/ 723]     layers.46.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]     layers.46.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]     layers.46.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 723]            layers.46.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]        layers.47.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]        layers.47.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]        layers.47.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]        layers.47.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]      layers.47.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 432/ 723]     layers.47.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]     layers.47.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]     layers.47.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 723]            layers.47.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]        layers.48.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]        layers.48.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]        layers.48.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]        layers.48.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]      layers.48.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 441/ 723]     layers.48.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]     layers.48.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]     layers.48.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 723]            layers.48.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]        layers.49.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]        layers.49.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]        layers.49.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]        layers.49.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]      layers.49.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 450/ 723]     layers.49.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]     layers.49.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]     layers.49.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 723]            layers.49.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]        layers.50.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]        layers.50.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]        layers.50.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]        layers.50.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]      layers.50.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 459/ 723]     layers.50.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]     layers.50.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]     layers.50.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 723]            layers.50.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]        layers.51.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]        layers.51.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]        layers.51.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]        layers.51.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]      layers.51.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 468/ 723]     layers.51.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]     layers.51.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]     layers.51.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 723]            layers.51.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]        layers.52.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]        layers.52.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]        layers.52.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]        layers.52.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]      layers.52.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 477/ 723]     layers.52.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]     layers.52.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]     layers.52.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 723]            layers.52.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]        layers.53.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]        layers.53.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]        layers.53.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]        layers.53.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]      layers.53.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 486/ 723]     layers.53.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]     layers.53.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]     layers.53.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 723]            layers.53.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]        layers.54.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]        layers.54.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]        layers.54.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]        layers.54.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]      layers.54.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 495/ 723]     layers.54.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]     layers.54.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]     layers.54.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 723]            layers.54.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]        layers.55.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]        layers.55.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]        layers.55.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]        layers.55.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]      layers.55.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 504/ 723]     layers.55.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]     layers.55.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]     layers.55.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 723]            layers.55.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]        layers.56.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]        layers.56.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]        layers.56.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]        layers.56.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]      layers.56.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 513/ 723]     layers.56.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]     layers.56.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]     layers.56.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 723]            layers.56.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]        layers.57.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]        layers.57.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]        layers.57.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]        layers.57.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]      layers.57.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 522/ 723]     layers.57.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]     layers.57.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]     layers.57.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 723]            layers.57.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]        layers.58.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]        layers.58.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]        layers.58.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]        layers.58.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]      layers.58.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 531/ 723]     layers.58.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]     layers.58.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]     layers.58.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 723]            layers.58.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]        layers.59.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]        layers.59.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]        layers.59.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]        layers.59.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]      layers.59.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 540/ 723]     layers.59.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]     layers.59.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]     layers.59.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 543/ 723]            layers.59.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]        layers.60.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]        layers.60.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]        layers.60.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]        layers.60.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]      layers.60.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 549/ 723]     layers.60.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]     layers.60.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]     layers.60.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 552/ 723]            layers.60.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]        layers.61.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]        layers.61.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]        layers.61.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]        layers.61.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]      layers.61.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 558/ 723]     layers.61.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]     layers.61.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]     layers.61.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 561/ 723]            layers.61.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]        layers.62.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]        layers.62.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]        layers.62.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]        layers.62.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]      layers.62.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 567/ 723]     layers.62.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]     layers.62.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]     layers.62.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 570/ 723]            layers.62.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]        layers.63.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]        layers.63.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]        layers.63.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]        layers.63.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]      layers.63.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 576/ 723]     layers.63.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]     layers.63.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]     layers.63.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 579/ 723]            layers.63.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]        layers.64.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]        layers.64.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]        layers.64.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]        layers.64.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]      layers.64.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 585/ 723]     layers.64.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]     layers.64.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]     layers.64.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 588/ 723]            layers.64.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]        layers.65.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]        layers.65.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]        layers.65.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]        layers.65.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]      layers.65.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 594/ 723]     layers.65.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]     layers.65.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]     layers.65.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 597/ 723]            layers.65.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]        layers.66.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]        layers.66.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]        layers.66.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]        layers.66.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]      layers.66.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 603/ 723]     layers.66.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]     layers.66.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]     layers.66.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 606/ 723]            layers.66.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]        layers.67.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]        layers.67.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]        layers.67.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]        layers.67.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]      layers.67.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 612/ 723]     layers.67.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]     layers.67.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]     layers.67.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 615/ 723]            layers.67.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]        layers.68.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]        layers.68.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]        layers.68.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]        layers.68.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]      layers.68.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 621/ 723]     layers.68.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]     layers.68.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]     layers.68.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 624/ 723]            layers.68.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]        layers.69.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]        layers.69.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]        layers.69.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]        layers.69.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]      layers.69.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 630/ 723]     layers.69.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]     layers.69.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]     layers.69.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 633/ 723]            layers.69.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]        layers.70.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]        layers.70.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]        layers.70.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]        layers.70.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]      layers.70.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 639/ 723]     layers.70.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]     layers.70.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]     layers.70.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 642/ 723]            layers.70.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]        layers.71.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]        layers.71.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]        layers.71.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]        layers.71.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]      layers.71.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 648/ 723]     layers.71.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]     layers.71.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]     layers.71.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 651/ 723]            layers.71.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]        layers.72.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]        layers.72.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]        layers.72.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]        layers.72.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]      layers.72.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 657/ 723]     layers.72.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]     layers.72.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]     layers.72.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 660/ 723]            layers.72.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]        layers.73.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]        layers.73.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]        layers.73.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]        layers.73.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]      layers.73.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 666/ 723]     layers.73.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]     layers.73.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]     layers.73.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 669/ 723]            layers.73.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]        layers.74.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]        layers.74.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]        layers.74.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]        layers.74.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]      layers.74.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 675/ 723]     layers.74.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]     layers.74.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]     layers.74.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 678/ 723]            layers.74.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]        layers.75.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]        layers.75.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]        layers.75.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]        layers.75.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]      layers.75.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 684/ 723]     layers.75.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]     layers.75.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]     layers.75.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 687/ 723]            layers.75.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]        layers.76.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]        layers.76.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]        layers.76.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]        layers.76.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]      layers.76.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 693/ 723]     layers.76.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 694/ 723]     layers.76.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 695/ 723]     layers.76.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 696/ 723]            layers.76.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]        layers.77.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]        layers.77.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]        layers.77.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]        layers.77.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]      layers.77.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 702/ 723]     layers.77.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 703/ 723]     layers.77.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 704/ 723]     layers.77.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 705/ 723]            layers.77.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]        layers.78.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]        layers.78.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]        layers.78.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]        layers.78.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]      layers.78.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 711/ 723]     layers.78.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 712/ 723]     layers.78.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 713/ 723]     layers.78.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 714/ 723]            layers.78.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]        layers.79.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]        layers.79.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]        layers.79.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]        layers.79.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]      layers.79.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 720/ 723]     layers.79.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 721/ 723]     layers.79.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 722/ 723]     layers.79.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 723/ 723]            layers.79.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 259329.44 ms\n",
      "main:    total time = 259329.45 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.bin ./models/13B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.bin ./models/30B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.bin ./models/65B/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576354\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love, learn and help others.\n",
      "I am a certified ESL teacher with 25 years experience teaching adults in New York City. My specialty is Business English, where I use my creative skills to train people in their workplace.\n",
      "I have also been a private tutor of music for over 30 years. In addition, I am a writer/poet and have written a book about my life entitled \"Finding the Way Home\".\n",
      "In this class, we will explore your inner world through writing. We will use poetry and other forms to express your inner feelings as well as outer experiences of daily living. This is a creative class that will help you find your voice!\n",
      "The meaning of life is to love, learn and help others.\n",
      "I am an ESL teacher with 25 years experience teaching adults in New York City. My specialty is Business English, where I use my creative skills to train people in their workplace.\n",
      "I have also been a private tutor of music for over 30 years. In addition, I am a writer/poet and have written a book about my life entitled \"Finding the Way Home\".\n",
      "In this class, we will explore your inner world through writing. We will use poetry and other forms to express your inner feelings as well as outer experiences of daily living. This is a creative class that will help you find your voice! I look forward to helping you find your own unique voice in all its power and glory!\n",
      "Very personable, good with kids. Would definitely recommend for tutoring.\n",
      "Victoria is very kind and warm. She has a great way with kids- my daughter loved working with her. Highly recommended!\n",
      "My 12 year old son had his first lesson with Victoria today and he really enjoyed it and was motivated to do something he loves, music. He wants more lessons. Thank you so much Victoria for your guidance!!!\n",
      "I have been a private tutor of Music for over 30 years. In addition, I am a writer/poet and have written a book about my life entitled \"Finding the Way Home\". Currently, I am an ESL teacher with 25 years experience teaching adults in New York City. My specialty is Business English where I use my creative skills to train people in their workplace.\n",
      "I have taught in many places including South Korea, China and India\n",
      "llama_print_timings:        load time =  3447.25 ms\n",
      "llama_print_timings:      sample time =   223.53 ms /   512 runs   (    0.44 ms per token,  2290.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1166.23 ms /   265 tokens (    4.40 ms per token,   227.23 tokens per second)\n",
      "llama_print_timings:        eval time = 24369.09 ms /   510 runs   (   47.78 ms per token,    20.93 tokens per second)\n",
      "llama_print_timings:       total time = 25840.17 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7a0912-2746-4e7d-adaf-92b60cb2ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576386\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find and do what we’re called to do, as best we can.\n",
      "I believe that when we find and do this, we’ll feel happiness and fulfillment in our lives.\n",
      "And I also believe that it would be a mistake for us to get so focused on finding and doing what we’re called to do that we lose sight of the other things that are meaningful in life — like spending time with family, friends, and loved ones; creating art, music, or literature (or just about anything else); traveling; eating good food; having fun; etc.\n",
      "And I also believe that it would be a mistake for us to get so focused on finding and doing what we’re called to do that we lose sight of the other things that are meaningful in life — like spending time with family, friends, and loved ones; creating art, music, or literature (or just about anything else); traveling; eating good food; having fun; etc.\n",
      "I believe it’s possible for us to do both of these things: Focus on finding and doing what we’re called to do, while at the same time enjoying life in all its richness, depth, and complexity.\n",
      "When I look back at my own life, I see that sometimes I got so focused on trying to figure out how to best serve humanity by becoming a writer or photographer, etc., that it took me away from the other things that were meaningful to me — like spending time with loved ones; traveling; eating good food; having fun; etc.\n",
      "But when I’m not so focused on trying to figure out how to best serve humanity by becoming a writer or photographer, etc., sometimes I get so caught up in the other things that are meaningful to me — like spending time with loved ones; traveling; eating good food; having fun; etc. — that it takes me away from trying to find out how best to serve humanity by becoming a writer or photographer, etc.\n",
      "I’m not saying we can’t do both of these things at the same time. I’m just saying that sometimes we need to remind ourselves that it’s possible for us to do both. Sometimes we focus too much on one thing and forget about the other. And vice versa.\n",
      "And I also believe that there are going to be times in our lives when we must choose between trying to\n",
      "llama_print_timings:        load time =   857.28 ms\n",
      "llama_print_timings:      sample time =   220.38 ms /   512 runs   (    0.43 ms per token,  2323.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1243.01 ms /   265 tokens (    4.69 ms per token,   213.19 tokens per second)\n",
      "llama_print_timings:        eval time = 26116.64 ms /   510 runs   (   51.21 ms per token,    19.53 tokens per second)\n",
      "llama_print_timings:       total time = 27661.70 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3324a21-7a43-4c68-83eb-52417e487c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576416\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make a difference in people’s lives. The way we do that is by showing up and being present in relationships, whether they are with friends or family members or with strangers.\n",
      "Beyond that, it also means being willing to listen, to be open to new ideas and ways of thinking, and to treat others with respect.\n",
      "You can’t just go into your day and expect to make a difference unless you have the right attitude and intentions.\n",
      "We all need to be reminded of this from time-to-time because it is so easy to get caught up in our own agendas and priorities that we lose sight of what matters most, which is others.\n",
      "I’m not suggesting that we should always tell people what they want to hear or go along with their wishes just to be nice. There are times when saying “no” is necessary and can even save lives.\n",
      "But the point I am making here is that there are so many things in our world today that need fixing, whether it’s a broken relationship, an ailing relative, or a broken system.\n",
      "We all have talents, skills, and abilities that we can use to make life better for others. It doesn’t take much time or effort, but the impact is profound.\n",
      "Think of just one person you know who needs your help right now. You don’t need to fly halfway around the world to do it. All you have to do is show up and be present in their life.\n",
      "They may not even realize that they need you, but you will know, because you can feel that they are suffering or struggling with something.\n",
      "It could be a parent who is lonely and needs someone to call and chat for an hour over the phone; it could be a neighbor who has no one to help them out when they have had surgery; it could even be your own child who just needs you to show up and listen to what he or she has to say.\n",
      "It’s not about solving people’s problems for them, but more about being there as an ear if needed, a shoulder to cry on, or a heart to listen with.\n",
      "We all need support from time-to-time, so be there for someone else and make a difference in their life today!\n",
      "Do you agree? Please leave your comments below.\n",
      "P.S. If you are looking for ways to feel happier right now, sign up for the\n",
      "llama_print_timings:        load time =   853.73 ms\n",
      "llama_print_timings:      sample time =   220.83 ms /   512 runs   (    0.43 ms per token,  2318.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1296.97 ms /   265 tokens (    4.89 ms per token,   204.32 tokens per second)\n",
      "llama_print_timings:        eval time = 27610.33 ms /   510 runs   (   54.14 ms per token,    18.47 tokens per second)\n",
      "llama_print_timings:       total time = 29208.72 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98a39e0-c339-4234-b6d2-49d642ffb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576448\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living it.\n",
      "—FRANCIS BACON, _A MIDSUMMER NIGHT'S DREAM_\n",
      "CHAPTER 1\n",
      "My name is Alice Bonney. I was born on a farm in New Hampshire in 1860. My parents were farmers and the youngest of their seven children. We all worked hard, but we were luckier than most because my father also had a job as a lumberjack during the winter months when there wasn't much work to be done on the farm.\n",
      "We didn't have fancy dinners or even many books, but what we lacked in material goods we made up for with love and attention to one another. We were all family and we were all important. Even though I was the youngest, I felt loved and cared about. My brothers and sisters were always looking out for me and protecting me from getting into trouble or hurting myself.\n",
      "My brother Andrew was my favorite. He was so strong that he could lift a heavy barrel of apples onto his shoulders and carry it to the barn by himself, and there wasn't much that he couldn't do. I loved being around him because he always made me feel safe and cared for.\n",
      "Andrew was fifteen years old when I was born. He loved playing on the farm with me when we weren't busy doing our chores or working in the fields. I would follow Andrew everywhere, and he let me tag along as long as I didn't get too close to the horses or into any danger.\n",
      "Andrew and I had a very special relationship, but it was different from what I had with my brothers and sisters. My siblings loved me because we were family—that is how you love a sister or brother. But Andrew and I had something more. He seemed to be my best friend even though he wasn't exactly my age. We spent hours playing together, and when he would get tired of working on the farm, he would take me with him into the woods. Sometimes we would just walk around in the forest and watch the leaves fall from the trees or listen for the birds singing as they flew through the branches above our heads. I loved being outside, especially walking along the trail behind our house with Andrew.\n",
      "I remember one day when he took me to see a big tree that was growing next to a little creek in the woods.\n",
      "llama_print_timings:        load time = 14698.51 ms\n",
      "llama_print_timings:      sample time =   221.21 ms /   512 runs   (    0.43 ms per token,  2314.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3301.91 ms /   265 tokens (   12.46 ms per token,    80.26 tokens per second)\n",
      "llama_print_timings:        eval time = 59356.18 ms /   510 runs   (  116.38 ms per token,     8.59 tokens per second)\n",
      "llama_print_timings:       total time = 62960.62 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0296e978-7c63-4d87-a1a9-dc41b49dc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576527\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m simply to enjoy it.\n",
      "Kinji Fukasaku - 1983\n",
      "A Japanese businessman's daughter, an American journalist, and others are trapped in a building with little food and no water as armed terrorists hold them hostage. The story unfolds from three different points of view: the terrorist leader, a government official, and an American negotiator sent to free the hostages.\n",
      "Sabu - 1954\n",
      "In this heart-warming story of childhood innocence, Sabu plays Abu, a boy who lives in the jungle with his elephant-keeper father. When his father is murdered by treacherous ivory hunters, Abu sets off on an adventure to find the killers and bring them to justice. Along the way he encounters a friendly lion cub and together they set out to find the killer's lair.\n",
      "The Lone Ranger - 1981\n",
      "Michael Horse plays Tonto, the Native American \"noble savage\" as a character of almost superhuman strength and intelligence, who is the faithful friend and ally of the white man who dresses in a black mask to fight for justice against villains and wrongdoers. He rides into town on his horse Silver, and fights crime using information provided by an informant named \"Kemo Sabe\" (translated as 'soggy head').\n",
      "The Lone Ranger - 1949 TV Show\n",
      "Starring Clayton Moore, Jay Silverheels, John Hart & Robert J. Wilkie. Created in 1933 by George Trendle and Fran Striker, The Lone Ranger debuted on Detroit radio station WXYZ as the lead-in to another western show, The Green Hornet.\n",
      "Lone Wolf and Cub - 1972 TV Show\n",
      "In this Japanese samurai drama, a ronin (a masterless samurai) must seek vengeance for his wronged lord by killing the men responsible. He travels with his wife and infant son, who is also trained in swordsmanship and martial arts. The series follows their adventures as they search for the men who killed their family members.\n",
      "The Lord Of The Rings - 2010 Video Game\n",
      "Based on Peter Jackson's movie trilogy of\n",
      "llama_print_timings:        load time =  1363.38 ms\n",
      "llama_print_timings:      sample time =   219.52 ms /   512 runs   (    0.43 ms per token,  2332.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3561.36 ms /   265 tokens (   13.44 ms per token,    74.41 tokens per second)\n",
      "llama_print_timings:        eval time = 64029.22 ms /   510 runs   (  125.55 ms per token,     7.97 tokens per second)\n",
      "llama_print_timings:       total time = 67891.12 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebca2498-774d-4f51-ac56-a9da8f359523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576598\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living, loving and helping one another.\n",
      "I have come to realize that there are three kinds of people in this world: those who love God, those who don’t think about God much at all, and those whose lives are devoted to trying to prove that God does not exist. I believe that God exists and that he is the creator of all things, both seen and unseen.\n",
      "To me, God is not a religion – he is a relationship. And this relationship can be experienced in many ways. The most important thing I have learned about this relationship (which has taken me nearly 60 years to learn) is that if you want it, you don’t need anyone else to help you get it.\n",
      "You see, God wants a love relationship with each of us as individuals. He doesn’t require that we go through any intermediary or priestly order in order for him to hear our prayers and care about our problems. That is one reason why I believe the Bible has stood the test of time – because it is not written by men (or women), but rather, God himself spoke his Word into the lives of many people who were just like us.\n",
      "This is what I find so hard to understand: that so many people can read about the life and times of Jesus in these ancient books called the New Testament and still reject the idea of a relationship with God through faith in the death and resurrection of his son, Jesus.\n",
      "I believe that the Bible (both Old and New Testaments) is the inspired Word of God – that he has revealed himself to mankind through it. I also believe that as Christians we have a responsibility to share this revelation with others. This is why I wrote my book – to share what I have learned about my relationship with God over the years, but also to show how Jesus can be relevant in our lives today.\n",
      "I want people to know that there are no pre-requisites for having a relationship with God other than faith and belief – even if they don’t understand it all at first. The Christian life is not always easy – but it is worth the effort when you have someone who cares about your life and wants what’s best for you, both now and in eternity.\n",
      "I also believe that God wants me to learn how to love others as much as he loves me. This means I have a responsibility not only to share my faith with others but also to show\n",
      "llama_print_timings:        load time =  1338.89 ms\n",
      "llama_print_timings:      sample time =   222.81 ms /   512 runs   (    0.44 ms per token,  2297.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3684.20 ms /   265 tokens (   13.90 ms per token,    71.93 tokens per second)\n",
      "llama_print_timings:        eval time = 65034.20 ms /   510 runs   (  127.52 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:       total time = 69022.37 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d246e2-1ed4-46f7-aa3e-0d4787cc3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576670\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it and to do good. I love my work, I enjoy every minute of it.\n",
      "I am a person who will treat you with respect, compassion and dignity. I am also a person who strives for perfection in everything that I do. It has been my experience throughout my life that nothing is accomplished without hard work and dedication. With those two elements anything can be achieved.\n",
      "I have always lived by the Golden Rule: \"Do unto others as you would have them do unto you.\" There are many ways to achieve this goal, but I believe the most effective way to treat everyone with respect.\n",
      "It is my commitment to provide the highest level of service while representing your best interest throughout the entire real estate transaction. My goal is to be an integral part of your success. As a team we will accomplish all of our goals.\n",
      "I am a member of the National Association of Realtors, Virginia Association of Realtors, Hampton Roads Realtors Association, and the Virginia Living Museum. I was also elected to serve on the Board of Directors for the Hampton Roads Realtors Association. It is my commitment to always be available to discuss your real estate needs, whether you are buying or selling.\n",
      "I have been married 29 years to my husband Steve. We have two sons: Matthew and David. Matt is a Senior at William & Mary in Williamsburg majoring in Business Management and Marketing. Dave is a Senior at Cox High School and will be following his brother in the fall of 2014.\n",
      "I was born and raised in New York City and moved to Virginia Beach, VA in 1987. I have been a Realtor since 2002 and love every minute of it. I am a people person and enjoy the challenge of finding my clients their perfect home or getting them top dollar for the sale of their home. It is always a pleasure to help people achieve their goals.\n",
      "I also enjoy playing tennis, traveling, cooking, and spending time with my family and friends.\n",
      "Please contact me if you are thinking about buying or selling a home in Virginia Beach, Chesapeake, Norfolk, Suffolk, Portsmouth, Hampton, Newport News, Yorktown, Smithfield or Isle of Wight! I am always available to assist you.\n",
      "\"We have lived in a dozen homes and\n",
      "llama_print_timings:        load time = 11754.70 ms\n",
      "llama_print_timings:      sample time =   221.06 ms /   512 runs   (    0.43 ms per token,  2316.06 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10006.45 ms /   265 tokens (   37.76 ms per token,    26.48 tokens per second)\n",
      "llama_print_timings:        eval time = 173974.19 ms /   510 runs   (  341.13 ms per token,     2.93 tokens per second)\n",
      "llama_print_timings:       total time = 184283.27 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce27ab44-b446-4126-8972-7472c658a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689576869\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to learn, experience and grow from every moment.\n",
      "I was born in Vancouver, Canada but my parents are from Hong Kong and we moved back when I was 2 months old. I grew up in a very different culture then most kids my age because I went to an International School that had many expats. There were kids from all over the world! It gave me such a great perspective on life at an early age.\n",
      "I love music, it’s been my passion since I can remember and still is to this day. I was in choir and band (played flute) through out middle school and high school. My favorite instrument is piano but I also play guitar and ukulele. Music to me is a language of emotion that anyone can speak. It’s the universal way we all express ourselves.\n",
      "I love food, everything about it makes me happy! From eating it, cooking it, smelling it, watching others eat it, going out for ice cream with friends… you get my drift! I was in a cooking club at school and loved every minute of it.\n",
      "I’m not sure where life will take me after graduation but all I know is that I want to help people. That’s why I’m studying Nutrition, because I believe food truly has the power to heal you in many ways. I also love learning about psychology and would like to incorporate that into my career. My main goal is to travel with my family and make sure we are all healthy and happy.\n",
      "I hope to inspire others to live a life they love and feel confident in every step of the way!\n",
      "Hi there! I’m Natalie, 20 years young, born and raised in Hong Kong. The only child and daughter of two very hard working but loving parents. I grew up with my family being extremely involved in my school education and extra curricular activities. That’s why I had a very close relationship with both my mom and dad. They have always been my best friends.\n",
      "I attended an International School for 13 years where majority of the people were expats. It was a fun environment and it made me very open minded to new cultures at a young age. Even though I love food, I did not enjoy cooking until late in high school when I joined a small Cooking Club. That’s when my passion for nutrition started.\n",
      "llama_print_timings:        load time =  2524.30 ms\n",
      "llama_print_timings:      sample time =   219.77 ms /   512 runs   (    0.43 ms per token,  2329.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10198.33 ms /   265 tokens (   38.48 ms per token,    25.98 tokens per second)\n",
      "llama_print_timings:        eval time = 175532.94 ms /   510 runs   (  344.18 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:       total time = 186032.61 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eec90d1-7b7b-4071-af17-849bcb505108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689577060\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and have fun.\n",
      "I’m not sure why I have to suffer so much because I have a rare disease, that makes me incapable of working and I don’t earn enough money to get proper treatment, so I’ll die earlier than you if I can’t find a solution or a way out….\n",
      "I believe we are all connected, everything is connected, it just takes time for us as humans to be able to feel or see those connections. We are all One. Everyone matters. All life matters. When a person dies from hunger, it’s not that their life doesn’t matter; it’s just that we don’t know what they were born to do.\n",
      "And I believe the meaning of life is to be happy and have fun.\n",
      "I'm a 35 year old female with two rare diseases: Sjogren's Syndrome (SS) and Mixed Connective Tissue Disease (MCTD). SS is an autoimmune disease that affects my saliva glands, which means I need to drink water a lot of times during the day. That makes me want to use the bathroom a lot. MCTD is a lupus-type disease that affects my joints, skin, and other organs. The combination of both diseases makes it difficult for me to get proper treatment or find a cure. I can’t work anymore because of these diseases; I just have enough money to eat and pay bills. I don't have health insurance and the government doesn't care about my condition, so I will probably die earlier than you do if I can’t find a way out.\n",
      "I'm a 35 year old female with two rare diseases: Sjogren's Syndrome (SS) and Mixed Connective Tissue Disease (MCTD). SS is an autoimmune disease that affects my saliva glands, which means I need to drink water a lot of times during the day. That makes me want to use the bathroom a lot. MCTD is a lupus-type disease that affects my joints, skin, and other organs. The combination of both diseases makes it difficult for me to get proper treatment or find a cure. I can’t work anymore because of these diseases; I just have enough money\n",
      "llama_print_timings:        load time =  2540.07 ms\n",
      "llama_print_timings:      sample time =   219.98 ms /   512 runs   (    0.43 ms per token,  2327.46 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10002.41 ms /   265 tokens (   37.74 ms per token,    26.49 tokens per second)\n",
      "llama_print_timings:        eval time = 173459.04 ms /   510 runs   (  340.12 ms per token,     2.94 tokens per second)\n",
      "llama_print_timings:       total time = 183763.05 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04719be2-a272-4fc9-aed8-193e4bfe9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689577248\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, and I feel that we have a responsibility as human beings to take care of each other.\n",
      "Find on Amazon: Drew Barrymore\n",
      "Each Other, Feel, Happy, Human Beings, Life, Meaning, Meaning Of Life, Responsibility, Take Care\n",
      "I think happiness is what makes you pretty. Period. Happy people are beautiful. They become like a mirror and they reflect that happiness. Drew Barrymore\n",
      "Beauty, Happiness, Beautiful, Mirror\n",
      "The only way to survive is to have fun: it's certainly the best way to work. Drew Barrymore\n",
      "Work, Best, Fun, Survive, Way, Only\n",
      "I am a woman in process. I'm just trying like everybody else. I try to take every conflict, every experience and learn from it. Life is never dull. Drew Barrymore\n",
      "Life, Experience, Woman, I Am, Trying\n",
      "I've always been very close with my mother. Drew Barrymore\n",
      "Mother, Mother's Day, Always, Close, Been\n",
      "My mom was onstage when she was pregnant with me. Drew Barrymore\n",
      "Mom, Me, She, Pregnant, Stage, Mom Was\n",
      "I don't know how to explain happiness. You have to be happy. Happiness explains itself. Drew Barrymore\n",
      "Happiness, Be Happy, Know, Explain, You\n",
      "Love is the hardest habit to break, and the most difficult to satisfy. Drew Barrymore\n",
      "Love, Love Is, Break, Habit, Most, Hardest\n",
      "I've always been a strong believer that if you don't have anything nice to say then don't say anything at all. Drew Barrymore\n",
      "Strong, Say, Nice, You, Always, Then\n",
      "You can't live your life blaming your failures on your femininity. But conversely, you can't go out into the world pretending you have the same clothes on as the guys. Because you don't. Drew Barrymore\n",
      "Life, Live, World, Clothes, You, Go\n",
      "I used to spend most of my time straining to see far-off things, but now I realize they were right there all along. Drew Barrymore\n",
      "Time, Realize, See, Right, Now, Most\n",
      "For me, the\n",
      "llama_print_timings:        load time = 25819.76 ms\n",
      "llama_print_timings:      sample time =   225.04 ms /   512 runs   (    0.44 ms per token,  2275.11 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20573.06 ms /   265 tokens (   77.63 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:        eval time = 369063.10 ms /   510 runs   (  723.65 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:       total time = 389943.22 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1730091a-3146-476d-ab57-49f8722c351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689577667\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give and receive love.\n",
      "I'm passionate about living consciously, being a positive influence in my community and in the world at large, practicing yoga and meditation, exploring new places, and helping others realize their potential for greatness.\n",
      "In 2014 I created The Unblocked Life where you can access videos, blogs, webinars, and courses about living a more conscious life. I'm currently working on my first book called \"Unblock Your Life\".\n",
      "I am an experienced yoga teacher, meditation facilitator, and coach with 10 years of experience in the wellness industry. I was born and raised in California where I lived for the majority of my life. I moved to Canada in 2014 and have been loving it ever since!\n",
      "I am a certified yoga instructor (500hrs), meditation facilitator, coach, and reiki practitioner. I have taught yoga at many different studios throughout the Lower Mainland such as Semperviva Yoga, Moksha Yoga, The Hot Box Yoga, Yogafit Studios, One Yoga For The People, The Wise Owl Wellness Center and more!\n",
      "I am also a certified Co-Active coach through the CTI institute. I love helping people make positive changes in their lives through coaching. My experience includes working as an Executive Career Coach for Rennie & Associates Real Estate, a Business and Life coach for Amped Fitness Inc., a health and wellness coach for The Wise Owl Wellness Center, a career coach for Simon Fraser University, and more.\n",
      "I teach yoga and meditation to help people connect with themselves. My classes are always challenging yet fun. I believe we must play as hard as we work! When you leave my class you will feel empowered, energized, and alive. You can expect me to push you beyond your comfort zone but only in a loving way, of course :)\n",
      "I coach people to help them unblock their minds and live more consciously. I believe we all have the power to create our best lives and I'm here to help you realize that power.\n",
      "The following is a list of my certifications and other qualifications:\n",
      "Co-Active Coaching Certification, CTI Institute\n",
      "Master Teacher \n",
      "llama_print_timings:        load time =  4772.72 ms\n",
      "llama_print_timings:      sample time =   222.81 ms /   512 runs   (    0.44 ms per token,  2297.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21379.34 ms /   265 tokens (   80.68 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:        eval time = 379107.41 ms /   510 runs   (  743.35 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 400790.71 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351608bd-64ae-4f26-9fb5-ccf3ee5fb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689578075\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find our gift, and give it away.\n",
      "I am a singer/songwriter who believes in the power of music and storytelling to heal us all. I want my songs to help people know that they are not alone in their struggles. I hope my lyrics inspire others to be authentic, and share their voices with the world.\n",
      "It's a crazy world out there. We all need each other. I strive for my music and writing to do some good in this world. I want it to leave people feeling hopeful and more connected with themselves and one another.\n",
      "I write songs about things that make me feel. When I sit down at the piano, sometimes a melody just comes out of nowhere. Sometimes I am moved by something or someone in my life and need to write a song about it. Whatever it is, when I connect with a feeling, then the words seem to flow from someplace deep inside me that knows what needs to be said without really planning it out ahead of time.\n",
      "I feel fortunate to have discovered a passion for writing songs at this point in my life, and am so excited to share my music with you!\n",
      "Thanks for visiting my website, and I hope you will check out more about me on the pages above!\n",
      "If you would like to be notified of future shows or new music releases, please join my email list. I promise never to spam you or sell your information to anyone else. You can always unsubscribe at any time with a single click.\n",
      "I'm honored that I was recently featured on The Good Men Project website! This article shares more about what inspired me to get into songwriting and where this journey has taken me so far. To read the full article, click here.\n",
      "New Song for International Women's Day 2019\n",
      "In celebration of International Women's Day 2019, I released a new song called \"The Voice.\" This song is about learning to trust and believe in our own inner guidance.\n",
      "I wrote this song last year for a contest that was looking for songs about the #MeToo movement. It did not win the contest, but it helped me find my voice as an artist, and I'm so glad I have decided to share it now!\n",
      "The Voice by Suzie Daggett\n",
      "\"The Voice\" has been released on CDBaby.com, iTunes\n",
      "llama_print_timings:        load time =  4827.28 ms\n",
      "llama_print_timings:      sample time =   221.63 ms /   512 runs   (    0.43 ms per token,  2310.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21476.01 ms /   265 tokens (   81.04 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:        eval time = 378004.79 ms /   510 runs   (  741.19 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time = 399785.19 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff34085-ceab-48a8-96cb-f47d3f8cc5b3",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "303e2727-09d0-47be-818f-2a6519aa5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689578483\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose and live it.\n",
      "I believe in being kind, having fun and living every day as if it were my last.\n",
      "I believe in having a little faith that everything will work out for the best.\n",
      "I believe in making mistakes, learning from them and becoming stronger because of them.\n",
      "I believe in forgiving others when they hurt you and forgiving yourself when you do something wrong.\n",
      "I believe in being thankful for all your blessings.\n",
      "I believe that love is an action, not a feeling; it’s about giving to someone else without expecting anything in return.\n",
      "I believe in following my heart and doing what makes me happy.\n",
      "I believe in the power of positivity and happiness; no matter how bad things look, you can always find something positive in every situation.\n",
      "I believe that everyone deserves a second chance; we are all human and make mistakes.\n",
      "I believe in being honest with people even if it’s hard to do so.\n",
      "I believe in being patient, because everything takes time.\n",
      "And I believe in not wasting your life away. Live for today because tomorrow is never promised.\n",
      "Previous Post When the storm comes…..\n",
      "Next Post Life happens when you least expect it!\n",
      "I love this post!! I truly agree with all of these beliefs! Thanks for sharing!\n",
      "Thanks so much, I’m glad that you liked my post!\n",
      "Good post and good advice. I’ll take it with me on my next journey.\n",
      "Great message, thankyou for sharing your thoughts. We must learn to love the person in the mirror first before we can truly love others.\n",
      "I love this list of beliefs. It’s really important to remind yourself constantly what you believe and why. I think it helps with everything! Great post!\n",
      "Thanks so much, I loved it as well!!\n",
      "Great post and I could not agree more with the last one.\n",
      "Thanks so much for your comment ���� yes, we can’t let our lives pass us by without living them to their full potential! Enjoy life! Thanks again!\n",
      "I’m glad you liked my post!! Thank you so much for your kind words! I agree with all of these as well. We need more positivity in this world and being positive is the key to happiness! Thanks again!\n",
      "Thanks so much!!! I appreciate it! Yes, sometimes we need a little encouragement from others (\n",
      "llama_print_timings:        load time =  8319.91 ms\n",
      "llama_print_timings:      sample time =   222.28 ms /   512 runs   (    0.43 ms per token,  2303.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1666.01 ms /   265 tokens (    6.29 ms per token,   159.06 tokens per second)\n",
      "llama_print_timings:        eval time = 84132.70 ms /   510 runs   (  164.97 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:       total time = 86101.85 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "061c4fdf-de31-47b9-a6f7-55873c2b690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689578579\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make your mark on this world. To do something that will have an impact in some way, whether it be positive or negative.\n",
      "I'm a very creative person; I've always been one! When I was 4 my dad bought me a camera and then gave me a course to learn how to use it. I just loved taking photos and even at the age of 10 I had an exhibition in our town. It was about landscapes.\n",
      "The first album cover I designed was for the band \"Rainbow\". After that, in my early twenties, I started to work with a lot of different artists and I worked for many years in England, but now I'm back in Holland. I have two small children and I'm married too, so it's really full-time!\n",
      "The job is quite a solitary one; you are on your own most of the time and that's both good and bad. It means that you get to be with yourself all the time but at the same time if you need help or advice you can't go to anyone for it straight away. You have to think about what you want to do and how you are going to do it because you don't have people around you to tell you what to do!\n",
      "I'm very passionate about my work, I love it when someone says \"Oh my God, that looks fantastic\" or when they say \"Wow!\" It gives me a lot of energy. That's why I like working with different artists as it makes me excited and enthusiastic to get up in the morning!\n",
      "It's very hard these days because I feel I need to be on top of everything, but at times you just can't do that. You have to stop for a moment and think: \"What are my priorities?\" Because then it will work out better in the end. My children come first, then my husband and family. Then everything else comes after that.\n",
      "I believe the meaning of life is to make your mark on this world. To do something that will have an impact in some way, whether it be positive or negative. In a way I don't think we can control what happens afterwards but at least while you are doing something you can know it was done with all your heart and soul; you gave everything to the job and that's important.\n",
      "I love my work because it is so creative, it allows\n",
      "llama_print_timings:        load time =  1754.99 ms\n",
      "llama_print_timings:      sample time =   220.80 ms /   512 runs   (    0.43 ms per token,  2318.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1637.40 ms /   265 tokens (    6.18 ms per token,   161.84 tokens per second)\n",
      "llama_print_timings:        eval time = 86164.48 ms /   510 runs   (  168.95 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time = 88103.44 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb37cbe3-16ab-4f63-9903-2e7951ce751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689578671\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and help others do the same!\n",
      "I am a very active person, I love dancing (Bachata), travelling, learning new languages and cultures.\n",
      "I have worked in tourism industry for 6 years and met lots of different people from all around the world. This gave me lot of opportunity to learn about their culture and traditions. In addition it was very interesting to compare differences between European way of living and Asian one. I enjoyed every moment spent with them, working together on projects or having fun in my spare time.\n",
      "I have been thinking for a long time now that life is too short to waste it doing things you don’t like!\n",
      "That’s why I want to change something in my life and start new challenges.\n",
      "Bachata is one of the most popular latin dances in Poland, but still not very well known abroad. My dream is to spread my passion for dancing Bachata around the world and show people how amazing it can be!\n",
      "I believe that every single person has something special inside himself, which he wants to share with others. I am sure that we all have our own mission in life and we just need some time to find it. My goal is to help you discover what makes you happy! We can achieve this together!\n",
      "It does not matter if you are a beginner or an experienced person, Bachata will give you lots of opportunities to learn something new about yourself. You’ll have the chance to make new friends and meet interesting people from all around the world. You will be able to improve your dancing skills in a fun environment!\n",
      "I am looking forward for meeting you on my classes!\n",
      "I want to show you how amazing and addictive Latin dances can be!\n",
      "You’ll have lots of opportunities to learn new things about yourself, meet new people from all around the world and improve your dancing skills.\n",
      "In addition we will have a lot of fun together!\n",
      "This is a great chance for you to have some fun in your free time. You will have a chance to learn how to dance Bachata from one of the best teachers in Poland.\n",
      "If you want to join us, please send me an email or fill up the contact form below.\n",
      "Classes are held every Saturday. The place and exact time is always announced on my Facebook page 2 weeks before each class.\n",
      "If you have any questions or would like to book a private lesson feel free to\n",
      "llama_print_timings:        load time =  1738.29 ms\n",
      "llama_print_timings:      sample time =   222.38 ms /   512 runs   (    0.43 ms per token,  2302.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1631.39 ms /   265 tokens (    6.16 ms per token,   162.44 tokens per second)\n",
      "llama_print_timings:        eval time = 84126.76 ms /   510 runs   (  164.95 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:       total time = 86062.20 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33cb1a63-f863-4a7f-b627-6c76e534f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689578761\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living and not in a system of belief.\n",
      "For me it is the way people have come together to form a community that willingly share their knowledge, skills and resources with each other without being asked or coerced. The way they care for each other when they are sad or sick or grieving for those who have passed over. I am talking about real communities not the artificial communities of Facebook where people hide behind a false identity of their own making to project an image of themselves that is in many cases far removed from reality and certainly not indicative of the true person within.\n",
      "I believe we are all capable of amazing feats when we work together toward a common goal. To me the meaning of life is found in being alive and in living life as it comes to us. To be happy, healthy, loved and loving, to laugh and cry, to smile and frown, to be sad and joyful. To learn something new every day and to share that knowledge with others.\n",
      "I believe the meaning of life is an internal journey through which we come to know ourselves intimately and in doing so realise who we are as individuals and our place within the universe. I don’t believe there is a “one size fits all” solution to this question, it is different for everyone, but what I do believe is that at some point on your journey you will find the answer to life’s meaning if you are willing to put in the effort required and look deep within yourself.\n",
      "I don’t have an answer to the meaning of life or even a suggestion. To me all of this question implies there must be something beyond our own existence, but I do not believe that is so. I believe we each live our lives as best we can on the journey through which we were born and when it ends it ends without hope or expectation beyond what we created while alive.\n",
      "This entry was posted in Blog Posts, Uncategorized and tagged Life, meaning of life, Philosophy, reality, Religion & Spirituality, universe. Bookmark the permalink.\n",
      "6 Responses to The Meaning Of Life\n",
      "I have always found that question somewhat ridiculous, what is the point in a meaning? I mean why would we need one if there isn’t really anything other than what we do with our lives and how we feel about it at the end of the day. I agree it is an internal thing but that doesn’t make\n",
      "llama_print_timings:        load time = 20215.65 ms\n",
      "llama_print_timings:      sample time =   222.87 ms /   512 runs   (    0.44 ms per token,  2297.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3990.08 ms /   265 tokens (   15.06 ms per token,    66.41 tokens per second)\n",
      "llama_print_timings:        eval time = 154776.40 ms /   510 runs   (  303.48 ms per token,     3.30 tokens per second)\n",
      "llama_print_timings:       total time = 159070.36 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250c511c-864d-4b20-8d87-86f2409b277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689578942\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\"\n",
      "\"If you have an ear, then get ready to listen to this message; if you have eyes to see, take a good look at what's coming...\" (Isaiah 32:3). \"Listen closely to my voice; pay attention and hear what I have to say. (Psalm 94:7)\n",
      "I believe God wants me to share his word with you. It is my calling, my gift from the Lord. He has given all of us gifts that we can use for Him. My gift is writing. I write to help people live more meaningful lives through Jesus Christ. The world is full of people who do not know how much God loves them. They are looking for love and acceptance but do not realize that they have it in the Lord. People need to be shown the truth, the way, and the life that only comes through a relationship with Jesus Christ. I pray that each person that reads my blog will come to know Christ as their Lord and Savior.\n",
      "\"Let your conversation be always full of grace, seasoned with salt, so that you may know how to answer everyone.\" (Colossians 4:6)\n",
      "I have been writing articles for the last 8 years but just started my blog a few months ago. I am excited about this new adventure and look forward to sharing God's word with others. Please feel free to share your comments, thoughts, or questions so that we can all grow together in Christ.\n",
      "\"I pray also that the eyes of your heart may be enlightened in order that you may know the hope to which he has called you, the riches of his glorious inheritance in his holy people.\" (Ephesians 1:18)\n",
      "\"You will seek me and find me when you seek me with all your heart. I will be found by you,\" declares the Lord...\"(Jeremiah 29:13-14a).\n",
      "\"The LORD is my light and my salvation--whom shall I fear? The LORD is the stronghold of my life--of whom shall I be afraid?\" (Psalm 27:1)\n",
      "\"Ask and it will be given to you; seek and you will find; knock and the door will be opened to you. For everyone who asks receives; he who seeks\n",
      "llama_print_timings:        load time =  2936.87 ms\n",
      "llama_print_timings:      sample time =   220.23 ms /   512 runs   (    0.43 ms per token,  2324.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4111.95 ms /   265 tokens (   15.52 ms per token,    64.45 tokens per second)\n",
      "llama_print_timings:        eval time = 154804.22 ms /   510 runs   (  303.54 ms per token,     3.29 tokens per second)\n",
      "llama_print_timings:       total time = 159217.54 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03f792b2-0f7e-4b93-9d3e-69d4479f6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689579106\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I love this quote so much because it perfectly sums up what I do. When people come for a reading, they have no idea what's in store for them and they may be nervous about being able to connect with Spirit but I promise you the message is always positive and validating of who you are as a soul and your purpose here on earth.\n",
      "My work is my gift to give away and I feel so blessed to do it every day. It's hard to believe that I can make a living doing what I love, something that has become part of me. Everyone has their own gifts and I hope you find yours.\n",
      "I recently read an interesting article in the New York Times about how some people are able to use their gift to help others even if they don't get paid for it. For instance one woman with Asperger's syndrome is able to help her boss by looking at his spreadsheets and making sure all of the numbers add up. There are also those who are very generous with their time, sharing what they know about computers or teaching someone a new skill like knitting. Even when you don't make money from your gift (in fact, sometimes not getting paid can be part of the problem) it still feels good to give.\n",
      "There are many ways we can all help others, even if it doesn't seem like much. Sometimes just being there for someone who needs an ear is enough and that simple act of kindness can make a big difference in their lives.\n",
      "If you are struggling with giving your gift away because you are scared or anxious about how people will react, remember that the message should be positive and uplifting. If you are struggling to connect with Spirit I have some tips on my website here but if all else fails I am happy to help. Even if you are just starting out as a psychic reader there is no reason to give up or feel discouraged.\n",
      "I'm grateful for those who helped me find and develop my own gift. It wasn't easy but it was worth the effort. When we open ourselves up to others, we can be a part of something bigger than ourselves: the human race. We are all connected in one way or another so we should try to remember that each day when we wake up.\n",
      "I know I have said this before but I love my job and I would not change\n",
      "llama_print_timings:        load time =  2941.33 ms\n",
      "llama_print_timings:      sample time =   221.11 ms /   512 runs   (    0.43 ms per token,  2315.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4139.26 ms /   265 tokens (   15.62 ms per token,    64.02 tokens per second)\n",
      "llama_print_timings:        eval time = 154786.81 ms /   510 runs   (  303.50 ms per token,     3.29 tokens per second)\n",
      "llama_print_timings:       total time = 159228.59 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd03d5a9-978f-454f-9dde-da67377225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689579270\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
