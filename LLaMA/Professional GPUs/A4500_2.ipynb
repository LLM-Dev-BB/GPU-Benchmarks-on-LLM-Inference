{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb85a07-5afc-4727-911d-8605898e1228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 05:02:51 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4500                On | 00000000:82:00.0 Off |                  Off |\n",
      "| 30%   26C    P8               16W / 200W|      1MiB / 20470MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4500                On | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   28C    P8               14W / 200W|      1MiB / 20470MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       263860676 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46540875-c224-40c2-84e4-d4e4beb38f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a092a0-5fbc-4c1b-804f-3a6c6648ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e05cc5-2958-4e64-958a-af0da1d86239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float, float)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kllama.cpp:2208:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koperation on ‘\u001b[01m\u001b[Kt_start_sample_us\u001b[m\u001b[K’ may be undefined [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsequence-point\u0007-Wsequence-point\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2208 |     int64_t t_start_sample_us = \u001b[01;35m\u001b[Kt_start_sample_us = ggml_time_us()\u001b[m\u001b[K;\n",
      "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fe2052-3531-43c7-88bb-ad96af10f8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570195\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find a purpose in life that you can live with passion.\n",
      "I am passionate about helping others realize their full potential and finding fulfillment in life. I am a certified Life Coach through the International Coaching Federation (ICF).\n",
      "My background includes 20+ years in management, marketing, sales, training, customer service and finance. I have worked as an Account Executive at two of the nation’s largest banks. My work experience has included leading teams to achieve aggressive sales goals and helping individuals become more confident in their personal and professional lives.\n",
      "I earned a Bachelor of Arts degree from Ripon College, with majors in Psychology and Business Management, and a Master of Science degree from the University of Wisconsin – Stevens Point. I have served on numerous board positions including the United Way, Boys & Girls Clubs, Rotary Club and the local YMCA.\n",
      "I am married to my best friend and we have two beautiful daughters. My family is active in our church and community. We love being outside, playing sports, spending time with friends, going on vacations and helping others find their path.\n",
      "My coaching style includes a holistic approach which includes goal setting, motivation techniques, life/stress management skills, communication styles, time management tips, conflict resolution strategies, team building techniques and more! I will not tell you what to do, but instead we will determine your goals together and explore options for ways to reach those goals.\n",
      "I am looking forward to the opportunity to work with you in achieving success in life. Let’s start making a difference in others lives today!\n",
      "Please click here to contact me about how I can help you find a purpose that you can live with passion!\n",
      "Making Positive Life Changes (MPLC) is an international organization for personal and professional growth, wellness, and personal development. Founded by Tina L. Cross in 2014, MPLC strives to help others achieve their goals through coaching, mentoring, training, speaking, teaching, writing and more!\n",
      "The meaning of life is to find a purpose in life that you can live with passion.\n",
      "Making Positive Life Changes (MPLC) is an international organization for personal and professional growth, wellness, and personal development. Founded by Tina L. Cross in 2014, MPLC strives to help others achieve their goals through\n",
      "llama_print_timings:        load time =  6019.90 ms\n",
      "llama_print_timings:      sample time =   402.76 ms /   512 runs   (    0.79 ms per token,  1271.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1177.46 ms /   265 tokens (    4.44 ms per token,   225.06 tokens per second)\n",
      "llama_print_timings:        eval time =  9945.76 ms /   510 runs   (   19.50 ms per token,    51.28 tokens per second)\n",
      "llama_print_timings:       total time = 11653.84 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed3c049-a417-4618-8137-61842a70bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570218\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. A lot of people spend their lives not living, they are waiting on something to happen, something to be done or someone to appear. Life is to be lived now, this very moment and if we aren’t doing that then we may as well be dead.\n",
      "I think that many people have never experienced the feeling of living life to its fullest potential, not because they are incompetent but because they have never been taught how to live life to the fullest. This is where I believe that I come into play.\n",
      "In life there will always be those who will try and bring you down and try to steal your joy away from you; however, I am not one of these people. Through my blogs, posts, social media, etc., I want to inspire others with the things that I have learned throughout my journey so they may live life to its fullest potential and never let anyone or anything bring them down ever again.\n",
      "I believe in living every day as if it were your last because you don’t know what might happen tomorrow, and if we do then we will live with no regrets because we will have lived our lives the way that we wanted to, not how society may want us to.\n",
      "I believe that life is precious and must be treasured for what it is; a blessing from God and nothing else! This means that we must live every day as if it were our last so that we can appreciate each moment of our lives. We also need to live without worry because there will always be someone who has it worse off than us.\n",
      "I believe in living life the way I want to, not how society or others may want me to live. I don’t care about money or material things; all I ever want is for people to be happy and live their lives to its fullest potential because that is what is important to me.\n",
      "I will never give up on my dreams and ambitions in life, no matter how hard the journey may be because if something is worth having it’s worth fighting for. This also means I will never give up on myself, even when others may want me to. It is through this that I have been able to do amazing things such as completing my master’s degree at Cambridge University and getting published in a book – The Oxford Handbook of Religious Education – which has made me one of the youngest people to ever write for such an important publication.\n",
      "I\n",
      "llama_print_timings:        load time =  1854.07 ms\n",
      "llama_print_timings:      sample time =   394.38 ms /   512 runs   (    0.77 ms per token,  1298.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1164.36 ms /   265 tokens (    4.39 ms per token,   227.59 tokens per second)\n",
      "llama_print_timings:        eval time = 10011.55 ms /   510 runs   (   19.63 ms per token,    50.94 tokens per second)\n",
      "llama_print_timings:       total time = 11697.63 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b873475-d98b-4fed-b8cb-217a32876265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570236\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out who you are and go for it.\n",
      "The first time I walked into this place as a student, I had no idea what was waiting for me on the other side. It would take some time before I started to understand everything that was offered here – from the world-class education, the supportive community to the numerous opportunities for international experiences and internships, to the chance to get involved in many different projects and activities.\n",
      "I was lucky enough to be a student at Vancouver Film School when it first opened its doors here more than 20 years ago. I am proud that we are now turning into a place where people from all over the world come together; not only students, but also staff, faculty members, industry professionals and so on.\n",
      "In my role as Vice President of Student Affairs, it is always my goal to help our students take advantage of every opportunity. I know that Vancouver Film School has one of the best reputations in the film business for training people who are not only well-prepared but also professional, polished and ready to work – from day one!\n",
      "I hope that we have all the right ingredients here to help our students make their dreams a reality. I am confident that our graduates will go on to do great things in film production and beyond. It is my privilege and honour to be part of this process. Let’s continue working together, as partners, for the benefit of all!\n",
      "As Vice President of Student Affairs, I have the unique opportunity to work with an amazing group of people every day – from students, to faculty members, staff, administrators and industry professionals. Together we are a dynamic team that creates a supportive community here at Vancouver Film School. We are all dedicated to helping our students go for it!\n",
      "I know first-hand what it means to be a student again because I was one myself just a few short years ago. It is an incredible feeling to be part of someone’s journey as they learn and grow, and to feel like you have made a difference in their lives. It is an honour for me to help our students do that here at Vancouver Film School – from day one!\n",
      "I am excited about the future for us all. We are on an amazing adventure together – let’s go for it!\n",
      "I know I make my best decisions when I have a solid support team backing me up, and I feel\n",
      "llama_print_timings:        load time =  1814.32 ms\n",
      "llama_print_timings:      sample time =   397.86 ms /   512 runs   (    0.78 ms per token,  1286.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1171.83 ms /   265 tokens (    4.42 ms per token,   226.14 tokens per second)\n",
      "llama_print_timings:        eval time =  9949.21 ms /   510 runs   (   19.51 ms per token,    51.26 tokens per second)\n",
      "llama_print_timings:       total time = 11646.10 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0953aa-40f9-4719-b657-cc607acd478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570254\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give, and my purpose in this world is to inspire.\n",
      "My life’s mission is to help people become better versions of themselves so they can make a bigger impact on their world by teaching them how to improve their lives through education & empowerment. If I can help one person in the world, then I am doing what I came here for and my purpose will be fulfilled.\n",
      "My passion for helping others is rooted from two very traumatic experiences that occurred when I was a teenager. The first involved a horrific car accident that completely changed my life. The second happened when the love of my life, at 21 years old suddenly passed away in his sleep with no explanation or reason why. These events shaped me and helped me realize that the only constant thing in this life is change so I should embrace it all.\n",
      "After graduating from college in 2013, I was completely lost about my future. I didn’t know what I wanted to do next with my life. All I knew was that I had a passion for helping people and making a difference so I started this blog in order to document everything that I learned along the way. It was a journal of sorts that would allow me to track my progress and share it with the world.\n",
      "In 2016, I decided to launch the #LoveYouMoreMovement which is designed to inspire & empower people to love themselves more so they can live their best lives ever! Through education & empowerment, I believe that everyone has the ability to change their life for the better.\n",
      "I am a self-professed nerd and introvert who loves learning about philosophy, spirituality, healthy living, & positive psychology in my free time (in addition to watching Netflix). In 2015, I decided that it was finally time to go after what I truly wanted to do with my life. So I packed up everything and moved across the country by myself to San Diego where I attended graduate school to get my Master’s in Human Resource Management & Business Administration (MBA) at National University of San Diego.\n",
      "I currently work as a Marketing Manager for a global technology company headquartered in San Diego, CA. My past experience includes working as a Human Resources Consultant for two Fortune 100 companies while I attended graduate school. Some of my passions include traveling the\n",
      "llama_print_timings:        load time = 11904.85 ms\n",
      "llama_print_timings:      sample time =   404.62 ms /   512 runs   (    0.79 ms per token,  1265.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1535.06 ms /   265 tokens (    5.79 ms per token,   172.63 tokens per second)\n",
      "llama_print_timings:        eval time = 14152.76 ms /   510 runs   (   27.75 ms per token,    36.04 tokens per second)\n",
      "llama_print_timings:       total time = 16220.06 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debb274e-57b1-45ee-832f-f019b7e363fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570287\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living it.\n",
      "Throughout my life, I have always been a believer that you can accomplish anything if you work hard enough and put your mind to it. After all, we are all human, and humans make mistakes all the time. The key is to learn from those mistakes, so they don't happen again. We have so much going on in our lives that sometimes the stresses of daily living can get overwhelming. When this happens, it is important to take a step back and reevaluate what really matters most.\n",
      "In April 2014, I was diagnosed with Stage IIIC Melanoma (the worst form of skin cancer). It was a very scary time for me as well as my family. The doctors told me that if the cancer spread to my lymph nodes and I was not eligible for surgery, there would be no known treatment available. Fortunately, the first doctor to treat me in this matter was one of the best oncologists in New York City and he took a very aggressive approach. Because of his expertise as well as my hard work, I am currently cancer free!\n",
      "I have always felt that we should try our best to be positive and surround ourselves with good people who encourage us to do great things. So often we get caught up in the day-to-day routine of life and forget what is really important. The simple fact of life is, we need to work hard every single day so that we can continue to live it!\n",
      "I grew up in a family with seven kids in the suburbs of New York City. My father was a businessman who started out as an electrician and then worked his way up through the ranks to become a general contractor, owning his own construction company. Throughout my childhood I watched my dad work extremely hard every day for our family. His dedication and work ethic inspired me to always do my best and put forth 100% effort in whatever I choose to accomplish in life.\n",
      "I have played tennis since I was a kid, but when I got to college at the University of Wisconsin-Whitewater, I began playing competitively on their club team. In the spring of 2014 after my freshman year, I decided to take some time off from school and tennis in order to focus exclusively on working hard in the gym and training\n",
      "llama_print_timings:        load time =  2472.83 ms\n",
      "llama_print_timings:      sample time =   402.97 ms /   512 runs   (    0.79 ms per token,  1270.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1539.27 ms /   265 tokens (    5.81 ms per token,   172.16 tokens per second)\n",
      "llama_print_timings:        eval time = 14048.89 ms /   510 runs   (   27.55 ms per token,    36.30 tokens per second)\n",
      "llama_print_timings:       total time = 16118.77 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b638a02-ecb5-4ae9-8d3d-3339ebadfd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570311\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\"\n",
      "~ Pablo Picasso ~\n",
      "\"We make a living by what we get, but we make a life by what we give.\"\n",
      "~ Winston Churchill ~\n",
      "\"You were made to be a blessing to someone. So let people see the light and love in you.\"\n",
      "~ Robert Collier ~\n",
      "\"The best way to find yourself is to lose yourself in the service of others.\"\n",
      "~ Mahatma Ghandi ~\n",
      "\"The biggest giveaway of your true self is when you are feeling good about what you have done for someone else.”\n",
      "“What lies behind us and what lies before us are tiny matters compared with what lies within us”\n",
      "~ Ralph Waldo Emerson ~\n",
      "\"Kindness in words creates confidence. Kindness in thinking creates profoundness. Kindness in giving creates love.\"\n",
      "~ Lao Tzu ~\n",
      "\"The best way to find yourself is to lose yourself in the service of others.\" ~ Mahatma Ghandi\n",
      "\"You can get everything you want if you help enough other people get what they want\"\n",
      "~ Zig Zigler ~\n",
      "~ Dr. Norman Vincent Peale ~\n",
      "\"There are two ways of spreading light; to be the candle or the mirror that reflects it\"\n",
      "“To the world, you may be just one person – but to one person, you may be the world.”\n",
      "~ Anonymous ~\n",
      "\"The ultimate measure of a man is not where he stands in moments of comfort and convenience, but where he stands at times of challenge and controversy.\" - Dr. Martin Luther King Jr.\n",
      "“The most beautiful people we have known are those who have known defeat, known suffering, known struggle, known loss, and have found their way out of the depths.” ~ Elisabeth Kübler-Ross\n",
      "\"You can accomplish anything when you put your heart into it.\"\n",
      "~ Shirley Temple ~\n",
      "“What lies behind us and what lies before us are tiny matters compared to what lies within us” - Ralph Waldo Emerson\n",
      "~ Robert Collier ~\n",
      "\"To touch the soul of another human being is to walk on holy ground.\" ~ Stephen Gaskin\n",
      "If I had my life to live over again, I'd make the same mistakes, only sooner. ~ Tallulah Bankhead\n",
      "“The greatest discovery of any generation is that a human can change his\n",
      "llama_print_timings:        load time =  2494.26 ms\n",
      "llama_print_timings:      sample time =   400.05 ms /   512 runs   (    0.78 ms per token,  1279.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1539.24 ms /   265 tokens (    5.81 ms per token,   172.16 tokens per second)\n",
      "llama_print_timings:        eval time = 14074.25 ms /   510 runs   (   27.60 ms per token,    36.24 tokens per second)\n",
      "llama_print_timings:       total time = 16141.11 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d85862-e4b7-45a1-81e2-5705a0e4db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570334\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I think we are all born with a purpose and that is to be happy. We are all searching for something in our lives, but don't know what it is.\n",
      "You could say that happiness is just an emotion and comes and goes at any time. But I believe that there is more to being happy than just being happy for that moment. I don't think you can be truly happy if your life isn't heading in the direction you want it too, or if your dreams aren't coming true.\n",
      "To find your purpose in life, I believe you have to be willing to try new things and open your mind to new ideas, and not just stick with one thing. There is a big world out there for us all to explore. You never know what might interest you or make you feel complete until you try it!\n",
      "I think that we can find ourselves through different activities such as: sports, music, art, etc. When I was younger, my grandmother signed me up for swimming lessons. Little did she know what this would mean to me. I fell in love with the water immediately and started swimming competitively at the age of 10, and that is when I first felt like I had found myself. Swimming became a huge part of my life, and still is today.\n",
      "I have been swimming for about 12 years now. When I was in high school I made it to the provincial championships, which is where I realized that I wanted to continue swimming.\n",
      "When I went off to university, I started swimming with a new team and realized that there were so many amazing people just like me who also loved the sport. Even now, being away from my family and friends, these teammates have become my new family and best friends. I was even lucky enough to travel to Hawaii for a competition last year, which was truly one of the most amazing experiences in my life.\n",
      "I think that if we all take time out of our busy lives to find something we love doing, and then do it everyday, then maybe there would be less sadness in this world. I have also found that the more you believe that you can accomplish your goals, the easier they are to achieve!\n",
      "So go out there and explore! You never know what could happen!\n",
      "Sometimes I think we get too busy with our own lives to realize how amazing it is to be\n",
      "llama_print_timings:        load time = 25753.72 ms\n",
      "llama_print_timings:      sample time =   399.33 ms /   512 runs   (    0.78 ms per token,  1282.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2667.64 ms /   265 tokens (   10.07 ms per token,    99.34 tokens per second)\n",
      "llama_print_timings:        eval time = 25982.32 ms /   510 runs   (   50.95 ms per token,    19.63 tokens per second)\n",
      "llama_print_timings:       total time = 29176.73 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "308f3918-b619-417e-909b-5611b85d4131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570394\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your purpose — which is as unique to each of us as a fingerprint — and then to\n",
      "give it away.\n",
      "The reason why you are here, in this place at this time, is to be the best version of yourself that you can be. And to share your gifts with the world.\n",
      "What are some examples of life purpose?\n",
      "Your purpose could be anything from a big-picture cause such as creating peace on Earth to something more personal like wanting to be a great parent.\n",
      "I believe that living out our life purpose brings us joy and fulfillment, which then ripples out into the world for everyone else’s highest good. This is the key to true happiness in life — and, I would say, the meaning of life itself.\n",
      "In fact, this is a message we’ve been given throughout history by many of our most revered spiritual teachers. Gandhi taught us that “the best way to find yourself is to lose yourself in the service of others.” Mother Teresa said, “If you are judging people, you have no time to love them.”\n",
      "So how can you discover your life’s purpose? Here are 10 clues.\n",
      "Clue #1: Your Purpose Feels Like Play\n",
      "When you do what you love, work feels like play. When you’re doing something that feeds your soul and makes you come alive, it doesn’t feel like “work” in the traditional sense. In fact, it feels more like fun or play. It fills you with energy rather than depletes you of it. It excites and inspires you.\n",
      "And when you do what you love, you don’t mind putting in long hours because you enjoy it so much. You also tend to be good at what you love doing. Why? Because you enjoy it so much that you’re willing to work hard at it. You practice and develop your skills. You find ways to keep improving.\n",
      "Clue #2: Your Purpose Feels Like a Calling\n",
      "It feels like something you are called or drawn toward, like a siren song. It calls you to itself. When you think about doing what you love for a living, you may have some fears about whether it’s possible or practical. But underneath those thoughts and fears is a feeling of rightness, of knowing that this is the path for you.\n",
      "You feel drawn toward your\n",
      "llama_print_timings:        load time =  5420.28 ms\n",
      "llama_print_timings:      sample time =   406.03 ms /   512 runs   (    0.79 ms per token,  1261.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2656.31 ms /   265 tokens (   10.02 ms per token,    99.76 tokens per second)\n",
      "llama_print_timings:        eval time = 25874.72 ms /   510 runs   (   50.73 ms per token,    19.71 tokens per second)\n",
      "llama_print_timings:       total time = 29064.84 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2949fb-1a3a-4d23-b1ae-eae1f43e6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570434\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "I think that we humans are a very strange species, in that we are the only one who can make themselves feel better by making others feel worse. There’s no other animal on earth that does that. No other creature will hunt down and kill another just for fun, or to prove they’re superior to them. We do it all the time. We also have the ability to feel empathy and sympathy for our fellow human beings.\n",
      "I think we as humans are at a cross-roads of sorts, where we can choose to be either good or evil. Every day we make choices that determine whether the world will become better or worse, just by how we treat each other. If you’re ever wondering why everyone is so depressed, all one has to do is look around at what our species does and says everyday. We’re not setting a very good example for ourselves.\n",
      "I think that if we want the world to be better, then it starts with us as individuals. We need to stop looking at others as different from us just because they don’t belong to a certain group or look a certain way. If we can’t get along with each other, how are we ever going to work together and save the planet before it’s too late?\n",
      "I believe that life is precious, no matter who you are or where you come from. Every single human being on earth deserves basic needs: food, shelter, water, education, a chance at happiness and love.\n",
      "I believe in kindness. I think it is the most powerful force in the universe. Kindness can turn a bad day into a good one; it can mend relationships that were long thought broken. It has no limits, and if we all gave it freely to everyone else around us without expecting anything in return, the world would be a much better place for it.\n",
      "I believe that we are all connected, whether we want to be or not. That means that what you do affects me just as much as what I do affects you. This is why we need to think about our actions and words before we act upon them, because they can do so much damage. There’s no excuse for being a jerk to someone else, and there’s certainly no excuse for letting someone be a jerk to you.\n",
      "I believe that life should be fun. I don’t think it was meant to be taken seriously all the\n",
      "llama_print_timings:        load time =  5121.26 ms\n",
      "llama_print_timings:      sample time =   400.00 ms /   512 runs   (    0.78 ms per token,  1279.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2665.32 ms /   265 tokens (   10.06 ms per token,    99.43 tokens per second)\n",
      "llama_print_timings:        eval time = 26051.97 ms /   510 runs   (   51.08 ms per token,    19.58 tokens per second)\n",
      "llama_print_timings:       total time = 29245.35 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ceb36cf-7029-4e71-b33a-6e3821f921fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570474\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33mCUDA error 2 at ggml-cuda.cu:3712: out of memory\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb3cc72b-00f2-40dd-bf3a-f85d9437c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570539\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33mCUDA error 2 at ggml-cuda.cu:3712: out of memory\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f065815a-fdaa-4a18-898e-092c4782e5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570554\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33mCUDA error 2 at ggml-cuda.cu:3712: out of memory\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fead9-7953-4e6a-9d5e-4597d24cd581",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa199684-93f1-4f1b-a3ed-295687409a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570569\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find it.\n",
      "I am a 16 year old girl who has been through a lot in my short life, but now I’m finally happy. It took me a while to get here though; I was diagnosed with depression when I was just 8 years old, which caused me to become suicidal and self harm. I have been hospitalised on two occasions since then, and each time I have promised myself that I would never do it again. But after having my first child at the age of 13, that’s exactly what I did. My depression got worse and I couldn’t cope with life any more. I couldn’t handle being a mum because it was too hard for me to care for her, so I decided to let her go.\n",
      "Afterwards, my life spiraled out of control completely. I wanted to die but I didn’t want to take my daughter with me; this is when the voices started telling me “you don’t deserve her” and “she would be better off without you”. Eventually, I came back from the brink of insanity but I was a completely different person. My life had changed so much that it almost felt like my previous self didn’t exist; all I could do was focus on putting myself together again to have any hope of ever being with my daughter again. In the past 5 years, I’ve been in and out of psychiatric hospitals and I’m still trying to get my life back on track. It has been a long journey but it is finally getting better.\n",
      "I never thought that I would be able to have a baby again because of my depression, so when my ex-boyfriend got his girlfriend pregnant 2 years ago (when she was only 15!) I told him not to even bother with her anymore and he started messaging me again. We met up in secret about once a week but never let anyone know we were seeing each other. After he met my daughter, who is now 3, it turned out that we had a lot of things in common and he was just as crazy as I am!\n",
      "One year ago this month, we made the decision to become an official couple; our families didn’t approve but they were also too scared for us. They are both pretty bad influences on each other haha! We are finally happy together and know that we will be\n",
      "llama_print_timings:        load time = 22755.70 ms\n",
      "llama_print_timings:      sample time =   403.32 ms /   512 runs   (    0.79 ms per token,  1269.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1157.94 ms /   265 tokens (    4.37 ms per token,   228.85 tokens per second)\n",
      "llama_print_timings:        eval time = 14312.85 ms /   510 runs   (   28.06 ms per token,    35.63 tokens per second)\n",
      "llama_print_timings:       total time = 16002.28 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97321c82-3020-46a0-a973-393e5d441670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570613\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give yourself away!\n",
      "I believe we are made in His image and called to be like Him and as we do that, we find our identity.\n",
      "I believe God wants us to have a relationship with him... not because he needs us but rather we need Him! To know this God person is the only way you will ever be able to understand your purpose and meaning of life on this planet.\n",
      "I believe we are called to love one another, serve one another and encourage each other in our faith walk.\n",
      "I believe that when Jesus comes back, it's going to be a glorious day! I can't wait for that day!\n",
      "I believe it’s a privilege as Christians to help people find their way towards God. It’s not our job but rather our calling and we are called to share what we have received with others so they too will receive the same peace, love and joy in knowing Jesus Christ. Our purpose is to make disciples of all nations!\n",
      "I believe that as Christians, it is a privilege to give to God's work through our tithes and offerings. It really does matter how we steward our money because it tells a story about what we value more than anything else in life. I believe God wants us to give cheerfully... not with guilt but rather out of love!\n",
      "I believe God has placed each of us on this planet for such a time as now and that He has equipped us with the ability to make a difference where ever we are. Every believer is called to ministry whether it be at your work place, school or in your home... everyone can make a positive impact!\n",
      "I believe God’s plan is much bigger than just one church building! I have seen this first hand as we partner with other ministries such as Mission Arlington and local churches. We are better together and together we do more for His Kingdom work!\n",
      "I believe that if you truly want to discover your purpose, calling or ministry then God will help you find it! He will put people in front of you who can help guide you along the way so be open to what he has planned! I am a firm believer that God has a unique plan for each one of us and when we allow him, our lives are more fulfilling.\n",
      "I believe that as Christians, it is our responsibility to share with others all they need to know about Jesus Christ and His love for them. The greatest\n",
      "llama_print_timings:        load time =  3646.68 ms\n",
      "llama_print_timings:      sample time =   404.34 ms /   512 runs   (    0.79 ms per token,  1266.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1164.04 ms /   265 tokens (    4.39 ms per token,   227.66 tokens per second)\n",
      "llama_print_timings:        eval time = 14289.34 ms /   510 runs   (   28.02 ms per token,    35.69 tokens per second)\n",
      "llama_print_timings:       total time = 15985.15 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d5c071-0cd0-4414-989d-227b8f796ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570638\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love others, and God.\n",
      "I don't have a religion, but I do believe in Jesus Christ. I believe that he was sent down from heaven by our creator to save the world. He showed us how to be good people, and he taught us how to live as christians. We are taught that if we ask for forgiveness of sins then we will go to heaven when we die. But God has also given us a gift called free will. If you follow Jesus' word's it does not matter what happens in the end because you have already been saved. You can only get into trouble by sinning, and if you do good things you will be rewarded by going to heaven after death.\n",
      "Postby Ginny » Thu Apr 09, 2009 3:57 pm\n",
      "Jesus Christ is the Son of God and he came down from Heaven for a purpose which was to save us from our sins and to give his life so we could live forever. He died on the cross and rose again to show that He can forgive sins by His blood shed on it. Jesus said, \"I am the way, the truth, and the life\" (John 14:6)\n",
      "As a Christian I believe in God, and Jesus Christ. And that his death was for us. So we don't have to pay the price of our sins ourselves. But He took it all upon himself so we could be forgiven. I also believe that there is only one true way to Heaven; and that is through Jesus.\n",
      "Postby Ginny » Fri Apr 10, 2009 4:05 pm\n",
      "I believe in God, and Jesus Christ because I have seen proof of their existence many times.\n",
      "A good example would be when my mom was in a bad car accident, the doctors said that she could not survive it but she did. Then later on after the accident she heard a voice say \"don't worry you will be fine\" and then that is exactly what happened. I believe because of this event Jesus Christ saved her life when she had no chance at all to survive.\n",
      "I also see other evidence every day when my mom prays for me or my sisters and it always seems like they get better. I know not everything is about God, but most things are from him.\n",
      "Postby Ginny » Sat Apr\n",
      "llama_print_timings:        load time =  3652.56 ms\n",
      "llama_print_timings:      sample time =   407.39 ms /   512 runs   (    0.80 ms per token,  1256.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1150.96 ms /   265 tokens (    4.34 ms per token,   230.24 tokens per second)\n",
      "llama_print_timings:        eval time = 14362.83 ms /   510 runs   (   28.16 ms per token,    35.51 tokens per second)\n",
      "llama_print_timings:       total time = 16048.78 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6041485b-3a5d-4877-8866-78fcb6a20717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570662\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift, the purpose God gave you when He created you. Then with all your heart, soul, mind and strength, give it away.\n",
      "We are each born for a reason, have a calling on our lives, a purpose that will bring us joy, fulfillment, and happiness. The problem is many never find their calling in life. Many of those who do find it don’t know how to give it away to others. They hoard it as if it were their rightful possession. I believe the reason God created each person unique with different gifts was so that we could share them with others.\n",
      "This is where we see the problem: If you find your gift, but keep it to yourself, how many people can benefit from it? One! And only one at a time. But if you discover your gift and give it away….imagine what might happen! The possibilities are truly endless.\n",
      "God created us all different for a reason. Every person on this earth has a purpose. In order to fulfill that purpose, we must discover our God-given gifts. And then we must learn how to share them with others. We need to give away what God gave us to give. What’s the point of having something if you don’t pass it on?\n",
      "I have a friend who is an amazing artist and singer-songwriter. She has had many opportunities given to her, but she holds back from sharing herself with others. Many people, including myself, would benefit from what God gave her. If she were to give her gift away, imagine how many people could be blessed by it?\n",
      "God didn’t create you so that you could hold on to your gifts and never pass them on to anyone else. He wants us to find our calling in life, then live out the dreams He has given us. And one of those dreams is to let others benefit from what God gave us.\n",
      "I believe we can all do this. We can each discover our gift, and give it away!\n",
      "What a wonderful idea! The more we learn about giving away that which we have been blessed with the more we receive in return.\n",
      "Good job. I like your blog. Keep up the good work.\n",
      "Very Inspiring words. You are so right God gives us every thing for a reason and he wants us to share it, what better way than through art! Art is truly a universal language\n",
      "llama_print_timings:        load time = 43573.03 ms\n",
      "llama_print_timings:      sample time =   396.20 ms /   512 runs   (    0.77 ms per token,  1292.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1512.60 ms /   265 tokens (    5.71 ms per token,   175.19 tokens per second)\n",
      "llama_print_timings:        eval time = 22008.02 ms /   510 runs   (   43.15 ms per token,    23.17 tokens per second)\n",
      "llama_print_timings:       total time = 24043.69 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "049482b4-3e67-44d7-b1dd-fb612c12b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570736\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living, loving and laughing. Laughter heals the soul and lightens our spirit.\n",
      "The most important thing is that we must be happy in ourselves so that others may be happy with us.\n",
      "To be happy it is not necessary that you become the President or PM of a country. All you need to do is to know the meaning of happiness, love, peace and success.\n",
      "I’ve learned that people will forget what you said, people will forget what you did, but people will never forget how you made them feel.\n",
      "The most important thing in life is not the triumphs or defeats, but rather to keep striving forward with determination and faith. It is this attitude of mind which determines life’s outcome.\n",
      "Happiness depends on your mindset. If you have a positive mind set then you can easily make yourself happy. Try to look at brighter side of every situation and you will start feeling happy.\n",
      "The meaning of happiness is a sense that one has attained a particular goal, something they want or desire. This might be the result of achieving a personal objective or the fulfillment of some other purpose in life. Happiness, for many people, is based on their accomplishments and achievements.\n",
      "Being happy does not necessarily mean being rich, famous or having a perfect family. It just means that we are at peace with ourself. Our soul is contented. We feel comfortable in our own skin.\n",
      "In order to be happy you have to create happiness for yourself – the more you can create for yourself the happier you will be.\n",
      "I’ve learned that people don’t always say what they mean or mean what they say, intentionally. So, I try not to take it personally when words are said and I try not to react offensively when someone says something I don’t agree with.\n",
      "In order to be happy you have to create happiness for yourself. The more you can create for yourself the happier you will be.\n",
      "Happiness is a very personal thing, and it involves being comfortable in your own skin. It means that you are able to enjoy where you are in life; you are content with what you have achieved and you aren’t trying to live up to the expectations of others.\n",
      "It isn’t always easy to be happy with who we are and what we have, but it is important that we try to do so.\n",
      "\n",
      "llama_print_timings:        load time =  6130.40 ms\n",
      "llama_print_timings:      sample time =   407.28 ms /   512 runs   (    0.80 ms per token,  1257.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1519.78 ms /   265 tokens (    5.74 ms per token,   174.37 tokens per second)\n",
      "llama_print_timings:        eval time = 22019.93 ms /   510 runs   (   43.18 ms per token,    23.16 tokens per second)\n",
      "llama_print_timings:       total time = 24074.40 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e85c20-c72d-4486-839e-539d620eafe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570772\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to grow, and that our purpose here on this planet is to help us do that. There's no other reason for this place. It is only a school in which we are all students, trying to learn lessons as best we can.\n",
      "I am not a human being; I am a human becoming.\n",
      "—HENRY DAVID THOREAU\n",
      "One of the most important questions you should ask yourself is: \"Why do I want to change?\" We know that there are plenty of people who really don't want to change, so if it's something you have to be coerced into doing by someone else or even your own sense of guilt, you probably won't succeed. But if you honestly feel the desire to make some positive changes in your life and you're ready and willing to take action, then that is where success can begin.\n",
      "Too many people fail at changing their lives because they lack the motivation to do so. That may be why many of us have a hard time staying with our New Year's resolutions. We make promises to ourselves on January 1 to lose weight, quit smoking, or save more money by the end of the year and we often give up before April 1. Why is that? Because people only want to change when they feel like it!\n",
      "But you can learn how to take control of your life even if you don't feel like it. That's really what this book is all about: learning how to make a powerful decision and stick with it, no matter what comes along. This process will help you develop the ability to choose your destiny rather than have your fate chosen for you by others or even by yourself. You may not always feel like changing at first, but when you learn the secrets I teach in these pages, you'll be able to change your life anyway—simply because you make that important decision.\n",
      "#  **The Power of Choice**\n",
      "We all have a choice about what we do and how we respond to challenges. But our ability to choose is often limited by the way we view ourselves and our circumstances. If we think that we are unworthy, unwanted or unable to change our situation, then we won't make an effort because it doesn't seem like anything will work out anyway.\n",
      "But when you know in your heart that you can accomplish something and that you have the power to do it, then nothing seems\n",
      "llama_print_timings:        load time =  6442.32 ms\n",
      "llama_print_timings:      sample time =   404.30 ms /   512 runs   (    0.79 ms per token,  1266.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1520.62 ms /   265 tokens (    5.74 ms per token,   174.27 tokens per second)\n",
      "llama_print_timings:        eval time = 21987.27 ms /   510 runs   (   43.11 ms per token,    23.20 tokens per second)\n",
      "llama_print_timings:       total time = 24039.72 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfd8cd6c-3373-46fc-bcc6-1e02a47036e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689570809\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A4500, compute capability 8.6\n",
      "  Device 1: NVIDIA RTX A4500, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA RTX A4500) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd4b46-4e7e-4784-ad51-e22437d4c41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
