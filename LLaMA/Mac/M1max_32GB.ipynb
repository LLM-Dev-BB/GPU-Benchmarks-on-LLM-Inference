{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     ggml-vocab.bin          \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191dea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "embd-input-test\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/common.cpp -o common.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/console.cpp -o console.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/grammar-parser.cpp -o grammar-parser.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml-alloc.c -o ggml-alloc.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-metal.o ggml-alloc.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o ggml-alloc.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders \n",
      "c++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329268\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12b67b440\n",
      "ggml_metal_init: loaded kernel_add_row                        0x12b67db10\n",
      "ggml_metal_init: loaded kernel_mul                            0x12b67cef0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12b67e590\n",
      "ggml_metal_init: loaded kernel_scale                          0x12b67ee90\n",
      "ggml_metal_init: loaded kernel_silu                           0x12b67f830\n",
      "ggml_metal_init: loaded kernel_relu                           0x12b67d2f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12b680160\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12b681170\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12b682450\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12b6826b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12b681960\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12b683a60\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12b682ee0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12b6841a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12b684ae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12b685420\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12b685d60\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12b686840\n",
      "ggml_metal_init: loaded kernel_norm                           0x12b687960\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12b687d60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12b6890b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12b689a80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12b68a470\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12b68af80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12b68b8d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12b68c290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12b68cc60\n",
      "ggml_metal_init: loaded kernel_rope                           0x12b68d590\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12b68e150\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12b68eff0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12b68fb80\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12b687180\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live in accordance with your own nature, and not that of another.\n",
      "The meaning of life is to live in accordance with your own nature, and not that of another.\n",
      "I’ve always been inspired by people who say things like “live your truth” or “follow your heart.” I like those inspirational words because they seem to be saying “Be yourself! Be true!” And, as someone who has struggled with depression and anxiety my whole life, I can attest that it is hard. It takes a lot of work!\n",
      "This is the first blog entry for our new website. We are excited about this project because we feel like it’s an important step in us becoming more open to the world; to be able to share our experiences with others who might be going through similar struggles, and to help them feel less alone.\n",
      "For me, I often get into a lot of trouble when I am not living according to my nature. I tend to take on different roles and characteristics in order to fit in (like the rebel or the artist), but when I do this, it always comes at a cost — I’m either unhappy with myself because I’m not being true to who I really am, or I’m upset that I have let someone else down. In my experience, these are also signs of depression and anxiety.\n",
      "I wish for us all to find our own truths, and live in accordance with them! But sometimes it takes a lot of work — there is no easy way out (except maybe drugs?). And I can’t tell you how many times I have tried to do just that: I have read books like The Power of Your Subconscious Mind by Dr. Joseph Murphy, and followed the 12 Step Program.\n",
      "I haven’t done so well with this last thing (because I don’t believe in a Higher Power), but it is something that I would like to do. There have been times when I have gone to church just to sit there and feel peaceful and at home, or as if everything will be okay.\n",
      "For me, the meaning of life is to live in accordance with my own nature, and not that of another. If I can live according to who I am — someone who has a tendency towards depression and anxiety — then I know that even when things are hard, I will get through it. That’s because I have\n",
      "llama_print_timings:        load time = 10692.58 ms\n",
      "llama_print_timings:      sample time =   673.62 ms /   512 runs   (    1.32 ms per token,   760.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4803.79 ms /   265 tokens (   18.13 ms per token,    55.16 tokens per second)\n",
      "llama_print_timings:        eval time = 10655.69 ms /   510 runs   (   20.89 ms per token,    47.86 tokens per second)\n",
      "llama_print_timings:       total time = 16198.78 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329295\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x14211f140\n",
      "ggml_metal_init: loaded kernel_add_row                        0x142121240\n",
      "ggml_metal_init: loaded kernel_mul                            0x142120720\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x142121cc0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1421226f0\n",
      "ggml_metal_init: loaded kernel_silu                           0x142122f10\n",
      "ggml_metal_init: loaded kernel_relu                           0x142120b20\n",
      "ggml_metal_init: loaded kernel_gelu                           0x142123840\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x142124770\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x142125ae0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x142125d40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x142124ff0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1421270f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x142126570\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x142127830\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x142128160\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x142128be0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1421293e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x142129ec0\n",
      "ggml_metal_init: loaded kernel_norm                           0x14212b120\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14212bd80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14212c6e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14212d0a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14212dab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14212e5b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x14212eee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x14212f730\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x142130100\n",
      "ggml_metal_init: loaded kernel_rope                           0x142130bd0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x142131750\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1421325d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x142133150\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x142133960\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose and live it.\n",
      "I believe that love has no boundaries, but only when you are open enough to receive it.\n",
      "I believe in karma and that things will always even out in the end.\n",
      "I believe in fate and destiny and that everything happens for a reason.\n",
      "I believe in luck and being in the right place at the right time.\n",
      "I believe in kindness and selflessness, especially when you have nothing to gain from it.\n",
      "I believe that money does not buy happiness, but it will certainly make your life more comfortable.\n",
      "I believe that life is a journey, which means you have a lot of roads ahead of you, so take them one at a time.\n",
      "I believe in second chances and the opportunity for new beginnings.\n",
      "I believe in hope; when times are toughest, it will keep you going, especially if you’ve lost faith in humanity.\n",
      "I believe that everyone has their own story to tell and it is important to listen to them all so we can learn something from each one of them.\n",
      "I believe the world would be a better place if people were more open-minded, but I know this will never happen as long as there are still those who want to control the masses with their own beliefs.\n",
      "I believe that love and happiness go hand in hand and we need both for us to feel complete and whole; otherwise, we’re only half of a person.\n",
      "I believe in miracles and I have experienced them firsthand many times over.\n",
      "The meaning of life is living your purpose.\n",
      "You were born with a purpose; find it and live it.\n",
      "Life is too short to waste time on things that do not make you happy or bring you any sort of joy.\n",
      "There are no guarantees in life, but there is always hope.\n",
      "If you want something, go for it. Never give up because if you keep trying long enough, you’ll eventually get what you want.\n",
      "Being kind to others does not mean that you have to be soft on them; rather, it means being considerate of their feelings and needs and never being mean or rude to them.\n",
      "You should always try to do the right thing in life because this is what will make you feel proud of yourself at the end of your journey on Earth; it’s also what will make your soul happy when you leave this world behind.\n",
      "I believe that we are all connected and\n",
      "llama_print_timings:        load time =   841.91 ms\n",
      "llama_print_timings:      sample time =   856.06 ms /   512 runs   (    1.67 ms per token,   598.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4819.77 ms /   265 tokens (   18.19 ms per token,    54.98 tokens per second)\n",
      "llama_print_timings:        eval time = 10796.96 ms /   510 runs   (   21.17 ms per token,    47.24 tokens per second)\n",
      "llama_print_timings:       total time = 16554.55 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329313\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x146630710\n",
      "ggml_metal_init: loaded kernel_add_row                        0x146632810\n",
      "ggml_metal_init: loaded kernel_mul                            0x146631cf0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x146633290\n",
      "ggml_metal_init: loaded kernel_scale                          0x146633cc0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1466344e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1466320f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x146634e10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x146635d40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1466370b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x146637310\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1466365c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1466386c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x146637b40\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x146638e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x146639730\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14663a1b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14663a9b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14663b490\n",
      "ggml_metal_init: loaded kernel_norm                           0x14663c6f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14663d350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14663dcb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14663e670\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14663f080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14663fb80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1466404b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x146640d00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1466416d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1466421a0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x146642d20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x146643ba0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x146644720\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x146644f30\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to create something that will outlast your existence.\n",
      "The first thing we must do in order to begin living our lives, is to be honest with ourselves about who we really are. This can be difficult because it takes courage and strength. It requires us to face the harsh reality of our existence and the truths behind why we’re here. We have to stop trying to avoid those unpleasantries that life throws at us and instead face them head on with an open mind, heart and soul.\n",
      "We must become willing to change, to evolve as human beings and learn how to live in a way that helps others live better lives. This also takes strength because when we face this truth, it’s painful. It’s almost like childbirth. You have to go through the pain, the labor pains of existence itself before you can bring forth all of your potential and begin living life as the most authentic version of yourself possible.\n",
      "When I say “live life,” I mean to live it in a way that is selfless rather than selfish. We must learn how to put others ahead of ourselves and help them live better lives. We have to be willing to serve humanity in some way, shape or form. This means we have to get out of our own heads and look at what’s going on around us instead of focusing solely on our own needs.\n",
      "I believe the meaning of life is to create something that will outlast your existence. Our purpose in this life, I think, is to create art — whether it be music, painting, writing or some other form of creativity. The ultimate goal should always be to leave something behind for others to enjoy and benefit from after our deaths.\n",
      "If you’re like me and have a tendency to focus on yourself and your own needs, this is an important lesson to learn because it means we must stop focusing so much on self-preservation and start thinking about other people more. It also takes the pressure off of trying to “be somebody” in order to live life fulfilling enough for ourselves — which is another big problem that we’ve all probably had at some point or another.\n",
      "In short, I believe that the meaning of life is to become a living, breathing work of art. Our purpose should be to create something that outlasts our existence and inspires others along the way. We have an opportunity here on Earth to make someone else’s day better\n",
      "llama_print_timings:        load time =   835.18 ms\n",
      "llama_print_timings:      sample time =   685.19 ms /   512 runs   (    1.34 ms per token,   747.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4782.64 ms /   265 tokens (   18.05 ms per token,    55.41 tokens per second)\n",
      "llama_print_timings:        eval time = 10774.12 ms /   510 runs   (   21.13 ms per token,    47.34 tokens per second)\n",
      "llama_print_timings:       total time = 16307.68 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329330\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x131e22fc0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x131e25270\n",
      "ggml_metal_init: loaded kernel_mul                            0x131e24650\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x131e25cd0\n",
      "ggml_metal_init: loaded kernel_scale                          0x131e26600\n",
      "ggml_metal_init: loaded kernel_silu                           0x131e248b0\n",
      "ggml_metal_init: loaded kernel_relu                           0x131e26f50\n",
      "ggml_metal_init: loaded kernel_gelu                           0x131e27880\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x131e288b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x131e29b60\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x131e29dc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x131e2aff0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x131e2b250\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x131e2a7b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x131e2ba70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x131e2c3b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x131e2ccf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x131e2d630\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x131e2e710\n",
      "ggml_metal_init: loaded kernel_norm                           0x131e2f200\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x131e2fe60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x131e30800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x131e311d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x131e31ba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x131e32690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x131e33020\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x131e339c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x131e34350\n",
      "ggml_metal_init: loaded kernel_rope                           0x131e34c70\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x131e35840\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x131e366c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x131e37240\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x131e2e970\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find and pursue your dreams, as long as they are within reason. If you’re a serial killer, you’ll go to jail.\n",
      "I would describe my personality as being very upbeat and positive, sometimes even oblivious to the realities of life but that’s okay because I think most people can relate to that. My friends say I am very kind and caring, which is a huge compliment considering they’re all crazy.\n",
      "I have so many dreams and aspirations it would take 10 pages to list them all. I guess the main ones are to travel the world, find my true love, raise a family and live a happy life.\n",
      "My biggest dream is to start a foundation for children in need of basic medical services. I’ve been inspired by the amazing work done by The Art of Giving Foundation which has made it possible for so many families around the world to have access to quality healthcare. We are blessed with good health and a lovely, happy family here in the US, and even though we don’t need any medical services there is nothing better than giving back!\n",
      "I believe that anyone who can find joy in the little things should be very grateful for their life – I try my best to find happiness in everything. One thing about me that you may not know is that I have a special way of finding joy and laughter during tough times, even when it’s not easy to do so. My mom was diagnosed with breast cancer last year and we went through chemo together for 6 months – she lost her hair, I cried almost every day, and the thought of losing my mom was one of the hardest things to deal with. But somehow, through that experience, I found a way to make light of it all and laugh about the funny situations that happened along the way.\n",
      "I love cooking for family and friends – especially when we have a big gathering. My biggest passion outside of work is traveling! I’ve been lucky enough to travel around the world and experience different cultures, cuisines, landscapes and people. I find that food, music and dance are all great ways to connect with others.\n",
      "I love how my mom has taught me to be a strong woman who can accomplish anything she sets her mind to – she is an amazing human being! My dad is the best example of kindness towards everyone he\n",
      "llama_print_timings:        load time =  4697.52 ms\n",
      "llama_print_timings:      sample time =  1612.47 ms /   512 runs   (    3.15 ms per token,   317.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5323.75 ms /   265 tokens (   20.09 ms per token,    49.78 tokens per second)\n",
      "llama_print_timings:        eval time = 37784.07 ms /   510 runs   (   74.09 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time = 44848.96 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329380\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x141e25e60\n",
      "ggml_metal_init: loaded kernel_add_row                        0x141e27f60\n",
      "ggml_metal_init: loaded kernel_mul                            0x141e27440\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x141e289e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x141e29410\n",
      "ggml_metal_init: loaded kernel_silu                           0x141e29c50\n",
      "ggml_metal_init: loaded kernel_relu                           0x141e27840\n",
      "ggml_metal_init: loaded kernel_gelu                           0x141e2a560\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x141e2b570\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x141e2b7d0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x141e2cb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x141e2dcc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x141e2d140\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x141e2d3a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x141e2e5d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x141e2eec0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x141e2f940\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x141e30160\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x141e30bf0\n",
      "ggml_metal_init: loaded kernel_norm                           0x141e31cf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x141e320f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x141e332f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x141e33c90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x141e346b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x141e351c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x141e35b50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x141e364f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x141e36eb0\n",
      "ggml_metal_init: loaded kernel_rope                           0x141e37940\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x141e38520\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x141e393d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x141e39f20\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x141e313b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.” – David Litton\n",
      "I’ve been thinking a lot about what my purpose in life is and what I can do with this one precious life that I have. What am I meant to do? How can I make the world a better place? And how can I use the resources I have as an individual—like my time, energy, money, talent, etc.—to make a difference?\n",
      "I’m grateful to be able to take the time to think about these things. Not everyone gets this chance. But when I consider that there are 1 billion people in our world who live on less than $1 per day and don’t have access to clean water, it makes me sad. And angry. It makes me want to do something big so that no one ever has to go without the basics again.\n",
      "So what is my gift? What am I meant to give away? For a long time I thought that my purpose in life was to make money and then donate all of it away. But now I see that there’s more to life than this. There’s so much meaning, passion, and purpose to be found through working with people.\n",
      "I love connecting with others on a personal level. When you can build trust and establish rapport with someone, the possibilities are endless. You can discover something new about yourself or help another person discover something new within themselves. This is why I’m so excited about my new career as a life coach! It gives me the chance to meet people where they are at, help them gain clarity on their goals and aspirations, motivate them to make changes in their lives, and provide support along the way. And I love doing it!\n",
      "I am grateful that I have this opportunity—to do something that makes my heart sing. To spend time with people who inspire me and who have a strong desire to change their lives for the better. It is incredible when you can sit down with someone, look them in the eye, and ask them some powerful questions that help them to see what’s really going on in their life. I love being able to hold space for others so they feel safe enough to be open and vulnerable. This type of connection is priceless!\n",
      "I don’t know exactly how far I will go with my career as a coach, but I hope that one day I can help people heal from their pain and\n",
      "llama_print_timings:        load time =  4111.05 ms\n",
      "llama_print_timings:      sample time =  1708.73 ms /   512 runs   (    3.34 ms per token,   299.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5304.40 ms /   265 tokens (   20.02 ms per token,    49.96 tokens per second)\n",
      "llama_print_timings:        eval time = 37881.80 ms /   510 runs   (   74.28 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time = 45032.74 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329430\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100c1b9f0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100c1daf0\n",
      "ggml_metal_init: loaded kernel_mul                            0x100c1cfd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100c1e570\n",
      "ggml_metal_init: loaded kernel_scale                          0x100c1efa0\n",
      "ggml_metal_init: loaded kernel_silu                           0x100c1f7e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x100c1d3d0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100c200f0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100c21100\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100c21360\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100c226b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100c23850\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100c22cd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100c22f30\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100c24160\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100c24a50\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100c254d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100c25cf0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100c26780\n",
      "ggml_metal_init: loaded kernel_norm                           0x100c27880\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100c27c80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100c28e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100c29820\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100c2a240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100c2ad50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100c2b6e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100c2c080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100c2ca40\n",
      "ggml_metal_init: loaded kernel_rope                           0x100c2d4d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100c2e0b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100c2ef60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100c2fab0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100c26f40\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.” ~ Pablo Picasso\n",
      "For more than 30 years, I have worked with thousands of people from all walks of life. My clients have been corporate executives and managers, small business owners, students, stay-at home moms, housewives, lawyers, doctors, nurses, teachers, professors, engineers, musicians, artists, athletes, retirees, college graduates, high school dropouts, single parents, married couples, and even the unemployed.\n",
      "Their common denominator? They were all looking for a way to change their life so they could live happier, more fulfilled lives. In other words, they wanted to find meaning in their lives. And while this might sound strange coming from a business consultant, I’ve been fortunate enough to have found my gift and I now live a very fulfilling life helping others achieve the same thing.\n",
      "I have learned that each of us has our own individual definition of what it means for your work to “mean something.” Some people want to make a difference in their world, but are uncertain how or where they can do so. Others believe they were put on this Earth to be a great success and become rich and famous. Still others believe that the meaning of life is simply to be happy, without having to explain why.\n",
      "I have learned that each person has his or her own individual definition of what it means for work to “mean something.” Some people want to make a difference in their world while others believe they were put on this earth to become rich and famous. Still others believe the meaning of life is simply to be happy without having to explain why.\n",
      "As you can see, there are many different ways our definition of meaning changes over time, as we move from one stage of life to another. For example:\n",
      "When I was a child, what made my work “mean something”? Well, when I was 7 years old and got a new bicycle for Christmas, it meant everything! In fact, the meaning of work in my life at that time was quite simple — get good grades so I could get more toys. For me, meaning in life came from what I was given or received.\n",
      "As an adult, things changed. It was no longer about getting something; rather, it became about giving something. When I decided to\n",
      "llama_print_timings:        load time =  3348.76 ms\n",
      "llama_print_timings:      sample time =  1603.81 ms /   512 runs   (    3.13 ms per token,   319.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5325.89 ms /   265 tokens (   20.10 ms per token,    49.76 tokens per second)\n",
      "llama_print_timings:        eval time = 37810.05 ms /   510 runs   (   74.14 ms per token,    13.49 tokens per second)\n",
      "llama_print_timings:       total time = 44867.18 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329478\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1396b36b0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1396b57b0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1396b4c90\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1396b6230\n",
      "ggml_metal_init: loaded kernel_scale                          0x1396b6c60\n",
      "ggml_metal_init: loaded kernel_silu                           0x1396b74a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1396b5090\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1396b7db0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1396b8dc0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1396b9020\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1396ba370\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1396bb510\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1396ba990\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1396babf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1396bbe20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1396bc710\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1396bd190\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1396bd9b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1396be440\n",
      "ggml_metal_init: loaded kernel_norm                           0x1396bf540\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1396bf940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1396c0b40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1396c14e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1396c1f00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1396c2a10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1396c33a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1396c3d40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1396c4700\n",
      "ggml_metal_init: loaded kernel_rope                           0x1396c5190\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1396c5d70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1396c6c20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1396c7770\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1396bec00\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to make your own unique contribution, and that the only real satisfaction comes from giving yourself completely to some endeavor, be it work or love or charity or even sport.\n",
      "I believe in the brotherhood and sisterhood of man, not as a matter of words but as a matter of readiness to help one another. I do not believe that any man is better than anyone else. We are all different and we will always be so; and by acknowledging those differences we can make both our own lives and the world at large more colourful and rewarding.\n",
      "I believe that we should live life as fully, as intensely, and as humanely as possible. And I think we've got to do it together. We need each other: the only way we can be is in a couple or a family or some kind of community. We need to belong to something bigger than ourselves.\n",
      "I believe that the great gift we have is that we are all different and that our differences mean something, not merely that they make us more interesting and challenging but that they give each one of us special gifts to contribute to life. That's what I call humanism. It means that we should never try to dominate or control one another, but work out together the meaning of our lives on this planet.\n",
      "I believe it is not just a matter of living longer; it's a matter of living better, and loving more and making each moment count.\n",
      "I think it's important that we are all individuals but at the same time, we need to belong to something bigger than ourselves. We need to be part of a family, or community or in love with somebody. It's very hard for us to live life on our own as an individual. I believe in the brotherhood and sisterhood of man not as a matter of words but as a matter of readiness to help one another, no matter what their background, their race or their color.\n",
      "I believe we are put here on this planet to do something good and useful and if you're lucky enough to be able to do it then I think you should pay taxes towards the poor people who aren't doing anything useful and don't have the ability to do anything useful for themselves. I don't believe that anybody has got a God given right to be rich or any other sort of thing, but if they're lucky enough to get it then they should pay taxes towards the poor people who\n",
      "llama_print_timings:        load time =  2098.79 ms\n",
      "llama_print_timings:      sample time =  1518.46 ms /   512 runs   (    2.97 ms per token,   337.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8685.11 ms /   265 tokens (   32.77 ms per token,    30.51 tokens per second)\n",
      "llama_print_timings:        eval time = 18449.43 ms /   510 runs   (   36.18 ms per token,    27.64 tokens per second)\n",
      "llama_print_timings:       total time = 28777.23 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329509\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x14b72e2d0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x14b7303c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x14b72f8d0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x14b730e50\n",
      "ggml_metal_init: loaded kernel_scale                          0x14b731880\n",
      "ggml_metal_init: loaded kernel_silu                           0x14b7320d0\n",
      "ggml_metal_init: loaded kernel_relu                           0x14b72fcd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14b732a00\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14b733920\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14b733d20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14b734e60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14b736150\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14b7355d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14b735830\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14b736a20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14b737360\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14b737ca0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14b7385e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14b7390c0\n",
      "ggml_metal_init: loaded kernel_norm                           0x14b73a1e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14b73a5e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14b73b790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14b73c150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14b73cb40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14b73d660\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x14b73e120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x14b73e980\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x14b73f380\n",
      "ggml_metal_init: loaded kernel_rope                           0x14b73fde0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x14b740980\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14b740be0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14b741a00\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14b739a00\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living. As you take action, you find out who you are and what your purpose is.\n",
      "I am a firm believer in being yourself and being proud of it.\n",
      "The most important thing about global warming is this: Even if the threat of catastrophe were removed from our midst, we would still need to deal with the crisis of global poverty, hunger, disease, and environmental degradation. And we need to do so in a way that doesn’t harm those who are least responsible for causing the problems.\n",
      "I’m not against the war on terrorism; I just don’t think it should be fought with terrorism.\n",
      "People often ask me if I’ve had my share of rejection and setbacks. Well, no more than anyone else. But I did learn early on that I could use those things to motivate me.\n",
      "I think that if you live long enough, you see that so many of the greatest discoveries were the result of accidents or chance happenings or just dumb luck – by people not particularly seeking them but who just happened upon them while looking for something else entirely different.\n",
      "What we need is a revolution in consciousness. We need to know that God loves all of creation, and if you have more love in your heart than hate, then it’s going to be a better world.\n",
      "I think the Bible teaches us how to live, but not how to live forever.\n",
      "The only way I know how to get things done is to do them myself. And I don’t need other people to tell me if they’re good or bad – I decide that.\n",
      "You have to believe in yourself when no one else does – that makes you a winner right there.\n",
      "When I was young, we had very little money. But this did not really bother my parents. They believed in education and hard work.\n",
      "I always felt like the most important thing is to love yourself for who you are. And I’ve never had a problem with that.\n",
      "I want to make sure the next generation has every opportunity to live up to their God-given potential. That means reforming our education system so it works for kids. It means making college affordable for all.\n",
      "You have to remember that we are trying to get away from the very things we were brought up with: injustice, inequality and lack of opportunity for growth.\n",
      "I was raised\n",
      "llama_print_timings:        load time =  1555.35 ms\n",
      "llama_print_timings:      sample time =  1428.23 ms /   512 runs   (    2.79 ms per token,   358.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8796.32 ms /   265 tokens (   33.19 ms per token,    30.13 tokens per second)\n",
      "llama_print_timings:        eval time = 18378.68 ms /   510 runs   (   36.04 ms per token,    27.75 tokens per second)\n",
      "llama_print_timings:       total time = 28725.63 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329540\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12521e7e0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1252208e0\n",
      "ggml_metal_init: loaded kernel_mul                            0x12521fdc0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x125221360\n",
      "ggml_metal_init: loaded kernel_scale                          0x125221d90\n",
      "ggml_metal_init: loaded kernel_silu                           0x1252225b0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1252201c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x125222ee0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x125223e10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x125225180\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1252253e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x125224690\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x125226790\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x125225c10\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x125226ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x125227800\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x125228280\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x125228a80\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x125229560\n",
      "ggml_metal_init: loaded kernel_norm                           0x12522a7c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12522b420\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12522bd80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12522c740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12522d150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12522dc50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12522e580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12522edd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12522f7a0\n",
      "ggml_metal_init: loaded kernel_rope                           0x125230270\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x125230df0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x125231c70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1252327f0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x125233000\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. I think that's what everyone really wants, no matter who they are.\n",
      "- Bill Murray, American actor born 1950\n",
      "I am the happiest man alive as a king or subject, for I have a heart that can receive all good, and return none but innocent smiles. And this is the crown of my life!\n",
      "Happiness consists more in conveniences of pleasure than in positiveness of enjoyment.\n",
      "- William Hazlitt, English essayist born 1778\n",
      "- Henry James, American novelist born 1843\n",
      "The only way to be happy is to love. Unless you love, your life will flash by.\n",
      "Happiness exists on earth before it exists in heaven.\n",
      "No one can make you feel inferior without your consent.\n",
      "- Eleanor Roosevelt, American politician and diplomat born 1884\n",
      "One of the most tragic things I know about human nature is that all of us tend to put off living. We are all dreaming of some magical rose garden over the horizon instead of enjoying the roses that are blooming outside our windows today.\n",
      "- Dale Carnegie, American writer and lecturer born 1888\n",
      "- Willa Cather, American novelist born 1873\n",
      "Happiness is a perfume you cannot pour on others without getting some on yourself.\n",
      "The art of being happy lies in the power of extracting happiness from common things.\n",
      "- Henry Ward Beecher, American clergyman born 1813\n",
      "Happiness is not having what you want but wanting what you have.\n",
      "- Anonymous\n",
      "There are two ways to be rich: One is to have all you want, the other is to enjoy all you have.\n",
      "The key to happiness? Become addicted to continual and never-ending improvement.\n",
      "- Anthony Robbins, American author born 1960\n",
      "If you don't like something change it; if you can't change it, change your attitude.\n",
      "- Maya Angelou, African-American poet born 1928\n",
      "It is not happy people who are thankful, but thankful people who are happy.\n",
      "The happiest moments of my life have been the few which I have passed at home in the bosom of my family.\n",
      "- Thomas Jefferson, American founding father\n",
      "llama_print_timings:        load time =  1529.42 ms\n",
      "llama_print_timings:      sample time =  1530.33 ms /   512 runs   (    2.99 ms per token,   334.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8562.19 ms /   265 tokens (   32.31 ms per token,    30.95 tokens per second)\n",
      "llama_print_timings:        eval time = 18463.72 ms /   510 runs   (   36.20 ms per token,    27.62 tokens per second)\n",
      "llama_print_timings:       total time = 28681.16 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329570\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329585\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to explore, learn and share with others.\n",
      "I believe that everyone has a calling inside themselves that they must follow. That thing that you are so passionate about is your calling. The only problem is that many people choose not to do it for one reason or another. I was lucky enough to find out what my calling was at an early age. I wanted to be a professional baseball player. I had the talent, I worked hard and I achieved that goal.\n",
      "I played professional baseball for 15 years. The first 10 I played in Japan then I came back to play the last five in the U.S. I’ve been retired since 2009 but one thing hasn’t changed. I still think about my calling.\n",
      "My passion is helping others. It doesn’t have to be baseball or even sports, it can be anything that you are passionate about. Helping others is such a rewarding feeling for me that I want people to experience the same joy that I do when they find what they are meant to do in life.\n",
      "I have dedicated my life to helping others. I’ve built up two schools and an orphanage in Kenya where I help children get off the street, educate them and give them a future. I’m also starting a non-profit in the U.S., as well as building an after school program for at risk kids.\n",
      "In the meantime, I work with my friend, Michael Bourn, on a website called BDJ Sports. We started the site because we wanted to create something that helped people get their mind off things and laugh every once in a while. It’s been a fun experience working with Mike and we have some cool stuff coming so make sure you check it out!\n",
      "I am also proud to be part of the Nike Sportswear team. I love being able to share my knowledge and passion for baseball with kids who want to learn how to play this great game. It’s funny, many people think that as a professional athlete I’m rich, but honestly I don’t have much money at all. The one thing that is priceless though is the opportunity to help others and that’s exactly what I get to do with Nike.\n",
      "I believe that it’s never too late for you to find your calling in life. It can be a hobby, business or whatever—something that makes you happy when\n",
      "llama_print_timings:        load time = 20723.30 ms\n",
      "llama_print_timings:      sample time =   364.74 ms /   512 runs   (    0.71 ms per token,  1403.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13444.64 ms /   265 tokens (   50.73 ms per token,    19.71 tokens per second)\n",
      "llama_print_timings:        eval time = 114078.62 ms /   510 runs   (  223.68 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time = 127936.19 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329734\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find a reason to live.\n",
      "Written by John Hurt (1940–2017)\n",
      "View a Detailed Biography of John Hurt\n",
      "View all 18 quotes by John Hurt\n",
      "Other People's Lives, p. 52\n",
      "Life has a way of sneaking up on you and biting you in the ass.\n",
      "View more quotes by John Hurt\n",
      "He was one of those people with an air of authority that always made me want to do exactly what they told me not to.\n",
      "I'm going to miss you, darling, but I don't know how long for… I just have a feeling about this one. I think it could be the big C. We'll see.\n",
      "View more quotes by John Hurt from The Elephant Man\n",
      "View more quotes by John Hurt (1940–2017)\n",
      "Life is hard, death is harder. Let's hope it's nothing too dreadful\n",
      "View more quotes by John Hurt from Indiana Jones and the Kingdom of the Crystal Skull (2008)\n",
      "I am a soldier in an army with a million men. I can fight but I cannot make decisions on my own.\n",
      "View more quotes by John Hurt from The Lord of the Rings: The Return of the King (2003)\n",
      "It's very strange, isn't it? All that time we spent together, you never told me your name. Why was that?\n",
      "I don't know! I guess because there wasn't much point. It didn't seem to matter who I was, only what I might become.\n",
      "View more quotes by John Hurt from V for Vendetta (2005)\n",
      "I'm afraid the next one will be worse.\n",
      "View more quotes by John Hurt from Alien (1979)\n",
      "We're all very lucky to have found each other, but we've got to go back now. I know it's scary... but if you could see what I've seen, and felt what I've felt, like I do, you'd be saying the same things! We'll miss you, we need you. But you've gotta do this. You don't have a choice\n",
      "View more quotes by John Hurt from Hellboy (2004)\n",
      "\n",
      "llama_print_timings:        load time = 20914.39 ms\n",
      "llama_print_timings:      sample time =   363.45 ms /   512 runs   (    0.71 ms per token,  1408.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13191.40 ms /   265 tokens (   49.78 ms per token,    20.09 tokens per second)\n",
      "llama_print_timings:        eval time = 114438.99 ms /   510 runs   (  224.39 ms per token,     4.46 tokens per second)\n",
      "llama_print_timings:       total time = 128042.03 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9469f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691329883\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.- Pablo Picasso\n",
      "Giving the gift of Life is the ultimate act of kindness and generosity! We are all born with a unique set of gifts that we can share with others. By giving the gift of life, we are able to provide someone else an opportunity to use their gifts for the betterment of humanity.\n",
      "Our purpose in life is to make our lives worthwhile by sharing our gifts and talents. Throughout my career I have had the pleasure of helping many people discover and develop their gifts. This process has provided me with great joy, satisfaction and fulfillment!\n",
      "I invite you to take a look at your own skills, knowledge and experiences that may benefit others. If you are interested in sharing this information in an online format, I would love to help you share your gift with the world.\n",
      "Gifts and talents can be shared in many different formats:\n",
      "Online Training/Courses\n",
      "Please contact me if you have any questions or would like more information about how you can begin sharing your unique gifts with others!\n",
      "Sarah D. McFadden, M.A., CLC\n",
      "Founder and CEO of The Lifeline Center\n",
      "Speaker, Educator & Author\n",
      "Certified Life Coach (CLC)\n",
      "Master's Degree in Counseling Psychology\n",
      "\"The greatest gift you can give is the gift of your self-esteem, because when you have it, everything else will fall into place.\" -Sarah Ban Breathnach\n",
      "\"Your work is to discover your world and then with all your heart give yourself to it.\" –Buddha\n",
      "\"Being deeply loved by someone gives you strength, while loving someone deeply gives you courage.\"-Lao Tzu\n",
      "“The purpose of our lives is to be happy.” - Dalai Lama\n",
      "\"If you want others to be happy, practice compassion. If you want to be happy, practice compassion.\" - Dalai Lama\n",
      "​\"A human being is a part of the whole called by us universe, a part limited in time and space. He experiences himself, his thoughts, and feelings as something separated from the rest, a kind of optical delusion of his consciousness. This delusion is a prison for us, restricting us to our personal desires and to affection for a few\n",
      "llama_print_timings:        load time = 17575.70 ms\n",
      "llama_print_timings:      sample time =   363.16 ms /   512 runs   (    0.71 ms per token,  1409.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9961.18 ms /   265 tokens (   37.59 ms per token,    26.60 tokens per second)\n",
      "llama_print_timings:        eval time = 113986.11 ms /   510 runs   (  223.50 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time = 124358.71 ms\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691330025\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.06 MB\n",
      "llama_model_load_internal: mem required  = 17993.06 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x140110fd0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x12f693a50\n",
      "ggml_metal_init: loaded kernel_mul                            0x12f693300\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12f6949a0\n",
      "ggml_metal_init: loaded kernel_scale                          0x12f6953d0\n",
      "ggml_metal_init: loaded kernel_silu                           0x12f695c10\n",
      "ggml_metal_init: loaded kernel_relu                           0x12f694130\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12f696490\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x140112830\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1401138b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x140114190\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1401131c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1401148d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12f6977d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12f697cf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12f6985a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12f698f00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12f699160\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12f69a120\n",
      "ggml_metal_init: loaded kernel_norm                           0x12f69ac60\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12f69b060\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12f69c1f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12f69cbc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12f69d590\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12f69e0f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12f69eaa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12f69f470\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12f6a09c0\n",
      "ggml_metal_init: loaded kernel_rope                           0x12f69fe10\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12f6a0070\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12f6a1730\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12f6a25b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12f6a3150\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1287.72 MB, offs =  17005117440, (17672.17 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (17688.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18470.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18686.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18942.34 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and healthy.\n",
      "I believe that everyone should have a chance at being happy in their own way.\n",
      "I believe you can find happiness within yourself, no one else can give it to you, and you can’t buy it.\n",
      "I believe sometimes people are not completely honest about what they think or feel.\n",
      "I believe we all have the ability to create our world for ourselves but that we should also take the time to help others around us that might need it.\n",
      "I believe happiness is a choice; however, some people choose to be unhappy and miserable. I think this is a terrible choice.\n",
      "I believe everyone has something they can teach me, though not everything I learn from them is good. I try to focus on the positive aspects of each person’s character.\n",
      "I believe we are all made up of our experiences in life; however, I also feel we are responsible for what we put into life and how we react to it.\n",
      "I believe everyone should be able to go after their dreams as long as they don’t hurt anyone.\n",
      "I believe we can change the world if we work together.\n",
      "Written by: Dani on October 15, 2012.\n",
      "This entry was posted in Belief, Dreams, Happiness, Life, Meaning of life and tagged belief, dreams, happiness, learning, life, meaning of life, teaching. Bookmark the permalink.\n",
      "← What I believe….\n",
      "What I believe…. →\n",
      "3 thoughts on “I believe….”\n",
      "Dani | October 15, 2012 at 6:59 pm\n",
      "Thanks! I appreciate your comment\n",
      "Matt Meadows | October 15, 2012 at 8:54 am\n",
      "This is a really inspiring piece and something that we can all learn from. Thank you for sharing.\n",
      "Dani | October 15, 2012 at 6:49 am\n",
      "I think it’s important to be reminded of all the good things in life when we are feeling down or sad. Hopefully my words will help others feel better about themselves and their lives as well. Thanks for your comment! Dani\n",
      "You have a very positive attitude towards life, which is something that I think everyone needs. I especially like what you say about people being able to teach us, regardless of whether they are older or\n",
      "llama_print_timings:        load time =  6185.89 ms\n",
      "llama_print_timings:      sample time =  1695.48 ms /   512 runs   (    3.31 ms per token,   301.98 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21388.00 ms /   265 tokens (   80.71 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:        eval time = 40815.16 ms /   510 runs   (   80.03 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:       total time = 64035.24 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691330096\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.06 MB\n",
      "llama_model_load_internal: mem required  = 17993.06 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13ee5c440\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10ee46880\n",
      "ggml_metal_init: loaded kernel_mul                            0x10ee46d70\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10ee477e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10ee48120\n",
      "ggml_metal_init: loaded kernel_silu                           0x10ee48a80\n",
      "ggml_metal_init: loaded kernel_relu                           0x10ee49330\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10ee49c10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10ee4a5a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10ee4ae60\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10ee50530\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10ee52690\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10ee51c70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10ee51ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10ee530d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10ee53a10\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10ee54350\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10ee54c90\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10ee55d40\n",
      "ggml_metal_init: loaded kernel_norm                           0x10ee566d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10ee56ad0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10ee57ca0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10ee586a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10ee59070\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10ee59b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10ee5a4c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10ee5aea0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10ee5b870\n",
      "ggml_metal_init: loaded kernel_rope                           0x10ee5c380\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10ee5cf00\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10ee5dd90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10ee5e910\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10ee55fa0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1287.72 MB, offs =  17005117440, (17672.17 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (17688.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18470.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18686.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18942.34 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to create your own meaning.\n",
      "This week, I’ve been listening to a free online course called “How To Find Your Purpose In Life”, by Mark Manson. He believes that finding your purpose in life isn’t all that hard, but he also believes it requires a few difficult decisions and actions. His ideas have made me think more deeply about what my own purpose is, so I decided to write some of those thoughts down here.\n",
      "The question “What is the meaning of life?” can be interpreted in two different ways; there’s what you want your life to mean (and how you will create that meaning), and then there’s the concept of an objective meaning behind all of existence. I think it makes more sense to take the first approach, because if there is a single objective meaning of life, we’ll never know it – in fact, Manson argues that trying to find this objective meaning will lead you astray from your true purpose.\n",
      "I believe the meaning of my life is to create meaning.\n",
      "This week I also read “The Life-Changing Magic Of Tidying Up” by Marie Kondo. In her book she encourages the reader to make a decision about what items in their home “spark joy”, and discard everything else. This concept really resonated with me, as I have been feeling frustrated lately with the clutter that has built up over the years in my house.\n",
      "Manson argues that finding your purpose requires you to make difficult decisions about what matters to you; Kondo argues that discarding things you don’t need allows you to focus on the things that matter. This is similar to an exercise I did a few months ago, where I made a list of 100 things that spark joy for me.\n",
      "I believe the meaning of my life is to create my own meaning. This will be accomplished in part by focusing on the items and activities that spark joy for me, discarding everything else, and making difficult decisions about what matters most to me.\n",
      "What is your purpose? How do you find it? What does this purpose require of you?\n",
      "This entry was posted in Personal Development and tagged life, meaning, philosophy on April 27, 2015 by will.\n",
      "← The Meaning Of Life The Meaning Of My Life →\n",
      "4 thoughts on “The Meaning Of Life”\n",
      "Pingback: The Meaning Of My\n",
      "llama_print_timings:        load time =  6437.06 ms\n",
      "llama_print_timings:      sample time =  1635.83 ms /   512 runs   (    3.19 ms per token,   312.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21333.83 ms /   265 tokens (   80.51 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:        eval time = 40898.99 ms /   510 runs   (   80.19 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time = 64000.62 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691330167\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.06 MB\n",
      "llama_model_load_internal: mem required  = 17993.06 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12e681cc0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x12e683db0\n",
      "ggml_metal_init: loaded kernel_mul                            0x12e6832c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12e684840\n",
      "ggml_metal_init: loaded kernel_scale                          0x12e685270\n",
      "ggml_metal_init: loaded kernel_silu                           0x12e685aa0\n",
      "ggml_metal_init: loaded kernel_relu                           0x12e6836c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12e6863b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12e687330\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12e688670\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12e6888d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12e689b40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12e689120\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12e689380\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12e68a5b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12e68aef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12e68b830\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12e68c170\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12e68d220\n",
      "ggml_metal_init: loaded kernel_norm                           0x12e68dd30\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12e68e9c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12e68f350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12e68fd40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12e690730\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12e6912a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12e6923f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12e692dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12e6938b0\n",
      "ggml_metal_init: loaded kernel_rope                           0x12e694060\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12e694f50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12e695ad0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12e696650\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12e696f60\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 16384.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1287.72 MB, offs =  17005117440, (17672.17 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (17688.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18470.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18686.34 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18942.34 / 21845.34)\n",
      "\n",
      "system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, and you can't just wait around for things to make you happy.\n",
      "—ALEC BALDWIN\n",
      "Happiness is an inside job.\n",
      "—WILLIE NELSON\n",
      "I have found that if you love life, life will love you back.\n",
      "—ARTHUR RUBINSTEIN\n",
      "HAPPINESS IS THE STATE OF MIND THAT PRECEDES every thought and action. It is the prerequisite of everything we want to attain or accomplish in life. Without happiness, our journey can become tedious and meaningless. But with it, anything is possible. Happiness allows us to live our dreams every day.\n",
      "The more we understand what makes us happy, the easier it becomes to achieve and maintain happiness on a consistent basis. If you are not consistently happy, then something must be out of balance. As you read this book, you will begin to recognize ways in which your thinking has been preventing you from experiencing sustainable happiness. You will come away with tools that can be used daily to create and maintain a state of happiness regardless of what is occurring on the outside.\n",
      "In our search for happiness we often look in all the wrong places, believing it is something \"out there\" or someone else who needs to change. We make ourselves miserable trying to find happiness externally when it can only be found within. If you are not happy with yourself, no one else can make you happy either. But if you are happy, then everyone around you benefits.\n",
      "Many people believe that life is supposed to be difficult and full of painful experiences. That's what they were taught by their parents or the people who raised them. They think that they have to suffer in order to grow as human beings. But I don't believe this is true. I believe it's possible to be happy without having to experience pain and suffering. In fact, you will find that your life becomes easier when you are happy. You begin to attract people and events into your life that support your happiness, rather than detract from it.\n",
      "Happiness creates an inner vibration that is very magnetic. It draws the right kind of experiences into our lives. If we don't get what we want in one way, we will in another way. We are always supported by a higher power to find happiness and fulfillment. When we stop trying so hard to be happy\n",
      "llama_print_timings:        load time =  6994.49 ms\n",
      "llama_print_timings:      sample time =  1160.46 ms /   512 runs   (    2.27 ms per token,   441.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21526.93 ms /   265 tokens (   81.23 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:        eval time = 40569.19 ms /   510 runs   (   79.55 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time = 63378.52 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d974327",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691330238\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.74 MB\n",
      "llama_model_load_internal: mem required  = 62533.74 MB (+  780.00 MB per state)\n"
     ]
    }
   ],
   "source": [
    "# try cpu\n",
    "!./main --color --no-mmap --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
