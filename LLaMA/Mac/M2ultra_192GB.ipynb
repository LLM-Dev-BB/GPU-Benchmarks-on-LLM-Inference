{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                     \u001b[1m\u001b[36m70B-v2\u001b[m\u001b[m                  \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m13B-v2\u001b[m\u001b[m                  \u001b[30m\u001b[43m7B\u001b[m\u001b[m                      \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                     \u001b[1m\u001b[36m7B-v2\u001b[m\u001b[m\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                     ggml-vocab.bin\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:   -framework Accelerate\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-metal.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "k_quants.o\n",
      "llama.o\n",
      "libembdinput.so\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "server\n",
      "simple\n",
      "vdot\n",
      "train-text-from-scratch\n",
      "embd-input-test\n",
      "build-info.h\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Darwin\n",
      "I UNAME_P:  arm\n",
      "I UNAME_M:  arm64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c llama.cpp -o llama.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/common.cpp -o common.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/console.cpp -o console.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -c examples/grammar-parser.cpp -o grammar-parser.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c -o k_quants.o k_quants.c\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml-alloc.c -o ggml-alloc.o\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-metal.o ggml-alloc.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-metal.o ggml-alloc.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-metal.o ggml-alloc.o -o train-text-from-scratch  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o simple  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o server  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders \n",
      "c++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o libembdinput.so  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders\n",
      "c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_METAL examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-metal.o ggml-alloc.o -o embd-input-test  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297405\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x120f06500\n",
      "ggml_metal_init: loaded kernel_add_row                        0x120f08be0\n",
      "ggml_metal_init: loaded kernel_mul                            0x120f08ea0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x120f096b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x120f0a150\n",
      "ggml_metal_init: loaded kernel_silu                           0x120f0a9d0\n",
      "ggml_metal_init: loaded kernel_relu                           0x120f08360\n",
      "ggml_metal_init: loaded kernel_gelu                           0x120f0b190\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x120f0bc70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x120f0c4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x120f0cba0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x120f0da20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x120f0ee70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x120e7f920\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x120e80730\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x120e80cf0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x120e816d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x120e82050\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x120e82a30\n",
      "ggml_metal_init: loaded kernel_norm                           0x120e83bf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x120f0fb20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x120f0ff20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x120f109f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x120f11420\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x120f11f80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x120f12ab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x120f13350\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x120f13d60\n",
      "ggml_metal_init: loaded kernel_rope                           0x120f13fc0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x120f14a40\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x120f15640\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x120f16500\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x120f170b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to understand, and to love.\n",
      "In a world where the meaning of life has become increasingly unclear, there’s no better place than the classroom to find the answers to this age-old question.\n",
      "Throughout our time at school, we are provided with countless opportunities to not only learn about the things that have shaped our society today — but more importantly, to learn about ourselves and how we fit into it all.\n",
      "We are given the chance to look outwards and see the bigger picture, and to also explore the world within us — our personal values and beliefs, which may or may not be shared by those around us.\n",
      "Throughout my time at school, I’ve realised that it’s only through understanding ourselves, our peers and society as a whole that we can truly find meaning in life.\n",
      "We have been given an opportunity to create the world of tomorrow, for ourselves and others — to better understand who we are and how we fit into this great big world — and most importantly, to make a change.\n",
      "I believe the meaning of life is to live it well.\n",
      "In a world where the meaning of life has become increasingly unclear, there’s no better place than the classroom to find answers to this age-old question. Throughout our time at school, we are provided with countless opportunities to not only learn about the things that have shaped our society today – but more importantly, to learn about ourselves and how we fit into it all. We are given the chance to look outwards and see the bigger picture, and to also explore the world within us — our personal values and beliefs, which may or may not be shared by those around us. Throughout my time at school, I’ve realised that it’s only through understanding ourselves, our peers and society as a whole that we can truly find meaning in life. We have been given an opportunity to create the world of tomorrow, for ourselves and others — to better understand who we are and how we fit into this great big world — and most importantly, to make a change. I believe the meaning of life is to live it well.\n",
      "I believe the meaning of life to be to love and to learn, but not necessarily in that order. In a world where people struggle to find their purpose and meaning beyond their work and home lives, there’s no better place than the classroom to find answers to\n",
      "llama_print_timings:        load time =   969.05 ms\n",
      "llama_print_timings:      sample time =   335.06 ms /   512 runs   (    0.65 ms per token,  1528.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3534.37 ms /   265 tokens (   13.34 ms per token,    74.98 tokens per second)\n",
      "llama_print_timings:        eval time =  6026.85 ms /   510 runs   (   11.82 ms per token,    84.62 tokens per second)\n",
      "llama_print_timings:       total time =  9938.64 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef52530a-c041-4855-8d88-a20e63dff79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297416\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13ea8bbb0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13ea8ddd0\n",
      "ggml_metal_init: loaded kernel_mul                            0x13ea8e080\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13ea8e8c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13ea8f360\n",
      "ggml_metal_init: loaded kernel_silu                           0x13ea8fba0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13ea8d550\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13ea90370\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13ea90e00\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13ea928f0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13ea91e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13ea91900\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13ea94060\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13ea934a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13ea947d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13ea952a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13ea95c20\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13ea96580\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13ea96f40\n",
      "ggml_metal_init: loaded kernel_norm                           0x13ea98100\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13ea98b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13ea99770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13ea9a1a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13ea9abb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13ea9b740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13ea9c270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13ea9cae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13ea9cf00\n",
      "ggml_metal_init: loaded kernel_rope                           0x13ea9e040\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13ea9e870\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13ea9f770\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13eaa0360\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13ea97720\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose and live it. It’s not about what you do; it’s about why you do it.\n",
      "If you want to feel more confident in yourself, take some time to consider why you are doing what you do. The reason that you get out of bed every morning, the thing that sets you on fire, is your purpose and finding it will give you life.\n",
      "A lot of people live their lives as if they don’t have a purpose. They just go through the motions, doing the stuff that they think other people want them to do without really thinking about whether or not it’s what they want to be doing. Their lives are not meaningful. And life is meant to be lived with meaning and purpose.\n",
      "If you don’t know why you are doing something, chances are you’re not going to like it much. It’s just a matter of time before you find yourself getting bored or fed up with the thing that you thought you wanted to do but really doesn’t suit your true passions and desires.\n",
      "I’m a big believer in finding your purpose, in life. I also believe that when you discover what it is you are meant to do, you will never regret getting out of bed every day. You will find yourself looking forward to the things that you have to do because they all tie into your purpose and help shape who you are and who you want to become.\n",
      "I’m a firm believer in purpose-driven living. I think it’s what life is about, finding out why you are here and how you can contribute to making this world a better place for everyone.\n",
      "In my opinion, there is no such thing as a pointless or meaningless job. There are just jobs that don’t suit your true purpose. We all have talents; it’s up to us to discover what those are so we can bring them to life and use them in the service of others.\n",
      "This is how you find your purpose. You need to stop thinking about yourself for a moment and instead think about other people, other beings – especially those around you that matter most to you. What do they want? Do they have dreams? Are there ways that you can help them fulfill their desires and reach their goals?\n",
      "When we look at the world from this perspective, it becomes a lot easier to find our true purpose because we realize that everyone else has one too.\n",
      "llama_print_timings:        load time =   549.40 ms\n",
      "llama_print_timings:      sample time =   317.94 ms /   512 runs   (    0.62 ms per token,  1610.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3523.46 ms /   265 tokens (   13.30 ms per token,    75.21 tokens per second)\n",
      "llama_print_timings:        eval time =  6016.55 ms /   510 runs   (   11.80 ms per token,    84.77 tokens per second)\n",
      "llama_print_timings:       total time =  9897.78 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de933c0-53d0-4fd4-9b98-724d2ebbd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297427\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 3647.96 MB\n",
      "llama_model_load_internal: mem required  = 3949.96 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100a26a50\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100a28c20\n",
      "ggml_metal_init: loaded kernel_mul                            0x100a28ec0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100a296e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x100a2a180\n",
      "ggml_metal_init: loaded kernel_silu                           0x100a2a9f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x100a283f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100a2b370\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100a2c320\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100a2c740\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100a2d920\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100a2cc00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100a2ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100a2e280\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100a2f760\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100a30110\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100a30a90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100a31410\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100a31ef0\n",
      "ggml_metal_init: loaded kernel_norm                           0x100a32ef0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100a33d40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100a34710\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100a35130\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100a35b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100a366e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100a37090\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100a378e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100a382f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x100a38dd0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100a39030\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100a39bb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100a3aa90\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100a326d0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.97 MB, ( 3648.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3658.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, ( 3916.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, ( 4048.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 4208.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift and put it in the service of mankind.\n",
      "I will do what I can, with what I have, where I am.\n",
      "I will make a difference!\n",
      "The mission of the Gifted Academy is to provide an academically rigorous education that promotes the development of our students' character, leadership skills and academic excellence in order to prepare them for success at college and beyond. We recognize that each child has different learning needs and we offer a variety of learning options to meet those needs.\n",
      "The Gifted Academy is a school designed for learners who demonstrate giftedness in one or more of the following areas: creativity, leadership, and/or intelligence. Our mission is to provide an academically rigorous education that promotes the development of our students' character, leadership skills, and academic excellence in order to prepare them for success at college and beyond. We recognize that each child has different learning needs; therefore, we offer a variety of learning options to meet those needs.\n",
      "The Gifted Academy is a school designed for learners who demonstrate giftedness in one or more of the following areas: creativity, leadership, and/or intelligence. Our mission is to provide an academically rigorous education that promotes the development of our students' character, leadership skills, and academic excellence in order to prepare them for success at college and beyond. We recognize that each child has different learning needs; therefore, we offer a variety of learning options to meet those needs.\n",
      "The Gifted Academy is one of only two public charter schools in Arizona that are specifically designed to serve gifted students. Our school was approved by the Arizona State Board for Charter Schools on March 31, 2015 and opened its doors to students for the first time on August 17, 2015. We serve kindergarten through eighth grade students who are currently enrolled in public schools within Maricopa County.\n",
      "The Gifted Academy is one of only two public charter schools in Arizona that are specifically designed to serve gifted students. Our school was approved by the Arizona State Board for Charter Schools on March 31, 2015 and opened its doors to students for the first time on August 17, 2015. We currently serve grades K-8 who are currently enrolled in public schools within Maricopa County.\n",
      "Our school is\n",
      "llama_print_timings:        load time =   563.41 ms\n",
      "llama_print_timings:      sample time =   317.42 ms /   512 runs   (    0.62 ms per token,  1613.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3520.69 ms /   265 tokens (   13.29 ms per token,    75.27 tokens per second)\n",
      "llama_print_timings:        eval time =  5992.06 ms /   510 runs   (   11.75 ms per token,    85.11 tokens per second)\n",
      "llama_print_timings:       total time =  9870.42 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297437\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x134e37640\n",
      "ggml_metal_init: loaded kernel_add_row                        0x134e39810\n",
      "ggml_metal_init: loaded kernel_mul                            0x134e39ac0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x134e3a2e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x134e3ad50\n",
      "ggml_metal_init: loaded kernel_silu                           0x134e3b5f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x134e39080\n",
      "ggml_metal_init: loaded kernel_gelu                           0x134e3bf10\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x134e3cff0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x134e3e360\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x134e3e5c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x134e3f8c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x134e3fb20\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x134e3ef00\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x134e40300\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x134e40c80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x134e415e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x134e41f60\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x134e42920\n",
      "ggml_metal_init: loaded kernel_norm                           0x134e43ac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x134e44550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x134e44b10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x134e45490\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x134e45ea0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x134e468d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x134e47a90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x134e48470\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x134e48e80\n",
      "ggml_metal_init: loaded kernel_rope                           0x134e490e0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x134e49bb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x134e4a780\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x134e4b670\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x134e43100\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out who you are and to be happy with that person. That's what I try to do, at least.\n",
      "I think my parents taught me a lot about being open-minded, inquisitive, and curious - they made sure we had a well-rounded upbringing without any prejudices or bigotry. It was very important for them that we learned to understand other cultures and be able to respect each individual as an equal.\n",
      "I think the whole world is going through this phase of being more open, where people aren't afraid of expressing themselves anymore.\n",
      "I love the idea that the body is a sculpture in its own right - all muscles are made up of fibres, and the brain has to work out how to coordinate them all at once! I also think it's really cool that we can live without food for 40 days.\n",
      "I enjoy reading books on philosophy, history, psychology, myths, spirituality, art, fashion, and travel. I often find myself going down these weird rabbit holes of researching interesting facts - which is where I get my ideas from!\n",
      "I do like to have fun and am very playful in everyday life; I love to make people laugh, dance, sing, and enjoy the simple pleasures of being alive.\n",
      "When I'm not working, you'll probably find me hiking with my dog, exploring new places, or reading a good book.\n",
      "I think a lot of people are afraid to express themselves because they don't want to be judged. I feel that it is important for everyone to have the confidence and courage to be who they truly are - to never give up on their dreams!\n",
      "If you do what you love, then you'll be successful... even if your work doesn't pay off financially at first, you will find the happiness within yourself. And in my experience, it always pays off in the end!\n",
      "I am a big proponent of body positivity - I think we need to stop judging ourselves and others as \"fat\" or \"skinny\", but rather see people for who they truly are on the inside. The same goes with race, gender, sexuality, religion, etc... we should all be more accepting and tolerant, instead of being prejudiced against others.\n",
      "I have always been a very ambitious person; I love to set goals for\n",
      "llama_print_timings:        load time =  2979.96 ms\n",
      "llama_print_timings:      sample time =   317.82 ms /   512 runs   (    0.62 ms per token,  1610.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3590.22 ms /   265 tokens (   13.55 ms per token,    73.81 tokens per second)\n",
      "llama_print_timings:        eval time = 18715.35 ms /   510 runs   (   36.70 ms per token,    27.25 tokens per second)\n",
      "llama_print_timings:       total time = 22663.29 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297463\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x112626570\n",
      "ggml_metal_init: loaded kernel_add_row                        0x112628790\n",
      "ggml_metal_init: loaded kernel_mul                            0x112628a40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x112629280\n",
      "ggml_metal_init: loaded kernel_scale                          0x112629d20\n",
      "ggml_metal_init: loaded kernel_silu                           0x11262a560\n",
      "ggml_metal_init: loaded kernel_relu                           0x112627f10\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11262ad30\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11262b7c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11262d2b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11262c7c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11262c2c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11262ea20\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11262de60\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11262f190\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11262fc60\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1126305e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x112630f40\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x112631900\n",
      "ggml_metal_init: loaded kernel_norm                           0x112632ac0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x112633550\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x112634130\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x112634b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x112635570\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x112636100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x112636c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1126374a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1126378c0\n",
      "ggml_metal_init: loaded kernel_rope                           0x112638a00\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x112639230\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11263a130\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11263ad20\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1126320e0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find happiness and fulfillment within yourself.\n",
      "I am a person who enjoys the simple things in life, I enjoy cooking for my family and friends. I also love to have fun, go dancing, travel, meet new people, explore new places. I believe in the saying \"Life is too short, so live it up\"! I'm passionate about what I do and take pride of myself in being creative.\n",
      "I am looking for a man who is honest and loyal. A man with a good heart, someone who makes me laugh and who enjoys life as much as I do. Someone who is not afraid to show his feelings and most importantly, I'm looking for my best friend who will be there for me through thick and thin!\n",
      "I am really interested in music, movies, food, travelling and meeting new people.\n",
      "Những tr��� mới tình yêu, những người đang thau thế về hơn 100 qua vụt không hết đứa đây. Những vợ chồng s��� tìm ra bài viết n��i tiếng ngay trên VietNamNet.\n",
      "I love the simple life, cooking at home and eating in with family and friends. I am very romantic and I love to travel. I have a good sense of humor and like to go out with my friends. I'm very easy going and get along well with people from all walks of life.\n",
      "I am looking for someone who is sincere, honest and caring. Who enjoys the simple things in life such as cooking at home, having a few friends over, traveling and taking photos. A man who can put a smile on my face when I'm down, make me laugh and have fun with. A man who is passionate about what he does and takes pride of himself for it. Most importantly, I want to meet someone who will be there for me through thick and thin!\n",
      "A person who likes to travel as much as I do, a good sense of humor, honest, caring and sincere. A man with a good heart that enjoys life as much as I do and is looking for the same things in life\n",
      "llama_print_timings:        load time =  1822.27 ms\n",
      "llama_print_timings:      sample time =   319.10 ms /   512 runs   (    0.62 ms per token,  1604.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3618.86 ms /   265 tokens (   13.66 ms per token,    73.23 tokens per second)\n",
      "llama_print_timings:        eval time = 18719.90 ms /   510 runs   (   36.71 ms per token,    27.24 tokens per second)\n",
      "llama_print_timings:       total time = 22698.51 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297488\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 12853.10 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x116526920\n",
      "ggml_metal_init: loaded kernel_add_row                        0x116528b40\n",
      "ggml_metal_init: loaded kernel_mul                            0x116528df0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1165295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11652a000\n",
      "ggml_metal_init: loaded kernel_silu                           0x11652a900\n",
      "ggml_metal_init: loaded kernel_relu                           0x1165282c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11652b0d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11652bb40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11652c610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11652d660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11652ebd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11652ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11652e1f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11652f660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11652ffe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x116530960\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1165312e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x116531ca0\n",
      "ggml_metal_init: loaded kernel_norm                           0x116532e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1165338d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x116533e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x116534810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x116535220\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x116535c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x116536de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1165377f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x116538200\n",
      "ggml_metal_init: loaded kernel_rope                           0x116538460\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x116538ec0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x116539a80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11653a990\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x116532480\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.11 MB, (12853.56 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, (12863.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.73 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find the answer, and that God has hidden it in a book.\n",
      "As I was growing up, my family would go to church every Sunday. We would participate in youth group events during the week, and we would pray before meals, but that was about as far as our spiritual lives went. I didn’t really think much of religion or God until high school when I began questioning my purpose here on earth. I had some friends who were Christians and I found myself becoming more drawn to them. When they asked if I wanted to attend a church youth group meeting, I jumped at the opportunity thinking it would help me find answers to my questions.\n",
      "The first time I went with them, we did a group activity that required us to write down our deepest secrets and then read them out loud. Everyone was silent as each person shared their secret. When it came time for me to share mine, I hesitated. I had never told anyone my secret before and didn’t know how they would react. After a few seconds of silence, the leader of the meeting asked if I wanted to continue or if I wanted to leave. Looking around at everyone watching me intently, I decided that I could tell them this one thing and then we could leave; it was really just too much pressure for me to handle. So as I opened my mouth to speak, I said “I don’t know who God is.”\n",
      "There were a few seconds of silence before the room erupted with gasps and murmurs. People started asking questions like: how do you not know? Have you never been to church? Do you ever pray? To which I responded that I had been baptized as an infant but never attended church, did not attend youth group meetings, and I rarely prayed.\n",
      "What was the meaning of life if there wasn’t a God?\n",
      "I had no idea what my purpose for living was or why we were here. The only thing I knew that would make me happy was to be in love. To fall in love with someone who loved you back, and then live happily ever after. But how could I find someone like that if I didn’t know who God was? All of my friends had found their best friends through dating or at school and had gotten married by the time they were 25. They seemed so happy and fulfilled in life, while I felt like nothing mattered to me – I didn’t even feel\n",
      "llama_print_timings:        load time =  1706.23 ms\n",
      "llama_print_timings:      sample time =   318.67 ms /   512 runs   (    0.62 ms per token,  1606.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3577.89 ms /   265 tokens (   13.50 ms per token,    74.07 tokens per second)\n",
      "llama_print_timings:        eval time = 18666.78 ms /   510 runs   (   36.60 ms per token,    27.32 tokens per second)\n",
      "llama_print_timings:       total time = 22602.75 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297512\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10f4110a0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10f4132c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x10f413570\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10f413db0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10f414850\n",
      "ggml_metal_init: loaded kernel_silu                           0x10f415090\n",
      "ggml_metal_init: loaded kernel_relu                           0x10f412a40\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10f415860\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10f4162f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10f417de0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10f4172f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10f416df0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10f419550\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10f418990\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10f419cc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10f41a790\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10f41b110\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10f41ba70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10f41c430\n",
      "ggml_metal_init: loaded kernel_norm                           0x10f41d5f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10f41e080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10f41ec60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10f41f690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10f4200a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10f420c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10f421760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10f421fd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10f4223f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x10f423530\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10f423d60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10f424c60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10f425850\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10f41cc10\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living.\n",
      "If we look at people who are happy, it seems that they do not get stuck in a particular time frame or place. They live in the moment as much as possible and appreciate where they are now. If they have a regret about something from their past, they accept it as part of life’s learning process and keep moving ahead. They don’t dwell on the past for too long or overthink the future.\n",
      "Those who are miserable, on the other hand, seem to live in some sort of time warp. They have an inability to be present in the moment because they are constantly trying to re-live a past that can never be again, and they are always thinking about a future that may not exist.\n",
      "We can learn from the lessons of the past while still being grateful for what we had and who we were at that time. We can also look forward with hope knowing that we have learned from those experiences and will make better choices in our lives because of them. But if we are constantly thinking about something that happened, we are not living now. If we are worrying about the future, we are not living now. The only place where life happens is right here and now.\n",
      "The most important thing about life is to live it well and enjoy it as much as possible. We don’t know how long we will have this gift of time so let us use it wisely. Let’s look for the joy in each moment, the happiness that surrounds us at all times. If you can find a way to be happy right now, life opens up with new possibilities and unexpected blessings.\n",
      "The meaning of life is found in living. The meaning of life is found in each moment. And there is no better time than right now.\n",
      "I believe the purpose of life is to be found in loving.\n",
      "We all want to love and feel loved in return, but many of us are so afraid of being hurt that we keep our hearts locked up tightly. We fear letting someone close enough to hurt us and end up hurting ourselves by keeping people out. It makes me sad when I see a beautiful flower wither because it is too scared or too proud to open itself up.\n",
      "If you are one who has closed your heart to the possibility of love, it is never too late to open yourself up and let someone in. There will be risks involved, but what better reason for living than love?\n",
      "\n",
      "llama_print_timings:        load time =  1703.97 ms\n",
      "llama_print_timings:      sample time =   318.59 ms /   512 runs   (    0.62 ms per token,  1607.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6021.14 ms /   265 tokens (   22.72 ms per token,    44.01 tokens per second)\n",
      "llama_print_timings:        eval time =  9678.55 ms /   510 runs   (   18.98 ms per token,    52.69 tokens per second)\n",
      "llama_print_timings:       total time = 16058.43 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297530\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x125e37650\n",
      "ggml_metal_init: loaded kernel_add_row                        0x125e39820\n",
      "ggml_metal_init: loaded kernel_mul                            0x125e39ae0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x125e3a2f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x125e3ad60\n",
      "ggml_metal_init: loaded kernel_silu                           0x125e3b5d0\n",
      "ggml_metal_init: loaded kernel_relu                           0x125e38fd0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x125e3bf20\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x125e3c7e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x125e3d250\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x125e3d620\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x125e3e610\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x125e3fa80\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x125e3eea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x125e40310\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x125e40c90\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x125e415f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x125e41f70\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x125e42930\n",
      "ggml_metal_init: loaded kernel_norm                           0x125e43b00\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x125e43f20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x125e452e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x125e45d20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x125e46760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x125e472f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x125e484f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x125e48750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x125e49a40\n",
      "ggml_metal_init: loaded kernel_rope                           0x125e49250\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x125e4a7d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x125e4b690\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x125e4c270\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x125e43300\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find our gift. The purpose of life is to give it away.”\n",
      "— Pablo Picasso (1881–1973), Spanish painter, sculptor, and printmaker\n",
      "My name is Katy Jangula, and I have been creating art for over 20 years now. My work has evolved from traditional watercolors into a more abstract expressionism style. Throughout my life, I’ve always tried to find ways to give back to the community through art. Over the last few years, I’ve had the opportunity to volunteer with several organizations such as:\n",
      "The American Cancer Society’s Relay for Life\n",
      "Making Strides Against Breast Cancer Walk in Minneapolis\n",
      "Walking Forward – Helping Women Fight Back Against Cancer\n",
      "Duluth AIDS Network (DAN)\n",
      "My involvement with DAN and the art therapy program has been very rewarding. I have seen first hand how creativity, art, and healing go together when one is dealing with a terminal diagnosis such as HIV or cancer. The people that participate in this group are so incredibly open to sharing their stories about their illnesses along with sharing their own unique talents. It has been an honor to work alongside them.\n",
      "My hope is that through my art I can continue to make a difference and help others cope with the hardships of life, whatever they may be.\n",
      "“Art washes away from the soul the dust of everyday life.” – Pablo Picasso (1881–1973), Spanish painter, sculptor, and printmaker\n",
      "I have always loved art and drawing since I was a little girl. My earliest inspiration came from my mother who has an incredible amount of talent as well. She still paints and draws every day and it’s something that she’s been able to do her entire life despite having lost her eyesight due to the complications of diabetes in 2005.\n",
      "My first real job was at age 16 when I worked for a local florist named Sunnyside Floral in Duluth, Minnesota. From there, I went on to sell paintings and art supplies at an art supply store in Duluth called Artistic Creations. That’s where my love of painting really began.\n",
      "At age 18, after high school, I moved to Las\n",
      "llama_print_timings:        load time =   990.07 ms\n",
      "llama_print_timings:      sample time =   317.00 ms /   512 runs   (    0.62 ms per token,  1615.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5991.21 ms /   265 tokens (   22.61 ms per token,    44.23 tokens per second)\n",
      "llama_print_timings:        eval time =  9683.89 ms /   510 runs   (   18.99 ms per token,    52.66 tokens per second)\n",
      "llama_print_timings:       total time = 16031.69 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297547\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7024.01 MB\n",
      "llama_model_load_internal: mem required  = 7390.01 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11c418ea0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11c41b070\n",
      "ggml_metal_init: loaded kernel_mul                            0x11c41b330\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11c41bb20\n",
      "ggml_metal_init: loaded kernel_scale                          0x11c41c600\n",
      "ggml_metal_init: loaded kernel_silu                           0x11c41ce70\n",
      "ggml_metal_init: loaded kernel_relu                           0x11c41a840\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11c41d7f0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11c41e7a0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11c41ebc0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11c41fda0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11c41f080\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11c4212b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11c420700\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11c421be0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11c422590\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11c422f10\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11c423890\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11c424370\n",
      "ggml_metal_init: loaded kernel_norm                           0x11c425370\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11c4261c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11c426b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11c4275b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11c427ff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11c428b60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11c429510\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11c429d60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11c42a770\n",
      "ggml_metal_init: loaded kernel_rope                           0x11c42b250\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11c42b4b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11c42c030\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11c42cf10\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11c424b50\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.02 MB, ( 7024.47 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 7036.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7438.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, ( 7600.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 7792.64 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find a purpose for your own life. It's not enough just to have a purpose, you need to know why you are here on earth, what you are doing here and why you do it.\n",
      "People who don't have a purpose can easily feel useless, unimportant or insignificant in the eyes of others. Without a purpose, we will lack direction. We also may tend to be depressed because we don't know how to make sense of our existence. Even though I believe that it is important to find a purpose for your own life, I also think the meaning of life has more than one answer. For example, you might have a different reason why you are on earth other than what I wrote above.\n",
      "I believe the meaning of life is to serve others. I don't mean to serve yourself by serving others, but to genuinely care about the wellbeing of your fellow human beings and even animals and nature.\n",
      "Serving others could be anything from giving back to society through volunteer work, a job that helps other people in need or simply being nice to someone in need. You can also serve others by helping them find their own purpose in life.\n",
      "You don't always have to go out of your way to do something for others. Sometimes just a simple smile at the right time can make somebody else's day and that makes you feel good about yourself as well.\n",
      "The question \"What is the meaning of life?\" has no single correct answer, but only an infinite number of different answers. The meaning of life is what you choose to make it. You create your own purpose in life and then act on it. If you don't know why you are here on earth yet or if you have a lack of direction in life, I encourage you to start thinking about your own life and what its true purpose might be. If you can't think of any reason for being alive you should seek out someone who knows you well enough to help you find answers to that question.\n",
      "If you are depressed or feel useless, remember that you have the power to change those feelings at any time. Simply decide to start looking for meaning in your own life and you will be able to do so because you are already alive and that is reason enough to make the most of it. Don't wait until something drastic happens before you start thinking about why you really want to live, rather take action right now.\n",
      "We all have different\n",
      "llama_print_timings:        load time =   995.96 ms\n",
      "llama_print_timings:      sample time =   317.42 ms /   512 runs   (    0.62 ms per token,  1613.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6014.30 ms /   265 tokens (   22.70 ms per token,    44.06 tokens per second)\n",
      "llama_print_timings:        eval time =  9706.53 ms /   510 runs   (   19.03 ms per token,    52.54 tokens per second)\n",
      "llama_print_timings:       total time = 16077.82 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297565\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11ca26920\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11ca28b40\n",
      "ggml_metal_init: loaded kernel_mul                            0x11ca28df0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11ca295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11ca2a000\n",
      "ggml_metal_init: loaded kernel_silu                           0x11ca2a900\n",
      "ggml_metal_init: loaded kernel_relu                           0x11ca282c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11ca2b0d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11ca2bb40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11ca2c610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11ca2d660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11ca2ebd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11ca2ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11ca2e1f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11ca2f660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11ca2ffe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11ca30960\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11ca312e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11ca31ca0\n",
      "ggml_metal_init: loaded kernel_norm                           0x11ca32e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11ca338d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11ca33e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11ca34810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11ca35220\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11ca35c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11ca36de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11ca377f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11ca38200\n",
      "ggml_metal_init: loaded kernel_rope                           0x11ca38460\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11ca38ec0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11ca39a80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11ca3a990\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11ca32480\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.70 MB, (24827.16 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (24839.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.33 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to have a deep sense of purpose in your life and to be able to make a difference.\n",
      "I don’t think we can ever know what our purpose here is, but we need to find it for ourselves – that will bring us much more fulfillment than just being a cog in the wheel.\n",
      "We all have different purposes though – some of us are here to help others and make the world a better place, others are here to be happy, raise families or enjoy what life has to offer.\n",
      "Whatever your purpose is you should always strive to achieve it. But sometimes we get distracted by our own wants and desires and forget why we’re really here. That’s when we need to take a step back and look at things in a different light.\n",
      "I hope that as you read this, you find your reason for being here – your purpose. And if not I hope it brings some clarity.\n",
      "The meaning of life is whatever you want it to be.\n",
      "You’re the one who decides what you want out of life and what defines success for you. If you let others tell you what your goals should be, they might give you some false ideas about how you should live – like getting a big house or fancy car. But if those things aren’t really important to you then why are you working towards them?\n",
      "We all have our own reasons for living and no one else can make that decision for us. If we let other people tell us what to do, then they will decide the meaning of life for us – not ourselves. But it’s up to every individual to determine their own purpose in life.\n",
      "You need to define your own values and goals – only you know which ones are right for you. And you shouldn’t let anyone else take that decision away from you or make you feel bad about the choices you’ve made. Everyone is different, so we all have different reasons for living and our own definition of success.\n",
      "Therefore there isn’t one set meaning of life – it varies from person to person because we are all different and everyone has their own values. That’s why we shouldn’t judge each other. We should embrace our differences, accept that they exist and allow others the freedom to choose their own path in life without forcing them down our own road.\n",
      "I believe it’s important to have a deep sense of purpose in life because then you will know what your goals are and be\n",
      "llama_print_timings:        load time =  5680.99 ms\n",
      "llama_print_timings:      sample time =   317.83 ms /   512 runs   (    0.62 ms per token,  1610.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6270.51 ms /   265 tokens (   23.66 ms per token,    42.26 tokens per second)\n",
      "llama_print_timings:        eval time = 35124.11 ms /   510 runs   (   68.87 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time = 41753.01 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297612\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104f26920\n",
      "ggml_metal_init: loaded kernel_add_row                        0x104f28b40\n",
      "ggml_metal_init: loaded kernel_mul                            0x104f28df0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104f295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x104f2a000\n",
      "ggml_metal_init: loaded kernel_silu                           0x104f2a900\n",
      "ggml_metal_init: loaded kernel_relu                           0x104f282c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104f2b0d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104f2bb40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104f2c610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104f2d660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104f2ebd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104f2ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x104f2e1f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x104f2f660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104f2ffe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x104f30960\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104f312e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104f31ca0\n",
      "ggml_metal_init: loaded kernel_norm                           0x104f32e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104f338d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104f33e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104f34810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104f35220\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104f35c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104f36de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104f377f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x104f38200\n",
      "ggml_metal_init: loaded kernel_rope                           0x104f38460\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104f38ec0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104f39a80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104f3a990\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x104f32480\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.70 MB, (24827.16 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (24839.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.33 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "As far as I can tell, I have no idea what I'm doing.\n",
      "I'm just trying to make my way through.\n",
      "I have no idea how I got this way.\n",
      "It's like I was born with all these things that I like and am interested in.\n",
      "And I don't think I can ever forget those things, which is good because they are the only things I know for sure.\n",
      "I want to be happy but I think it has a lot to do with being insecure about myself.\n",
      "So I'm not going to talk about how I could find happiness by doing x or y because that would be too easy, and not fun enough.\n",
      "I just have to say, happiness is the meaning of life. It doesn't matter what you choose to do, as long as it makes you happy.\n",
      "And if I seem like a little kid in an adult world, well...\n",
      "So here I am, trying to find my path in this crazy, crazy world.\n",
      "I think that's all there is to it.\n",
      "I hope you enjoy my journey. :)\n",
      "Posted by Nicole M at 10:42 PM No comments:\n",
      "Labels: adventure, being happy, blog, discovery, life, meaning of life, rant, thoughts\n",
      "Nicole M. (1995)\n",
      "I'm a teenager who enjoys writing and discovering new things in the world. I'm looking for my place in this crazy world so that one day, it can all make sense.\n",
      "I'll be posting about what I think is important in life; happiness. My posts will cover topics like how to be happy and how to live a meaningful life. As well as just general rants/thoughts on the world around me. I hope you enjoy! :)\n",
      "Follow @nicolem713\n",
      "Writing about life, in order to find out what is important in life.\n",
      "Blog Archive October (2) September (4) August (2) July (1) June (2) May (3) April (2) March (4) February (9) January (5) December (5) November (7) October (8) September (6) August (7) July (3)\n",
      "My other blog for writing: www.nicolem713.wordpress.com\n",
      "© Nicole M.\n",
      "llama_print_timings:        load time =  3843.74 ms\n",
      "llama_print_timings:      sample time =   317.44 ms /   512 runs   (    0.62 ms per token,  1612.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6264.30 ms /   265 tokens (   23.64 ms per token,    42.30 tokens per second)\n",
      "llama_print_timings:        eval time = 35384.15 ms /   510 runs   (   69.38 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time = 42006.00 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297658\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 24826.69 MB\n",
      "llama_model_load_internal: mem required  = 25192.69 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13b7272c0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13b7294e0\n",
      "ggml_metal_init: loaded kernel_mul                            0x13b729790\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13b729fd0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13b72aa70\n",
      "ggml_metal_init: loaded kernel_silu                           0x13b72b2b0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13b728c60\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13b72ba80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13b72c510\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13b72e000\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13b72d510\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13b72d010\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13b72f770\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13b72ebb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13b72fee0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13b7309b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13b731330\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13b731c90\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13b732650\n",
      "ggml_metal_init: loaded kernel_norm                           0x13b733810\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13b7342a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13b734e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13b7358b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13b7362c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13b736e50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13b737980\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13b7381f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13b738610\n",
      "ggml_metal_init: loaded kernel_rope                           0x13b739750\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13b739f80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13b73ae80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13b73ba70\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13b732e30\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   312.50 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.70 MB, (24827.16 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (24839.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (25241.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (25403.33 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (25595.33 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "I’m not really sure how this came about, but as a child I was always fascinated by death. It wasn’t just the curiosity that every kid has about why they have to die too (that’s pretty much a given), it was more than that. As a kid, my mind would wander into the darkness of what happens after life and why we do the things we do.\n",
      "Why do we hurt those who love us so? Why are there so many wars? What is the purpose of all this pain?\n",
      "I felt like I knew it in my gut that there was more to this than meets the eye. So, as a child, when death came knocking at our door, I would ask my grandmother if she believed in heaven and hell. She said she did and that she had seen both places.\n",
      "She believed in heaven because she used to visit her friends who had passed on before her in heaven and she also told me about the time she went to hell. Apparently, she was walking along a river bank and saw someone being dragged underneath the water by an evil spirit, this person was screaming for help, but no one would help because they were all afraid of what might happen if they did.\n",
      "My grandmother took off her shoes and dove into the river and pulled out the man from the evil clutches of hell. She didn’t know the man; she had never seen him before in her life, but she knew it was the right thing to do.\n",
      "I couldn’t help but wonder if that meant I would end up in heaven or hell because I was too scared to ask for help with my fears and anxieties. In fact, I wouldn’t even tell anyone about them until I was an adult. I thought maybe if I didn’t tell then no one could be mean to me.\n",
      "I guess all this thinking about death led me into studying psychology when I got older; I wanted to figure out the human mind and what made us tick. It wasn’t really working for me, so I decided to go back to school in my late twenties to study counselling, which was a much better fit.\n",
      "My grandmother always used to say that when you die, your soul is weighed against a feather of truth and if the scales balance out then you will enter heaven and if it doesn’t, you go to\n",
      "llama_print_timings:        load time =  3917.64 ms\n",
      "llama_print_timings:      sample time =   318.28 ms /   512 runs   (    0.62 ms per token,  1608.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6292.35 ms /   265 tokens (   23.74 ms per token,    42.11 tokens per second)\n",
      "llama_print_timings:        eval time = 35357.01 ms /   510 runs   (   69.33 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time = 42008.97 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6537f",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16c67651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297704\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.06 MB\n",
      "llama_model_load_internal: mem required  = 17993.06 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x105b11260\n",
      "ggml_metal_init: loaded kernel_add_row                        0x105b13480\n",
      "ggml_metal_init: loaded kernel_mul                            0x105b13730\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x105b13f70\n",
      "ggml_metal_init: loaded kernel_scale                          0x105b14a10\n",
      "ggml_metal_init: loaded kernel_silu                           0x105b15250\n",
      "ggml_metal_init: loaded kernel_relu                           0x105b12c00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x105b15a20\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x105b164b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x105b17fa0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x105b174b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x105b16fb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x105b19710\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x105b18b50\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x105b19e80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x105b1a950\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x105b1b2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x105b1bc30\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x105b1c5f0\n",
      "ggml_metal_init: loaded kernel_norm                           0x105b1d7b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x105b1e240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x105b1ee20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x105b1f850\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x105b20260\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x105b20df0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x105b21920\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x105b22190\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x105b225b0\n",
      "ggml_metal_init: loaded kernel_rope                           0x105b236f0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x105b23f20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x105b24e20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x105b25a10\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x105b1cdd0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.06 MB, (17505.52 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (17521.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18303.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18519.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18775.69 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "My happiness comes from helping others and knowing that my efforts have made a difference in someone's life.\n",
      "When you think about it, most people are unhappy with their life because they don't know what they want - they don't have a goal. They have no motivation in life to do anything, so they get up, go to work, come home, watch TV and go to bed. Then they repeat the process 5 days a week.\n",
      "It is so simple really, you just have to know what it is that makes you happy - then do that thing!\n",
      "My goal is to help people find their goal...to make them understand that happiness can be found in your life if you look for it and work at it every day.\n",
      "I am a very lucky person. I get paid to do the things I love doing, which include: helping people to feel better about themselves, teaching others how to become more successful, working with companies to build their brand and increase sales, connecting with other entrepreneurs who are looking for ways to be more efficient in their businesses, and finding new ways to express my creativity.\n",
      "I am a very happy person because I do the things that make me happy...and I get paid to do them!\n",
      "How can you be happy? It's simple: you have to find your passion, then do it every day of your life.\n",
      "And if you don't know what your passion is, there are so many ways to find out! Just ask yourself this question: What would I do even if I didn't get paid for it?\n",
      "I recently read an article called \"Finding Your Dream Job\" by Tara-Nicholle Nelson. The author points out that the idea of finding your dream job is a myth and that it doesn't exist. Instead, she suggests you find something you love doing - and then do it!\n",
      "Here are some of her tips to help you get started:\n",
      "1) Ask yourself what you would do even if you didn't get paid for it. Then do it every day!\n",
      "2) Don't let your fear hold you back from trying new things. The only way you can find out if you love something is by doing it...so go ahead and try it. If you don't like it, then move on to the next thing.\n",
      "3) Find a coach/mentor that believes in you. They can help\n",
      "llama_print_timings:        load time =  4239.04 ms\n",
      "llama_print_timings:      sample time =   319.51 ms /   512 runs   (    0.62 ms per token,  1602.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13238.78 ms /   265 tokens (   49.96 ms per token,    20.02 tokens per second)\n",
      "llama_print_timings:        eval time = 20371.66 ms /   510 runs   (   39.94 ms per token,    25.03 tokens per second)\n",
      "llama_print_timings:       total time = 33968.66 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "148d30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297743\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.06 MB\n",
      "llama_model_load_internal: mem required  = 17993.06 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x102926920\n",
      "ggml_metal_init: loaded kernel_add_row                        0x102928b40\n",
      "ggml_metal_init: loaded kernel_mul                            0x102928df0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1029295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10292a000\n",
      "ggml_metal_init: loaded kernel_silu                           0x10292a900\n",
      "ggml_metal_init: loaded kernel_relu                           0x1029282c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10292b0d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10292bb40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10292c610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10292d660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10292ebd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10292ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10292e1f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10292f660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10292ffe0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x102930960\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1029312e0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x102931ca0\n",
      "ggml_metal_init: loaded kernel_norm                           0x102932e40\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1029338d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x102933e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x102934810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x102935220\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x102935c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x102936de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1029377f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x102938200\n",
      "ggml_metal_init: loaded kernel_rope                           0x102938460\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x102938ec0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x102939a80\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10293a990\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x102932480\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.06 MB, (17505.52 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (17521.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18303.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18519.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18775.69 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to enjoy it—that is all, nothing more.\n",
      "—OSCAR WILDE\n",
      "I'm a big believer that we can't make others happy if we aren't already happy ourselves. So this book will begin with a discussion about happiness itself and how it relates to our daily lives and the lives of those around us. We'll look at what makes people happy and what doesn't. We'll explore whether or not money buys happiness, why it's important that we feel good about ourselves in order to be happy, and how a state of mind called \"flow\" can affect our overall sense of contentment.\n",
      "We'll also learn how the brain is hardwired to make us happy (if only we'd let it) and how even our genes have an impact on whether or not our lives are filled with cheer—or angst. We'll look at the most important factors that affect happiness, both in America as well as around the globe.\n",
      "And finally, we'll learn how to be happier in our own lives—by improving our self-esteem, by finding meaningful work, by living more healthily, and by forging lasting relationships with family and friends. We'll also see that it's possible to be happy even when we feel sad or down. In other words, we can learn to be content even when we aren't necessarily feeling great.\n",
      "In the next chapter, I'll discuss how happiness has been defined in different eras and cultures around the world. We'll look at the ancient Greeks and their ideas about what it means to lead a good life (and why they may have been on to something after all). And we'll see that Eastern philosophies on contentment and joy are often quite different from those of Western countries—and yet, in many ways, just as valid.\n",
      "We will also explore the evolutionary basis for happiness, and see why the need to feel good is hardwired into our very genes. And finally, we'll look at some recent studies that have made headlines around the world, including those that show that money can buy you happiness—but only up to a point.\n",
      "In chapter 3, we'll delve more deeply into the nature of happiness itself. We'll see why it is so important for us to feel good about ourselves and how this sense of self-worth can be a\n",
      "llama_print_timings:        load time =  2828.25 ms\n",
      "llama_print_timings:      sample time =   322.78 ms /   512 runs   (    0.63 ms per token,  1586.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13287.83 ms /   265 tokens (   50.14 ms per token,    19.94 tokens per second)\n",
      "llama_print_timings:        eval time = 20416.33 ms /   510 runs   (   40.03 ms per token,    24.98 tokens per second)\n",
      "llama_print_timings:       total time = 34065.85 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297780\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 17505.06 MB\n",
      "llama_model_load_internal: mem required  = 17993.06 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13162a850\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13162ca70\n",
      "ggml_metal_init: loaded kernel_mul                            0x13162cd20\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13162d4f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x13162df30\n",
      "ggml_metal_init: loaded kernel_silu                           0x13162e830\n",
      "ggml_metal_init: loaded kernel_relu                           0x13162c1f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13162f000\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13162fa70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x131630540\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x131631590\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x131632b00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x131632d60\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x131632120\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x131633590\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x131633f10\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x131634890\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x131635210\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x131635bd0\n",
      "ggml_metal_init: loaded kernel_norm                           0x131636d70\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x131637800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x131637dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x131638740\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x131639150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x131639b80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13163ad10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13163b720\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13163c130\n",
      "ggml_metal_init: loaded kernel_rope                           0x13163c390\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13163cdf0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13163d9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13163e8c0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1316363b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   166.63 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.06 MB, (17505.52 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (17521.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (18303.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (18519.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (18775.69 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift and give it away.\n",
      "In addition, the meaning of my life is to help others find their gift and give it away as well.\n",
      "I have always been one who cares for other people’s emotional needs but never fully understood why until recently. My purpose in life has been revealed to me by the Holy Spirit which I will explain further on.\n",
      "First, let me share my story.\n",
      "My father used to come home drunk and beat my mother. He eventually left us when I was 13 years old. While he was away, we suffered from poverty and had to do without a lot of necessities, but I never felt sorry for myself. Instead, I found ways to help others.\n",
      "I have always been sensitive to other people’s needs, even as a child. I remember one day in grade school when the teacher said something that hurt my friend’s feelings and I was quick to defend him.\n",
      "In high school, a good friend of mine became very sick with a disease called Guillain-Barre Syndrome which left her partially paralyzed for several months and unable to breathe on her own without the help of a ventilator. My whole class knew about it because I told them. But most importantly, my friend knew that if she ever needed me, I would be there for her no matter what.\n",
      "I started to cry every time I saw her in the hospital and I wanted to do something more than just visiting her. So I asked the nurses how I could help. They told me to play with other patients on the children’s floor. I went back to my classroom and shared this story with my teacher and a few friends. Then, every day after school for several weeks, we would visit my friend and then go upstairs to play games in the children’s ward. My friend felt so good about what we were doing that she decided to return to high school earlier than expected – without the aid of her ventilator.\n",
      "I remember how good I felt when I saw her at graduation with a smile on her face. She was back and no longer sick!\n",
      "Another time, I was in a friend’s car when he almost hit a child who ran out into the street. I realized that God had spared that child from being hit by a car for some special reason – to fulfill his or her purpose in life. This made me think about\n",
      "llama_print_timings:        load time =  2849.38 ms\n",
      "llama_print_timings:      sample time =   317.95 ms /   512 runs   (    0.62 ms per token,  1610.29 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13107.07 ms /   265 tokens (   49.46 ms per token,    20.22 tokens per second)\n",
      "llama_print_timings:        eval time = 20394.21 ms /   510 runs   (   39.99 ms per token,    25.01 tokens per second)\n",
      "llama_print_timings:       total time = 33857.96 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb0e7",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297817\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.74 MB\n",
      "llama_model_load_internal: mem required  = 62533.74 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x129a26920\n",
      "ggml_metal_init: loaded kernel_add_row                        0x129a28b40\n",
      "ggml_metal_init: loaded kernel_mul                            0x129a28df0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x129a295c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x129a2a000\n",
      "ggml_metal_init: loaded kernel_silu                           0x129a2a900\n",
      "ggml_metal_init: loaded kernel_relu                           0x129a282c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x129a2b0d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x129a2bb40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x129a2c610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x129a2d660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x129a2ebd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x129a2ee30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x129a2e1f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x129a2f660\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104c11290\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x104c11710\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104c10120\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104c15120\n",
      "ggml_metal_init: loaded kernel_norm                           0x104c16140\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104c16d80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104c17340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104c17cc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104c186b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104c19100\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104c19c90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104c1a6a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x104c1b0b0\n",
      "ggml_metal_init: loaded kernel_rope                           0x104c1b760\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104c1b9c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104c1d130\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104c1e040\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x104c1ec10\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   406.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.75 MB, (62046.20 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (62062.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (62844.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (63060.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (63316.38 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "One of my favorite authors, Pablo Picasso, once said, “The meaning of life is to find your gift, and the purpose of life is to give it away.” My name is David Lee Sexton Jr., I was born in 1967, and if you’re doing the math that would make me 50 years old. You see, I have a very personal story, and I’d like to share it with you.\n",
      "I grew up in Wichita Falls, Texas and came from what most people would call a broken family. My parents divorced when I was three or four years old and by the time I was seven years old my father had moved to California. My dad wasn’t around as much as he should have been, but I understood why he left. And I’m not mad about it at all because I know that he did what he thought was best for me and my sisters, and he still does today.\n",
      "I grew up in a very poor family. We moved around a lot and lived in some pretty bad areas where we had to be careful as young kids, but I’m not bitter about our living conditions either because I realize that we didn’t know any better. My mother always did the best she could with what she had, and I appreciate her for that.\n",
      "At a very early age, I was diagnosed with epilepsy and started having seizures. And when I was 14 years old my parents took me to the doctor because of my migraine headaches. I underwent two brain surgeries at Texas A&M University where they removed a portion of my brain, but the surgery didn’t seem to work. So, they ended up putting me on anti-seizure medication for the next 26 years.\n",
      "I graduated from high school and went to college in San Angelo, Texas where I studied business administration. I had to drop out of college after one year because I was having financial difficulties and couldn’t afford my tuition. I moved back home where I worked at a convenience store for five years and eventually became the manager.\n",
      "I got married to my high school sweetheart in 1989, but we divorced four years later. Then, I went through several other failed relationships before I met my second husband in 2003\n",
      "llama_print_timings:        load time = 15306.85 ms\n",
      "llama_print_timings:      sample time =   335.13 ms /   512 runs   (    0.65 ms per token,  1527.76 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14185.67 ms /   265 tokens (   53.53 ms per token,    18.68 tokens per second)\n",
      "llama_print_timings:        eval time = 77727.71 ms /   510 runs   (  152.41 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:       total time = 92293.25 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691297925\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.74 MB\n",
      "llama_model_load_internal: mem required  = 62533.74 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10552b940\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10552db60\n",
      "ggml_metal_init: loaded kernel_mul                            0x10552de10\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10552e650\n",
      "ggml_metal_init: loaded kernel_scale                          0x10552f0f0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10552f930\n",
      "ggml_metal_init: loaded kernel_relu                           0x10552d2e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x105530100\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x105530b90\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x105532680\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x105531b90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x105531690\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x105533df0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x105533230\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x105534560\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x105535030\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1055359b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x105536310\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x105536cd0\n",
      "ggml_metal_init: loaded kernel_norm                           0x105537e90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x105538920\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x105539500\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x105539f30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10553a940\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10553b4d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10553c000\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10553c870\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10553cc90\n",
      "ggml_metal_init: loaded kernel_rope                           0x10553ddd0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10553e600\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10553f500\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1055400f0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1055374b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   406.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.75 MB, (62046.20 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (62062.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (62844.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (63060.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (63316.38 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a meaning.\n",
      "To be an instrument, an agent for good in this world. To find ways each day to make people’s lives just a little bit better and to learn something new, both about myself and others every single day.\n",
      "Life is not always kind, but we must be. I believe that the best way to have the most impact on this world is to love those around us and to serve them.\n",
      "I was born in Salt Lake City, Utah, and grew up with a wonderful family in West Jordan, Utah. After high school, I served a two-year mission for The Church of Jesus Christ of Latter-day Saints in Recife, Brazil. I loved it there and learned so much about how to serve others and how to be happy.\n",
      "After returning from my mission, I attended Brigham Young University where I was lucky enough to get to know some of the greatest people on this planet and was blessed with a degree in Spanish Education.\n",
      "I met my wife while attending BYU and we have been happily married for 12 years. We have four incredible children: three boys and one little girl. I love being their father, husband to my wife and friend to people around me. I am always looking for ways to help people improve themselves and the world in which they live.\n",
      "I believe that there is so much good to be done out there if we just put our minds to it. It is my goal to do as much of that good as possible, and I would love you to join me on this journey.\n",
      "Please enjoy reading some of my posts and feel free to contact me with any questions or comments you might have.\n",
      "I’m a writer who loves working with words. I love sharing stories and teaching others about various topics.\n",
      "My goal is to help people live better lives, both in the sense that they can improve themselves as individuals and in helping them have a more positive impact on those around them.\n",
      "In addition, my goal is to make a living doing something I love. To be able to write for a living, and to be able to give back financially to others around me, would be an incredible blessing.\n",
      "I am always looking to expand my horizons and learn new things, so if you’ve got something that needs written or some other service that needs rendered, please feel free to contact me.\n",
      "In addition to writing for my website, I also work as\n",
      "llama_print_timings:        load time = 10364.65 ms\n",
      "llama_print_timings:      sample time =   324.84 ms /   512 runs   (    0.63 ms per token,  1576.17 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13946.61 ms /   265 tokens (   52.63 ms per token,    19.00 tokens per second)\n",
      "llama_print_timings:        eval time = 77746.27 ms /   510 runs   (  152.44 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:       total time = 92060.77 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71c4a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298027\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_head_kv  = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 62045.74 MB\n",
      "llama_model_load_internal: mem required  = 62533.74 MB (+  780.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x117078120\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11707a2f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x11707a590\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11707adb0\n",
      "ggml_metal_init: loaded kernel_scale                          0x11707b850\n",
      "ggml_metal_init: loaded kernel_silu                           0x11707c0c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x117079ac0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11707ca40\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11707d9f0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11707de10\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11707eff0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11707e2d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x117080500\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11707f950\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x117080e30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1170817e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x117082160\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x117082ae0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1170835c0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1170845c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x117085410\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x117085de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x117086800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x117087240\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x117087db0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x117088760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x117088fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1170899c0\n",
      "ggml_metal_init: loaded kernel_rope                           0x11708a4a0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11708a700\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11708b280\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11708c160\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x117083da0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   406.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.75 MB, (62046.20 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.17 MB, (62062.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (62844.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (63060.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (63316.38 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it fully, no matter what you do.\n",
      "My name is Tina and I’m a 32-year old freelance writer from Belgrade, Serbia. Since English was always my favourite subject in school, I started writing at an early age. As years went by, my hobby turned into a profession as I was offered to write for local magazines and newspapers. This gave me the opportunity to explore various topics that interest me.\n",
      "I write about everything ranging from technology, traveling and health to relationships, entertainment and fashion. But what really sparks my interest is history, culture and psychology. With time I realized there are plenty of bloggers out there who are passionate about writing just like me, and I decided it’s time for me to start my own blog and share my thoughts with the world.\n",
      "I hope you will enjoy reading my posts as much as I enjoy writing them! Feel free to comment and let me know what you think!\n",
      "Love this. Can’t wait to read your next post.\n",
      "You are welcome. I look forward to following your blog. If there is ever anything I can do for you please let me know.\n",
      "Thank you very much, I hope you will enjoy reading it!\n",
      "I love the idea behind your blog and all the posts that you have written so far. Looking foward to the next one! ���� Take care!\n",
      "Hi Tina, my name is Ahmad, I live in Jordan. I’m a student at Al-Zaytoonah University of Jordan, faculty of foreign languages and translation. The reason that made me write this message to you is because I’m really impressed by your blog, it’s very helpful for people and it’s not common especially here in my country.\n",
      "Thank you very much for your kind words, Ahmad! It means a lot to me. ���� I wish you all the best with your studies!\n",
      "Hey Tina! Just came across your blog and love what you are about! Keep up the good work.\n",
      "I’m glad you liked it! Thanks for stopping by! ���� And thank you for your kind words, I really appreciate it.\n",
      "Hi Tina. My name is Danny and I am a blogger from Ireland. Thank you very much for dropping into my blog recently and leaving a comment\n",
      "llama_print_timings:        load time = 10238.20 ms\n",
      "llama_print_timings:      sample time =   316.50 ms /   512 runs   (    0.62 ms per token,  1617.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14076.43 ms /   265 tokens (   53.12 ms per token,    18.83 tokens per second)\n",
      "llama_print_timings:        eval time = 77851.62 ms /   510 runs   (  152.65 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:       total time = 92287.71 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0bef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298130\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.96 MB\n",
      "llama_model_load_internal: mem required  = 35839.96 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x107926900\n",
      "ggml_metal_init: loaded kernel_add_row                        0x107928b20\n",
      "ggml_metal_init: loaded kernel_mul                            0x107928dd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1079295a0\n",
      "ggml_metal_init: loaded kernel_scale                          0x107929fe0\n",
      "ggml_metal_init: loaded kernel_silu                           0x10792a8e0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1079282a0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10792b0b0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10792bb20\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10792c5f0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10792d640\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10792ebb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10792ee10\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10792e1d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10792f640\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10792ffc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x107930940\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1079312c0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x107931c80\n",
      "ggml_metal_init: loaded kernel_norm                           0x107932e20\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1079338b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x107933e70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1079347f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x107935200\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x107935c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x107936dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1079377d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1079381e0\n",
      "ggml_metal_init: loaded kernel_rope                           0x107938440\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x107938ea0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x107939a60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10793a970\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x107932460\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.97 MB, (35091.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (35115.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (36397.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (36738.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (37122.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and feel blessed everyday. It’s important to have goals for our future and work hard to achieve them, but it’s equally as vital that we enjoy the moment while striving toward these targets. We only get one opportunity at this thing called life and I don’t intend on wasting mine being unhappy or unsatisfied.\n",
      "I believe we all have a purpose for being here. For some people, their purpose is to become a CEO of a Fortune 500 company while for others it’s to be the best parent they can possibly be. No one person’s purpose is any less important than another’s. I believe that you need to have good intentions in all that you do and treat your fellow human beings with kindness, empathy, compassion, acceptance, and love.\n",
      "I also believe it’s so important to surround yourself with positive people who lift you up rather than bring you down. It’s amazing how much of an effect the company we keep has on our lives; both good and bad. I have been fortunate enough to be surrounded by some pretty incredible human beings in my life.\n",
      "I believe that being happy is a choice, just as being sad is a choice. No one can “make” you feel anything without your permission. Happiness comes from within; not from any material thing. Too many people look for happiness in all the wrong places and then have the nerve to get angry when they don’t find it there.\n",
      "I believe that each day is a gift and an opportunity to make someone else’s life better, if only by a smile or kind word. I believe in random acts of kindness. They are like little gifts you leave behind you for people to unwrap at their leisure. These gifts are never expected or anticipated; they just sort of happen. And when they do, the giver usually gets more out of it than the receiver does.\n",
      "I believe that a good deed is its own reward and that we shouldn’t expect anything in return for doing one. I also believe that there is no such thing as a selfless act. When you do something nice for someone else, even if you don’t expect anything in return (and you shouldn’t), it always comes back to you in some way.\n",
      "I believe that life isn’t fair; it’s just life\n",
      "llama_print_timings:        load time =  8433.00 ms\n",
      "llama_print_timings:      sample time =   316.73 ms /   512 runs   (    0.62 ms per token,  1616.51 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25995.91 ms /   265 tokens (   98.10 ms per token,    10.19 tokens per second)\n",
      "llama_print_timings:        eval time = 35939.74 ms /   510 runs   (   70.47 ms per token,    14.19 tokens per second)\n",
      "llama_print_timings:       total time = 62290.80 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298201\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.96 MB\n",
      "llama_model_load_internal: mem required  = 35839.96 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104727050\n",
      "ggml_metal_init: loaded kernel_add_row                        0x104729220\n",
      "ggml_metal_init: loaded kernel_mul                            0x1047294c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104729ce0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10472a780\n",
      "ggml_metal_init: loaded kernel_silu                           0x10472aff0\n",
      "ggml_metal_init: loaded kernel_relu                           0x1047289f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10472b970\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10472c920\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10472cd40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10472df20\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10472d200\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10472f430\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10472e880\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10472fd60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104730710\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x104731090\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104731a10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1047324f0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1047334f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104734340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104734d10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104735730\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104736170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104736ce0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104737690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104737ee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1047388f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x1047393d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104739630\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10473a1b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10473b090\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x104732cd0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.97 MB, (35091.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (35115.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (36397.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (36738.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (37122.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "The Dalai Lama, when asked\n",
      "What's the meaning of life?\n",
      "In 2013, a group called The Wellbeing Project set out to answer one question: what makes people happy and healthy? They surveyed thousands of residents across the country—and then compared those responses with hard data about education levels, income, marital status, ethnicity, commute times, crime rates, and more.\n",
      "Their findings were surprising. The most important factor in whether or not someone is happy and healthy? Not money, not a good job, not even marriage. It's social relationships—which matter twice as much as your physical health when it comes to well-being.\n",
      "\"It's the quality of our relationships that matters,\" said one researcher. \"Those who reported having five or more close friends were far more likely to say they were 'thriving.' The evidence shows that people with a network of close friends are happier and healthier.\"\n",
      "So how do you make friends in a new city?\n",
      "In the beginning, it's easy: your coworkers at the office, your neighbors, members of your local church, book club, or sports league. But what happens when you can't find that group of friends who just get you—the ones you can talk to about anything and everything, the people with whom you want to spend every single second?\n",
      "That was my problem. I made friends easily and I had some amazing groups at work and in my neighborhood. But I couldn't seem to make those deeper connections that last longer than a few weeks—the type of friendships that last decades, not months, the kind where you can say anything without judgment or fear. The kind that fill your soul up with their laughter when they are around you and leave you feeling lonely when they're gone.\n",
      "That was my problem . . . until I started to play the Friendship Game.\n",
      "WHAT IS THE FRIENDSHIP GAME?\n",
      "The Friendship Game is all about making meaningful, lasting friendships with people who make your life better and more fun. If you want to have a richer, more fulfilling existence—the kind where you feel like you're part of something bigger than yourself—then this book is for you.\n",
      "The Friendship Game will help you create a network of friends who fill up your soul\n",
      "llama_print_timings:        load time =  5626.31 ms\n",
      "llama_print_timings:      sample time =   317.09 ms /   512 runs   (    0.62 ms per token,  1614.68 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25953.20 ms /   265 tokens (   97.94 ms per token,    10.21 tokens per second)\n",
      "llama_print_timings:        eval time = 35907.34 ms /   510 runs   (   70.41 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:       total time = 62216.38 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298269\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 35090.96 MB\n",
      "llama_model_load_internal: mem required  = 35839.96 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1540a2d50\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1540a4f70\n",
      "ggml_metal_init: loaded kernel_mul                            0x1540a5220\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1540a59f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x1540a6430\n",
      "ggml_metal_init: loaded kernel_silu                           0x1540a6d30\n",
      "ggml_metal_init: loaded kernel_relu                           0x1540a46f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1540a7500\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x1540a7f70\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1540a8a40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x1540a9a90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1540ab000\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1540ab260\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1540aa620\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x1540aba90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1540ac410\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1540acd90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1540ad710\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1540ae0d0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1540af270\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1540afd00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1540b02c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1540b0c40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1540b1650\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1540b2080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1540b3210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1540b3c20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x1540b4630\n",
      "ggml_metal_init: loaded kernel_rope                           0x1540b4890\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1540b52f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1540b5eb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1540b6dc0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1540ae8b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35090.97 MB, (35091.42 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (35115.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (36397.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (36738.59 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (37122.59 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\" Pablo Picasso\n",
      "\"I believe that the only courage anybody ever needs is the courage to follow your own dreams.\" Oprah Winfrey\n",
      "\"Earth and sky, woods and fields, lakes and rivers, the mountain and the sea, are excellent schoolmasters, and teach some of us more than we can ever learn from books.\" John Lubbock\n",
      "\"I always did something I was a little not ready to do. I think that's how you grow. When there's that moment of 'Wow, I'm not really sure I can do this,' and you push through those moments, that's when you have a breakthrough.\" Marissa Mayer\n",
      "\"I've missed more than 9000 shots in my career. I've lost almost 300 games. Twenty six times I've been trusted to take the game winning shot and missed. I've failed over and over and over again in my life. And that is why I succeed.\" Michael Jordan\n",
      "\"Happiness doesn’t depend on any external conditions, it is governed by our mental attitude.\" Dale Carnegie\n",
      "\"I am what you might call abstractly fond of children; but for any one child in particular I have never cared and never shall. They are bad enough in their infancy, and worse when they grow up.\" Mary Wortley Montagu\n",
      "\"The most important thing is not to stop questioning. Curiosity has its own reason for existing.\" Albert Einstein\n",
      "\"A person's a person no matter how small.\" Dr Seuss\n",
      "\"It's funny; all you have to do is talk to somebody and you find something in common.\" David Lee Roth\n",
      "\"The man with the greatest potential is the man who is always positive.\" Muhammad Ali\n",
      "\"When I was growing up, my house was filled with books. My mother's passion for the written word quite literally trickled down to me when I was a child.\" Dana Perino\n",
      "\"I would define, in brief compass, the true object of education, as the development of power and cultivation of mind in accordance with nature.\" John Ruskin\n",
      "\"In war there is no substitute for victory.\" General Douglas MacArthur\n",
      "\"Do not wait to strike till the iron is hot; but make it hot by striking.\" William Butler Yeats\n",
      "llama_print_timings:        load time =  5664.77 ms\n",
      "llama_print_timings:      sample time =   319.66 ms /   512 runs   (    0.62 ms per token,  1601.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 26200.49 ms /   265 tokens (   98.87 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:        eval time = 35976.47 ms /   510 runs   (   70.54 ms per token,    14.18 tokens per second)\n",
      "llama_print_timings:       total time = 62536.10 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce3e8",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5122b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298338\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.25 MB\n",
      "llama_model_load_internal: mem required  = 125274.25 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x104c13ad0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x104c15ca0\n",
      "ggml_metal_init: loaded kernel_mul                            0x104c15f60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x104c16750\n",
      "ggml_metal_init: loaded kernel_scale                          0x104c152b0\n",
      "ggml_metal_init: loaded kernel_silu                           0x104c155d0\n",
      "ggml_metal_init: loaded kernel_relu                           0x104c17190\n",
      "ggml_metal_init: loaded kernel_gelu                           0x104c18410\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x104c19460\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x104c1a800\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104c19ae0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104c19d40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x104c1b380\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x104c1be30\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x104c1c7b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x104c1d110\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x104c1da90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x104c1e410\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x104c1edd0\n",
      "ggml_metal_init: loaded kernel_norm                           0x104c1ffa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104c203c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x104c21780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x104c221c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x104c22c00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x104c23790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x104c24990\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x104c24bf0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x104c25ee0\n",
      "ggml_metal_init: loaded kernel_rope                           0x104c256f0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x104d24180\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x104d255e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x104d25c70\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x104d26860\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.28 MB, offs = 115439812608, (125025.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (125049.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (126331.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (126672.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (127056.91 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and make others happy.\n",
      "The purpose of our lives is to be happy. ~ Dalai Lama\n",
      "Happiness is a mental or emotional state of well-being defined by positive or pleasant emotions ranging from contentment to intense joy.[1] Happy mental states may also reflect judgements by a person about their overall well-being.[2][3] A variety of biological, psychological, economic, religious and philosophical approaches have striven to define happiness and identify its sources. Various research groups, including positive psychology and happiness economics are employing the scientific method to research questions about what \"happiness\" is, and how it might be attained.\n",
      "The United Nations declared 20 March the International Day of Happiness to recognise the relevance of happiness and wellbeing as universal goals.\n",
      "In philosophy, utilitarian theories of ethics have argued that the moral worth of an action is determined solely by its contribution to overall \"utility\" in maximizing happiness or pleasure as summed among all people. This general orientation toward promoting happiness for the greatest number of people is a common theme in ethical theory.\n",
      "2 General definitions\n",
      "3 Philosophy and religion\n",
      "4 Biological approach\n",
      "5 Psychological approach\n",
      "6 Economic approach (Utilitarianism)\n",
      "7 Religious approach\n",
      "8 Happiness economics\n",
      "9 National comparisons of life satisfaction\n",
      "10 Theories of happiness\n",
      "10.1 Utilitarians\n",
      "10.2 Aristotelian\n",
      "10.3 Hedonistic\n",
      "10.4 Stoic\n",
      "Happy young mother and child, detail of a tondo by Correggio, 1514\n",
      "The \"pursuit of happiness\" is enshrined in the US Declaration of Independence as an unalienable right, but scholars have long been divided on what it means. In 2013, the General Assembly of the United Nations issued a non-binding resolution that called for member states to measure and take into account citizen happiness and wellbeing rather than simply economic measures when making policy decisions.[4]\n",
      "Happiness is notoriously difficult to define. While most people agree they know it when they see it, there is little consensus on what constitutes the state of being happy, or how happiness might be achieved. Happiness has been said to lie at\n",
      "llama_print_timings:        load time = 44976.86 ms\n",
      "llama_print_timings:      sample time =   334.27 ms /   512 runs   (    0.65 ms per token,  1531.70 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32076.02 ms /   265 tokens (  121.04 ms per token,     8.26 tokens per second)\n",
      "llama_print_timings:        eval time = 152238.19 ms /   510 runs   (  298.51 ms per token,     3.35 tokens per second)\n",
      "llama_print_timings:       total time = 184693.08 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86588417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298568\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.25 MB\n",
      "llama_model_load_internal: mem required  = 125274.25 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x100b09560\n",
      "ggml_metal_init: loaded kernel_add_row                        0x100b0bcc0\n",
      "ggml_metal_init: loaded kernel_mul                            0x100b0bf80\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x100b0c790\n",
      "ggml_metal_init: loaded kernel_scale                          0x100b0d230\n",
      "ggml_metal_init: loaded kernel_silu                           0x100b0dab0\n",
      "ggml_metal_init: loaded kernel_relu                           0x100b0b440\n",
      "ggml_metal_init: loaded kernel_gelu                           0x100b0e270\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x100b0ed50\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x100b0f5c0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x100b0fc80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x100b10b00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x100b11f50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x100b11370\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x100b127e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x100b13160\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x100b13ac0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x100b14440\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x100b14e00\n",
      "ggml_metal_init: loaded kernel_norm                           0x100b15fd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x100b163f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x100b177b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x100b181f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x100b18c30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x100b197c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x100b1a9c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x100b1ac20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x100b1bf10\n",
      "ggml_metal_init: loaded kernel_rope                           0x100b1b720\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x100b1cca0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x100b1db60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x100b1e740\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x100b157d0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.28 MB, offs = 115439812608, (125025.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (125049.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (126331.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (126672.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (127056.91 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift.\n",
      "The purpose of life is to give it away.”\n",
      "I am a writer, teacher and speaker who believes that learning to love ourselves and each other is our greatest challenge on this earth. When we do so, we find true meaning in life.\n",
      "My work at the intersection of spirituality, psychology, and social justice is rooted in my own journey as a child of divorce, an immigrant from Iran, a refugee from revolution, a woman who has struggled with depression and addiction, a mother whose son died suddenly, a wife who survived her husband’s infidelity, a seeker who has been on a lifelong quest for meaning.\n",
      "I draw upon the depth of my experience to offer a unique perspective that is not always comfortable or easy, but is life-changing and transformative. I believe that our most difficult experiences can be the doorway to wisdom, compassion and strength, if we are willing to walk through it. It is this courage that allows us to find meaning in suffering, to make a difference in the lives of others and to leave an enduring legacy for future generations.\n",
      "I would love to hear from you! Please leave me a message below:\n",
      "Please keep me informed about upcoming events and programs.\n",
      "Sign-up for Sharon’s eNewsletter\n",
      "For media, interview or speaking requests, please use the form above or click here to email me directly.\n",
      "If you are looking for information on my books, visit my author website at www.sharonsalzbergbooks.com.\n",
      "Click HERE to download a high resolution photo of Sharon Salzberg. Photo credit: Robert Ullman. Please include photographer’s name and credit when using this image. Thank you!\n",
      "Pingback: 2013 in review | Sharon Salzberg\n",
      "Pingback: The Power of Mindfulness-Based Stress Reduction: An Interview with Dr. Bob Stahl | CALM CHATTER: tips for a happy, healthy mind\n",
      "Dear Sharon Salzberg,\n",
      "I am writing to you after reading in your memoir “Loving Kindness” about the time that you spent with Sri K. Pattabhi Jois at his Ashtanga Yoga Research Institute in Mysore, India. I would very much love to hear more about this experience as I have recently\n",
      "llama_print_timings:        load time = 46523.83 ms\n",
      "llama_print_timings:      sample time =   327.19 ms /   512 runs   (    0.64 ms per token,  1564.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31684.15 ms /   265 tokens (  119.56 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:        eval time = 153002.44 ms /   510 runs   (  300.00 ms per token,     3.33 tokens per second)\n",
      "llama_print_timings:       total time = 185058.99 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e1e0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 953 (f514d1b)\n",
      "main: seed  = 1691298801\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_head_kv  = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size = 124525.25 MB\n",
      "llama_model_load_internal: mem required  = 125274.25 MB (+ 1280.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x122111530\n",
      "ggml_metal_init: loaded kernel_add_row                        0x122113700\n",
      "ggml_metal_init: loaded kernel_mul                            0x1221139c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1221141b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x122112d10\n",
      "ggml_metal_init: loaded kernel_silu                           0x122113030\n",
      "ggml_metal_init: loaded kernel_relu                           0x122114bf0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x122115e70\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x122116ec0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x122118260\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x122117540\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x1221177a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x122118de0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x122119890\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12211a210\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12211ab70\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10510b1e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10510be40\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10510c500\n",
      "ggml_metal_init: loaded kernel_norm                           0x10510d650\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10510e280\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10510e6a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10510f160\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10510fb90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1051105c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x105111160\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x105111b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x105112580\n",
      "ggml_metal_init: loaded kernel_rope                           0x105113a00\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x105114080\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x105114f90\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12211c0f0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12211c6c0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 147456.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   500.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.28 MB, offs = 115439812608, (125025.73 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    24.17 MB, (125049.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1282.00 MB, (126331.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   341.00 MB, (126672.91 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   384.00 MB, (127056.91 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love and be loved. All of us want to connect with others in a deep, heartfelt way. We want to know that we’re valued and appreciated for who we are.\n",
      "But we don’t always get what we want. In fact, sometimes our relationships fall short of our expectations. It hurts when you give love and it isn’t returned. When this happens, do you blame yourself? Do you think that if only you weren’t so demanding or needy, everything would be fine?\n",
      "It might seem like the answer is to stop expecting so much from others. But that doesn’t make sense – why should we expect less of those we love and care about? The problem isn’t that our expectations are too high; it’s that we may not have communicated them clearly enough, or perhaps we haven’t been clear on what is important to us in the first place.\n",
      "You might be wondering: How do I communicate my needs so they will be understood? What if I don’t even know what my needs are? These questions are important and will be addressed in future blogs. But for now, let’s start by looking at how you can stop blaming yourself when your relationships feel less than fulfilling.\n",
      "Stop Blaming Yourself: 4 Steps to Help You Change the Way You Think About Relationships\n",
      "Step One: Identify what is important to you in a relationship. What do you value? This may take some deep thinking. It’s helpful to write down your thoughts and feelings, or talk with someone whose opinion you value. Remember that there are no “wrong” answers – just what works for you.\n",
      "Step Two: Identify how others affect you when they don’t meet your needs. In other words, what happens when relationships feel unfulfilling? Do you get upset and angry, or do you withdraw? Take some time to reflect on this question as well.\n",
      "Step Three: Ask yourself if the way you respond is helpful in getting your needs met. If it doesn’t work, then it’s not useful! You may want to change what you’re doing so that you feel better and are more effective at communicating with others. For example, when relationships feel unfulfilling do you get angry and lash out? This might cause the other person to respond in kind, which leads to a cycle of negativity.\n",
      "Step Four\n",
      "llama_print_timings:        load time = 48958.78 ms\n",
      "llama_print_timings:      sample time =   328.47 ms /   512 runs   (    0.64 ms per token,  1558.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31904.30 ms /   265 tokens (  120.39 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:        eval time = 153851.40 ms /   510 runs   (  301.67 ms per token,     3.31 tokens per second)\n",
      "llama_print_timings:       total time = 186129.54 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959fbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
