{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Thu Dec 21 21:05:58 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:25:00.0 Off |                  Off |\n",
      "| 30%   30C    P8              23W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:41:00.0 Off |                  Off |\n",
      "| 31%   29C    P8              20W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        On  | 00000000:61:00.0 Off |                  Off |\n",
      "| 30%   27C    P8              13W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        On  | 00000000:81:00.0 Off |                  Off |\n",
      "| 31%   32C    P8              25W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        On  | 00000000:A1:00.0 Off |                  Off |\n",
      "| 32%   31C    P8              21W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        On  | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   26C    P8              13W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       1056617192 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 14321, done.\u001b[K\n",
      "remote: Counting objects: 100% (4354/4354), done.\u001b[K\n",
      "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
      "remote: Total 14321 (delta 4263), reused 4197 (delta 4177), pack-reused 9967\u001b[K\n",
      "Receiving objects: 100% (14321/14321), 16.39 MiB | 21.54 MiB/s, done.\n",
      "Resolving deltas: 100% (10051/10051), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f20a13-e11c-4e56-98b9-4a896510f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   9941      0 --:--:-- --:--:-- --:--:--  9939\n",
      "Downloading tokenizer\n",
      "--2023-12-21 18:40:30--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-12-21 18:40:30 (10.4 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-12-21 18:40:30--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 18:40:30 (51.3 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-12-21 18:40:30--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[===================>]  12.55G  28.9MB/s    in 9m 21s  \n",
      "\n",
      "2023-12-21 18:49:52 (22.9 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-12-21 18:49:52--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 18:49:52 (43.6 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 18:49:52--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 18:49:52 (170 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-12-21 18:50:16--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  52.3MB/s    in 5m 35s  \n",
      "\n",
      "2023-12-21 18:55:51 (37.1 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-21 18:55:51--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  29.8MB/s    in 8m 9s   \n",
      "\n",
      "2023-12-21 19:04:01 (25.4 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-21 19:04:01--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:04:02 (44.2 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 19:04:02--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:04:02 (226 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-12-21 19:05:09--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  82.3MB/s    in 4m 4s   \n",
      "\n",
      "2023-12-21 19:09:15 (63.5 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:09:15--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  59.7MB/s    in 4m 30s  \n",
      "\n",
      "2023-12-21 19:13:45 (57.5 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:13:45--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  13.8MB/s    in 17m 29s \n",
      "\n",
      "2023-12-21 19:31:15 (14.8 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:31:15--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  96.3MB/s    in 3m 32s  \n",
      "\n",
      "2023-12-21 19:34:47 (73.3 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:34:47--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:34:47 (81.6 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 19:34:47--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:34:48 (153 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-12-21 19:37:36--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  94.6MB/s    in 3m 7s   \n",
      "\n",
      "2023-12-21 19:40:44 (83.2 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:40:44--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  80.0MB/s    in 3m 23s  \n",
      "\n",
      "2023-12-21 19:44:07 (76.7 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:44:07--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  86.1MB/s    in 3m 11s  \n",
      "\n",
      "2023-12-21 19:47:18 (81.7 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:47:18--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  91.2MB/s    in 3m 55s  \n",
      "\n",
      "2023-12-21 19:51:14 (66.2 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:51:14--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  47.4MB/s    in 4m 48s  \n",
      "\n",
      "2023-12-21 19:56:02 (54.1 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:56:02--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  95.3MB/s    in 3m 2s   \n",
      "\n",
      "2023-12-21 19:59:05 (85.7 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:59:05--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  49.4MB/s    in 5m 12s  \n",
      "\n",
      "2023-12-21 20:04:17 (49.9 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:04:17--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  68.6MB/s    in 3m 40s  \n",
      "\n",
      "2023-12-21 20:07:58 (70.7 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:07:58--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 20:07:58 (42.1 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 20:07:58--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 20:07:59 (490 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B\t7B-v2\t\t\t  ggml-vocab-mpt.gguf\n",
      "13B-v2\tggml-vocab-aquila.gguf\t  ggml-vocab-refact.gguf\n",
      "30B\tggml-vocab-baichuan.gguf  ggml-vocab-stablelm-3b-4e1t.gguf\n",
      "65B\tggml-vocab-falcon.gguf\t  ggml-vocab-starcoder.gguf\n",
      "70B-v2\tggml-vocab-gpt-neox.gguf  tokenizer.model\n",
      "7B\tggml-vocab-llama.gguf\t  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# Obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376847f6-3509-4a11-b211-ec1fb0d73ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13283  100 13283    0     0  41071      0 --:--:-- --:--:-- --:--:-- 41123\n"
     ]
    }
   ],
   "source": [
    "# If you encounter the error \"does not appear to have a file named config.json\" when converting the models to ggml FP16 format, try to convert the model to huggingface format to get the config.json file.\n",
    "!curl -o convert_llama_weights_to_hf.py https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bee9eb-b358-40f2-8582-f93dad231f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ff2a39-7f7d-4285-9652-a89b9503005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp tokenizer.model 7B/\n",
    "!cp tokenizer.model 13B/\n",
    "!cp tokenizer.model 30B/\n",
    "!cp tokenizer.model 65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc57513-b8fa-46ba-8315-9f118b998c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.36.2)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: protobuf>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall accelerate # If you have this package, uninstall it first, then use `convert to hf model` to get the config.json.\n",
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636f3946-8a2b-4a9d-a23a-9e880e96ad65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/7B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/13B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/30B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/65B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n"
     ]
    }
   ],
   "source": [
    "# We don't need these models actually. We only need this to figure out the config.json error.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/7B/ --model_size 7B --output_dir models/7B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/13B/ --model_size 13B --output_dir models/13B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/30B/ --model_size 30B --output_dir models/30B/ # Surprisingly, it still solves the problem although you can't find the config.json file.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/65B/ --model_size 65B --output_dir models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbf7dba-9d3e-4c40-95c8-58ccc63d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit your params.json file if the \"vocab_size\" mismatch\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/7B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/7B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/13B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/13B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/30B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/30B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/65B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/65B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/7B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [4096]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 4096]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "skipping tensor blk.0.attn_rot_embd\n",
      "skipping tensor blk.1.attn_rot_embd\n",
      "skipping tensor blk.2.attn_rot_embd\n",
      "skipping tensor blk.3.attn_rot_embd\n",
      "skipping tensor blk.4.attn_rot_embd\n",
      "skipping tensor blk.5.attn_rot_embd\n",
      "skipping tensor blk.6.attn_rot_embd\n",
      "skipping tensor blk.7.attn_rot_embd\n",
      "skipping tensor blk.8.attn_rot_embd\n",
      "skipping tensor blk.9.attn_rot_embd\n",
      "skipping tensor blk.10.attn_rot_embd\n",
      "skipping tensor blk.11.attn_rot_embd\n",
      "skipping tensor blk.12.attn_rot_embd\n",
      "skipping tensor blk.13.attn_rot_embd\n",
      "skipping tensor blk.14.attn_rot_embd\n",
      "skipping tensor blk.15.attn_rot_embd\n",
      "skipping tensor blk.16.attn_rot_embd\n",
      "skipping tensor blk.17.attn_rot_embd\n",
      "skipping tensor blk.18.attn_rot_embd\n",
      "skipping tensor blk.19.attn_rot_embd\n",
      "skipping tensor blk.20.attn_rot_embd\n",
      "skipping tensor blk.21.attn_rot_embd\n",
      "skipping tensor blk.22.attn_rot_embd\n",
      "skipping tensor blk.23.attn_rot_embd\n",
      "skipping tensor blk.24.attn_rot_embd\n",
      "skipping tensor blk.25.attn_rot_embd\n",
      "skipping tensor blk.26.attn_rot_embd\n",
      "skipping tensor blk.27.attn_rot_embd\n",
      "skipping tensor blk.28.attn_rot_embd\n",
      "skipping tensor blk.29.attn_rot_embd\n",
      "skipping tensor blk.30.attn_rot_embd\n",
      "skipping tensor blk.31.attn_rot_embd\n",
      "Writing models/7B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 11/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 12/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 13/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 14/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 15/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 16/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 17/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 18/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 19/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 20/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 21/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 22/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 23/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 24/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 25/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 26/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 28/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 29/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 31/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 32/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 33/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 34/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 35/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 37/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 38/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 40/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 41/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 42/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 43/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 44/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 46/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 47/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 49/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 50/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 51/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 52/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 53/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 55/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 56/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 58/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 59/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 60/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 61/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 62/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 64/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 65/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 67/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 68/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 69/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 70/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 71/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 73/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 74/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 76/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 77/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 78/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 79/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 80/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 82/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 83/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 85/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 86/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 87/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 88/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 89/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 91/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 92/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 94/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 95/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 96/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 97/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 98/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[100/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[101/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[102/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[103/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[104/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[105/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[106/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[108/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[109/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[110/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[111/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[112/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[113/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[114/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[115/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[116/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[117/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[118/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[119/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[120/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[121/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[122/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[123/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[124/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[125/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[126/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[127/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[128/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[129/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[130/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[131/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[132/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[133/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[134/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[135/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[136/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[137/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[138/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[139/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[140/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[141/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[142/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[143/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[144/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[145/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[146/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[147/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[148/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[149/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[150/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[151/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[152/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[153/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[154/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[155/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[156/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[158/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[159/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[160/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[162/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[163/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[164/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[165/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[166/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[167/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[168/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[169/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[170/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[171/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[172/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[173/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[174/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[175/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[176/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[177/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[178/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[179/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[180/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[181/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[182/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[183/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[184/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[185/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[186/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[187/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[188/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[189/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[190/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[191/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[192/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[193/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[194/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[195/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[196/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[197/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[198/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[199/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[200/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[201/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[203/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[204/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[205/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[206/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[207/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[209/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[210/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[211/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[212/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[213/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[214/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[215/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[216/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[217/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[218/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[219/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[220/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[221/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[222/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[223/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[224/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[225/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[226/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[227/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[228/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[229/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[230/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[231/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[232/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[233/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[234/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[235/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[236/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[237/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[238/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[239/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[240/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[241/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[242/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[243/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[244/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[245/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[246/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[247/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[248/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[249/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[250/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[251/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[252/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10\n",
      "[253/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10\n",
      "[254/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  10\n",
      "[255/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[256/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[257/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[258/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[259/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[260/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10\n",
      "[261/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10\n",
      "[262/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10\n",
      "[263/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  10\n",
      "[264/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[265/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[266/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[267/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[268/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[269/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10\n",
      "[270/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10\n",
      "[271/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10\n",
      "[272/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  10\n",
      "[273/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[274/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[275/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[276/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[277/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[278/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10\n",
      "[279/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  11\n",
      "[280/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  11\n",
      "[281/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  11\n",
      "[282/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[283/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[284/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[285/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[286/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11\n",
      "[287/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  11\n",
      "[288/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  11\n",
      "[289/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  11\n",
      "[290/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  11\n",
      "[291/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "Wrote models/7B/ggml-model-f16.gguf\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "params = Params(n_vocab=32000, n_embd=5120, n_layer=40, n_ctx=2048, n_ff=13824, n_head=40, n_head_kv=40, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/13B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 5120]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [5120]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 5120]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [5120]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [5120]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [5120]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [5120]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [5120]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [5120]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [5120]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [5120]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [5120]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [5120]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [5120]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [5120]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [5120]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [5120]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [5120]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [5120]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [5120]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [5120]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [5120]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [5120]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [5120]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [5120]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [5120]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [5120]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [5120]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [5120]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [5120]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [5120]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [5120]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [5120]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [5120]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [5120]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [5120]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [5120]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [5120]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [5120]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [5120]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [5120]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [5120]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [5120]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [5120]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/13B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/363] Writing tensor token_embd.weight                      | size  32000 x   5120  | type F16  | T+   0\n",
      "[  2/363] Writing tensor output_norm.weight                     | size   5120           | type F32  | T+   0\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type F16  | T+   0\n",
      "[  4/363] Writing tensor blk.0.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  5/363] Writing tensor blk.0.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  6/363] Writing tensor blk.0.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  7/363] Writing tensor blk.0.attn_output.weight               | size   5120 x   5120  | type F16  | T+   1\n",
      "[  8/363] Writing tensor blk.0.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[  9/363] Writing tensor blk.0.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   1\n",
      "[ 10/363] Writing tensor blk.0.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 11/363] Writing tensor blk.0.attn_norm.weight                 | size   5120           | type F32  | T+   1\n",
      "[ 12/363] Writing tensor blk.0.ffn_norm.weight                  | size   5120           | type F32  | T+   1\n",
      "[ 13/363] Writing tensor blk.1.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 14/363] Writing tensor blk.1.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 15/363] Writing tensor blk.1.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 16/363] Writing tensor blk.1.attn_output.weight               | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 17/363] Writing tensor blk.1.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 18/363] Writing tensor blk.1.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   3\n",
      "[ 19/363] Writing tensor blk.1.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 20/363] Writing tensor blk.1.attn_norm.weight                 | size   5120           | type F32  | T+   3\n",
      "[ 21/363] Writing tensor blk.1.ffn_norm.weight                  | size   5120           | type F32  | T+   3\n",
      "[ 22/363] Writing tensor blk.2.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 23/363] Writing tensor blk.2.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 24/363] Writing tensor blk.2.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 25/363] Writing tensor blk.2.attn_output.weight               | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 26/363] Writing tensor blk.2.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 27/363] Writing tensor blk.2.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   4\n",
      "[ 28/363] Writing tensor blk.2.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 29/363] Writing tensor blk.2.attn_norm.weight                 | size   5120           | type F32  | T+   4\n",
      "[ 30/363] Writing tensor blk.2.ffn_norm.weight                  | size   5120           | type F32  | T+   4\n",
      "[ 31/363] Writing tensor blk.3.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 32/363] Writing tensor blk.3.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 33/363] Writing tensor blk.3.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 34/363] Writing tensor blk.3.attn_output.weight               | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 35/363] Writing tensor blk.3.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 36/363] Writing tensor blk.3.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   5\n",
      "[ 37/363] Writing tensor blk.3.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 38/363] Writing tensor blk.3.attn_norm.weight                 | size   5120           | type F32  | T+   6\n",
      "[ 39/363] Writing tensor blk.3.ffn_norm.weight                  | size   5120           | type F32  | T+   6\n",
      "[ 40/363] Writing tensor blk.4.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 41/363] Writing tensor blk.4.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 42/363] Writing tensor blk.4.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 43/363] Writing tensor blk.4.attn_output.weight               | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 44/363] Writing tensor blk.4.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 45/363] Writing tensor blk.4.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   7\n",
      "[ 46/363] Writing tensor blk.4.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   8\n",
      "[ 47/363] Writing tensor blk.4.attn_norm.weight                 | size   5120           | type F32  | T+   8\n",
      "[ 48/363] Writing tensor blk.4.ffn_norm.weight                  | size   5120           | type F32  | T+   8\n",
      "[ 49/363] Writing tensor blk.5.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 50/363] Writing tensor blk.5.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 51/363] Writing tensor blk.5.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 52/363] Writing tensor blk.5.attn_output.weight               | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 53/363] Writing tensor blk.5.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   8\n",
      "[ 54/363] Writing tensor blk.5.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   8\n",
      "[ 55/363] Writing tensor blk.5.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   9\n",
      "[ 56/363] Writing tensor blk.5.attn_norm.weight                 | size   5120           | type F32  | T+   9\n",
      "[ 57/363] Writing tensor blk.5.ffn_norm.weight                  | size   5120           | type F32  | T+   9\n",
      "[ 58/363] Writing tensor blk.6.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 59/363] Writing tensor blk.6.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 60/363] Writing tensor blk.6.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 61/363] Writing tensor blk.6.attn_output.weight               | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 62/363] Writing tensor blk.6.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+  10\n",
      "[ 63/363] Writing tensor blk.6.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+  10\n",
      "[ 64/363] Writing tensor blk.6.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  10\n",
      "[ 65/363] Writing tensor blk.6.attn_norm.weight                 | size   5120           | type F32  | T+  10\n",
      "[ 66/363] Writing tensor blk.6.ffn_norm.weight                  | size   5120           | type F32  | T+  10\n",
      "[ 67/363] Writing tensor blk.7.attn_q.weight                    | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 68/363] Writing tensor blk.7.attn_k.weight                    | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 69/363] Writing tensor blk.7.attn_v.weight                    | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 70/363] Writing tensor blk.7.attn_output.weight               | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 71/363] Writing tensor blk.7.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+  11\n",
      "[ 72/363] Writing tensor blk.7.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+  12\n",
      "[ 73/363] Writing tensor blk.7.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  12\n",
      "[ 74/363] Writing tensor blk.7.attn_norm.weight                 | size   5120           | type F32  | T+  12\n",
      "[ 75/363] Writing tensor blk.7.ffn_norm.weight                  | size   5120           | type F32  | T+  12\n",
      "[ 76/363] Writing tensor blk.8.attn_q.weight                    | size   5120 x   5120  | type F16  | T+  12\n",
      "[ 77/363] Writing tensor blk.8.attn_k.weight                    | size   5120 x   5120  | type F16  | T+  12\n",
      "[ 78/363] Writing tensor blk.8.attn_v.weight                    | size   5120 x   5120  | type F16  | T+  12\n",
      "[ 79/363] Writing tensor blk.8.attn_output.weight               | size   5120 x   5120  | type F16  | T+  12\n",
      "[ 80/363] Writing tensor blk.8.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+  13\n",
      "[ 81/363] Writing tensor blk.8.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+  13\n",
      "[ 82/363] Writing tensor blk.8.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  13\n",
      "[ 83/363] Writing tensor blk.8.attn_norm.weight                 | size   5120           | type F32  | T+  13\n",
      "[ 84/363] Writing tensor blk.8.ffn_norm.weight                  | size   5120           | type F32  | T+  13\n",
      "[ 85/363] Writing tensor blk.9.attn_q.weight                    | size   5120 x   5120  | type F16  | T+  13\n",
      "[ 86/363] Writing tensor blk.9.attn_k.weight                    | size   5120 x   5120  | type F16  | T+  13\n",
      "[ 87/363] Writing tensor blk.9.attn_v.weight                    | size   5120 x   5120  | type F16  | T+  13\n",
      "[ 88/363] Writing tensor blk.9.attn_output.weight               | size   5120 x   5120  | type F16  | T+  14\n",
      "[ 89/363] Writing tensor blk.9.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+  14\n",
      "[ 90/363] Writing tensor blk.9.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+  15\n",
      "[ 91/363] Writing tensor blk.9.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  15\n",
      "[ 92/363] Writing tensor blk.9.attn_norm.weight                 | size   5120           | type F32  | T+  15\n",
      "[ 93/363] Writing tensor blk.9.ffn_norm.weight                  | size   5120           | type F32  | T+  15\n",
      "[ 94/363] Writing tensor blk.10.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[ 95/363] Writing tensor blk.10.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[ 96/363] Writing tensor blk.10.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[ 97/363] Writing tensor blk.10.attn_output.weight              | size   5120 x   5120  | type F16  | T+  16\n",
      "[ 98/363] Writing tensor blk.10.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  16\n",
      "[ 99/363] Writing tensor blk.10.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  17\n",
      "[100/363] Writing tensor blk.10.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  19\n",
      "[101/363] Writing tensor blk.10.attn_norm.weight                | size   5120           | type F32  | T+  19\n",
      "[102/363] Writing tensor blk.10.ffn_norm.weight                 | size   5120           | type F32  | T+  19\n",
      "[103/363] Writing tensor blk.11.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[104/363] Writing tensor blk.11.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[105/363] Writing tensor blk.11.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[106/363] Writing tensor blk.11.attn_output.weight              | size   5120 x   5120  | type F16  | T+  19\n",
      "[107/363] Writing tensor blk.11.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  19\n",
      "[108/363] Writing tensor blk.11.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  19\n",
      "[109/363] Writing tensor blk.11.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  19\n",
      "[110/363] Writing tensor blk.11.attn_norm.weight                | size   5120           | type F32  | T+  20\n",
      "[111/363] Writing tensor blk.11.ffn_norm.weight                 | size   5120           | type F32  | T+  20\n",
      "[112/363] Writing tensor blk.12.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[113/363] Writing tensor blk.12.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[114/363] Writing tensor blk.12.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[115/363] Writing tensor blk.12.attn_output.weight              | size   5120 x   5120  | type F16  | T+  20\n",
      "[116/363] Writing tensor blk.12.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  20\n",
      "[117/363] Writing tensor blk.12.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  20\n",
      "[118/363] Writing tensor blk.12.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  21\n",
      "[119/363] Writing tensor blk.12.attn_norm.weight                | size   5120           | type F32  | T+  21\n",
      "[120/363] Writing tensor blk.12.ffn_norm.weight                 | size   5120           | type F32  | T+  21\n",
      "[121/363] Writing tensor blk.13.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[122/363] Writing tensor blk.13.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[123/363] Writing tensor blk.13.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[124/363] Writing tensor blk.13.attn_output.weight              | size   5120 x   5120  | type F16  | T+  21\n",
      "[125/363] Writing tensor blk.13.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  21\n",
      "[126/363] Writing tensor blk.13.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  21\n",
      "[127/363] Writing tensor blk.13.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  22\n",
      "[128/363] Writing tensor blk.13.attn_norm.weight                | size   5120           | type F32  | T+  22\n",
      "[129/363] Writing tensor blk.13.ffn_norm.weight                 | size   5120           | type F32  | T+  22\n",
      "[130/363] Writing tensor blk.14.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[131/363] Writing tensor blk.14.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[132/363] Writing tensor blk.14.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[133/363] Writing tensor blk.14.attn_output.weight              | size   5120 x   5120  | type F16  | T+  22\n",
      "[134/363] Writing tensor blk.14.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  22\n",
      "[135/363] Writing tensor blk.14.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  23\n",
      "[136/363] Writing tensor blk.14.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  23\n",
      "[137/363] Writing tensor blk.14.attn_norm.weight                | size   5120           | type F32  | T+  23\n",
      "[138/363] Writing tensor blk.14.ffn_norm.weight                 | size   5120           | type F32  | T+  23\n",
      "[139/363] Writing tensor blk.15.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[140/363] Writing tensor blk.15.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[141/363] Writing tensor blk.15.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[142/363] Writing tensor blk.15.attn_output.weight              | size   5120 x   5120  | type F16  | T+  23\n",
      "[143/363] Writing tensor blk.15.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  23\n",
      "[144/363] Writing tensor blk.15.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  24\n",
      "[145/363] Writing tensor blk.15.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  24\n",
      "[146/363] Writing tensor blk.15.attn_norm.weight                | size   5120           | type F32  | T+  24\n",
      "[147/363] Writing tensor blk.15.ffn_norm.weight                 | size   5120           | type F32  | T+  24\n",
      "[148/363] Writing tensor blk.16.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[149/363] Writing tensor blk.16.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[150/363] Writing tensor blk.16.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[151/363] Writing tensor blk.16.attn_output.weight              | size   5120 x   5120  | type F16  | T+  24\n",
      "[152/363] Writing tensor blk.16.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  25\n",
      "[153/363] Writing tensor blk.16.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  26\n",
      "[154/363] Writing tensor blk.16.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  27\n",
      "[155/363] Writing tensor blk.16.attn_norm.weight                | size   5120           | type F32  | T+  27\n",
      "[156/363] Writing tensor blk.16.ffn_norm.weight                 | size   5120           | type F32  | T+  27\n",
      "[157/363] Writing tensor blk.17.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[158/363] Writing tensor blk.17.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[159/363] Writing tensor blk.17.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[160/363] Writing tensor blk.17.attn_output.weight              | size   5120 x   5120  | type F16  | T+  27\n",
      "[161/363] Writing tensor blk.17.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  27\n",
      "[162/363] Writing tensor blk.17.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  28\n",
      "[163/363] Writing tensor blk.17.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  28\n",
      "[164/363] Writing tensor blk.17.attn_norm.weight                | size   5120           | type F32  | T+  28\n",
      "[165/363] Writing tensor blk.17.ffn_norm.weight                 | size   5120           | type F32  | T+  28\n",
      "[166/363] Writing tensor blk.18.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[167/363] Writing tensor blk.18.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[168/363] Writing tensor blk.18.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[169/363] Writing tensor blk.18.attn_output.weight              | size   5120 x   5120  | type F16  | T+  28\n",
      "[170/363] Writing tensor blk.18.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  29\n",
      "[171/363] Writing tensor blk.18.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  29\n",
      "[172/363] Writing tensor blk.18.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  30\n",
      "[173/363] Writing tensor blk.18.attn_norm.weight                | size   5120           | type F32  | T+  30\n",
      "[174/363] Writing tensor blk.18.ffn_norm.weight                 | size   5120           | type F32  | T+  30\n",
      "[175/363] Writing tensor blk.19.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[176/363] Writing tensor blk.19.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[177/363] Writing tensor blk.19.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[178/363] Writing tensor blk.19.attn_output.weight              | size   5120 x   5120  | type F16  | T+  30\n",
      "[179/363] Writing tensor blk.19.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  30\n",
      "[180/363] Writing tensor blk.19.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  31\n",
      "[181/363] Writing tensor blk.19.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  31\n",
      "[182/363] Writing tensor blk.19.attn_norm.weight                | size   5120           | type F32  | T+  31\n",
      "[183/363] Writing tensor blk.19.ffn_norm.weight                 | size   5120           | type F32  | T+  31\n",
      "[184/363] Writing tensor blk.20.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[185/363] Writing tensor blk.20.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[186/363] Writing tensor blk.20.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[187/363] Writing tensor blk.20.attn_output.weight              | size   5120 x   5120  | type F16  | T+  31\n",
      "[188/363] Writing tensor blk.20.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  32\n",
      "[189/363] Writing tensor blk.20.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  32\n",
      "[190/363] Writing tensor blk.20.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  32\n",
      "[191/363] Writing tensor blk.20.attn_norm.weight                | size   5120           | type F32  | T+  32\n",
      "[192/363] Writing tensor blk.20.ffn_norm.weight                 | size   5120           | type F32  | T+  32\n",
      "[193/363] Writing tensor blk.21.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[194/363] Writing tensor blk.21.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[195/363] Writing tensor blk.21.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[196/363] Writing tensor blk.21.attn_output.weight              | size   5120 x   5120  | type F16  | T+  32\n",
      "[197/363] Writing tensor blk.21.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  33\n",
      "[198/363] Writing tensor blk.21.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  34\n",
      "[199/363] Writing tensor blk.21.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  35\n",
      "[200/363] Writing tensor blk.21.attn_norm.weight                | size   5120           | type F32  | T+  36\n",
      "[201/363] Writing tensor blk.21.ffn_norm.weight                 | size   5120           | type F32  | T+  36\n",
      "[202/363] Writing tensor blk.22.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  36\n",
      "[203/363] Writing tensor blk.22.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  36\n",
      "[204/363] Writing tensor blk.22.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  36\n",
      "[205/363] Writing tensor blk.22.attn_output.weight              | size   5120 x   5120  | type F16  | T+  36\n",
      "[206/363] Writing tensor blk.22.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  36\n",
      "[207/363] Writing tensor blk.22.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  37\n",
      "[208/363] Writing tensor blk.22.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  37\n",
      "[209/363] Writing tensor blk.22.attn_norm.weight                | size   5120           | type F32  | T+  38\n",
      "[210/363] Writing tensor blk.22.ffn_norm.weight                 | size   5120           | type F32  | T+  38\n",
      "[211/363] Writing tensor blk.23.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  38\n",
      "[212/363] Writing tensor blk.23.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  38\n",
      "[213/363] Writing tensor blk.23.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  38\n",
      "[214/363] Writing tensor blk.23.attn_output.weight              | size   5120 x   5120  | type F16  | T+  38\n",
      "[215/363] Writing tensor blk.23.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  38\n",
      "[216/363] Writing tensor blk.23.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  39\n",
      "[217/363] Writing tensor blk.23.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  39\n",
      "[218/363] Writing tensor blk.23.attn_norm.weight                | size   5120           | type F32  | T+  39\n",
      "[219/363] Writing tensor blk.23.ffn_norm.weight                 | size   5120           | type F32  | T+  39\n",
      "[220/363] Writing tensor blk.24.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  39\n",
      "[221/363] Writing tensor blk.24.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  39\n",
      "[222/363] Writing tensor blk.24.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  39\n",
      "[223/363] Writing tensor blk.24.attn_output.weight              | size   5120 x   5120  | type F16  | T+  39\n",
      "[224/363] Writing tensor blk.24.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  39\n",
      "[225/363] Writing tensor blk.24.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  40\n",
      "[226/363] Writing tensor blk.24.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  40\n",
      "[227/363] Writing tensor blk.24.attn_norm.weight                | size   5120           | type F32  | T+  40\n",
      "[228/363] Writing tensor blk.24.ffn_norm.weight                 | size   5120           | type F32  | T+  40\n",
      "[229/363] Writing tensor blk.25.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  40\n",
      "[230/363] Writing tensor blk.25.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  40\n",
      "[231/363] Writing tensor blk.25.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  40\n",
      "[232/363] Writing tensor blk.25.attn_output.weight              | size   5120 x   5120  | type F16  | T+  40\n",
      "[233/363] Writing tensor blk.25.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  40\n",
      "[234/363] Writing tensor blk.25.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  41\n",
      "[235/363] Writing tensor blk.25.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  42\n",
      "[236/363] Writing tensor blk.25.attn_norm.weight                | size   5120           | type F32  | T+  42\n",
      "[237/363] Writing tensor blk.25.ffn_norm.weight                 | size   5120           | type F32  | T+  42\n",
      "[238/363] Writing tensor blk.26.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  42\n",
      "[239/363] Writing tensor blk.26.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  42\n",
      "[240/363] Writing tensor blk.26.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  42\n",
      "[241/363] Writing tensor blk.26.attn_output.weight              | size   5120 x   5120  | type F16  | T+  42\n",
      "[242/363] Writing tensor blk.26.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  42\n",
      "[243/363] Writing tensor blk.26.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  42\n",
      "[244/363] Writing tensor blk.26.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  43\n",
      "[245/363] Writing tensor blk.26.attn_norm.weight                | size   5120           | type F32  | T+  43\n",
      "[246/363] Writing tensor blk.26.ffn_norm.weight                 | size   5120           | type F32  | T+  43\n",
      "[247/363] Writing tensor blk.27.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  43\n",
      "[248/363] Writing tensor blk.27.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  43\n",
      "[249/363] Writing tensor blk.27.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  43\n",
      "[250/363] Writing tensor blk.27.attn_output.weight              | size   5120 x   5120  | type F16  | T+  43\n",
      "[251/363] Writing tensor blk.27.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  43\n",
      "[252/363] Writing tensor blk.27.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  43\n",
      "[253/363] Writing tensor blk.27.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  43\n",
      "[254/363] Writing tensor blk.27.attn_norm.weight                | size   5120           | type F32  | T+  44\n",
      "[255/363] Writing tensor blk.27.ffn_norm.weight                 | size   5120           | type F32  | T+  44\n",
      "[256/363] Writing tensor blk.28.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  44\n",
      "[257/363] Writing tensor blk.28.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  44\n",
      "[258/363] Writing tensor blk.28.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  45\n",
      "[259/363] Writing tensor blk.28.attn_output.weight              | size   5120 x   5120  | type F16  | T+  45\n",
      "[260/363] Writing tensor blk.28.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  46\n",
      "[261/363] Writing tensor blk.28.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  46\n",
      "[262/363] Writing tensor blk.28.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  46\n",
      "[263/363] Writing tensor blk.28.attn_norm.weight                | size   5120           | type F32  | T+  47\n",
      "[264/363] Writing tensor blk.28.ffn_norm.weight                 | size   5120           | type F32  | T+  47\n",
      "[265/363] Writing tensor blk.29.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  47\n",
      "[266/363] Writing tensor blk.29.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  47\n",
      "[267/363] Writing tensor blk.29.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  47\n",
      "[268/363] Writing tensor blk.29.attn_output.weight              | size   5120 x   5120  | type F16  | T+  47\n",
      "[269/363] Writing tensor blk.29.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  47\n",
      "[270/363] Writing tensor blk.29.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  47\n",
      "[271/363] Writing tensor blk.29.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  48\n",
      "[272/363] Writing tensor blk.29.attn_norm.weight                | size   5120           | type F32  | T+  48\n",
      "[273/363] Writing tensor blk.29.ffn_norm.weight                 | size   5120           | type F32  | T+  48\n",
      "[274/363] Writing tensor blk.30.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  48\n",
      "[275/363] Writing tensor blk.30.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  48\n",
      "[276/363] Writing tensor blk.30.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  48\n",
      "[277/363] Writing tensor blk.30.attn_output.weight              | size   5120 x   5120  | type F16  | T+  48\n",
      "[278/363] Writing tensor blk.30.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  48\n",
      "[279/363] Writing tensor blk.30.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  49\n",
      "[280/363] Writing tensor blk.30.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  50\n",
      "[281/363] Writing tensor blk.30.attn_norm.weight                | size   5120           | type F32  | T+  50\n",
      "[282/363] Writing tensor blk.30.ffn_norm.weight                 | size   5120           | type F32  | T+  50\n",
      "[283/363] Writing tensor blk.31.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  50\n",
      "[284/363] Writing tensor blk.31.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  50\n",
      "[285/363] Writing tensor blk.31.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  50\n",
      "[286/363] Writing tensor blk.31.attn_output.weight              | size   5120 x   5120  | type F16  | T+  50\n",
      "[287/363] Writing tensor blk.31.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  50\n",
      "[288/363] Writing tensor blk.31.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  50\n",
      "[289/363] Writing tensor blk.31.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  51\n",
      "[290/363] Writing tensor blk.31.attn_norm.weight                | size   5120           | type F32  | T+  51\n",
      "[291/363] Writing tensor blk.31.ffn_norm.weight                 | size   5120           | type F32  | T+  51\n",
      "[292/363] Writing tensor blk.32.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  51\n",
      "[293/363] Writing tensor blk.32.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  51\n",
      "[294/363] Writing tensor blk.32.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  51\n",
      "[295/363] Writing tensor blk.32.attn_output.weight              | size   5120 x   5120  | type F16  | T+  51\n",
      "[296/363] Writing tensor blk.32.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  51\n",
      "[297/363] Writing tensor blk.32.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  51\n",
      "[298/363] Writing tensor blk.32.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  52\n",
      "[299/363] Writing tensor blk.32.attn_norm.weight                | size   5120           | type F32  | T+  52\n",
      "[300/363] Writing tensor blk.32.ffn_norm.weight                 | size   5120           | type F32  | T+  52\n",
      "[301/363] Writing tensor blk.33.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  52\n",
      "[302/363] Writing tensor blk.33.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  52\n",
      "[303/363] Writing tensor blk.33.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  52\n",
      "[304/363] Writing tensor blk.33.attn_output.weight              | size   5120 x   5120  | type F16  | T+  52\n",
      "[305/363] Writing tensor blk.33.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  52\n",
      "[306/363] Writing tensor blk.33.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  53\n",
      "[307/363] Writing tensor blk.33.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  53\n",
      "[308/363] Writing tensor blk.33.attn_norm.weight                | size   5120           | type F32  | T+  53\n",
      "[309/363] Writing tensor blk.33.ffn_norm.weight                 | size   5120           | type F32  | T+  53\n",
      "[310/363] Writing tensor blk.34.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  53\n",
      "[311/363] Writing tensor blk.34.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  53\n",
      "[312/363] Writing tensor blk.34.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  54\n",
      "[313/363] Writing tensor blk.34.attn_output.weight              | size   5120 x   5120  | type F16  | T+  54\n",
      "[314/363] Writing tensor blk.34.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  54\n",
      "[315/363] Writing tensor blk.34.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  54\n",
      "[316/363] Writing tensor blk.34.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  55\n",
      "[317/363] Writing tensor blk.34.attn_norm.weight                | size   5120           | type F32  | T+  55\n",
      "[318/363] Writing tensor blk.34.ffn_norm.weight                 | size   5120           | type F32  | T+  55\n",
      "[319/363] Writing tensor blk.35.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  55\n",
      "[320/363] Writing tensor blk.35.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  55\n",
      "[321/363] Writing tensor blk.35.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  55\n",
      "[322/363] Writing tensor blk.35.attn_output.weight              | size   5120 x   5120  | type F16  | T+  55\n",
      "[323/363] Writing tensor blk.35.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  55\n",
      "[324/363] Writing tensor blk.35.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  56\n",
      "[325/363] Writing tensor blk.35.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  56\n",
      "[326/363] Writing tensor blk.35.attn_norm.weight                | size   5120           | type F32  | T+  56\n",
      "[327/363] Writing tensor blk.35.ffn_norm.weight                 | size   5120           | type F32  | T+  56\n",
      "[328/363] Writing tensor blk.36.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  56\n",
      "[329/363] Writing tensor blk.36.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  56\n",
      "[330/363] Writing tensor blk.36.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  56\n",
      "[331/363] Writing tensor blk.36.attn_output.weight              | size   5120 x   5120  | type F16  | T+  56\n",
      "[332/363] Writing tensor blk.36.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  56\n",
      "[333/363] Writing tensor blk.36.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  57\n",
      "[334/363] Writing tensor blk.36.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  58\n",
      "[335/363] Writing tensor blk.36.attn_norm.weight                | size   5120           | type F32  | T+  58\n",
      "[336/363] Writing tensor blk.36.ffn_norm.weight                 | size   5120           | type F32  | T+  58\n",
      "[337/363] Writing tensor blk.37.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  58\n",
      "[338/363] Writing tensor blk.37.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  58\n",
      "[339/363] Writing tensor blk.37.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  58\n",
      "[340/363] Writing tensor blk.37.attn_output.weight              | size   5120 x   5120  | type F16  | T+  58\n",
      "[341/363] Writing tensor blk.37.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  58\n",
      "[342/363] Writing tensor blk.37.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  59\n",
      "[343/363] Writing tensor blk.37.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  59\n",
      "[344/363] Writing tensor blk.37.attn_norm.weight                | size   5120           | type F32  | T+  59\n",
      "[345/363] Writing tensor blk.37.ffn_norm.weight                 | size   5120           | type F32  | T+  59\n",
      "[346/363] Writing tensor blk.38.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  59\n",
      "[347/363] Writing tensor blk.38.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  59\n",
      "[348/363] Writing tensor blk.38.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  59\n",
      "[349/363] Writing tensor blk.38.attn_output.weight              | size   5120 x   5120  | type F16  | T+  59\n",
      "[350/363] Writing tensor blk.38.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  60\n",
      "[351/363] Writing tensor blk.38.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  61\n",
      "[352/363] Writing tensor blk.38.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  61\n",
      "[353/363] Writing tensor blk.38.attn_norm.weight                | size   5120           | type F32  | T+  61\n",
      "[354/363] Writing tensor blk.38.ffn_norm.weight                 | size   5120           | type F32  | T+  61\n",
      "[355/363] Writing tensor blk.39.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  61\n",
      "[356/363] Writing tensor blk.39.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  61\n",
      "[357/363] Writing tensor blk.39.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  61\n",
      "[358/363] Writing tensor blk.39.attn_output.weight              | size   5120 x   5120  | type F16  | T+  61\n",
      "[359/363] Writing tensor blk.39.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  62\n",
      "[360/363] Writing tensor blk.39.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  62\n",
      "[361/363] Writing tensor blk.39.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  62\n",
      "[362/363] Writing tensor blk.39.attn_norm.weight                | size   5120           | type F32  | T+  62\n",
      "[363/363] Writing tensor blk.39.ffn_norm.weight                 | size   5120           | type F32  | T+  62\n",
      "Wrote models/13B/ggml-model-f16.gguf\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "params = Params(n_vocab=32000, n_embd=6656, n_layer=60, n_ctx=2048, n_ff=17920, n_head=52, n_head_kv=52, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/30B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 6656]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [6656]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 6656]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [6656]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [6656]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [6656]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [6656]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [6656]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [6656]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [6656]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [6656]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [6656]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [6656]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [6656]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [6656]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [6656]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [6656]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [6656]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [6656]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [6656]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [6656]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [6656]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [6656]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [6656]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [6656]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [6656]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [6656]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [6656]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [6656]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [6656]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [6656]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [6656]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [6656]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [6656]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [6656]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [6656]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [6656]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [6656]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [6656]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [6656]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [6656]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [6656]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [6656]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [6656]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [6656]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [6656]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [6656]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [6656]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [6656]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [6656]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [6656]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [6656]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [6656]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [6656]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [6656]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [6656]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [6656]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [6656]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [6656]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [6656]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [6656]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [6656]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [6656]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [6656]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/30B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/543] Writing tensor token_embd.weight                      | size  32000 x   6656  | type F16  | T+   1\n",
      "[  2/543] Writing tensor output_norm.weight                     | size   6656           | type F32  | T+   1\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type F16  | T+   1\n",
      "[  4/543] Writing tensor blk.0.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[  5/543] Writing tensor blk.0.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[  6/543] Writing tensor blk.0.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[  7/543] Writing tensor blk.0.attn_output.weight               | size   6656 x   6656  | type F16  | T+   2\n",
      "[  8/543] Writing tensor blk.0.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   2\n",
      "[  9/543] Writing tensor blk.0.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   2\n",
      "[ 10/543] Writing tensor blk.0.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   2\n",
      "[ 11/543] Writing tensor blk.0.attn_norm.weight                 | size   6656           | type F32  | T+   3\n",
      "[ 12/543] Writing tensor blk.0.ffn_norm.weight                  | size   6656           | type F32  | T+   3\n",
      "[ 13/543] Writing tensor blk.1.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 14/543] Writing tensor blk.1.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 15/543] Writing tensor blk.1.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 16/543] Writing tensor blk.1.attn_output.weight               | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 17/543] Writing tensor blk.1.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   4\n",
      "[ 18/543] Writing tensor blk.1.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   5\n",
      "[ 19/543] Writing tensor blk.1.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   5\n",
      "[ 20/543] Writing tensor blk.1.attn_norm.weight                 | size   6656           | type F32  | T+   5\n",
      "[ 21/543] Writing tensor blk.1.ffn_norm.weight                  | size   6656           | type F32  | T+   5\n",
      "[ 22/543] Writing tensor blk.2.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 23/543] Writing tensor blk.2.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 24/543] Writing tensor blk.2.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 25/543] Writing tensor blk.2.attn_output.weight               | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 26/543] Writing tensor blk.2.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   6\n",
      "[ 27/543] Writing tensor blk.2.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   7\n",
      "[ 28/543] Writing tensor blk.2.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   7\n",
      "[ 29/543] Writing tensor blk.2.attn_norm.weight                 | size   6656           | type F32  | T+   7\n",
      "[ 30/543] Writing tensor blk.2.ffn_norm.weight                  | size   6656           | type F32  | T+   7\n",
      "[ 31/543] Writing tensor blk.3.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 32/543] Writing tensor blk.3.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 33/543] Writing tensor blk.3.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 34/543] Writing tensor blk.3.attn_output.weight               | size   6656 x   6656  | type F16  | T+   8\n",
      "[ 35/543] Writing tensor blk.3.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   8\n",
      "[ 36/543] Writing tensor blk.3.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   9\n",
      "[ 37/543] Writing tensor blk.3.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   9\n",
      "[ 38/543] Writing tensor blk.3.attn_norm.weight                 | size   6656           | type F32  | T+   9\n",
      "[ 39/543] Writing tensor blk.3.ffn_norm.weight                  | size   6656           | type F32  | T+   9\n",
      "[ 40/543] Writing tensor blk.4.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 41/543] Writing tensor blk.4.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 42/543] Writing tensor blk.4.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 43/543] Writing tensor blk.4.attn_output.weight               | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 44/543] Writing tensor blk.4.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  10\n",
      "[ 45/543] Writing tensor blk.4.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  11\n",
      "[ 46/543] Writing tensor blk.4.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 47/543] Writing tensor blk.4.attn_norm.weight                 | size   6656           | type F32  | T+  12\n",
      "[ 48/543] Writing tensor blk.4.ffn_norm.weight                  | size   6656           | type F32  | T+  12\n",
      "[ 49/543] Writing tensor blk.5.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 50/543] Writing tensor blk.5.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 51/543] Writing tensor blk.5.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 52/543] Writing tensor blk.5.attn_output.weight               | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 53/543] Writing tensor blk.5.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 54/543] Writing tensor blk.5.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  13\n",
      "[ 55/543] Writing tensor blk.5.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  14\n",
      "[ 56/543] Writing tensor blk.5.attn_norm.weight                 | size   6656           | type F32  | T+  15\n",
      "[ 57/543] Writing tensor blk.5.ffn_norm.weight                  | size   6656           | type F32  | T+  15\n",
      "[ 58/543] Writing tensor blk.6.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 59/543] Writing tensor blk.6.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 60/543] Writing tensor blk.6.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 61/543] Writing tensor blk.6.attn_output.weight               | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 62/543] Writing tensor blk.6.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  15\n",
      "[ 63/543] Writing tensor blk.6.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  16\n",
      "[ 64/543] Writing tensor blk.6.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  16\n",
      "[ 65/543] Writing tensor blk.6.attn_norm.weight                 | size   6656           | type F32  | T+  16\n",
      "[ 66/543] Writing tensor blk.6.ffn_norm.weight                  | size   6656           | type F32  | T+  16\n",
      "[ 67/543] Writing tensor blk.7.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 68/543] Writing tensor blk.7.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 69/543] Writing tensor blk.7.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 70/543] Writing tensor blk.7.attn_output.weight               | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 71/543] Writing tensor blk.7.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  16\n",
      "[ 72/543] Writing tensor blk.7.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  17\n",
      "[ 73/543] Writing tensor blk.7.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  18\n",
      "[ 74/543] Writing tensor blk.7.attn_norm.weight                 | size   6656           | type F32  | T+  18\n",
      "[ 75/543] Writing tensor blk.7.ffn_norm.weight                  | size   6656           | type F32  | T+  18\n",
      "[ 76/543] Writing tensor blk.8.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  18\n",
      "[ 77/543] Writing tensor blk.8.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  19\n",
      "[ 78/543] Writing tensor blk.8.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  19\n",
      "[ 79/543] Writing tensor blk.8.attn_output.weight               | size   6656 x   6656  | type F16  | T+  19\n",
      "[ 80/543] Writing tensor blk.8.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  19\n",
      "[ 81/543] Writing tensor blk.8.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  20\n",
      "[ 82/543] Writing tensor blk.8.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  20\n",
      "[ 83/543] Writing tensor blk.8.attn_norm.weight                 | size   6656           | type F32  | T+  21\n",
      "[ 84/543] Writing tensor blk.8.ffn_norm.weight                  | size   6656           | type F32  | T+  21\n",
      "[ 85/543] Writing tensor blk.9.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  21\n",
      "[ 86/543] Writing tensor blk.9.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  22\n",
      "[ 87/543] Writing tensor blk.9.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  23\n",
      "[ 88/543] Writing tensor blk.9.attn_output.weight               | size   6656 x   6656  | type F16  | T+  24\n",
      "[ 89/543] Writing tensor blk.9.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  25\n",
      "[ 90/543] Writing tensor blk.9.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  26\n",
      "[ 91/543] Writing tensor blk.9.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  27\n",
      "[ 92/543] Writing tensor blk.9.attn_norm.weight                 | size   6656           | type F32  | T+  27\n",
      "[ 93/543] Writing tensor blk.9.ffn_norm.weight                  | size   6656           | type F32  | T+  27\n",
      "[ 94/543] Writing tensor blk.10.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[ 95/543] Writing tensor blk.10.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[ 96/543] Writing tensor blk.10.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[ 97/543] Writing tensor blk.10.attn_output.weight              | size   6656 x   6656  | type F16  | T+  27\n",
      "[ 98/543] Writing tensor blk.10.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  27\n",
      "[ 99/543] Writing tensor blk.10.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  28\n",
      "[100/543] Writing tensor blk.10.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  29\n",
      "[101/543] Writing tensor blk.10.attn_norm.weight                | size   6656           | type F32  | T+  29\n",
      "[102/543] Writing tensor blk.10.ffn_norm.weight                 | size   6656           | type F32  | T+  29\n",
      "[103/543] Writing tensor blk.11.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[104/543] Writing tensor blk.11.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[105/543] Writing tensor blk.11.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[106/543] Writing tensor blk.11.attn_output.weight              | size   6656 x   6656  | type F16  | T+  29\n",
      "[107/543] Writing tensor blk.11.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  29\n",
      "[108/543] Writing tensor blk.11.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  30\n",
      "[109/543] Writing tensor blk.11.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  30\n",
      "[110/543] Writing tensor blk.11.attn_norm.weight                | size   6656           | type F32  | T+  30\n",
      "[111/543] Writing tensor blk.11.ffn_norm.weight                 | size   6656           | type F32  | T+  30\n",
      "[112/543] Writing tensor blk.12.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[113/543] Writing tensor blk.12.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[114/543] Writing tensor blk.12.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[115/543] Writing tensor blk.12.attn_output.weight              | size   6656 x   6656  | type F16  | T+  30\n",
      "[116/543] Writing tensor blk.12.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  31\n",
      "[117/543] Writing tensor blk.12.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  31\n",
      "[118/543] Writing tensor blk.12.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  33\n",
      "[119/543] Writing tensor blk.12.attn_norm.weight                | size   6656           | type F32  | T+  33\n",
      "[120/543] Writing tensor blk.12.ffn_norm.weight                 | size   6656           | type F32  | T+  33\n",
      "[121/543] Writing tensor blk.13.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[122/543] Writing tensor blk.13.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[123/543] Writing tensor blk.13.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[124/543] Writing tensor blk.13.attn_output.weight              | size   6656 x   6656  | type F16  | T+  33\n",
      "[125/543] Writing tensor blk.13.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  33\n",
      "[126/543] Writing tensor blk.13.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  34\n",
      "[127/543] Writing tensor blk.13.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  34\n",
      "[128/543] Writing tensor blk.13.attn_norm.weight                | size   6656           | type F32  | T+  34\n",
      "[129/543] Writing tensor blk.13.ffn_norm.weight                 | size   6656           | type F32  | T+  34\n",
      "[130/543] Writing tensor blk.14.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[131/543] Writing tensor blk.14.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[132/543] Writing tensor blk.14.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[133/543] Writing tensor blk.14.attn_output.weight              | size   6656 x   6656  | type F16  | T+  34\n",
      "[134/543] Writing tensor blk.14.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  35\n",
      "[135/543] Writing tensor blk.14.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  35\n",
      "[136/543] Writing tensor blk.14.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  37\n",
      "[137/543] Writing tensor blk.14.attn_norm.weight                | size   6656           | type F32  | T+  37\n",
      "[138/543] Writing tensor blk.14.ffn_norm.weight                 | size   6656           | type F32  | T+  37\n",
      "[139/543] Writing tensor blk.15.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[140/543] Writing tensor blk.15.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[141/543] Writing tensor blk.15.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[142/543] Writing tensor blk.15.attn_output.weight              | size   6656 x   6656  | type F16  | T+  37\n",
      "[143/543] Writing tensor blk.15.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  37\n",
      "[144/543] Writing tensor blk.15.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  38\n",
      "[145/543] Writing tensor blk.15.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  38\n",
      "[146/543] Writing tensor blk.15.attn_norm.weight                | size   6656           | type F32  | T+  38\n",
      "[147/543] Writing tensor blk.15.ffn_norm.weight                 | size   6656           | type F32  | T+  38\n",
      "[148/543] Writing tensor blk.16.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  38\n",
      "[149/543] Writing tensor blk.16.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  38\n",
      "[150/543] Writing tensor blk.16.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  38\n",
      "[151/543] Writing tensor blk.16.attn_output.weight              | size   6656 x   6656  | type F16  | T+  38\n",
      "[152/543] Writing tensor blk.16.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  39\n",
      "[153/543] Writing tensor blk.16.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  40\n",
      "[154/543] Writing tensor blk.16.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  41\n",
      "[155/543] Writing tensor blk.16.attn_norm.weight                | size   6656           | type F32  | T+  41\n",
      "[156/543] Writing tensor blk.16.ffn_norm.weight                 | size   6656           | type F32  | T+  41\n",
      "[157/543] Writing tensor blk.17.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[158/543] Writing tensor blk.17.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[159/543] Writing tensor blk.17.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[160/543] Writing tensor blk.17.attn_output.weight              | size   6656 x   6656  | type F16  | T+  41\n",
      "[161/543] Writing tensor blk.17.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  41\n",
      "[162/543] Writing tensor blk.17.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  44\n",
      "[163/543] Writing tensor blk.17.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  45\n",
      "[164/543] Writing tensor blk.17.attn_norm.weight                | size   6656           | type F32  | T+  45\n",
      "[165/543] Writing tensor blk.17.ffn_norm.weight                 | size   6656           | type F32  | T+  45\n",
      "[166/543] Writing tensor blk.18.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  45\n",
      "[167/543] Writing tensor blk.18.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  45\n",
      "[168/543] Writing tensor blk.18.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  45\n",
      "[169/543] Writing tensor blk.18.attn_output.weight              | size   6656 x   6656  | type F16  | T+  45\n",
      "[170/543] Writing tensor blk.18.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  46\n",
      "[171/543] Writing tensor blk.18.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  48\n",
      "[172/543] Writing tensor blk.18.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  48\n",
      "[173/543] Writing tensor blk.18.attn_norm.weight                | size   6656           | type F32  | T+  48\n",
      "[174/543] Writing tensor blk.18.ffn_norm.weight                 | size   6656           | type F32  | T+  48\n",
      "[175/543] Writing tensor blk.19.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[176/543] Writing tensor blk.19.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[177/543] Writing tensor blk.19.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[178/543] Writing tensor blk.19.attn_output.weight              | size   6656 x   6656  | type F16  | T+  48\n",
      "[179/543] Writing tensor blk.19.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  49\n",
      "[180/543] Writing tensor blk.19.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  50\n",
      "[181/543] Writing tensor blk.19.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  51\n",
      "[182/543] Writing tensor blk.19.attn_norm.weight                | size   6656           | type F32  | T+  51\n",
      "[183/543] Writing tensor blk.19.ffn_norm.weight                 | size   6656           | type F32  | T+  51\n",
      "[184/543] Writing tensor blk.20.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[185/543] Writing tensor blk.20.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[186/543] Writing tensor blk.20.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[187/543] Writing tensor blk.20.attn_output.weight              | size   6656 x   6656  | type F16  | T+  51\n",
      "[188/543] Writing tensor blk.20.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  52\n",
      "[189/543] Writing tensor blk.20.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  53\n",
      "[190/543] Writing tensor blk.20.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  54\n",
      "[191/543] Writing tensor blk.20.attn_norm.weight                | size   6656           | type F32  | T+  54\n",
      "[192/543] Writing tensor blk.20.ffn_norm.weight                 | size   6656           | type F32  | T+  54\n",
      "[193/543] Writing tensor blk.21.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[194/543] Writing tensor blk.21.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[195/543] Writing tensor blk.21.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[196/543] Writing tensor blk.21.attn_output.weight              | size   6656 x   6656  | type F16  | T+  54\n",
      "[197/543] Writing tensor blk.21.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  54\n",
      "[198/543] Writing tensor blk.21.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  56\n",
      "[199/543] Writing tensor blk.21.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  56\n",
      "[200/543] Writing tensor blk.21.attn_norm.weight                | size   6656           | type F32  | T+  56\n",
      "[201/543] Writing tensor blk.21.ffn_norm.weight                 | size   6656           | type F32  | T+  56\n",
      "[202/543] Writing tensor blk.22.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[203/543] Writing tensor blk.22.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[204/543] Writing tensor blk.22.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[205/543] Writing tensor blk.22.attn_output.weight              | size   6656 x   6656  | type F16  | T+  56\n",
      "[206/543] Writing tensor blk.22.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  56\n",
      "[207/543] Writing tensor blk.22.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  57\n",
      "[208/543] Writing tensor blk.22.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  57\n",
      "[209/543] Writing tensor blk.22.attn_norm.weight                | size   6656           | type F32  | T+  57\n",
      "[210/543] Writing tensor blk.22.ffn_norm.weight                 | size   6656           | type F32  | T+  57\n",
      "[211/543] Writing tensor blk.23.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[212/543] Writing tensor blk.23.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[213/543] Writing tensor blk.23.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[214/543] Writing tensor blk.23.attn_output.weight              | size   6656 x   6656  | type F16  | T+  58\n",
      "[215/543] Writing tensor blk.23.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  58\n",
      "[216/543] Writing tensor blk.23.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  58\n",
      "[217/543] Writing tensor blk.23.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  59\n",
      "[218/543] Writing tensor blk.23.attn_norm.weight                | size   6656           | type F32  | T+  59\n",
      "[219/543] Writing tensor blk.23.ffn_norm.weight                 | size   6656           | type F32  | T+  59\n",
      "[220/543] Writing tensor blk.24.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[221/543] Writing tensor blk.24.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[222/543] Writing tensor blk.24.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[223/543] Writing tensor blk.24.attn_output.weight              | size   6656 x   6656  | type F16  | T+  59\n",
      "[224/543] Writing tensor blk.24.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  59\n",
      "[225/543] Writing tensor blk.24.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  60\n",
      "[226/543] Writing tensor blk.24.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  60\n",
      "[227/543] Writing tensor blk.24.attn_norm.weight                | size   6656           | type F32  | T+  60\n",
      "[228/543] Writing tensor blk.24.ffn_norm.weight                 | size   6656           | type F32  | T+  60\n",
      "[229/543] Writing tensor blk.25.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[230/543] Writing tensor blk.25.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[231/543] Writing tensor blk.25.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[232/543] Writing tensor blk.25.attn_output.weight              | size   6656 x   6656  | type F16  | T+  60\n",
      "[233/543] Writing tensor blk.25.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  60\n",
      "[234/543] Writing tensor blk.25.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  61\n",
      "[235/543] Writing tensor blk.25.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  61\n",
      "[236/543] Writing tensor blk.25.attn_norm.weight                | size   6656           | type F32  | T+  61\n",
      "[237/543] Writing tensor blk.25.ffn_norm.weight                 | size   6656           | type F32  | T+  61\n",
      "[238/543] Writing tensor blk.26.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  61\n",
      "[239/543] Writing tensor blk.26.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  61\n",
      "[240/543] Writing tensor blk.26.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[241/543] Writing tensor blk.26.attn_output.weight              | size   6656 x   6656  | type F16  | T+  62\n",
      "[242/543] Writing tensor blk.26.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  62\n",
      "[243/543] Writing tensor blk.26.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  62\n",
      "[244/543] Writing tensor blk.26.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  63\n",
      "[245/543] Writing tensor blk.26.attn_norm.weight                | size   6656           | type F32  | T+  63\n",
      "[246/543] Writing tensor blk.26.ffn_norm.weight                 | size   6656           | type F32  | T+  63\n",
      "[247/543] Writing tensor blk.27.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[248/543] Writing tensor blk.27.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[249/543] Writing tensor blk.27.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[250/543] Writing tensor blk.27.attn_output.weight              | size   6656 x   6656  | type F16  | T+  63\n",
      "[251/543] Writing tensor blk.27.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  63\n",
      "[252/543] Writing tensor blk.27.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  65\n",
      "[253/543] Writing tensor blk.27.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  66\n",
      "[254/543] Writing tensor blk.27.attn_norm.weight                | size   6656           | type F32  | T+  66\n",
      "[255/543] Writing tensor blk.27.ffn_norm.weight                 | size   6656           | type F32  | T+  66\n",
      "[256/543] Writing tensor blk.28.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[257/543] Writing tensor blk.28.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[258/543] Writing tensor blk.28.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[259/543] Writing tensor blk.28.attn_output.weight              | size   6656 x   6656  | type F16  | T+  66\n",
      "[260/543] Writing tensor blk.28.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  66\n",
      "[261/543] Writing tensor blk.28.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  67\n",
      "[262/543] Writing tensor blk.28.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  69\n",
      "[263/543] Writing tensor blk.28.attn_norm.weight                | size   6656           | type F32  | T+  69\n",
      "[264/543] Writing tensor blk.28.ffn_norm.weight                 | size   6656           | type F32  | T+  69\n",
      "[265/543] Writing tensor blk.29.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[266/543] Writing tensor blk.29.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[267/543] Writing tensor blk.29.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[268/543] Writing tensor blk.29.attn_output.weight              | size   6656 x   6656  | type F16  | T+  70\n",
      "[269/543] Writing tensor blk.29.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  70\n",
      "[270/543] Writing tensor blk.29.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  70\n",
      "[271/543] Writing tensor blk.29.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  71\n",
      "[272/543] Writing tensor blk.29.attn_norm.weight                | size   6656           | type F32  | T+  71\n",
      "[273/543] Writing tensor blk.29.ffn_norm.weight                 | size   6656           | type F32  | T+  71\n",
      "[274/543] Writing tensor blk.30.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  71\n",
      "[275/543] Writing tensor blk.30.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  71\n",
      "[276/543] Writing tensor blk.30.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  71\n",
      "[277/543] Writing tensor blk.30.attn_output.weight              | size   6656 x   6656  | type F16  | T+  71\n",
      "[278/543] Writing tensor blk.30.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  71\n",
      "[279/543] Writing tensor blk.30.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  72\n",
      "[280/543] Writing tensor blk.30.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  74\n",
      "[281/543] Writing tensor blk.30.attn_norm.weight                | size   6656           | type F32  | T+  74\n",
      "[282/543] Writing tensor blk.30.ffn_norm.weight                 | size   6656           | type F32  | T+  74\n",
      "[283/543] Writing tensor blk.31.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  74\n",
      "[284/543] Writing tensor blk.31.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  74\n",
      "[285/543] Writing tensor blk.31.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  74\n",
      "[286/543] Writing tensor blk.31.attn_output.weight              | size   6656 x   6656  | type F16  | T+  74\n",
      "[287/543] Writing tensor blk.31.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  74\n",
      "[288/543] Writing tensor blk.31.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  75\n",
      "[289/543] Writing tensor blk.31.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  76\n",
      "[290/543] Writing tensor blk.31.attn_norm.weight                | size   6656           | type F32  | T+  76\n",
      "[291/543] Writing tensor blk.31.ffn_norm.weight                 | size   6656           | type F32  | T+  76\n",
      "[292/543] Writing tensor blk.32.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[293/543] Writing tensor blk.32.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[294/543] Writing tensor blk.32.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[295/543] Writing tensor blk.32.attn_output.weight              | size   6656 x   6656  | type F16  | T+  76\n",
      "[296/543] Writing tensor blk.32.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  76\n",
      "[297/543] Writing tensor blk.32.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  77\n",
      "[298/543] Writing tensor blk.32.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  78\n",
      "[299/543] Writing tensor blk.32.attn_norm.weight                | size   6656           | type F32  | T+  78\n",
      "[300/543] Writing tensor blk.32.ffn_norm.weight                 | size   6656           | type F32  | T+  78\n",
      "[301/543] Writing tensor blk.33.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  78\n",
      "[302/543] Writing tensor blk.33.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  78\n",
      "[303/543] Writing tensor blk.33.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  78\n",
      "[304/543] Writing tensor blk.33.attn_output.weight              | size   6656 x   6656  | type F16  | T+  78\n",
      "[305/543] Writing tensor blk.33.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  78\n",
      "[306/543] Writing tensor blk.33.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  79\n",
      "[307/543] Writing tensor blk.33.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  80\n",
      "[308/543] Writing tensor blk.33.attn_norm.weight                | size   6656           | type F32  | T+  80\n",
      "[309/543] Writing tensor blk.33.ffn_norm.weight                 | size   6656           | type F32  | T+  80\n",
      "[310/543] Writing tensor blk.34.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[311/543] Writing tensor blk.34.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[312/543] Writing tensor blk.34.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[313/543] Writing tensor blk.34.attn_output.weight              | size   6656 x   6656  | type F16  | T+  80\n",
      "[314/543] Writing tensor blk.34.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  80\n",
      "[315/543] Writing tensor blk.34.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  82\n",
      "[316/543] Writing tensor blk.34.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  82\n",
      "[317/543] Writing tensor blk.34.attn_norm.weight                | size   6656           | type F32  | T+  82\n",
      "[318/543] Writing tensor blk.34.ffn_norm.weight                 | size   6656           | type F32  | T+  82\n",
      "[319/543] Writing tensor blk.35.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[320/543] Writing tensor blk.35.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[321/543] Writing tensor blk.35.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[322/543] Writing tensor blk.35.attn_output.weight              | size   6656 x   6656  | type F16  | T+  82\n",
      "[323/543] Writing tensor blk.35.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  82\n",
      "[324/543] Writing tensor blk.35.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  83\n",
      "[325/543] Writing tensor blk.35.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  84\n",
      "[326/543] Writing tensor blk.35.attn_norm.weight                | size   6656           | type F32  | T+  84\n",
      "[327/543] Writing tensor blk.35.ffn_norm.weight                 | size   6656           | type F32  | T+  84\n",
      "[328/543] Writing tensor blk.36.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  84\n",
      "[329/543] Writing tensor blk.36.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  84\n",
      "[330/543] Writing tensor blk.36.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  85\n",
      "[331/543] Writing tensor blk.36.attn_output.weight              | size   6656 x   6656  | type F16  | T+  85\n",
      "[332/543] Writing tensor blk.36.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  85\n",
      "[333/543] Writing tensor blk.36.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  86\n",
      "[334/543] Writing tensor blk.36.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  87\n",
      "[335/543] Writing tensor blk.36.attn_norm.weight                | size   6656           | type F32  | T+  88\n",
      "[336/543] Writing tensor blk.36.ffn_norm.weight                 | size   6656           | type F32  | T+  88\n",
      "[337/543] Writing tensor blk.37.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  88\n",
      "[338/543] Writing tensor blk.37.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  89\n",
      "[339/543] Writing tensor blk.37.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[340/543] Writing tensor blk.37.attn_output.weight              | size   6656 x   6656  | type F16  | T+  90\n",
      "[341/543] Writing tensor blk.37.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  90\n",
      "[342/543] Writing tensor blk.37.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  91\n",
      "[343/543] Writing tensor blk.37.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  91\n",
      "[344/543] Writing tensor blk.37.attn_norm.weight                | size   6656           | type F32  | T+  92\n",
      "[345/543] Writing tensor blk.37.ffn_norm.weight                 | size   6656           | type F32  | T+  92\n",
      "[346/543] Writing tensor blk.38.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  92\n",
      "[347/543] Writing tensor blk.38.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  93\n",
      "[348/543] Writing tensor blk.38.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  94\n",
      "[349/543] Writing tensor blk.38.attn_output.weight              | size   6656 x   6656  | type F16  | T+  94\n",
      "[350/543] Writing tensor blk.38.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  95\n",
      "[351/543] Writing tensor blk.38.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  95\n",
      "[352/543] Writing tensor blk.38.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  96\n",
      "[353/543] Writing tensor blk.38.attn_norm.weight                | size   6656           | type F32  | T+  97\n",
      "[354/543] Writing tensor blk.38.ffn_norm.weight                 | size   6656           | type F32  | T+  97\n",
      "[355/543] Writing tensor blk.39.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  97\n",
      "[356/543] Writing tensor blk.39.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  98\n",
      "[357/543] Writing tensor blk.39.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  99\n",
      "[358/543] Writing tensor blk.39.attn_output.weight              | size   6656 x   6656  | type F16  | T+  99\n",
      "[359/543] Writing tensor blk.39.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  99\n",
      "[360/543] Writing tensor blk.39.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  99\n",
      "[361/543] Writing tensor blk.39.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 101\n",
      "[362/543] Writing tensor blk.39.attn_norm.weight                | size   6656           | type F32  | T+ 101\n",
      "[363/543] Writing tensor blk.39.ffn_norm.weight                 | size   6656           | type F32  | T+ 101\n",
      "[364/543] Writing tensor blk.40.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 101\n",
      "[365/543] Writing tensor blk.40.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 101\n",
      "[366/543] Writing tensor blk.40.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 101\n",
      "[367/543] Writing tensor blk.40.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 101\n",
      "[368/543] Writing tensor blk.40.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 101\n",
      "[369/543] Writing tensor blk.40.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 102\n",
      "[370/543] Writing tensor blk.40.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 102\n",
      "[371/543] Writing tensor blk.40.attn_norm.weight                | size   6656           | type F32  | T+ 102\n",
      "[372/543] Writing tensor blk.40.ffn_norm.weight                 | size   6656           | type F32  | T+ 102\n",
      "[373/543] Writing tensor blk.41.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 102\n",
      "[374/543] Writing tensor blk.41.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 102\n",
      "[375/543] Writing tensor blk.41.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 102\n",
      "[376/543] Writing tensor blk.41.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 102\n",
      "[377/543] Writing tensor blk.41.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 103\n",
      "[378/543] Writing tensor blk.41.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 105\n",
      "[379/543] Writing tensor blk.41.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 105\n",
      "[380/543] Writing tensor blk.41.attn_norm.weight                | size   6656           | type F32  | T+ 105\n",
      "[381/543] Writing tensor blk.41.ffn_norm.weight                 | size   6656           | type F32  | T+ 105\n",
      "[382/543] Writing tensor blk.42.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 105\n",
      "[383/543] Writing tensor blk.42.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 105\n",
      "[384/543] Writing tensor blk.42.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 105\n",
      "[385/543] Writing tensor blk.42.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 106\n",
      "[386/543] Writing tensor blk.42.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 106\n",
      "[387/543] Writing tensor blk.42.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 107\n",
      "[388/543] Writing tensor blk.42.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 108\n",
      "[389/543] Writing tensor blk.42.attn_norm.weight                | size   6656           | type F32  | T+ 108\n",
      "[390/543] Writing tensor blk.42.ffn_norm.weight                 | size   6656           | type F32  | T+ 108\n",
      "[391/543] Writing tensor blk.43.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 108\n",
      "[392/543] Writing tensor blk.43.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 108\n",
      "[393/543] Writing tensor blk.43.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 108\n",
      "[394/543] Writing tensor blk.43.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 108\n",
      "[395/543] Writing tensor blk.43.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 108\n",
      "[396/543] Writing tensor blk.43.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 110\n",
      "[397/543] Writing tensor blk.43.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 111\n",
      "[398/543] Writing tensor blk.43.attn_norm.weight                | size   6656           | type F32  | T+ 111\n",
      "[399/543] Writing tensor blk.43.ffn_norm.weight                 | size   6656           | type F32  | T+ 111\n",
      "[400/543] Writing tensor blk.44.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 111\n",
      "[401/543] Writing tensor blk.44.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 111\n",
      "[402/543] Writing tensor blk.44.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 111\n",
      "[403/543] Writing tensor blk.44.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 111\n",
      "[404/543] Writing tensor blk.44.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 111\n",
      "[405/543] Writing tensor blk.44.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 112\n",
      "[406/543] Writing tensor blk.44.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 113\n",
      "[407/543] Writing tensor blk.44.attn_norm.weight                | size   6656           | type F32  | T+ 113\n",
      "[408/543] Writing tensor blk.44.ffn_norm.weight                 | size   6656           | type F32  | T+ 113\n",
      "[409/543] Writing tensor blk.45.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 113\n",
      "[410/543] Writing tensor blk.45.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 113\n",
      "[411/543] Writing tensor blk.45.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 113\n",
      "[412/543] Writing tensor blk.45.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 113\n",
      "[413/543] Writing tensor blk.45.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 113\n",
      "[414/543] Writing tensor blk.45.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 114\n",
      "[415/543] Writing tensor blk.45.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 115\n",
      "[416/543] Writing tensor blk.45.attn_norm.weight                | size   6656           | type F32  | T+ 115\n",
      "[417/543] Writing tensor blk.45.ffn_norm.weight                 | size   6656           | type F32  | T+ 115\n",
      "[418/543] Writing tensor blk.46.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 115\n",
      "[419/543] Writing tensor blk.46.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 115\n",
      "[420/543] Writing tensor blk.46.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 115\n",
      "[421/543] Writing tensor blk.46.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 115\n",
      "[422/543] Writing tensor blk.46.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 115\n",
      "[423/543] Writing tensor blk.46.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 116\n",
      "[424/543] Writing tensor blk.46.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 116\n",
      "[425/543] Writing tensor blk.46.attn_norm.weight                | size   6656           | type F32  | T+ 116\n",
      "[426/543] Writing tensor blk.46.ffn_norm.weight                 | size   6656           | type F32  | T+ 116\n",
      "[427/543] Writing tensor blk.47.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 116\n",
      "[428/543] Writing tensor blk.47.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 116\n",
      "[429/543] Writing tensor blk.47.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 116\n",
      "[430/543] Writing tensor blk.47.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 116\n",
      "[431/543] Writing tensor blk.47.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 116\n",
      "[432/543] Writing tensor blk.47.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 117\n",
      "[433/543] Writing tensor blk.47.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 119\n",
      "[434/543] Writing tensor blk.47.attn_norm.weight                | size   6656           | type F32  | T+ 119\n",
      "[435/543] Writing tensor blk.47.ffn_norm.weight                 | size   6656           | type F32  | T+ 119\n",
      "[436/543] Writing tensor blk.48.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 119\n",
      "[437/543] Writing tensor blk.48.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 119\n",
      "[438/543] Writing tensor blk.48.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 119\n",
      "[439/543] Writing tensor blk.48.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 119\n",
      "[440/543] Writing tensor blk.48.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 119\n",
      "[441/543] Writing tensor blk.48.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 120\n",
      "[442/543] Writing tensor blk.48.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 120\n",
      "[443/543] Writing tensor blk.48.attn_norm.weight                | size   6656           | type F32  | T+ 120\n",
      "[444/543] Writing tensor blk.48.ffn_norm.weight                 | size   6656           | type F32  | T+ 120\n",
      "[445/543] Writing tensor blk.49.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 120\n",
      "[446/543] Writing tensor blk.49.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 120\n",
      "[447/543] Writing tensor blk.49.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 120\n",
      "[448/543] Writing tensor blk.49.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 120\n",
      "[449/543] Writing tensor blk.49.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 121\n",
      "[450/543] Writing tensor blk.49.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 122\n",
      "[451/543] Writing tensor blk.49.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 122\n",
      "[452/543] Writing tensor blk.49.attn_norm.weight                | size   6656           | type F32  | T+ 122\n",
      "[453/543] Writing tensor blk.49.ffn_norm.weight                 | size   6656           | type F32  | T+ 122\n",
      "[454/543] Writing tensor blk.50.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 122\n",
      "[455/543] Writing tensor blk.50.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 122\n",
      "[456/543] Writing tensor blk.50.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 122\n",
      "[457/543] Writing tensor blk.50.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 122\n",
      "[458/543] Writing tensor blk.50.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 122\n",
      "[459/543] Writing tensor blk.50.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 124\n",
      "[460/543] Writing tensor blk.50.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 124\n",
      "[461/543] Writing tensor blk.50.attn_norm.weight                | size   6656           | type F32  | T+ 125\n",
      "[462/543] Writing tensor blk.50.ffn_norm.weight                 | size   6656           | type F32  | T+ 125\n",
      "[463/543] Writing tensor blk.51.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 125\n",
      "[464/543] Writing tensor blk.51.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 125\n",
      "[465/543] Writing tensor blk.51.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 125\n",
      "[466/543] Writing tensor blk.51.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 125\n",
      "[467/543] Writing tensor blk.51.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 125\n",
      "[468/543] Writing tensor blk.51.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 126\n",
      "[469/543] Writing tensor blk.51.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 126\n",
      "[470/543] Writing tensor blk.51.attn_norm.weight                | size   6656           | type F32  | T+ 127\n",
      "[471/543] Writing tensor blk.51.ffn_norm.weight                 | size   6656           | type F32  | T+ 127\n",
      "[472/543] Writing tensor blk.52.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 127\n",
      "[473/543] Writing tensor blk.52.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 127\n",
      "[474/543] Writing tensor blk.52.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 127\n",
      "[475/543] Writing tensor blk.52.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 127\n",
      "[476/543] Writing tensor blk.52.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 127\n",
      "[477/543] Writing tensor blk.52.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 128\n",
      "[478/543] Writing tensor blk.52.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 128\n",
      "[479/543] Writing tensor blk.52.attn_norm.weight                | size   6656           | type F32  | T+ 128\n",
      "[480/543] Writing tensor blk.52.ffn_norm.weight                 | size   6656           | type F32  | T+ 128\n",
      "[481/543] Writing tensor blk.53.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 128\n",
      "[482/543] Writing tensor blk.53.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 129\n",
      "[483/543] Writing tensor blk.53.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 129\n",
      "[484/543] Writing tensor blk.53.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 129\n",
      "[485/543] Writing tensor blk.53.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 130\n",
      "[486/543] Writing tensor blk.53.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 130\n",
      "[487/543] Writing tensor blk.53.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 130\n",
      "[488/543] Writing tensor blk.53.attn_norm.weight                | size   6656           | type F32  | T+ 130\n",
      "[489/543] Writing tensor blk.53.ffn_norm.weight                 | size   6656           | type F32  | T+ 130\n",
      "[490/543] Writing tensor blk.54.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 130\n",
      "[491/543] Writing tensor blk.54.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 131\n",
      "[492/543] Writing tensor blk.54.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 131\n",
      "[493/543] Writing tensor blk.54.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 131\n",
      "[494/543] Writing tensor blk.54.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 131\n",
      "[495/543] Writing tensor blk.54.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 132\n",
      "[496/543] Writing tensor blk.54.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 132\n",
      "[497/543] Writing tensor blk.54.attn_norm.weight                | size   6656           | type F32  | T+ 132\n",
      "[498/543] Writing tensor blk.54.ffn_norm.weight                 | size   6656           | type F32  | T+ 132\n",
      "[499/543] Writing tensor blk.55.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 132\n",
      "[500/543] Writing tensor blk.55.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 132\n",
      "[501/543] Writing tensor blk.55.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 132\n",
      "[502/543] Writing tensor blk.55.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 132\n",
      "[503/543] Writing tensor blk.55.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 133\n",
      "[504/543] Writing tensor blk.55.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 134\n",
      "[505/543] Writing tensor blk.55.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 134\n",
      "[506/543] Writing tensor blk.55.attn_norm.weight                | size   6656           | type F32  | T+ 134\n",
      "[507/543] Writing tensor blk.55.ffn_norm.weight                 | size   6656           | type F32  | T+ 134\n",
      "[508/543] Writing tensor blk.56.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 134\n",
      "[509/543] Writing tensor blk.56.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 134\n",
      "[510/543] Writing tensor blk.56.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 134\n",
      "[511/543] Writing tensor blk.56.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 134\n",
      "[512/543] Writing tensor blk.56.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 135\n",
      "[513/543] Writing tensor blk.56.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 135\n",
      "[514/543] Writing tensor blk.56.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 136\n",
      "[515/543] Writing tensor blk.56.attn_norm.weight                | size   6656           | type F32  | T+ 136\n",
      "[516/543] Writing tensor blk.56.ffn_norm.weight                 | size   6656           | type F32  | T+ 136\n",
      "[517/543] Writing tensor blk.57.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 136\n",
      "[518/543] Writing tensor blk.57.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 136\n",
      "[519/543] Writing tensor blk.57.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 136\n",
      "[520/543] Writing tensor blk.57.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 136\n",
      "[521/543] Writing tensor blk.57.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 136\n",
      "[522/543] Writing tensor blk.57.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 137\n",
      "[523/543] Writing tensor blk.57.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 137\n",
      "[524/543] Writing tensor blk.57.attn_norm.weight                | size   6656           | type F32  | T+ 137\n",
      "[525/543] Writing tensor blk.57.ffn_norm.weight                 | size   6656           | type F32  | T+ 137\n",
      "[526/543] Writing tensor blk.58.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 137\n",
      "[527/543] Writing tensor blk.58.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 137\n",
      "[528/543] Writing tensor blk.58.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 137\n",
      "[529/543] Writing tensor blk.58.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 137\n",
      "[530/543] Writing tensor blk.58.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 138\n",
      "[531/543] Writing tensor blk.58.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 139\n",
      "[532/543] Writing tensor blk.58.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 139\n",
      "[533/543] Writing tensor blk.58.attn_norm.weight                | size   6656           | type F32  | T+ 139\n",
      "[534/543] Writing tensor blk.58.ffn_norm.weight                 | size   6656           | type F32  | T+ 139\n",
      "[535/543] Writing tensor blk.59.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 139\n",
      "[536/543] Writing tensor blk.59.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 139\n",
      "[537/543] Writing tensor blk.59.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 139\n",
      "[538/543] Writing tensor blk.59.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 139\n",
      "[539/543] Writing tensor blk.59.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 140\n",
      "[540/543] Writing tensor blk.59.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 140\n",
      "[541/543] Writing tensor blk.59.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 141\n",
      "[542/543] Writing tensor blk.59.attn_norm.weight                | size   6656           | type F32  | T+ 141\n",
      "[543/543] Writing tensor blk.59.ffn_norm.weight                 | size   6656           | type F32  | T+ 141\n",
      "Wrote models/30B/ggml-model-f16.gguf\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "params = Params(n_vocab=32000, n_embd=8192, n_layer=80, n_ctx=4096, n_ff=22016, n_head=64, n_head_kv=64, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/65B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 8192]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [8192]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 8192]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [8192]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [8192]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [8192]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [8192]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [8192]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [8192]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [8192]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [8192]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [8192]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [8192]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [8192]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [8192]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [8192]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [8192]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [8192]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [8192]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [8192]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [8192]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [8192]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [8192]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [8192]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [8192]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [8192]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [8192]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [8192]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [8192]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [8192]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [8192]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [8192]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [8192]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [8192]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [8192]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [8192]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [8192]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [8192]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [8192]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [8192]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [8192]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [8192]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [8192]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [8192]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [8192]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [8192]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [8192]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [8192]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [8192]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [8192]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [8192]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [8192]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [8192]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [8192]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [8192]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [8192]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [8192]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [8192]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [8192]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [8192]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [8192]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [8192]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [8192]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.60.attention.wq.weight                    -> blk.60.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wk.weight                    -> blk.60.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wv.weight                    -> blk.60.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wo.weight                    -> blk.60.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.60.feed_forward.w1.weight                 -> blk.60.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.60.feed_forward.w2.weight                 -> blk.60.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.60.feed_forward.w3.weight                 -> blk.60.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.60.attention_norm.weight                  -> blk.60.attn_norm.weight                  | F16    | [8192]\n",
      "layers.60.ffn_norm.weight                        -> blk.60.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.61.attention.wq.weight                    -> blk.61.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wk.weight                    -> blk.61.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wv.weight                    -> blk.61.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wo.weight                    -> blk.61.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.61.feed_forward.w1.weight                 -> blk.61.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.61.feed_forward.w2.weight                 -> blk.61.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.61.feed_forward.w3.weight                 -> blk.61.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.61.attention_norm.weight                  -> blk.61.attn_norm.weight                  | F16    | [8192]\n",
      "layers.61.ffn_norm.weight                        -> blk.61.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.62.attention.wq.weight                    -> blk.62.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wk.weight                    -> blk.62.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wv.weight                    -> blk.62.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wo.weight                    -> blk.62.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.62.feed_forward.w1.weight                 -> blk.62.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.62.feed_forward.w2.weight                 -> blk.62.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.62.feed_forward.w3.weight                 -> blk.62.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.62.attention_norm.weight                  -> blk.62.attn_norm.weight                  | F16    | [8192]\n",
      "layers.62.ffn_norm.weight                        -> blk.62.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.63.attention.wq.weight                    -> blk.63.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wk.weight                    -> blk.63.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wv.weight                    -> blk.63.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wo.weight                    -> blk.63.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.63.feed_forward.w1.weight                 -> blk.63.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.63.feed_forward.w2.weight                 -> blk.63.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.63.feed_forward.w3.weight                 -> blk.63.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.63.attention_norm.weight                  -> blk.63.attn_norm.weight                  | F16    | [8192]\n",
      "layers.63.ffn_norm.weight                        -> blk.63.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.64.attention.wq.weight                    -> blk.64.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wk.weight                    -> blk.64.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wv.weight                    -> blk.64.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wo.weight                    -> blk.64.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.64.feed_forward.w1.weight                 -> blk.64.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.64.feed_forward.w2.weight                 -> blk.64.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.64.feed_forward.w3.weight                 -> blk.64.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.64.attention_norm.weight                  -> blk.64.attn_norm.weight                  | F16    | [8192]\n",
      "layers.64.ffn_norm.weight                        -> blk.64.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.65.attention.wq.weight                    -> blk.65.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wk.weight                    -> blk.65.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wv.weight                    -> blk.65.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wo.weight                    -> blk.65.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.65.feed_forward.w1.weight                 -> blk.65.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.65.feed_forward.w2.weight                 -> blk.65.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.65.feed_forward.w3.weight                 -> blk.65.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.65.attention_norm.weight                  -> blk.65.attn_norm.weight                  | F16    | [8192]\n",
      "layers.65.ffn_norm.weight                        -> blk.65.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.66.attention.wq.weight                    -> blk.66.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wk.weight                    -> blk.66.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wv.weight                    -> blk.66.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wo.weight                    -> blk.66.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.66.feed_forward.w1.weight                 -> blk.66.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.66.feed_forward.w2.weight                 -> blk.66.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.66.feed_forward.w3.weight                 -> blk.66.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.66.attention_norm.weight                  -> blk.66.attn_norm.weight                  | F16    | [8192]\n",
      "layers.66.ffn_norm.weight                        -> blk.66.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.67.attention.wq.weight                    -> blk.67.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wk.weight                    -> blk.67.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wv.weight                    -> blk.67.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wo.weight                    -> blk.67.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.67.feed_forward.w1.weight                 -> blk.67.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.67.feed_forward.w2.weight                 -> blk.67.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.67.feed_forward.w3.weight                 -> blk.67.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.67.attention_norm.weight                  -> blk.67.attn_norm.weight                  | F16    | [8192]\n",
      "layers.67.ffn_norm.weight                        -> blk.67.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.68.attention.wq.weight                    -> blk.68.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wk.weight                    -> blk.68.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wv.weight                    -> blk.68.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wo.weight                    -> blk.68.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.68.feed_forward.w1.weight                 -> blk.68.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.68.feed_forward.w2.weight                 -> blk.68.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.68.feed_forward.w3.weight                 -> blk.68.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.68.attention_norm.weight                  -> blk.68.attn_norm.weight                  | F16    | [8192]\n",
      "layers.68.ffn_norm.weight                        -> blk.68.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.69.attention.wq.weight                    -> blk.69.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wk.weight                    -> blk.69.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wv.weight                    -> blk.69.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wo.weight                    -> blk.69.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.69.feed_forward.w1.weight                 -> blk.69.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.69.feed_forward.w2.weight                 -> blk.69.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.69.feed_forward.w3.weight                 -> blk.69.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.69.attention_norm.weight                  -> blk.69.attn_norm.weight                  | F16    | [8192]\n",
      "layers.69.ffn_norm.weight                        -> blk.69.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.70.attention.wq.weight                    -> blk.70.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wk.weight                    -> blk.70.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wv.weight                    -> blk.70.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wo.weight                    -> blk.70.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.70.feed_forward.w1.weight                 -> blk.70.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.70.feed_forward.w2.weight                 -> blk.70.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.70.feed_forward.w3.weight                 -> blk.70.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.70.attention_norm.weight                  -> blk.70.attn_norm.weight                  | F16    | [8192]\n",
      "layers.70.ffn_norm.weight                        -> blk.70.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.71.attention.wq.weight                    -> blk.71.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wk.weight                    -> blk.71.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wv.weight                    -> blk.71.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wo.weight                    -> blk.71.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.71.feed_forward.w1.weight                 -> blk.71.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.71.feed_forward.w2.weight                 -> blk.71.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.71.feed_forward.w3.weight                 -> blk.71.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.71.attention_norm.weight                  -> blk.71.attn_norm.weight                  | F16    | [8192]\n",
      "layers.71.ffn_norm.weight                        -> blk.71.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.72.attention.wq.weight                    -> blk.72.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wk.weight                    -> blk.72.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wv.weight                    -> blk.72.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wo.weight                    -> blk.72.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.72.feed_forward.w1.weight                 -> blk.72.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.72.feed_forward.w2.weight                 -> blk.72.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.72.feed_forward.w3.weight                 -> blk.72.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.72.attention_norm.weight                  -> blk.72.attn_norm.weight                  | F16    | [8192]\n",
      "layers.72.ffn_norm.weight                        -> blk.72.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.73.attention.wq.weight                    -> blk.73.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wk.weight                    -> blk.73.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wv.weight                    -> blk.73.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wo.weight                    -> blk.73.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.73.feed_forward.w1.weight                 -> blk.73.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.73.feed_forward.w2.weight                 -> blk.73.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.73.feed_forward.w3.weight                 -> blk.73.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.73.attention_norm.weight                  -> blk.73.attn_norm.weight                  | F16    | [8192]\n",
      "layers.73.ffn_norm.weight                        -> blk.73.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.74.attention.wq.weight                    -> blk.74.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wk.weight                    -> blk.74.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wv.weight                    -> blk.74.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wo.weight                    -> blk.74.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.74.feed_forward.w1.weight                 -> blk.74.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.74.feed_forward.w2.weight                 -> blk.74.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.74.feed_forward.w3.weight                 -> blk.74.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.74.attention_norm.weight                  -> blk.74.attn_norm.weight                  | F16    | [8192]\n",
      "layers.74.ffn_norm.weight                        -> blk.74.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.75.attention.wq.weight                    -> blk.75.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wk.weight                    -> blk.75.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wv.weight                    -> blk.75.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wo.weight                    -> blk.75.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.75.feed_forward.w1.weight                 -> blk.75.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.75.feed_forward.w2.weight                 -> blk.75.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.75.feed_forward.w3.weight                 -> blk.75.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.75.attention_norm.weight                  -> blk.75.attn_norm.weight                  | F16    | [8192]\n",
      "layers.75.ffn_norm.weight                        -> blk.75.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.76.attention.wq.weight                    -> blk.76.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wk.weight                    -> blk.76.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wv.weight                    -> blk.76.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wo.weight                    -> blk.76.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.76.feed_forward.w1.weight                 -> blk.76.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.76.feed_forward.w2.weight                 -> blk.76.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.76.feed_forward.w3.weight                 -> blk.76.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.76.attention_norm.weight                  -> blk.76.attn_norm.weight                  | F16    | [8192]\n",
      "layers.76.ffn_norm.weight                        -> blk.76.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.77.attention.wq.weight                    -> blk.77.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wk.weight                    -> blk.77.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wv.weight                    -> blk.77.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wo.weight                    -> blk.77.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.77.feed_forward.w1.weight                 -> blk.77.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.77.feed_forward.w2.weight                 -> blk.77.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.77.feed_forward.w3.weight                 -> blk.77.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.77.attention_norm.weight                  -> blk.77.attn_norm.weight                  | F16    | [8192]\n",
      "layers.77.ffn_norm.weight                        -> blk.77.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.78.attention.wq.weight                    -> blk.78.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wk.weight                    -> blk.78.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wv.weight                    -> blk.78.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wo.weight                    -> blk.78.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.78.feed_forward.w1.weight                 -> blk.78.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.78.feed_forward.w2.weight                 -> blk.78.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.78.feed_forward.w3.weight                 -> blk.78.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.78.attention_norm.weight                  -> blk.78.attn_norm.weight                  | F16    | [8192]\n",
      "layers.78.ffn_norm.weight                        -> blk.78.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.79.attention.wq.weight                    -> blk.79.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wk.weight                    -> blk.79.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wv.weight                    -> blk.79.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wo.weight                    -> blk.79.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.79.feed_forward.w1.weight                 -> blk.79.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.79.feed_forward.w2.weight                 -> blk.79.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.79.feed_forward.w3.weight                 -> blk.79.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.79.attention_norm.weight                  -> blk.79.attn_norm.weight                  | F16    | [8192]\n",
      "layers.79.ffn_norm.weight                        -> blk.79.ffn_norm.weight                   | F16    | [8192]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/65B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/723] Writing tensor token_embd.weight                      | size  32000 x   8192  | type F16  | T+   1\n",
      "[  2/723] Writing tensor output_norm.weight                     | size   8192           | type F32  | T+   1\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type F16  | T+   1\n",
      "[  4/723] Writing tensor blk.0.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  5/723] Writing tensor blk.0.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  6/723] Writing tensor blk.0.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  7/723] Writing tensor blk.0.attn_output.weight               | size   8192 x   8192  | type F16  | T+   2\n",
      "[  8/723] Writing tensor blk.0.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   2\n",
      "[  9/723] Writing tensor blk.0.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   2\n",
      "[ 10/723] Writing tensor blk.0.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   2\n",
      "[ 11/723] Writing tensor blk.0.attn_norm.weight                 | size   8192           | type F32  | T+   2\n",
      "[ 12/723] Writing tensor blk.0.ffn_norm.weight                  | size   8192           | type F32  | T+   2\n",
      "[ 13/723] Writing tensor blk.1.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 14/723] Writing tensor blk.1.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 15/723] Writing tensor blk.1.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 16/723] Writing tensor blk.1.attn_output.weight               | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 17/723] Writing tensor blk.1.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   3\n",
      "[ 18/723] Writing tensor blk.1.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   3\n",
      "[ 19/723] Writing tensor blk.1.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   3\n",
      "[ 20/723] Writing tensor blk.1.attn_norm.weight                 | size   8192           | type F32  | T+   3\n",
      "[ 21/723] Writing tensor blk.1.ffn_norm.weight                  | size   8192           | type F32  | T+   3\n",
      "[ 22/723] Writing tensor blk.2.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 23/723] Writing tensor blk.2.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   4\n",
      "[ 24/723] Writing tensor blk.2.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   4\n",
      "[ 25/723] Writing tensor blk.2.attn_output.weight               | size   8192 x   8192  | type F16  | T+   4\n",
      "[ 26/723] Writing tensor blk.2.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   4\n",
      "[ 27/723] Writing tensor blk.2.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   5\n",
      "[ 28/723] Writing tensor blk.2.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   5\n",
      "[ 29/723] Writing tensor blk.2.attn_norm.weight                 | size   8192           | type F32  | T+   5\n",
      "[ 30/723] Writing tensor blk.2.ffn_norm.weight                  | size   8192           | type F32  | T+   5\n",
      "[ 31/723] Writing tensor blk.3.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 32/723] Writing tensor blk.3.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 33/723] Writing tensor blk.3.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 34/723] Writing tensor blk.3.attn_output.weight               | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 35/723] Writing tensor blk.3.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   5\n",
      "[ 36/723] Writing tensor blk.3.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   6\n",
      "[ 37/723] Writing tensor blk.3.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   6\n",
      "[ 38/723] Writing tensor blk.3.attn_norm.weight                 | size   8192           | type F32  | T+   6\n",
      "[ 39/723] Writing tensor blk.3.ffn_norm.weight                  | size   8192           | type F32  | T+   6\n",
      "[ 40/723] Writing tensor blk.4.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 41/723] Writing tensor blk.4.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 42/723] Writing tensor blk.4.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 43/723] Writing tensor blk.4.attn_output.weight               | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 44/723] Writing tensor blk.4.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   6\n",
      "[ 45/723] Writing tensor blk.4.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   7\n",
      "[ 46/723] Writing tensor blk.4.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   7\n",
      "[ 47/723] Writing tensor blk.4.attn_norm.weight                 | size   8192           | type F32  | T+   7\n",
      "[ 48/723] Writing tensor blk.4.ffn_norm.weight                  | size   8192           | type F32  | T+   7\n",
      "[ 49/723] Writing tensor blk.5.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 50/723] Writing tensor blk.5.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 51/723] Writing tensor blk.5.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 52/723] Writing tensor blk.5.attn_output.weight               | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 53/723] Writing tensor blk.5.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   8\n",
      "[ 54/723] Writing tensor blk.5.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   8\n",
      "[ 55/723] Writing tensor blk.5.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   9\n",
      "[ 56/723] Writing tensor blk.5.attn_norm.weight                 | size   8192           | type F32  | T+   9\n",
      "[ 57/723] Writing tensor blk.5.ffn_norm.weight                  | size   8192           | type F32  | T+   9\n",
      "[ 58/723] Writing tensor blk.6.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 59/723] Writing tensor blk.6.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 60/723] Writing tensor blk.6.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 61/723] Writing tensor blk.6.attn_output.weight               | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 62/723] Writing tensor blk.6.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   9\n",
      "[ 63/723] Writing tensor blk.6.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  10\n",
      "[ 64/723] Writing tensor blk.6.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  11\n",
      "[ 65/723] Writing tensor blk.6.attn_norm.weight                 | size   8192           | type F32  | T+  11\n",
      "[ 66/723] Writing tensor blk.6.ffn_norm.weight                  | size   8192           | type F32  | T+  11\n",
      "[ 67/723] Writing tensor blk.7.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 68/723] Writing tensor blk.7.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 69/723] Writing tensor blk.7.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 70/723] Writing tensor blk.7.attn_output.weight               | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 71/723] Writing tensor blk.7.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  11\n",
      "[ 72/723] Writing tensor blk.7.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  12\n",
      "[ 73/723] Writing tensor blk.7.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 74/723] Writing tensor blk.7.attn_norm.weight                 | size   8192           | type F32  | T+  12\n",
      "[ 75/723] Writing tensor blk.7.ffn_norm.weight                  | size   8192           | type F32  | T+  12\n",
      "[ 76/723] Writing tensor blk.8.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 77/723] Writing tensor blk.8.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 78/723] Writing tensor blk.8.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 79/723] Writing tensor blk.8.attn_output.weight               | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 80/723] Writing tensor blk.8.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  13\n",
      "[ 81/723] Writing tensor blk.8.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  13\n",
      "[ 82/723] Writing tensor blk.8.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  13\n",
      "[ 83/723] Writing tensor blk.8.attn_norm.weight                 | size   8192           | type F32  | T+  13\n",
      "[ 84/723] Writing tensor blk.8.ffn_norm.weight                  | size   8192           | type F32  | T+  13\n",
      "[ 85/723] Writing tensor blk.9.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 86/723] Writing tensor blk.9.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 87/723] Writing tensor blk.9.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 88/723] Writing tensor blk.9.attn_output.weight               | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 89/723] Writing tensor blk.9.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  14\n",
      "[ 90/723] Writing tensor blk.9.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  15\n",
      "[ 91/723] Writing tensor blk.9.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  16\n",
      "[ 92/723] Writing tensor blk.9.attn_norm.weight                 | size   8192           | type F32  | T+  17\n",
      "[ 93/723] Writing tensor blk.9.ffn_norm.weight                  | size   8192           | type F32  | T+  17\n",
      "[ 94/723] Writing tensor blk.10.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 95/723] Writing tensor blk.10.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 96/723] Writing tensor blk.10.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 97/723] Writing tensor blk.10.attn_output.weight              | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 98/723] Writing tensor blk.10.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  18\n",
      "[ 99/723] Writing tensor blk.10.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  18\n",
      "[100/723] Writing tensor blk.10.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  18\n",
      "[101/723] Writing tensor blk.10.attn_norm.weight                | size   8192           | type F32  | T+  18\n",
      "[102/723] Writing tensor blk.10.ffn_norm.weight                 | size   8192           | type F32  | T+  18\n",
      "[103/723] Writing tensor blk.11.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[104/723] Writing tensor blk.11.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[105/723] Writing tensor blk.11.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[106/723] Writing tensor blk.11.attn_output.weight              | size   8192 x   8192  | type F16  | T+  18\n",
      "[107/723] Writing tensor blk.11.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  19\n",
      "[108/723] Writing tensor blk.11.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  20\n",
      "[109/723] Writing tensor blk.11.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  20\n",
      "[110/723] Writing tensor blk.11.attn_norm.weight                | size   8192           | type F32  | T+  21\n",
      "[111/723] Writing tensor blk.11.ffn_norm.weight                 | size   8192           | type F32  | T+  21\n",
      "[112/723] Writing tensor blk.12.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[113/723] Writing tensor blk.12.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[114/723] Writing tensor blk.12.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[115/723] Writing tensor blk.12.attn_output.weight              | size   8192 x   8192  | type F16  | T+  21\n",
      "[116/723] Writing tensor blk.12.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  21\n",
      "[117/723] Writing tensor blk.12.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  22\n",
      "[118/723] Writing tensor blk.12.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  22\n",
      "[119/723] Writing tensor blk.12.attn_norm.weight                | size   8192           | type F32  | T+  23\n",
      "[120/723] Writing tensor blk.12.ffn_norm.weight                 | size   8192           | type F32  | T+  23\n",
      "[121/723] Writing tensor blk.13.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  23\n",
      "[122/723] Writing tensor blk.13.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  23\n",
      "[123/723] Writing tensor blk.13.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  23\n",
      "[124/723] Writing tensor blk.13.attn_output.weight              | size   8192 x   8192  | type F16  | T+  23\n",
      "[125/723] Writing tensor blk.13.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  23\n",
      "[126/723] Writing tensor blk.13.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  24\n",
      "[127/723] Writing tensor blk.13.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  24\n",
      "[128/723] Writing tensor blk.13.attn_norm.weight                | size   8192           | type F32  | T+  24\n",
      "[129/723] Writing tensor blk.13.ffn_norm.weight                 | size   8192           | type F32  | T+  24\n",
      "[130/723] Writing tensor blk.14.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[131/723] Writing tensor blk.14.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[132/723] Writing tensor blk.14.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[133/723] Writing tensor blk.14.attn_output.weight              | size   8192 x   8192  | type F16  | T+  24\n",
      "[134/723] Writing tensor blk.14.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  24\n",
      "[135/723] Writing tensor blk.14.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  25\n",
      "[136/723] Writing tensor blk.14.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  25\n",
      "[137/723] Writing tensor blk.14.attn_norm.weight                | size   8192           | type F32  | T+  25\n",
      "[138/723] Writing tensor blk.14.ffn_norm.weight                 | size   8192           | type F32  | T+  25\n",
      "[139/723] Writing tensor blk.15.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[140/723] Writing tensor blk.15.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[141/723] Writing tensor blk.15.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[142/723] Writing tensor blk.15.attn_output.weight              | size   8192 x   8192  | type F16  | T+  25\n",
      "[143/723] Writing tensor blk.15.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  25\n",
      "[144/723] Writing tensor blk.15.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  26\n",
      "[145/723] Writing tensor blk.15.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  26\n",
      "[146/723] Writing tensor blk.15.attn_norm.weight                | size   8192           | type F32  | T+  26\n",
      "[147/723] Writing tensor blk.15.ffn_norm.weight                 | size   8192           | type F32  | T+  26\n",
      "[148/723] Writing tensor blk.16.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[149/723] Writing tensor blk.16.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[150/723] Writing tensor blk.16.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[151/723] Writing tensor blk.16.attn_output.weight              | size   8192 x   8192  | type F16  | T+  26\n",
      "[152/723] Writing tensor blk.16.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  26\n",
      "[153/723] Writing tensor blk.16.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  27\n",
      "[154/723] Writing tensor blk.16.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  27\n",
      "[155/723] Writing tensor blk.16.attn_norm.weight                | size   8192           | type F32  | T+  27\n",
      "[156/723] Writing tensor blk.16.ffn_norm.weight                 | size   8192           | type F32  | T+  27\n",
      "[157/723] Writing tensor blk.17.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[158/723] Writing tensor blk.17.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[159/723] Writing tensor blk.17.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[160/723] Writing tensor blk.17.attn_output.weight              | size   8192 x   8192  | type F16  | T+  27\n",
      "[161/723] Writing tensor blk.17.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  28\n",
      "[162/723] Writing tensor blk.17.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  28\n",
      "[163/723] Writing tensor blk.17.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  28\n",
      "[164/723] Writing tensor blk.17.attn_norm.weight                | size   8192           | type F32  | T+  28\n",
      "[165/723] Writing tensor blk.17.ffn_norm.weight                 | size   8192           | type F32  | T+  28\n",
      "[166/723] Writing tensor blk.18.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[167/723] Writing tensor blk.18.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[168/723] Writing tensor blk.18.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[169/723] Writing tensor blk.18.attn_output.weight              | size   8192 x   8192  | type F16  | T+  28\n",
      "[170/723] Writing tensor blk.18.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  29\n",
      "[171/723] Writing tensor blk.18.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  29\n",
      "[172/723] Writing tensor blk.18.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  29\n",
      "[173/723] Writing tensor blk.18.attn_norm.weight                | size   8192           | type F32  | T+  30\n",
      "[174/723] Writing tensor blk.18.ffn_norm.weight                 | size   8192           | type F32  | T+  30\n",
      "[175/723] Writing tensor blk.19.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[176/723] Writing tensor blk.19.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[177/723] Writing tensor blk.19.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[178/723] Writing tensor blk.19.attn_output.weight              | size   8192 x   8192  | type F16  | T+  30\n",
      "[179/723] Writing tensor blk.19.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  30\n",
      "[180/723] Writing tensor blk.19.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  31\n",
      "[181/723] Writing tensor blk.19.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  33\n",
      "[182/723] Writing tensor blk.19.attn_norm.weight                | size   8192           | type F32  | T+  34\n",
      "[183/723] Writing tensor blk.19.ffn_norm.weight                 | size   8192           | type F32  | T+  34\n",
      "[184/723] Writing tensor blk.20.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[185/723] Writing tensor blk.20.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  35\n",
      "[186/723] Writing tensor blk.20.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[187/723] Writing tensor blk.20.attn_output.weight              | size   8192 x   8192  | type F16  | T+  36\n",
      "[188/723] Writing tensor blk.20.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  36\n",
      "[189/723] Writing tensor blk.20.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  36\n",
      "[190/723] Writing tensor blk.20.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  36\n",
      "[191/723] Writing tensor blk.20.attn_norm.weight                | size   8192           | type F32  | T+  37\n",
      "[192/723] Writing tensor blk.20.ffn_norm.weight                 | size   8192           | type F32  | T+  37\n",
      "[193/723] Writing tensor blk.21.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[194/723] Writing tensor blk.21.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[195/723] Writing tensor blk.21.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[196/723] Writing tensor blk.21.attn_output.weight              | size   8192 x   8192  | type F16  | T+  37\n",
      "[197/723] Writing tensor blk.21.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  37\n",
      "[198/723] Writing tensor blk.21.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  38\n",
      "[199/723] Writing tensor blk.21.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  38\n",
      "[200/723] Writing tensor blk.21.attn_norm.weight                | size   8192           | type F32  | T+  39\n",
      "[201/723] Writing tensor blk.21.ffn_norm.weight                 | size   8192           | type F32  | T+  39\n",
      "[202/723] Writing tensor blk.22.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[203/723] Writing tensor blk.22.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[204/723] Writing tensor blk.22.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[205/723] Writing tensor blk.22.attn_output.weight              | size   8192 x   8192  | type F16  | T+  39\n",
      "[206/723] Writing tensor blk.22.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  39\n",
      "[207/723] Writing tensor blk.22.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  39\n",
      "[208/723] Writing tensor blk.22.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  39\n",
      "[209/723] Writing tensor blk.22.attn_norm.weight                | size   8192           | type F32  | T+  40\n",
      "[210/723] Writing tensor blk.22.ffn_norm.weight                 | size   8192           | type F32  | T+  40\n",
      "[211/723] Writing tensor blk.23.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[212/723] Writing tensor blk.23.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[213/723] Writing tensor blk.23.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[214/723] Writing tensor blk.23.attn_output.weight              | size   8192 x   8192  | type F16  | T+  40\n",
      "[215/723] Writing tensor blk.23.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  40\n",
      "[216/723] Writing tensor blk.23.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  40\n",
      "[217/723] Writing tensor blk.23.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  41\n",
      "[218/723] Writing tensor blk.23.attn_norm.weight                | size   8192           | type F32  | T+  42\n",
      "[219/723] Writing tensor blk.23.ffn_norm.weight                 | size   8192           | type F32  | T+  42\n",
      "[220/723] Writing tensor blk.24.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  42\n",
      "[221/723] Writing tensor blk.24.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  42\n",
      "[222/723] Writing tensor blk.24.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[223/723] Writing tensor blk.24.attn_output.weight              | size   8192 x   8192  | type F16  | T+  43\n",
      "[224/723] Writing tensor blk.24.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  43\n",
      "[225/723] Writing tensor blk.24.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  44\n",
      "[226/723] Writing tensor blk.24.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  45\n",
      "[227/723] Writing tensor blk.24.attn_norm.weight                | size   8192           | type F32  | T+  45\n",
      "[228/723] Writing tensor blk.24.ffn_norm.weight                 | size   8192           | type F32  | T+  45\n",
      "[229/723] Writing tensor blk.25.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  45\n",
      "[230/723] Writing tensor blk.25.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  45\n",
      "[231/723] Writing tensor blk.25.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[232/723] Writing tensor blk.25.attn_output.weight              | size   8192 x   8192  | type F16  | T+  46\n",
      "[233/723] Writing tensor blk.25.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  46\n",
      "[234/723] Writing tensor blk.25.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  46\n",
      "[235/723] Writing tensor blk.25.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  46\n",
      "[236/723] Writing tensor blk.25.attn_norm.weight                | size   8192           | type F32  | T+  46\n",
      "[237/723] Writing tensor blk.25.ffn_norm.weight                 | size   8192           | type F32  | T+  46\n",
      "[238/723] Writing tensor blk.26.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[239/723] Writing tensor blk.26.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[240/723] Writing tensor blk.26.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[241/723] Writing tensor blk.26.attn_output.weight              | size   8192 x   8192  | type F16  | T+  47\n",
      "[242/723] Writing tensor blk.26.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  47\n",
      "[243/723] Writing tensor blk.26.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  47\n",
      "[244/723] Writing tensor blk.26.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  47\n",
      "[245/723] Writing tensor blk.26.attn_norm.weight                | size   8192           | type F32  | T+  48\n",
      "[246/723] Writing tensor blk.26.ffn_norm.weight                 | size   8192           | type F32  | T+  48\n",
      "[247/723] Writing tensor blk.27.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[248/723] Writing tensor blk.27.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[249/723] Writing tensor blk.27.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[250/723] Writing tensor blk.27.attn_output.weight              | size   8192 x   8192  | type F16  | T+  48\n",
      "[251/723] Writing tensor blk.27.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  48\n",
      "[252/723] Writing tensor blk.27.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  48\n",
      "[253/723] Writing tensor blk.27.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  48\n",
      "[254/723] Writing tensor blk.27.attn_norm.weight                | size   8192           | type F32  | T+  49\n",
      "[255/723] Writing tensor blk.27.ffn_norm.weight                 | size   8192           | type F32  | T+  49\n",
      "[256/723] Writing tensor blk.28.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[257/723] Writing tensor blk.28.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[258/723] Writing tensor blk.28.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[259/723] Writing tensor blk.28.attn_output.weight              | size   8192 x   8192  | type F16  | T+  49\n",
      "[260/723] Writing tensor blk.28.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  49\n",
      "[261/723] Writing tensor blk.28.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  50\n",
      "[262/723] Writing tensor blk.28.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  50\n",
      "[263/723] Writing tensor blk.28.attn_norm.weight                | size   8192           | type F32  | T+  50\n",
      "[264/723] Writing tensor blk.28.ffn_norm.weight                 | size   8192           | type F32  | T+  50\n",
      "[265/723] Writing tensor blk.29.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  50\n",
      "[266/723] Writing tensor blk.29.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  50\n",
      "[267/723] Writing tensor blk.29.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  50\n",
      "[268/723] Writing tensor blk.29.attn_output.weight              | size   8192 x   8192  | type F16  | T+  50\n",
      "[269/723] Writing tensor blk.29.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  51\n",
      "[270/723] Writing tensor blk.29.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  51\n",
      "[271/723] Writing tensor blk.29.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  51\n",
      "[272/723] Writing tensor blk.29.attn_norm.weight                | size   8192           | type F32  | T+  51\n",
      "[273/723] Writing tensor blk.29.ffn_norm.weight                 | size   8192           | type F32  | T+  51\n",
      "[274/723] Writing tensor blk.30.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[275/723] Writing tensor blk.30.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[276/723] Writing tensor blk.30.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[277/723] Writing tensor blk.30.attn_output.weight              | size   8192 x   8192  | type F16  | T+  51\n",
      "[278/723] Writing tensor blk.30.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  52\n",
      "[279/723] Writing tensor blk.30.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  52\n",
      "[280/723] Writing tensor blk.30.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  52\n",
      "[281/723] Writing tensor blk.30.attn_norm.weight                | size   8192           | type F32  | T+  52\n",
      "[282/723] Writing tensor blk.30.ffn_norm.weight                 | size   8192           | type F32  | T+  52\n",
      "[283/723] Writing tensor blk.31.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[284/723] Writing tensor blk.31.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[285/723] Writing tensor blk.31.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[286/723] Writing tensor blk.31.attn_output.weight              | size   8192 x   8192  | type F16  | T+  52\n",
      "[287/723] Writing tensor blk.31.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  53\n",
      "[288/723] Writing tensor blk.31.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  54\n",
      "[289/723] Writing tensor blk.31.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  54\n",
      "[290/723] Writing tensor blk.31.attn_norm.weight                | size   8192           | type F32  | T+  54\n",
      "[291/723] Writing tensor blk.31.ffn_norm.weight                 | size   8192           | type F32  | T+  54\n",
      "[292/723] Writing tensor blk.32.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  54\n",
      "[293/723] Writing tensor blk.32.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  54\n",
      "[294/723] Writing tensor blk.32.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  54\n",
      "[295/723] Writing tensor blk.32.attn_output.weight              | size   8192 x   8192  | type F16  | T+  54\n",
      "[296/723] Writing tensor blk.32.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  54\n",
      "[297/723] Writing tensor blk.32.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  55\n",
      "[298/723] Writing tensor blk.32.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  55\n",
      "[299/723] Writing tensor blk.32.attn_norm.weight                | size   8192           | type F32  | T+  55\n",
      "[300/723] Writing tensor blk.32.ffn_norm.weight                 | size   8192           | type F32  | T+  55\n",
      "[301/723] Writing tensor blk.33.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[302/723] Writing tensor blk.33.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[303/723] Writing tensor blk.33.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[304/723] Writing tensor blk.33.attn_output.weight              | size   8192 x   8192  | type F16  | T+  55\n",
      "[305/723] Writing tensor blk.33.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  56\n",
      "[306/723] Writing tensor blk.33.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  57\n",
      "[307/723] Writing tensor blk.33.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  57\n",
      "[308/723] Writing tensor blk.33.attn_norm.weight                | size   8192           | type F32  | T+  58\n",
      "[309/723] Writing tensor blk.33.ffn_norm.weight                 | size   8192           | type F32  | T+  58\n",
      "[310/723] Writing tensor blk.34.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[311/723] Writing tensor blk.34.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[312/723] Writing tensor blk.34.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[313/723] Writing tensor blk.34.attn_output.weight              | size   8192 x   8192  | type F16  | T+  58\n",
      "[314/723] Writing tensor blk.34.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  58\n",
      "[315/723] Writing tensor blk.34.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  58\n",
      "[316/723] Writing tensor blk.34.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  59\n",
      "[317/723] Writing tensor blk.34.attn_norm.weight                | size   8192           | type F32  | T+  59\n",
      "[318/723] Writing tensor blk.34.ffn_norm.weight                 | size   8192           | type F32  | T+  59\n",
      "[319/723] Writing tensor blk.35.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  59\n",
      "[320/723] Writing tensor blk.35.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  59\n",
      "[321/723] Writing tensor blk.35.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  59\n",
      "[322/723] Writing tensor blk.35.attn_output.weight              | size   8192 x   8192  | type F16  | T+  59\n",
      "[323/723] Writing tensor blk.35.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  59\n",
      "[324/723] Writing tensor blk.35.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  59\n",
      "[325/723] Writing tensor blk.35.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  60\n",
      "[326/723] Writing tensor blk.35.attn_norm.weight                | size   8192           | type F32  | T+  60\n",
      "[327/723] Writing tensor blk.35.ffn_norm.weight                 | size   8192           | type F32  | T+  60\n",
      "[328/723] Writing tensor blk.36.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[329/723] Writing tensor blk.36.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[330/723] Writing tensor blk.36.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[331/723] Writing tensor blk.36.attn_output.weight              | size   8192 x   8192  | type F16  | T+  60\n",
      "[332/723] Writing tensor blk.36.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  60\n",
      "[333/723] Writing tensor blk.36.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  61\n",
      "[334/723] Writing tensor blk.36.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  61\n",
      "[335/723] Writing tensor blk.36.attn_norm.weight                | size   8192           | type F32  | T+  61\n",
      "[336/723] Writing tensor blk.36.ffn_norm.weight                 | size   8192           | type F32  | T+  61\n",
      "[337/723] Writing tensor blk.37.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  61\n",
      "[338/723] Writing tensor blk.37.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  61\n",
      "[339/723] Writing tensor blk.37.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  61\n",
      "[340/723] Writing tensor blk.37.attn_output.weight              | size   8192 x   8192  | type F16  | T+  61\n",
      "[341/723] Writing tensor blk.37.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  62\n",
      "[342/723] Writing tensor blk.37.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  62\n",
      "[343/723] Writing tensor blk.37.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  64\n",
      "[344/723] Writing tensor blk.37.attn_norm.weight                | size   8192           | type F32  | T+  64\n",
      "[345/723] Writing tensor blk.37.ffn_norm.weight                 | size   8192           | type F32  | T+  64\n",
      "[346/723] Writing tensor blk.38.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[347/723] Writing tensor blk.38.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[348/723] Writing tensor blk.38.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[349/723] Writing tensor blk.38.attn_output.weight              | size   8192 x   8192  | type F16  | T+  64\n",
      "[350/723] Writing tensor blk.38.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  64\n",
      "[351/723] Writing tensor blk.38.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  65\n",
      "[352/723] Writing tensor blk.38.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  65\n",
      "[353/723] Writing tensor blk.38.attn_norm.weight                | size   8192           | type F32  | T+  65\n",
      "[354/723] Writing tensor blk.38.ffn_norm.weight                 | size   8192           | type F32  | T+  65\n",
      "[355/723] Writing tensor blk.39.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[356/723] Writing tensor blk.39.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[357/723] Writing tensor blk.39.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[358/723] Writing tensor blk.39.attn_output.weight              | size   8192 x   8192  | type F16  | T+  65\n",
      "[359/723] Writing tensor blk.39.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  65\n",
      "[360/723] Writing tensor blk.39.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  66\n",
      "[361/723] Writing tensor blk.39.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  66\n",
      "[362/723] Writing tensor blk.39.attn_norm.weight                | size   8192           | type F32  | T+  66\n",
      "[363/723] Writing tensor blk.39.ffn_norm.weight                 | size   8192           | type F32  | T+  66\n",
      "[364/723] Writing tensor blk.40.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[365/723] Writing tensor blk.40.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[366/723] Writing tensor blk.40.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[367/723] Writing tensor blk.40.attn_output.weight              | size   8192 x   8192  | type F16  | T+  66\n",
      "[368/723] Writing tensor blk.40.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  67\n",
      "[369/723] Writing tensor blk.40.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  67\n",
      "[370/723] Writing tensor blk.40.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  68\n",
      "[371/723] Writing tensor blk.40.attn_norm.weight                | size   8192           | type F32  | T+  70\n",
      "[372/723] Writing tensor blk.40.ffn_norm.weight                 | size   8192           | type F32  | T+  70\n",
      "[373/723] Writing tensor blk.41.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[374/723] Writing tensor blk.41.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[375/723] Writing tensor blk.41.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[376/723] Writing tensor blk.41.attn_output.weight              | size   8192 x   8192  | type F16  | T+  70\n",
      "[377/723] Writing tensor blk.41.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  71\n",
      "[378/723] Writing tensor blk.41.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  71\n",
      "[379/723] Writing tensor blk.41.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  71\n",
      "[380/723] Writing tensor blk.41.attn_norm.weight                | size   8192           | type F32  | T+  72\n",
      "[381/723] Writing tensor blk.41.ffn_norm.weight                 | size   8192           | type F32  | T+  72\n",
      "[382/723] Writing tensor blk.42.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  72\n",
      "[383/723] Writing tensor blk.42.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  72\n",
      "[384/723] Writing tensor blk.42.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  72\n",
      "[385/723] Writing tensor blk.42.attn_output.weight              | size   8192 x   8192  | type F16  | T+  72\n",
      "[386/723] Writing tensor blk.42.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  72\n",
      "[387/723] Writing tensor blk.42.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  72\n",
      "[388/723] Writing tensor blk.42.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  73\n",
      "[389/723] Writing tensor blk.42.attn_norm.weight                | size   8192           | type F32  | T+  74\n",
      "[390/723] Writing tensor blk.42.ffn_norm.weight                 | size   8192           | type F32  | T+  74\n",
      "[391/723] Writing tensor blk.43.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[392/723] Writing tensor blk.43.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[393/723] Writing tensor blk.43.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[394/723] Writing tensor blk.43.attn_output.weight              | size   8192 x   8192  | type F16  | T+  74\n",
      "[395/723] Writing tensor blk.43.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  74\n",
      "[396/723] Writing tensor blk.43.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  74\n",
      "[397/723] Writing tensor blk.43.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  75\n",
      "[398/723] Writing tensor blk.43.attn_norm.weight                | size   8192           | type F32  | T+  75\n",
      "[399/723] Writing tensor blk.43.ffn_norm.weight                 | size   8192           | type F32  | T+  75\n",
      "[400/723] Writing tensor blk.44.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[401/723] Writing tensor blk.44.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[402/723] Writing tensor blk.44.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[403/723] Writing tensor blk.44.attn_output.weight              | size   8192 x   8192  | type F16  | T+  75\n",
      "[404/723] Writing tensor blk.44.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  75\n",
      "[405/723] Writing tensor blk.44.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  76\n",
      "[406/723] Writing tensor blk.44.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  76\n",
      "[407/723] Writing tensor blk.44.attn_norm.weight                | size   8192           | type F32  | T+  77\n",
      "[408/723] Writing tensor blk.44.ffn_norm.weight                 | size   8192           | type F32  | T+  77\n",
      "[409/723] Writing tensor blk.45.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[410/723] Writing tensor blk.45.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[411/723] Writing tensor blk.45.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[412/723] Writing tensor blk.45.attn_output.weight              | size   8192 x   8192  | type F16  | T+  77\n",
      "[413/723] Writing tensor blk.45.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  77\n",
      "[414/723] Writing tensor blk.45.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  78\n",
      "[415/723] Writing tensor blk.45.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  79\n",
      "[416/723] Writing tensor blk.45.attn_norm.weight                | size   8192           | type F32  | T+  80\n",
      "[417/723] Writing tensor blk.45.ffn_norm.weight                 | size   8192           | type F32  | T+  80\n",
      "[418/723] Writing tensor blk.46.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[419/723] Writing tensor blk.46.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[420/723] Writing tensor blk.46.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[421/723] Writing tensor blk.46.attn_output.weight              | size   8192 x   8192  | type F16  | T+  80\n",
      "[422/723] Writing tensor blk.46.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  80\n",
      "[423/723] Writing tensor blk.46.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  81\n",
      "[424/723] Writing tensor blk.46.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  81\n",
      "[425/723] Writing tensor blk.46.attn_norm.weight                | size   8192           | type F32  | T+  82\n",
      "[426/723] Writing tensor blk.46.ffn_norm.weight                 | size   8192           | type F32  | T+  82\n",
      "[427/723] Writing tensor blk.47.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[428/723] Writing tensor blk.47.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[429/723] Writing tensor blk.47.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[430/723] Writing tensor blk.47.attn_output.weight              | size   8192 x   8192  | type F16  | T+  82\n",
      "[431/723] Writing tensor blk.47.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  82\n",
      "[432/723] Writing tensor blk.47.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  82\n",
      "[433/723] Writing tensor blk.47.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  83\n",
      "[434/723] Writing tensor blk.47.attn_norm.weight                | size   8192           | type F32  | T+  83\n",
      "[435/723] Writing tensor blk.47.ffn_norm.weight                 | size   8192           | type F32  | T+  83\n",
      "[436/723] Writing tensor blk.48.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[437/723] Writing tensor blk.48.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[438/723] Writing tensor blk.48.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[439/723] Writing tensor blk.48.attn_output.weight              | size   8192 x   8192  | type F16  | T+  83\n",
      "[440/723] Writing tensor blk.48.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  83\n",
      "[441/723] Writing tensor blk.48.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  84\n",
      "[442/723] Writing tensor blk.48.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  85\n",
      "[443/723] Writing tensor blk.48.attn_norm.weight                | size   8192           | type F32  | T+  85\n",
      "[444/723] Writing tensor blk.48.ffn_norm.weight                 | size   8192           | type F32  | T+  85\n",
      "[445/723] Writing tensor blk.49.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[446/723] Writing tensor blk.49.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[447/723] Writing tensor blk.49.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[448/723] Writing tensor blk.49.attn_output.weight              | size   8192 x   8192  | type F16  | T+  85\n",
      "[449/723] Writing tensor blk.49.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  85\n",
      "[450/723] Writing tensor blk.49.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  85\n",
      "[451/723] Writing tensor blk.49.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  86\n",
      "[452/723] Writing tensor blk.49.attn_norm.weight                | size   8192           | type F32  | T+  86\n",
      "[453/723] Writing tensor blk.49.ffn_norm.weight                 | size   8192           | type F32  | T+  86\n",
      "[454/723] Writing tensor blk.50.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  86\n",
      "[455/723] Writing tensor blk.50.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  86\n",
      "[456/723] Writing tensor blk.50.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  86\n",
      "[457/723] Writing tensor blk.50.attn_output.weight              | size   8192 x   8192  | type F16  | T+  86\n",
      "[458/723] Writing tensor blk.50.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  86\n",
      "[459/723] Writing tensor blk.50.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  86\n",
      "[460/723] Writing tensor blk.50.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  87\n",
      "[461/723] Writing tensor blk.50.attn_norm.weight                | size   8192           | type F32  | T+  87\n",
      "[462/723] Writing tensor blk.50.ffn_norm.weight                 | size   8192           | type F32  | T+  87\n",
      "[463/723] Writing tensor blk.51.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[464/723] Writing tensor blk.51.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[465/723] Writing tensor blk.51.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[466/723] Writing tensor blk.51.attn_output.weight              | size   8192 x   8192  | type F16  | T+  87\n",
      "[467/723] Writing tensor blk.51.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  87\n",
      "[468/723] Writing tensor blk.51.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  87\n",
      "[469/723] Writing tensor blk.51.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  88\n",
      "[470/723] Writing tensor blk.51.attn_norm.weight                | size   8192           | type F32  | T+  88\n",
      "[471/723] Writing tensor blk.51.ffn_norm.weight                 | size   8192           | type F32  | T+  88\n",
      "[472/723] Writing tensor blk.52.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[473/723] Writing tensor blk.52.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[474/723] Writing tensor blk.52.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[475/723] Writing tensor blk.52.attn_output.weight              | size   8192 x   8192  | type F16  | T+  88\n",
      "[476/723] Writing tensor blk.52.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  88\n",
      "[477/723] Writing tensor blk.52.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  89\n",
      "[478/723] Writing tensor blk.52.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  89\n",
      "[479/723] Writing tensor blk.52.attn_norm.weight                | size   8192           | type F32  | T+  89\n",
      "[480/723] Writing tensor blk.52.ffn_norm.weight                 | size   8192           | type F32  | T+  89\n",
      "[481/723] Writing tensor blk.53.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[482/723] Writing tensor blk.53.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[483/723] Writing tensor blk.53.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[484/723] Writing tensor blk.53.attn_output.weight              | size   8192 x   8192  | type F16  | T+  89\n",
      "[485/723] Writing tensor blk.53.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  89\n",
      "[486/723] Writing tensor blk.53.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  89\n",
      "[487/723] Writing tensor blk.53.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  90\n",
      "[488/723] Writing tensor blk.53.attn_norm.weight                | size   8192           | type F32  | T+  90\n",
      "[489/723] Writing tensor blk.53.ffn_norm.weight                 | size   8192           | type F32  | T+  90\n",
      "[490/723] Writing tensor blk.54.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  90\n",
      "[491/723] Writing tensor blk.54.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  90\n",
      "[492/723] Writing tensor blk.54.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  90\n",
      "[493/723] Writing tensor blk.54.attn_output.weight              | size   8192 x   8192  | type F16  | T+  90\n",
      "[494/723] Writing tensor blk.54.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  90\n",
      "[495/723] Writing tensor blk.54.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  91\n",
      "[496/723] Writing tensor blk.54.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  91\n",
      "[497/723] Writing tensor blk.54.attn_norm.weight                | size   8192           | type F32  | T+  91\n",
      "[498/723] Writing tensor blk.54.ffn_norm.weight                 | size   8192           | type F32  | T+  91\n",
      "[499/723] Writing tensor blk.55.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  91\n",
      "[500/723] Writing tensor blk.55.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  91\n",
      "[501/723] Writing tensor blk.55.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  91\n",
      "[502/723] Writing tensor blk.55.attn_output.weight              | size   8192 x   8192  | type F16  | T+  91\n",
      "[503/723] Writing tensor blk.55.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  92\n",
      "[504/723] Writing tensor blk.55.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  93\n",
      "[505/723] Writing tensor blk.55.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  93\n",
      "[506/723] Writing tensor blk.55.attn_norm.weight                | size   8192           | type F32  | T+  93\n",
      "[507/723] Writing tensor blk.55.ffn_norm.weight                 | size   8192           | type F32  | T+  93\n",
      "[508/723] Writing tensor blk.56.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[509/723] Writing tensor blk.56.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[510/723] Writing tensor blk.56.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[511/723] Writing tensor blk.56.attn_output.weight              | size   8192 x   8192  | type F16  | T+  93\n",
      "[512/723] Writing tensor blk.56.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  93\n",
      "[513/723] Writing tensor blk.56.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  94\n",
      "[514/723] Writing tensor blk.56.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  95\n",
      "[515/723] Writing tensor blk.56.attn_norm.weight                | size   8192           | type F32  | T+  97\n",
      "[516/723] Writing tensor blk.56.ffn_norm.weight                 | size   8192           | type F32  | T+  97\n",
      "[517/723] Writing tensor blk.57.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  97\n",
      "[518/723] Writing tensor blk.57.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  97\n",
      "[519/723] Writing tensor blk.57.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  97\n",
      "[520/723] Writing tensor blk.57.attn_output.weight              | size   8192 x   8192  | type F16  | T+  97\n",
      "[521/723] Writing tensor blk.57.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  97\n",
      "[522/723] Writing tensor blk.57.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  98\n",
      "[523/723] Writing tensor blk.57.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  98\n",
      "[524/723] Writing tensor blk.57.attn_norm.weight                | size   8192           | type F32  | T+  98\n",
      "[525/723] Writing tensor blk.57.ffn_norm.weight                 | size   8192           | type F32  | T+  98\n",
      "[526/723] Writing tensor blk.58.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  98\n",
      "[527/723] Writing tensor blk.58.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  98\n",
      "[528/723] Writing tensor blk.58.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  98\n",
      "[529/723] Writing tensor blk.58.attn_output.weight              | size   8192 x   8192  | type F16  | T+  98\n",
      "[530/723] Writing tensor blk.58.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  98\n",
      "[531/723] Writing tensor blk.58.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  98\n",
      "[532/723] Writing tensor blk.58.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  99\n",
      "[533/723] Writing tensor blk.58.attn_norm.weight                | size   8192           | type F32  | T+  99\n",
      "[534/723] Writing tensor blk.58.ffn_norm.weight                 | size   8192           | type F32  | T+  99\n",
      "[535/723] Writing tensor blk.59.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  99\n",
      "[536/723] Writing tensor blk.59.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  99\n",
      "[537/723] Writing tensor blk.59.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  99\n",
      "[538/723] Writing tensor blk.59.attn_output.weight              | size   8192 x   8192  | type F16  | T+  99\n",
      "[539/723] Writing tensor blk.59.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  99\n",
      "[540/723] Writing tensor blk.59.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 100\n",
      "[541/723] Writing tensor blk.59.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 100\n",
      "[542/723] Writing tensor blk.59.attn_norm.weight                | size   8192           | type F32  | T+ 100\n",
      "[543/723] Writing tensor blk.59.ffn_norm.weight                 | size   8192           | type F32  | T+ 100\n",
      "[544/723] Writing tensor blk.60.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[545/723] Writing tensor blk.60.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[546/723] Writing tensor blk.60.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[547/723] Writing tensor blk.60.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 101\n",
      "[548/723] Writing tensor blk.60.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 101\n",
      "[549/723] Writing tensor blk.60.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 101\n",
      "[550/723] Writing tensor blk.60.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 101\n",
      "[551/723] Writing tensor blk.60.attn_norm.weight                | size   8192           | type F32  | T+ 102\n",
      "[552/723] Writing tensor blk.60.ffn_norm.weight                 | size   8192           | type F32  | T+ 102\n",
      "[553/723] Writing tensor blk.61.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[554/723] Writing tensor blk.61.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[555/723] Writing tensor blk.61.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[556/723] Writing tensor blk.61.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 102\n",
      "[557/723] Writing tensor blk.61.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 102\n",
      "[558/723] Writing tensor blk.61.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 102\n",
      "[559/723] Writing tensor blk.61.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 102\n",
      "[560/723] Writing tensor blk.61.attn_norm.weight                | size   8192           | type F32  | T+ 103\n",
      "[561/723] Writing tensor blk.61.ffn_norm.weight                 | size   8192           | type F32  | T+ 103\n",
      "[562/723] Writing tensor blk.62.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 103\n",
      "[563/723] Writing tensor blk.62.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 103\n",
      "[564/723] Writing tensor blk.62.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 103\n",
      "[565/723] Writing tensor blk.62.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 103\n",
      "[566/723] Writing tensor blk.62.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 104\n",
      "[567/723] Writing tensor blk.62.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 104\n",
      "[568/723] Writing tensor blk.62.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 104\n",
      "[569/723] Writing tensor blk.62.attn_norm.weight                | size   8192           | type F32  | T+ 104\n",
      "[570/723] Writing tensor blk.62.ffn_norm.weight                 | size   8192           | type F32  | T+ 104\n",
      "[571/723] Writing tensor blk.63.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[572/723] Writing tensor blk.63.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[573/723] Writing tensor blk.63.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[574/723] Writing tensor blk.63.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 104\n",
      "[575/723] Writing tensor blk.63.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 105\n",
      "[576/723] Writing tensor blk.63.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 106\n",
      "[577/723] Writing tensor blk.63.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 108\n",
      "[578/723] Writing tensor blk.63.attn_norm.weight                | size   8192           | type F32  | T+ 108\n",
      "[579/723] Writing tensor blk.63.ffn_norm.weight                 | size   8192           | type F32  | T+ 108\n",
      "[580/723] Writing tensor blk.64.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 108\n",
      "[581/723] Writing tensor blk.64.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[582/723] Writing tensor blk.64.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 110\n",
      "[583/723] Writing tensor blk.64.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 111\n",
      "[584/723] Writing tensor blk.64.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 111\n",
      "[585/723] Writing tensor blk.64.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 112\n",
      "[586/723] Writing tensor blk.64.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 113\n",
      "[587/723] Writing tensor blk.64.attn_norm.weight                | size   8192           | type F32  | T+ 114\n",
      "[588/723] Writing tensor blk.64.ffn_norm.weight                 | size   8192           | type F32  | T+ 114\n",
      "[589/723] Writing tensor blk.65.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[590/723] Writing tensor blk.65.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[591/723] Writing tensor blk.65.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[592/723] Writing tensor blk.65.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 115\n",
      "[593/723] Writing tensor blk.65.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 115\n",
      "[594/723] Writing tensor blk.65.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 116\n",
      "[595/723] Writing tensor blk.65.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 118\n",
      "[596/723] Writing tensor blk.65.attn_norm.weight                | size   8192           | type F32  | T+ 119\n",
      "[597/723] Writing tensor blk.65.ffn_norm.weight                 | size   8192           | type F32  | T+ 119\n",
      "[598/723] Writing tensor blk.66.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[599/723] Writing tensor blk.66.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[600/723] Writing tensor blk.66.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[601/723] Writing tensor blk.66.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 120\n",
      "[602/723] Writing tensor blk.66.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 120\n",
      "[603/723] Writing tensor blk.66.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 121\n",
      "[604/723] Writing tensor blk.66.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 122\n",
      "[605/723] Writing tensor blk.66.attn_norm.weight                | size   8192           | type F32  | T+ 123\n",
      "[606/723] Writing tensor blk.66.ffn_norm.weight                 | size   8192           | type F32  | T+ 123\n",
      "[607/723] Writing tensor blk.67.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 123\n",
      "[608/723] Writing tensor blk.67.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 123\n",
      "[609/723] Writing tensor blk.67.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 123\n",
      "[610/723] Writing tensor blk.67.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 124\n",
      "[611/723] Writing tensor blk.67.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 124\n",
      "[612/723] Writing tensor blk.67.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 124\n",
      "[613/723] Writing tensor blk.67.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 124\n",
      "[614/723] Writing tensor blk.67.attn_norm.weight                | size   8192           | type F32  | T+ 124\n",
      "[615/723] Writing tensor blk.67.ffn_norm.weight                 | size   8192           | type F32  | T+ 124\n",
      "[616/723] Writing tensor blk.68.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 124\n",
      "[617/723] Writing tensor blk.68.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 124\n",
      "[618/723] Writing tensor blk.68.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 124\n",
      "[619/723] Writing tensor blk.68.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 124\n",
      "[620/723] Writing tensor blk.68.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 125\n",
      "[621/723] Writing tensor blk.68.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 125\n",
      "[622/723] Writing tensor blk.68.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 125\n",
      "[623/723] Writing tensor blk.68.attn_norm.weight                | size   8192           | type F32  | T+ 125\n",
      "[624/723] Writing tensor blk.68.ffn_norm.weight                 | size   8192           | type F32  | T+ 125\n",
      "[625/723] Writing tensor blk.69.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[626/723] Writing tensor blk.69.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[627/723] Writing tensor blk.69.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[628/723] Writing tensor blk.69.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 125\n",
      "[629/723] Writing tensor blk.69.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 125\n",
      "[630/723] Writing tensor blk.69.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 126\n",
      "[631/723] Writing tensor blk.69.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 126\n",
      "[632/723] Writing tensor blk.69.attn_norm.weight                | size   8192           | type F32  | T+ 126\n",
      "[633/723] Writing tensor blk.69.ffn_norm.weight                 | size   8192           | type F32  | T+ 126\n",
      "[634/723] Writing tensor blk.70.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[635/723] Writing tensor blk.70.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[636/723] Writing tensor blk.70.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[637/723] Writing tensor blk.70.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 126\n",
      "[638/723] Writing tensor blk.70.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 127\n",
      "[639/723] Writing tensor blk.70.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 127\n",
      "[640/723] Writing tensor blk.70.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 127\n",
      "[641/723] Writing tensor blk.70.attn_norm.weight                | size   8192           | type F32  | T+ 127\n",
      "[642/723] Writing tensor blk.70.ffn_norm.weight                 | size   8192           | type F32  | T+ 127\n",
      "[643/723] Writing tensor blk.71.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 127\n",
      "[644/723] Writing tensor blk.71.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 127\n",
      "[645/723] Writing tensor blk.71.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 127\n",
      "[646/723] Writing tensor blk.71.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 127\n",
      "[647/723] Writing tensor blk.71.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 128\n",
      "[648/723] Writing tensor blk.71.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 128\n",
      "[649/723] Writing tensor blk.71.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 129\n",
      "[650/723] Writing tensor blk.71.attn_norm.weight                | size   8192           | type F32  | T+ 129\n",
      "[651/723] Writing tensor blk.71.ffn_norm.weight                 | size   8192           | type F32  | T+ 129\n",
      "[652/723] Writing tensor blk.72.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 129\n",
      "[653/723] Writing tensor blk.72.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 129\n",
      "[654/723] Writing tensor blk.72.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 129\n",
      "[655/723] Writing tensor blk.72.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 129\n",
      "[656/723] Writing tensor blk.72.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 129\n",
      "[657/723] Writing tensor blk.72.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 130\n",
      "[658/723] Writing tensor blk.72.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 130\n",
      "[659/723] Writing tensor blk.72.attn_norm.weight                | size   8192           | type F32  | T+ 130\n",
      "[660/723] Writing tensor blk.72.ffn_norm.weight                 | size   8192           | type F32  | T+ 130\n",
      "[661/723] Writing tensor blk.73.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 130\n",
      "[662/723] Writing tensor blk.73.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 130\n",
      "[663/723] Writing tensor blk.73.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 130\n",
      "[664/723] Writing tensor blk.73.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 130\n",
      "[665/723] Writing tensor blk.73.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 131\n",
      "[666/723] Writing tensor blk.73.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 131\n",
      "[667/723] Writing tensor blk.73.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 131\n",
      "[668/723] Writing tensor blk.73.attn_norm.weight                | size   8192           | type F32  | T+ 131\n",
      "[669/723] Writing tensor blk.73.ffn_norm.weight                 | size   8192           | type F32  | T+ 131\n",
      "[670/723] Writing tensor blk.74.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 131\n",
      "[671/723] Writing tensor blk.74.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 131\n",
      "[672/723] Writing tensor blk.74.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 131\n",
      "[673/723] Writing tensor blk.74.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 131\n",
      "[674/723] Writing tensor blk.74.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 131\n",
      "[675/723] Writing tensor blk.74.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 132\n",
      "[676/723] Writing tensor blk.74.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 132\n",
      "[677/723] Writing tensor blk.74.attn_norm.weight                | size   8192           | type F32  | T+ 132\n",
      "[678/723] Writing tensor blk.74.ffn_norm.weight                 | size   8192           | type F32  | T+ 132\n",
      "[679/723] Writing tensor blk.75.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 132\n",
      "[680/723] Writing tensor blk.75.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 132\n",
      "[681/723] Writing tensor blk.75.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 132\n",
      "[682/723] Writing tensor blk.75.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 132\n",
      "[683/723] Writing tensor blk.75.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 132\n",
      "[684/723] Writing tensor blk.75.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 132\n",
      "[685/723] Writing tensor blk.75.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 134\n",
      "[686/723] Writing tensor blk.75.attn_norm.weight                | size   8192           | type F32  | T+ 134\n",
      "[687/723] Writing tensor blk.75.ffn_norm.weight                 | size   8192           | type F32  | T+ 134\n",
      "[688/723] Writing tensor blk.76.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 134\n",
      "[689/723] Writing tensor blk.76.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 134\n",
      "[690/723] Writing tensor blk.76.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 134\n",
      "[691/723] Writing tensor blk.76.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 135\n",
      "[692/723] Writing tensor blk.76.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 135\n",
      "[693/723] Writing tensor blk.76.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 136\n",
      "[694/723] Writing tensor blk.76.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 137\n",
      "[695/723] Writing tensor blk.76.attn_norm.weight                | size   8192           | type F32  | T+ 138\n",
      "[696/723] Writing tensor blk.76.ffn_norm.weight                 | size   8192           | type F32  | T+ 138\n",
      "[697/723] Writing tensor blk.77.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 138\n",
      "[698/723] Writing tensor blk.77.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 139\n",
      "[699/723] Writing tensor blk.77.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 139\n",
      "[700/723] Writing tensor blk.77.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 139\n",
      "[701/723] Writing tensor blk.77.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 139\n",
      "[702/723] Writing tensor blk.77.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 140\n",
      "[703/723] Writing tensor blk.77.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 142\n",
      "[704/723] Writing tensor blk.77.attn_norm.weight                | size   8192           | type F32  | T+ 142\n",
      "[705/723] Writing tensor blk.77.ffn_norm.weight                 | size   8192           | type F32  | T+ 142\n",
      "[706/723] Writing tensor blk.78.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 142\n",
      "[707/723] Writing tensor blk.78.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 143\n",
      "[708/723] Writing tensor blk.78.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 143\n",
      "[709/723] Writing tensor blk.78.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 144\n",
      "[710/723] Writing tensor blk.78.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 144\n",
      "[711/723] Writing tensor blk.78.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 145\n",
      "[712/723] Writing tensor blk.78.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 145\n",
      "[713/723] Writing tensor blk.78.attn_norm.weight                | size   8192           | type F32  | T+ 146\n",
      "[714/723] Writing tensor blk.78.ffn_norm.weight                 | size   8192           | type F32  | T+ 146\n",
      "[715/723] Writing tensor blk.79.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 146\n",
      "[716/723] Writing tensor blk.79.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 147\n",
      "[717/723] Writing tensor blk.79.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 147\n",
      "[718/723] Writing tensor blk.79.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 148\n",
      "[719/723] Writing tensor blk.79.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 148\n",
      "[720/723] Writing tensor blk.79.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 149\n",
      "[721/723] Writing tensor blk.79.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 149\n",
      "[722/723] Writing tensor blk.79.attn_norm.weight                | size   8192           | type F32  | T+ 150\n",
      "[723/723] Writing tensor blk.79.ffn_norm.weight                 | size   8192           | type F32  | T+ 150\n",
      "Wrote models/65B/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/7B/ggml-model-f16.gguf' to './models/7B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 1714336 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
      "[   4/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   9/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  22/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  82/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 109/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 262/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 271/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 280/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 282/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 290/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 14076.22 ms\n",
      "main:    total time = 14076.22 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/13B/ggml-model-f16.gguf' to './models/13B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_quantize_internal: meta size = 1718656 bytes\n",
      "[   1/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   312.50 MiB ->    87.89 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   312.50 MiB ->   128.17 MiB | hist: \n",
      "[   4/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  12/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  13/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  21/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  22/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  30/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  31/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  39/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  40/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  48/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  49/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  57/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  58/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  66/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  67/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  75/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  76/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  84/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  85/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  93/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  94/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 102/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 109/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 111/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 120/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 129/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 138/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 147/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 156/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 165/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 174/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 183/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 192/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 201/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 210/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 219/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 228/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 237/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 246/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 255/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 264/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 273/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 282/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 291/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 300/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 309/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 318/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 327/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 334/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 336/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 343/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 345/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 352/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 354/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 361/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 362/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 363/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 24449.57 ms\n",
      "main:    total time = 24449.57 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/30B/ggml-model-f16.gguf' to './models/30B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llama_model_quantize_internal: meta size = 1729408 bytes\n",
      "[   1/ 543]                    token_embd.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   406.25 MiB ->   114.26 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                   output_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   406.25 MiB ->   166.63 MiB | hist: \n",
      "[   4/ 543]                  blk.0.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]                  blk.0.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]                  blk.0.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]             blk.0.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]                blk.0.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 543]                blk.0.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 543]                  blk.0.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 543]               blk.0.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  12/ 543]                blk.0.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  13/ 543]                  blk.1.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]                  blk.1.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]                  blk.1.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]             blk.1.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]                blk.1.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 543]                blk.1.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]                  blk.1.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]               blk.1.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  21/ 543]                blk.1.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  22/ 543]                  blk.2.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]                  blk.2.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]                  blk.2.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]             blk.2.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]                blk.2.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 543]                blk.2.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]                  blk.2.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]               blk.2.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  30/ 543]                blk.2.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  31/ 543]                  blk.3.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]                  blk.3.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]                  blk.3.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]             blk.3.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]                blk.3.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 543]                blk.3.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  37/ 543]                  blk.3.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 543]               blk.3.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  39/ 543]                blk.3.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  40/ 543]                  blk.4.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]                  blk.4.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]                  blk.4.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]             blk.4.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]                blk.4.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 543]                blk.4.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]                  blk.4.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]               blk.4.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  48/ 543]                blk.4.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  49/ 543]                  blk.5.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]                  blk.5.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]                  blk.5.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]             blk.5.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]                blk.5.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 543]                blk.5.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]                  blk.5.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]               blk.5.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  57/ 543]                blk.5.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  58/ 543]                  blk.6.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]                  blk.6.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]                  blk.6.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]             blk.6.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]                blk.6.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 543]                blk.6.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]                  blk.6.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]               blk.6.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  66/ 543]                blk.6.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  67/ 543]                  blk.7.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]                  blk.7.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]                  blk.7.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]             blk.7.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]                blk.7.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 543]                blk.7.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]                  blk.7.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]               blk.7.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  75/ 543]                blk.7.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  76/ 543]                  blk.8.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]                  blk.8.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]                  blk.8.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]             blk.8.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]                blk.8.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 543]                blk.8.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]                  blk.8.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]               blk.8.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  84/ 543]                blk.8.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  85/ 543]                  blk.9.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]                  blk.9.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]                  blk.9.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]             blk.9.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]                blk.9.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 543]                blk.9.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]                  blk.9.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]               blk.9.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  93/ 543]                blk.9.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  94/ 543]                 blk.10.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]                 blk.10.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]                 blk.10.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]            blk.10.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]               blk.10.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 543]               blk.10.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]                 blk.10.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]              blk.10.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 102/ 543]               blk.10.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]                 blk.11.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]                 blk.11.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]                 blk.11.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]            blk.11.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]               blk.11.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 543]               blk.11.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]                 blk.11.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]              blk.11.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 111/ 543]               blk.11.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]                 blk.12.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]                 blk.12.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]                 blk.12.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]            blk.12.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]               blk.12.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 543]               blk.12.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]                 blk.12.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]              blk.12.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 120/ 543]               blk.12.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]                 blk.13.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]                 blk.13.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]                 blk.13.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]            blk.13.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]               blk.13.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 543]               blk.13.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]                 blk.13.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]              blk.13.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 129/ 543]               blk.13.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]                 blk.14.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]                 blk.14.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]                 blk.14.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]            blk.14.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]               blk.14.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 543]               blk.14.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 543]                 blk.14.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 543]              blk.14.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 138/ 543]               blk.14.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]                 blk.15.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]                 blk.15.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]                 blk.15.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]            blk.15.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]               blk.15.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 543]               blk.15.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]                 blk.15.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]              blk.15.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 147/ 543]               blk.15.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]                 blk.16.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]                 blk.16.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]                 blk.16.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]            blk.16.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]               blk.16.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 543]               blk.16.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]                 blk.16.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]              blk.16.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 156/ 543]               blk.16.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]                 blk.17.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]                 blk.17.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]                 blk.17.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]            blk.17.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]               blk.17.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 543]               blk.17.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]                 blk.17.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]              blk.17.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 165/ 543]               blk.17.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]                 blk.18.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]                 blk.18.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]                 blk.18.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]            blk.18.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]               blk.18.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 543]               blk.18.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]                 blk.18.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]              blk.18.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 174/ 543]               blk.18.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]                 blk.19.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]                 blk.19.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]                 blk.19.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]            blk.19.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]               blk.19.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 543]               blk.19.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 181/ 543]                 blk.19.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 543]              blk.19.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 183/ 543]               blk.19.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]                 blk.20.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]                 blk.20.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]                 blk.20.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]            blk.20.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]               blk.20.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 543]               blk.20.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 190/ 543]                 blk.20.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 543]              blk.20.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 192/ 543]               blk.20.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]                 blk.21.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]                 blk.21.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]                 blk.21.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]            blk.21.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]               blk.21.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 543]               blk.21.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]                 blk.21.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]              blk.21.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 201/ 543]               blk.21.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]                 blk.22.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]                 blk.22.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]                 blk.22.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]            blk.22.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]               blk.22.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 543]               blk.22.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]                 blk.22.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]              blk.22.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 210/ 543]               blk.22.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]                 blk.23.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]                 blk.23.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]                 blk.23.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]            blk.23.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]               blk.23.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 543]               blk.23.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]                 blk.23.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]              blk.23.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 219/ 543]               blk.23.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]                 blk.24.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]                 blk.24.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]                 blk.24.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]            blk.24.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]               blk.24.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 543]               blk.24.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]                 blk.24.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]              blk.24.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 228/ 543]               blk.24.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]                 blk.25.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]                 blk.25.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]                 blk.25.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]            blk.25.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]               blk.25.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 543]               blk.25.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]                 blk.25.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]              blk.25.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 237/ 543]               blk.25.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]                 blk.26.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]                 blk.26.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]                 blk.26.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]            blk.26.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]               blk.26.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 543]               blk.26.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]                 blk.26.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]              blk.26.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 246/ 543]               blk.26.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]                 blk.27.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]                 blk.27.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]                 blk.27.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]            blk.27.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]               blk.27.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 543]               blk.27.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]                 blk.27.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]              blk.27.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 255/ 543]               blk.27.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]                 blk.28.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]                 blk.28.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]                 blk.28.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]            blk.28.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]               blk.28.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 543]               blk.28.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]                 blk.28.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]              blk.28.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 264/ 543]               blk.28.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]                 blk.29.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]                 blk.29.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]                 blk.29.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]            blk.29.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]               blk.29.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 543]               blk.29.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]                 blk.29.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]              blk.29.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 273/ 543]               blk.29.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]                 blk.30.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]                 blk.30.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]                 blk.30.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]            blk.30.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]               blk.30.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 543]               blk.30.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]                 blk.30.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]              blk.30.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 282/ 543]               blk.30.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]                 blk.31.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]                 blk.31.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]                 blk.31.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]            blk.31.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]               blk.31.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 543]               blk.31.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]                 blk.31.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]              blk.31.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 291/ 543]               blk.31.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]                 blk.32.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]                 blk.32.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]                 blk.32.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]            blk.32.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]               blk.32.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 543]               blk.32.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]                 blk.32.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]              blk.32.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 300/ 543]               blk.32.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]                 blk.33.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]                 blk.33.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]                 blk.33.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]            blk.33.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]               blk.33.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 543]               blk.33.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]                 blk.33.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]              blk.33.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 309/ 543]               blk.33.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]                 blk.34.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]                 blk.34.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]                 blk.34.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]            blk.34.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]               blk.34.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 543]               blk.34.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]                 blk.34.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]              blk.34.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 318/ 543]               blk.34.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]                 blk.35.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]                 blk.35.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]                 blk.35.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]            blk.35.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]               blk.35.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 543]               blk.35.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]                 blk.35.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]              blk.35.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 327/ 543]               blk.35.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]                 blk.36.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]                 blk.36.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]                 blk.36.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]            blk.36.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]               blk.36.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 543]               blk.36.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]                 blk.36.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]              blk.36.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 336/ 543]               blk.36.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]                 blk.37.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]                 blk.37.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]                 blk.37.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]            blk.37.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]               blk.37.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 543]               blk.37.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]                 blk.37.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]              blk.37.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 345/ 543]               blk.37.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]                 blk.38.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]                 blk.38.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]                 blk.38.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]            blk.38.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]               blk.38.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 543]               blk.38.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]                 blk.38.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]              blk.38.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 354/ 543]               blk.38.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]                 blk.39.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]                 blk.39.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]                 blk.39.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]            blk.39.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]               blk.39.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 543]               blk.39.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]                 blk.39.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]              blk.39.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 363/ 543]               blk.39.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]                 blk.40.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]                 blk.40.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]                 blk.40.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]            blk.40.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]               blk.40.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 543]               blk.40.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]                 blk.40.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]              blk.40.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 372/ 543]               blk.40.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]                 blk.41.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]                 blk.41.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]                 blk.41.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]            blk.41.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]               blk.41.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 543]               blk.41.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]                 blk.41.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]              blk.41.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 381/ 543]               blk.41.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]                 blk.42.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]                 blk.42.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]                 blk.42.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]            blk.42.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]               blk.42.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 543]               blk.42.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]                 blk.42.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]              blk.42.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 390/ 543]               blk.42.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]                 blk.43.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]                 blk.43.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]                 blk.43.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]            blk.43.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]               blk.43.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 543]               blk.43.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]                 blk.43.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]              blk.43.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 399/ 543]               blk.43.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]                 blk.44.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]                 blk.44.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]                 blk.44.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]            blk.44.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]               blk.44.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 543]               blk.44.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]                 blk.44.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]              blk.44.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 408/ 543]               blk.44.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]                 blk.45.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]                 blk.45.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]                 blk.45.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]            blk.45.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]               blk.45.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 543]               blk.45.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]                 blk.45.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]              blk.45.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 417/ 543]               blk.45.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]                 blk.46.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]                 blk.46.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]                 blk.46.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]            blk.46.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]               blk.46.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 543]               blk.46.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]                 blk.46.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]              blk.46.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 426/ 543]               blk.46.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]                 blk.47.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]                 blk.47.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]                 blk.47.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]            blk.47.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]               blk.47.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 543]               blk.47.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]                 blk.47.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]              blk.47.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 435/ 543]               blk.47.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]                 blk.48.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]                 blk.48.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]                 blk.48.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]            blk.48.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]               blk.48.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 543]               blk.48.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]                 blk.48.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]              blk.48.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 444/ 543]               blk.48.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]                 blk.49.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]                 blk.49.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]                 blk.49.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]            blk.49.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]               blk.49.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 543]               blk.49.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]                 blk.49.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]              blk.49.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 453/ 543]               blk.49.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]                 blk.50.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]                 blk.50.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]                 blk.50.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]            blk.50.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]               blk.50.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 543]               blk.50.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]                 blk.50.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]              blk.50.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 462/ 543]               blk.50.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]                 blk.51.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]                 blk.51.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]                 blk.51.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]            blk.51.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]               blk.51.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 543]               blk.51.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]                 blk.51.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]              blk.51.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 471/ 543]               blk.51.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]                 blk.52.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]                 blk.52.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]                 blk.52.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]            blk.52.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]               blk.52.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 543]               blk.52.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]                 blk.52.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]              blk.52.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 480/ 543]               blk.52.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]                 blk.53.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]                 blk.53.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]                 blk.53.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]            blk.53.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]               blk.53.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 543]               blk.53.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]                 blk.53.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]              blk.53.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 489/ 543]               blk.53.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]                 blk.54.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]                 blk.54.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]                 blk.54.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]            blk.54.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]               blk.54.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 543]               blk.54.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]                 blk.54.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]              blk.54.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 498/ 543]               blk.54.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]                 blk.55.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]                 blk.55.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]                 blk.55.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]            blk.55.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]               blk.55.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 543]               blk.55.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]                 blk.55.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]              blk.55.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 507/ 543]               blk.55.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]                 blk.56.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]                 blk.56.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]                 blk.56.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]            blk.56.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]               blk.56.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 543]               blk.56.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 514/ 543]                 blk.56.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 543]              blk.56.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 516/ 543]               blk.56.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]                 blk.57.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]                 blk.57.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]                 blk.57.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]            blk.57.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]               blk.57.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 543]               blk.57.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 523/ 543]                 blk.57.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 543]              blk.57.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 525/ 543]               blk.57.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]                 blk.58.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]                 blk.58.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]                 blk.58.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]            blk.58.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]               blk.58.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 543]               blk.58.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 532/ 543]                 blk.58.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 543]              blk.58.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 534/ 543]               blk.58.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]                 blk.59.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]                 blk.59.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]                 blk.59.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]            blk.59.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]               blk.59.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 540/ 543]               blk.59.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 541/ 543]                 blk.59.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 542/ 543]              blk.59.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 543/ 543]               blk.59.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 85369.32 ms\n",
      "main:    total time = 85369.32 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/65B/ggml-model-f16.gguf' to './models/65B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llama_model_quantize_internal: meta size = 1740160 bytes\n",
      "[   1/ 723]                    token_embd.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   500.00 MiB ->   140.62 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                   output_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   500.00 MiB ->   205.08 MiB | hist: \n",
      "[   4/ 723]                  blk.0.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]                  blk.0.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]                  blk.0.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]             blk.0.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]                blk.0.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[   9/ 723]                blk.0.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 723]                  blk.0.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  11/ 723]               blk.0.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  12/ 723]                blk.0.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  13/ 723]                  blk.1.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]                  blk.1.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]                  blk.1.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]             blk.1.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]                blk.1.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 723]                blk.1.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]                  blk.1.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]               blk.1.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  21/ 723]                blk.1.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  22/ 723]                  blk.2.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]                  blk.2.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]                  blk.2.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]             blk.2.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]                blk.2.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 723]                blk.2.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]                  blk.2.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]               blk.2.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  30/ 723]                blk.2.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  31/ 723]                  blk.3.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]                  blk.3.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]                  blk.3.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]             blk.3.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]                blk.3.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 723]                blk.3.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]                  blk.3.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]               blk.3.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  39/ 723]                blk.3.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  40/ 723]                  blk.4.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]                  blk.4.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]                  blk.4.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]             blk.4.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]                blk.4.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 723]                blk.4.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]                  blk.4.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]               blk.4.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  48/ 723]                blk.4.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  49/ 723]                  blk.5.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]                  blk.5.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]                  blk.5.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]             blk.5.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]                blk.5.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 723]                blk.5.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]                  blk.5.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]               blk.5.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  57/ 723]                blk.5.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  58/ 723]                  blk.6.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]                  blk.6.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]                  blk.6.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]             blk.6.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]                blk.6.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 723]                blk.6.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]                  blk.6.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]               blk.6.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  66/ 723]                blk.6.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  67/ 723]                  blk.7.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]                  blk.7.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]                  blk.7.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]             blk.7.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]                blk.7.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 723]                blk.7.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]                  blk.7.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]               blk.7.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  75/ 723]                blk.7.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  76/ 723]                  blk.8.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]                  blk.8.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]                  blk.8.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]             blk.8.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]                blk.8.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 723]                blk.8.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]                  blk.8.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]               blk.8.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  84/ 723]                blk.8.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  85/ 723]                  blk.9.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]                  blk.9.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]                  blk.9.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]             blk.9.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]                blk.9.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 723]                blk.9.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 723]                  blk.9.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 723]               blk.9.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  93/ 723]                blk.9.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  94/ 723]                 blk.10.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]                 blk.10.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]                 blk.10.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]            blk.10.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]               blk.10.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 723]               blk.10.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 723]                 blk.10.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 723]              blk.10.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 102/ 723]               blk.10.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]                 blk.11.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]                 blk.11.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]                 blk.11.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]            blk.11.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]               blk.11.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 723]               blk.11.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]                 blk.11.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]              blk.11.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 111/ 723]               blk.11.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]                 blk.12.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]                 blk.12.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]                 blk.12.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]            blk.12.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]               blk.12.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 723]               blk.12.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]                 blk.12.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]              blk.12.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 120/ 723]               blk.12.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]                 blk.13.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]                 blk.13.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]                 blk.13.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]            blk.13.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]               blk.13.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 723]               blk.13.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]                 blk.13.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]              blk.13.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 129/ 723]               blk.13.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]                 blk.14.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]                 blk.14.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]                 blk.14.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]            blk.14.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]               blk.14.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 723]               blk.14.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]                 blk.14.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]              blk.14.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 138/ 723]               blk.14.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]                 blk.15.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]                 blk.15.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]                 blk.15.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]            blk.15.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]               blk.15.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 723]               blk.15.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 723]                 blk.15.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 723]              blk.15.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 147/ 723]               blk.15.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]                 blk.16.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]                 blk.16.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]                 blk.16.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]            blk.16.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]               blk.16.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 723]               blk.16.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]                 blk.16.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]              blk.16.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 156/ 723]               blk.16.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]                 blk.17.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]                 blk.17.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]                 blk.17.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]            blk.17.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]               blk.17.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 723]               blk.17.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]                 blk.17.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]              blk.17.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 165/ 723]               blk.17.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]                 blk.18.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]                 blk.18.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]                 blk.18.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]            blk.18.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]               blk.18.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 723]               blk.18.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]                 blk.18.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]              blk.18.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 174/ 723]               blk.18.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]                 blk.19.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]                 blk.19.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]                 blk.19.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]            blk.19.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]               blk.19.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 723]               blk.19.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]                 blk.19.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]              blk.19.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 183/ 723]               blk.19.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]                 blk.20.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]                 blk.20.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]                 blk.20.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]            blk.20.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]               blk.20.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 723]               blk.20.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]                 blk.20.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]              blk.20.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 192/ 723]               blk.20.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]                 blk.21.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]                 blk.21.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]                 blk.21.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]            blk.21.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]               blk.21.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 723]               blk.21.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]                 blk.21.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]              blk.21.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 201/ 723]               blk.21.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]                 blk.22.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]                 blk.22.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]                 blk.22.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]            blk.22.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]               blk.22.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 723]               blk.22.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]                 blk.22.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]              blk.22.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 210/ 723]               blk.22.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]                 blk.23.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]                 blk.23.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]                 blk.23.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]            blk.23.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]               blk.23.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 723]               blk.23.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]                 blk.23.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]              blk.23.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 219/ 723]               blk.23.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]                 blk.24.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]                 blk.24.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]                 blk.24.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]            blk.24.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]               blk.24.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 723]               blk.24.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]                 blk.24.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]              blk.24.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 228/ 723]               blk.24.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]                 blk.25.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]                 blk.25.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]                 blk.25.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]            blk.25.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]               blk.25.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 723]               blk.25.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]                 blk.25.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]              blk.25.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 237/ 723]               blk.25.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]                 blk.26.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]                 blk.26.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]                 blk.26.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]            blk.26.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]               blk.26.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 723]               blk.26.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]                 blk.26.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]              blk.26.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 246/ 723]               blk.26.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]                 blk.27.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]                 blk.27.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]                 blk.27.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]            blk.27.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]               blk.27.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 723]               blk.27.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]                 blk.27.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]              blk.27.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 255/ 723]               blk.27.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]                 blk.28.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]                 blk.28.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]                 blk.28.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]            blk.28.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]               blk.28.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 723]               blk.28.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]                 blk.28.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]              blk.28.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 264/ 723]               blk.28.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]                 blk.29.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]                 blk.29.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]                 blk.29.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]            blk.29.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]               blk.29.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 723]               blk.29.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]                 blk.29.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]              blk.29.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 273/ 723]               blk.29.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]                 blk.30.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]                 blk.30.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]                 blk.30.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]            blk.30.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]               blk.30.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 723]               blk.30.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]                 blk.30.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]              blk.30.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 282/ 723]               blk.30.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]                 blk.31.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]                 blk.31.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]                 blk.31.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]            blk.31.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]               blk.31.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 723]               blk.31.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]                 blk.31.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]              blk.31.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 291/ 723]               blk.31.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]                 blk.32.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]                 blk.32.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]                 blk.32.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]            blk.32.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]               blk.32.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 723]               blk.32.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]                 blk.32.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]              blk.32.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 300/ 723]               blk.32.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]                 blk.33.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]                 blk.33.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]                 blk.33.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]            blk.33.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]               blk.33.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 723]               blk.33.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]                 blk.33.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]              blk.33.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 309/ 723]               blk.33.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]                 blk.34.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]                 blk.34.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]                 blk.34.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]            blk.34.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]               blk.34.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 723]               blk.34.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]                 blk.34.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]              blk.34.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 318/ 723]               blk.34.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]                 blk.35.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]                 blk.35.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]                 blk.35.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]            blk.35.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]               blk.35.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 723]               blk.35.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]                 blk.35.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]              blk.35.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 327/ 723]               blk.35.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]                 blk.36.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]                 blk.36.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]                 blk.36.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]            blk.36.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]               blk.36.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 723]               blk.36.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]                 blk.36.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]              blk.36.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 336/ 723]               blk.36.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]                 blk.37.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]                 blk.37.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]                 blk.37.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]            blk.37.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]               blk.37.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 723]               blk.37.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]                 blk.37.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]              blk.37.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 345/ 723]               blk.37.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]                 blk.38.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]                 blk.38.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]                 blk.38.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]            blk.38.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]               blk.38.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 723]               blk.38.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]                 blk.38.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]              blk.38.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 354/ 723]               blk.38.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]                 blk.39.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]                 blk.39.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]                 blk.39.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]            blk.39.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]               blk.39.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 723]               blk.39.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]                 blk.39.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]              blk.39.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 363/ 723]               blk.39.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]                 blk.40.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]                 blk.40.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]                 blk.40.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]            blk.40.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]               blk.40.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 723]               blk.40.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]                 blk.40.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]              blk.40.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 372/ 723]               blk.40.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]                 blk.41.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]                 blk.41.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]                 blk.41.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]            blk.41.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]               blk.41.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 723]               blk.41.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]                 blk.41.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]              blk.41.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 381/ 723]               blk.41.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]                 blk.42.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]                 blk.42.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]                 blk.42.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]            blk.42.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]               blk.42.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 723]               blk.42.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]                 blk.42.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]              blk.42.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 390/ 723]               blk.42.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]                 blk.43.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]                 blk.43.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]                 blk.43.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]            blk.43.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]               blk.43.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 723]               blk.43.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]                 blk.43.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]              blk.43.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 399/ 723]               blk.43.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]                 blk.44.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]                 blk.44.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]                 blk.44.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]            blk.44.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]               blk.44.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 723]               blk.44.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]                 blk.44.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]              blk.44.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 408/ 723]               blk.44.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]                 blk.45.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]                 blk.45.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]                 blk.45.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]            blk.45.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]               blk.45.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 723]               blk.45.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]                 blk.45.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]              blk.45.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 417/ 723]               blk.45.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]                 blk.46.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]                 blk.46.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]                 blk.46.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]            blk.46.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]               blk.46.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 723]               blk.46.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]                 blk.46.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]              blk.46.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 426/ 723]               blk.46.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]                 blk.47.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]                 blk.47.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]                 blk.47.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]            blk.47.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]               blk.47.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 723]               blk.47.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]                 blk.47.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]              blk.47.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 435/ 723]               blk.47.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]                 blk.48.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]                 blk.48.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]                 blk.48.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]            blk.48.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]               blk.48.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 723]               blk.48.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]                 blk.48.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]              blk.48.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 444/ 723]               blk.48.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]                 blk.49.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]                 blk.49.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]                 blk.49.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]            blk.49.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]               blk.49.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 723]               blk.49.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]                 blk.49.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]              blk.49.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 453/ 723]               blk.49.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]                 blk.50.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]                 blk.50.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]                 blk.50.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]            blk.50.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]               blk.50.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 723]               blk.50.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]                 blk.50.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]              blk.50.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 462/ 723]               blk.50.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]                 blk.51.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]                 blk.51.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]                 blk.51.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]            blk.51.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]               blk.51.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 723]               blk.51.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]                 blk.51.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]              blk.51.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 471/ 723]               blk.51.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]                 blk.52.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]                 blk.52.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]                 blk.52.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]            blk.52.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]               blk.52.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 723]               blk.52.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]                 blk.52.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]              blk.52.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 480/ 723]               blk.52.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]                 blk.53.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]                 blk.53.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]                 blk.53.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]            blk.53.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]               blk.53.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 723]               blk.53.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]                 blk.53.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]              blk.53.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 489/ 723]               blk.53.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]                 blk.54.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]                 blk.54.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]                 blk.54.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]            blk.54.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]               blk.54.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 723]               blk.54.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]                 blk.54.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]              blk.54.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 498/ 723]               blk.54.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]                 blk.55.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]                 blk.55.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]                 blk.55.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]            blk.55.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]               blk.55.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 723]               blk.55.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]                 blk.55.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]              blk.55.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 507/ 723]               blk.55.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]                 blk.56.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]                 blk.56.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]                 blk.56.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]            blk.56.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]               blk.56.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 723]               blk.56.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]                 blk.56.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]              blk.56.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 516/ 723]               blk.56.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]                 blk.57.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]                 blk.57.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]                 blk.57.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]            blk.57.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]               blk.57.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 723]               blk.57.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]                 blk.57.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]              blk.57.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 525/ 723]               blk.57.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]                 blk.58.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]                 blk.58.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]                 blk.58.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]            blk.58.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]               blk.58.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 723]               blk.58.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]                 blk.58.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]              blk.58.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 534/ 723]               blk.58.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]                 blk.59.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]                 blk.59.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]                 blk.59.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]            blk.59.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]               blk.59.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 540/ 723]               blk.59.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]                 blk.59.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]              blk.59.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 543/ 723]               blk.59.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]                 blk.60.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]                 blk.60.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]                 blk.60.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]            blk.60.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]               blk.60.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 549/ 723]               blk.60.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]                 blk.60.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]              blk.60.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 552/ 723]               blk.60.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]                 blk.61.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]                 blk.61.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]                 blk.61.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]            blk.61.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]               blk.61.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 558/ 723]               blk.61.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]                 blk.61.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]              blk.61.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 561/ 723]               blk.61.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]                 blk.62.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]                 blk.62.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]                 blk.62.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]            blk.62.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]               blk.62.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 567/ 723]               blk.62.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]                 blk.62.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]              blk.62.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 570/ 723]               blk.62.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]                 blk.63.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]                 blk.63.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]                 blk.63.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]            blk.63.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]               blk.63.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 576/ 723]               blk.63.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]                 blk.63.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]              blk.63.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 579/ 723]               blk.63.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]                 blk.64.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]                 blk.64.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]                 blk.64.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]            blk.64.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]               blk.64.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 585/ 723]               blk.64.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]                 blk.64.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]              blk.64.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 588/ 723]               blk.64.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]                 blk.65.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]                 blk.65.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]                 blk.65.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]            blk.65.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]               blk.65.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 594/ 723]               blk.65.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]                 blk.65.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]              blk.65.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 597/ 723]               blk.65.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]                 blk.66.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]                 blk.66.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]                 blk.66.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]            blk.66.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]               blk.66.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 603/ 723]               blk.66.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]                 blk.66.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]              blk.66.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 606/ 723]               blk.66.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]                 blk.67.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]                 blk.67.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]                 blk.67.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]            blk.67.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]               blk.67.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 612/ 723]               blk.67.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]                 blk.67.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]              blk.67.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 615/ 723]               blk.67.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]                 blk.68.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]                 blk.68.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]                 blk.68.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]            blk.68.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]               blk.68.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 621/ 723]               blk.68.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]                 blk.68.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]              blk.68.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 624/ 723]               blk.68.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]                 blk.69.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]                 blk.69.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]                 blk.69.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]            blk.69.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]               blk.69.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 630/ 723]               blk.69.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]                 blk.69.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]              blk.69.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 633/ 723]               blk.69.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]                 blk.70.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]                 blk.70.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]                 blk.70.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]            blk.70.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]               blk.70.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 639/ 723]               blk.70.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]                 blk.70.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]              blk.70.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 642/ 723]               blk.70.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]                 blk.71.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]                 blk.71.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]                 blk.71.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]            blk.71.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]               blk.71.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 648/ 723]               blk.71.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]                 blk.71.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]              blk.71.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 651/ 723]               blk.71.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]                 blk.72.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]                 blk.72.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]                 blk.72.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]            blk.72.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]               blk.72.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 657/ 723]               blk.72.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]                 blk.72.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]              blk.72.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 660/ 723]               blk.72.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]                 blk.73.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]                 blk.73.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]                 blk.73.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]            blk.73.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]               blk.73.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 666/ 723]               blk.73.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]                 blk.73.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]              blk.73.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 669/ 723]               blk.73.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]                 blk.74.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]                 blk.74.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]                 blk.74.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]            blk.74.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]               blk.74.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 675/ 723]               blk.74.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]                 blk.74.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]              blk.74.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 678/ 723]               blk.74.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]                 blk.75.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]                 blk.75.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]                 blk.75.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]            blk.75.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]               blk.75.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 684/ 723]               blk.75.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]                 blk.75.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]              blk.75.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 687/ 723]               blk.75.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]                 blk.76.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]                 blk.76.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]                 blk.76.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]            blk.76.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]               blk.76.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 693/ 723]               blk.76.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 694/ 723]                 blk.76.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 695/ 723]              blk.76.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 696/ 723]               blk.76.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]                 blk.77.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]                 blk.77.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]                 blk.77.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]            blk.77.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]               blk.77.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 702/ 723]               blk.77.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 703/ 723]                 blk.77.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 704/ 723]              blk.77.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 705/ 723]               blk.77.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]                 blk.78.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]                 blk.78.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]                 blk.78.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]            blk.78.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]               blk.78.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 711/ 723]               blk.78.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 712/ 723]                 blk.78.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 713/ 723]              blk.78.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 714/ 723]               blk.78.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]                 blk.79.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]                 blk.79.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]                 blk.79.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]            blk.79.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]               blk.79.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 720/ 723]               blk.79.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 721/ 723]                 blk.79.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 722/ 723]              blk.79.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 723/ 723]               blk.79.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 78846.09 ms\n",
      "main:    total time = 78846.09 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.gguf ./models/13B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.gguf ./models/30B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.gguf ./models/65B/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS:  \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
      "removed 'build-info.o'\n",
      "removed 'common.o'\n",
      "removed 'console.o'\n",
      "removed 'ggml-alloc.o'\n",
      "removed 'ggml-backend.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml-quants.o'\n",
      "removed 'ggml.o'\n",
      "removed 'grammar-parser.o'\n",
      "removed 'llama.o'\n",
      "removed 'sampling.o'\n",
      "removed 'train.o'\n",
      "removed 'tests/test-c.o'\n",
      "removed 'benchmark-matmult'\n",
      "removed 'common/build-info.cpp'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'vdot'\n",
      "removed 'q8dot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'convert-llama2c-to-ggml'\n",
      "removed 'simple'\n",
      "removed 'batched'\n",
      "removed 'batched-bench'\n",
      "removed 'save-load-state'\n",
      "removed 'server'\n",
      "removed 'gguf'\n",
      "removed 'llama-bench'\n",
      "removed 'libllava.a'\n",
      "removed 'llava-cli'\n",
      "removed 'baby-llama'\n",
      "removed 'beam-search'\n",
      "removed 'speculative'\n",
      "removed 'infill'\n",
      "removed 'tokenize'\n",
      "removed 'parallel'\n",
      "removed 'finetune'\n",
      "removed 'export-lora'\n",
      "removed 'lookahead'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
      "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
      "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib   -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib  -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88ef08-68f5-4655-ac49-d0368c11b004",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33724f57-0378-4652-86b1-2b4858d56528",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db5a686-a0d6-4af2-9753-fd4e6b42a706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703191766\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.98 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   70.42 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 3577.55 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.06 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, having begun to make both at about the same time, and to spend it as fast, is by this process inevitably sliding down her inclined plane; and speedily must either stop or tumble headlong over. But we will not anticipate. It may be doubtful (in the popular sense) how far private enterprise (and that alone) has a right to engage in making paper money: but that England can get rid of all she has made, is quite certain; and at this moment it were hard to say which was more embarrassing, to have too little, or too much of the sort. \n",
      " Chapter the First—Mysterious Ways \n",
      " ' _O fredd, o fredd!_ \" exclaimed Mr. George Frederick Stooker (for so he was called), \"you really make me tremble with cold and horror.\" In this exclamation (and it was not addressed to Mr. Bumble) Mr. Bumble would have read much more than met his eye; for the fact is, that there _is_ no Mr. George Frederick Stooker: that being a fictitious name invented by Mr. Bumble in his own mind as soon as he heard it pronounced by the magistrate's clerk: which, in default of anything more probable, had been communicated to him with reference to this matter; and on which, when applied to any particular person or persons, a new impression was made upon Mr. Bumble's mind, according to circumstances. On one occasion it would have meant an old woman who had stolen her nephew's plate: another time it might have signified a man who kept a public house for the sale of beer; on another it was applied to some person or persons supposed to have taken part in a robbery with a view to murder. It is astonishing how far Mr. Bumble would carry his thoughts, even when he had no other information than that which Mr. George Frederick Stooker (for so he was called) communicated by expressing the words which we have rendered into English. ' _O fredd! o fredd!_ \" cried Mr. Bumble, as if the man were before his eyes; and so it might be in fact to one who had never heard the name but once before. \"He has escaped with my girl.\" \n",
      " Chapter the Second—Mr. Weller Takes a Walk on Business \n",
      " Mr. Joseph Willet was at that time seated in his usual place in front of the White Hart, and smoking a long meerschaum pipe, in which there were more than three quarters of an ounce of tobacco. The heat of the weather had induced him to relinquish his knife-case for this more portable and convenient implement; but he took care not to be without the knife: a circumstance which is a sufficient demonstration that Joseph Willet, although possessed of some good qualities (which we shall have occasion to mention hereafter), was still by no means an enlightened man. \n",
      " Mr. Weller had taken his station on a low seat in front of the inn door—a post of great importance, which is generally filled up with gentlemen of various degrees and descriptions of intelligence, from the mere idler down to the dullest traveller on business; from the most learned man who has ever set eyes upon a newspaper through pure curiosity, to that very simple personage the publican. But Mr. Weller was a very good sort of man, possessing several excellent qualities: in particular he had an insatiable appetite for reading—the quality which usually distinguishes the man who can write, from the one who cannot; and this passion being indulged only by way of variety to his smoking propensity, (of which the reader will hereafter have an opportunity of seeing some proof,) made him extremely popular with all who frequented the inn, where he was looked upon as a sort of resident instructor in every department of human knowledge. \n",
      " Mr. Weller's eyes were bent on his pipe; but being at once observant and sociable, they took in much more than he appeared to see:—he heard a great deal that he did not hear, and saw the face of one who was speaking, although the speaker kept his head down on his breast. \n",
      " The face thus viewed by Mr. Weller's eyes was one which had often before been surveyed in this way, and will doubtless be again; for the face was a young gentleman's—a youth with an open countenance, expressive of frankness and goodwill; a well-proportioned body, and a head finely shaped, though unadorned by either locks or curls. \n",
      " Now this handsome personage,\n",
      "llama_print_timings:        load time =    3563.62 ms\n",
      "llama_print_timings:      sample time =     162.15 ms /  1024 runs   (    0.16 ms per token,  6315.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1114.48 ms /   494 tokens (    2.26 ms per token,   443.26 tokens per second)\n",
      "llama_print_timings:        eval time =   28539.48 ms /  1023 runs   (   27.90 ms per token,    35.85 tokens per second)\n",
      "llama_print_timings:       total time =   30066.36 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9324415-e69d-4ed2-8b56-2094995fbe8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703191835\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.98 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   70.42 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 3577.55 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.06 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England had begun to struggle, by hesitating and by retrograding, towards the cultivation of the dairy, that she might have butter for breakfast. \n",
      " Mr. Wringhim, who had been indefatigable at the pump during the last session of Parliament, now found himself without a patron; or at least one not very willing to employ him; so he made up his mind to return to the Highlands, and continue to work in that quarter. He set out for Aberdeen on foot; which was the more ungrateful, because the Earl, though unwilling to renew his civilities towards him, had furnished him with an excellent horse on which to ride home; but Wringhim had a strong objection against being beholden to his family. \n",
      " Now, if ever, this was a proper occasion for the narrator of Mr. Wringhim's story to apologise for himself; but, as he has hitherto said little about himself in this part of his work, and now very little indeed, we hope our readers will not find him ungrateful; besides which, if they do, they have a great deal more to make up their minds to than he has. \n",
      " When Mr. Wringhim left London on foot, with the intention of returning to his family in the Highlands, the summer was beginning to draw towards its close. It had not been very sultry; but the sunshine, when it came upon him in the early part of each day, seemed almost too hot for his complexion. He made several stops on his way northward; and one afternoon, as he sat by himself in a solitary inn, where there was only a small party at dinner (for the place was out of fashion with tourists), the weather being somewhat overcast, it may have been about three o'clock, the sun began to be so very hot that it gave him something to think of. He had already got his little bundle of manuscripts and papers together; he now thought, on consulting his watch, that there was time enough for him to write a postscript or two to some letters which were lying at an inn in Aberdeen, and also time enough to make up another little parcel, containing a few things to send off by the carrier from Aberdeen to Dr. Quixwood's. \n",
      " Mr. Wringhim sat down and wrote these letters; but after writing them he found himself still disinclined to go on with his journey, and instead of despatching this second bundle he put it in his pocket, where he took it out again as the evening fell upon him. He now began to write a long postscript, which might possibly be sufficient by itself for one of those letters; but when he had written half through it, and was still sitting there at the table in that inn, thinking about what else he could say on this postscript, another thought struck him; so he put down his quill, took up a paper-knife which lay upon the table before him, cut off a bit of the letter from one corner to the other, and wrote two or three words upon it. He then folded it up into a small packet, tied it with a knot in a string he happened to have by him, sealed it over, and put it into his pocket-book along with those other things; and when night came on he went to bed. \n",
      " Chapter VI - A Letter from the Past\n",
      "When Mr. Wringhim woke next morning, he was rather surprised to see a packet of paper tied up in the same way as he had seen some packets of manuscript lying about at Dr. Quixwood's; and when he took it out, it proved to be not only that which he had put there on purpose, but another, written from another hand.\n",
      "This letter was written in a different character of handwriting from that of the other two letters (the first one which Mr. Wringhim sent, being, as before said, written by himself), and this time it ran thus:– \n",
      "My dear Mr. Wringhim,–Your long absence has been a heavy affliction to me; but I have endeavoured not to let the fear of your anger prevent my writing to you again and again. However, at length it was impossible for me any longer to be silent, when I thought how your mind would be so grievously vexed on account of that matter in which I wrote to you before; and besides that, I felt a great anxiety about yourself. So, my dearest master, I send this paper again by the hands of Mr. Slattery. It is not very long, but it will give you an opportunity of making amends for all the past faults of your life; so I beg and beseech you to write to me without any\n",
      "llama_print_timings:        load time =    3866.04 ms\n",
      "llama_print_timings:      sample time =     156.71 ms /  1024 runs   (    0.15 ms per token,  6534.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1085.31 ms /   494 tokens (    2.20 ms per token,   455.17 tokens per second)\n",
      "llama_print_timings:        eval time =   27522.26 ms /  1023 runs   (   26.90 ms per token,    37.17 tokens per second)\n",
      "llama_print_timings:       total time =   29005.21 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0cf40d6-f822-4ba8-a315-c3819dd2af9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703191879\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.98 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   70.42 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 3577.55 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.06 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was ardent in her enthusiasms: her king was young; her queen was handsome; her Prince of Wales was a man about town and a hearty sportsman: her Earl Marshal had been raised from the ranks; she was proud of her peerage, her nobility, and her gentry; fond of her commonalty, and still more fond of her pauperism: her people were a great nation: her religion (of whatever sect) was a unanimous one: she had no want of public or private spirit: she was full of faith and fervour: her literature was in vigorous exercise; her press teemed with political writings; the stage presented masterpieces; the opera, ballet, and concert room afforded constant sources of gratification to the people: her national vanity was flattered by the attention paid to her by other nations; she had been long the arbiter of Europe’s destinies: her commerce and manufactures were extensive and in high reputation. Above all, she had not a king but George the Third! \n",
      " France was no sluggard on the hill either: the revolution which had broken out in her sister had made the government nervous: they were in perpetual apprehension of some similar commotion in their own dominions; and, indeed, the very first effort at reform produced a dangerous insurrection, headed by one of their best generals. The Jacobins were formidable menaces: all classes felt the necessity of mutual support against them: there was but one remedy for this terrible malady – an infusion of blood from time to time. France had plenty of such antivenin on hand in the person of Robespierre and his crew, who were not a little flattered by their popularity with the Jacobins. \n",
      " The French Revolution was at that period in its meridian splendor: all eyes were turned towards Paris; every one listened with anxiety to its latest news: the French were looked on as the greatest people upon earth: the great and good of other nations flocked into Paris for the sake of seeing, hearing, and becoming acquainted. The English in particular could not resist this temptation; and London was full of persons whose conversation had a flavour of Paris about it. \n",
      " When the King of England made his famous speech in Parliament on the first day of George III’s reign, (25 October 1760), there was a general feeling of satisfaction with the government throughout the nation: he had always been considered as the best Englishman amongst kings; and now they were sure he would be the most useful. They also felt happy that they could congratulate him on an event which gave so much delight to all Europe, the accession of a French king – the Duke de Choiseul, who, by his wit, energy, and zeal for the glory and interests of France, was looked upon as the personification of the kingdom. All Europe had great expectations from the new French government: they were about to give laws which would be admired by all other nations – to reform the administration – to relieve their own debased finances; in fact, to make themselves masters of Europe and the world! They thought they should see Choiseul at least as great a man as Louis XIV. – his ancestor: but, instead of this, he became only the most insignificant and contemptible creature on earth – the most despised minister that ever disgraced the annals of France. \n",
      " It was not to be expected that George III’s first speech to Parliament should have had a more favorable reception; but they had always been treated with great leniency: his majesty had never made any remarks upon their conduct – they had always thought there would be no necessity for him to do it, until now! When the king came to give his opinion of Mr. Pelham’s plan of reforming the administration and the revenue of the kingdom, the minister could not but feel that the whole system of administration was now at stake, and that, unless he carried his measures into effect with the utmost decision and firmness, his own government would be lost: in short, his fate hung upon it. He made no hesitation, therefore, to tell His Majesty that a great revolution must take place before he could consent to give up the principle of ministerial responsibility; and if Mr. Pelham were to make use of those powers with which his majesty had entrusted him, in order to reform abuses – the measure was an indispensable one. But the king saw at once that such a step must be attended with consequences so fatal as to render it quite impossible to proceed on it: he therefore stopped short in his speech by saying that there were some objections which it would not be proper for him, personally, to state;\n",
      "llama_print_timings:        load time =    3600.85 ms\n",
      "llama_print_timings:      sample time =     157.64 ms /  1024 runs   (    0.15 ms per token,  6495.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1084.34 ms /   494 tokens (    2.20 ms per token,   455.58 tokens per second)\n",
      "llama_print_timings:        eval time =   27640.58 ms /  1023 runs   (   27.02 ms per token,    37.01 tokens per second)\n",
      "llama_print_timings:       total time =   29124.30 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a124de-0e22-49fc-8ea3-d324d831a961",
   "metadata": {},
   "source": [
    "### 7B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518024e7-d24d-4701-a788-6445d04b3ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703191923\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.13 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  250.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 12603.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, from the same cause and by natural impulse, had got headache, and taken arnica, as becomes a nation which has some wits among its members. France was, nevertheless, at this very time busily employed in compounding a new set of troubles for her own behoof, and, incidentally, for the behoof of Europe generally. \n",
      " On a certain evening of autumn, a French gentleman who had been long resident in England, and who wrote himself Monsieur de Villefort, took up his residence once more at a hotel in Paris, from which he proceeded to an hotel near the barracks in the Champs Elysees; while another French gentleman, who called himself Count Antonicq, though his friends knew him as General Quosquevit (who had been in London about seven months back), was making preparations for a journey into Switzerland. \n",
      " The two travellers, from what we know of the world and its ways, might easily have met if they had cared to do so; but there is no record that they did. They may have recognised one another without either knowing or caring who the other was: yet in such a case we are apt to doubt whether any mutual recognition took place. In the case of the Frenchmen, who were both young men with good heads upon their shoulders and strong legs beneath them, the chances were that they met somewhere on the road between Dover and Calais; or again, it might have been at Boulogne, where there is always a great deal of business to be done: perhaps in Switzerland itself, when the Count came down into Italy; but somehow the two travellers, whose names we have mentioned, did not meet.\n",
      "At about one o'clock in the morning on this particular night Monsieur de Villefort returned from his journey in company with the lady whom he loved so deeply as to wish himself dead rather than separated from her for a moment: but when they reached their hotel there was no room to be had at all that day; and therefore it was decided that they should drive round to a certain hotel in the Champs Elysees. As the young man who called himself Count Antonicq also returned on this night, he went there direct from his hotel, while Monsieur de Villefort and Madame de Saint Meran (for that was the name of his sweetheart) made their way through the streets to their own lodgings; where a large room had been prepared for them with four beds in it. \n",
      " At about three o'clock on this night we find Count Antonicq coming along the Champs Elysees as fast as ever he could drive, in his travelling carriage; but, of course, he was much more careful and steady than Monsieur de Villefort, who, having nothing to prevent him from making any pace he chose, kept going faster and faster.\n",
      "Madame de Saint Meran was in one of the rooms at this hotel where there were four beds, but where there were two other persons besides herself and the Count; so that the servants had all they could do to manage matters properly. \n",
      " The room was very pleasant: for though it looked out upon the street, the windows had shutters which could be closed up whenever occasion required, and the rooms were well furnished and scrupulously cleaned; and in every way suitable for such a journey as that of Madame de Saint Meran to Switzerland.\n",
      "Madame de Saint Meran was young, pretty, very clever and very rich: indeed she had two or three millions which she could not spend in her lifetime without getting into the greatest trouble about it, if we may believe what everybody says; and it is very easy to see how Monsieur de Villefort came to love her so much.\n",
      "Monsieur de Saint Meran, who was a marquis, having spent all his fortune before he had any children, Madame de Saint Meran lived in a very retired manner at Geneva with the young count who was scarcely more than three years old and not much of a child yet; but, like everybody else, she thought it would be far pleasanter to bring her little boy out into society again. \n",
      " 'In spite of the difference in rank,' she said, 'my husband has always behaved very handsomely towards me: indeed I am almost certain that he was only too glad to get rid of a child who would never have brought him in any money; but I am sure, if he had not been so good-natured, the Countess de Montreuil and the Baroness d'A—— would have done it for me: indeed I rather think they did. But then they might be a little jealous of my coming to Paris again with their own sons: besides that I think the baroness will not be able to bear up against the shock of seeing us in town without having the whole story of our marriage told over and over\n",
      "llama_print_timings:        load time =   28929.45 ms\n",
      "llama_print_timings:      sample time =     211.27 ms /  1024 runs   (    0.21 ms per token,  4846.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1065.99 ms /   494 tokens (    2.16 ms per token,   463.42 tokens per second)\n",
      "llama_print_timings:        eval time =   33573.95 ms /  1023 runs   (   32.82 ms per token,    30.47 tokens per second)\n",
      "llama_print_timings:       total time =   35166.11 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4b98bb-a046-4be5-9f98-cc5f64724a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703191998\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.13 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  250.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 12603.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, who had taken a more direct course, had suffered much in her roads, and was now, by great efforts and valuable assistances from her kinswoman, emerging into comparative prosperity. The French Government was the most corrupt on record; that of Great Britain one of the cleanest. France was the terror of Europe; England was a rallying-place for all mankind who wished to be free. \n",
      " In this year it so happened (which is more than can be said of many years) that the two countries, divided as they were in other respects, should resemble each other very closely in one particular – which was, that the same political party prevailed at both Courts. The English ministers were all Whigs, the French all Jacobins; for though the French called themselves Royalists, yet, having established an abstract idea of monarchy to oppose the King himself, who was as real a monarch as any man ever had been, they were really Republicans. \n",
      " Thus then, in this year 1793, the two countries were at peace; or at least their armies lay asunder, while each side armed its people for self-defence against the other – not an unusual state of things, by any means. England had done her utmost to stop the French from carrying out a design which they have since made good, in spite of all opposition – I mean that of making themselves the terror of Europe and of setting up an army of emigrants in every country that hates them as much as we do. \n",
      " The English Government was far too clever to give France any pretext for declaring war; or if they did make one, it was so very trifling a pretence, that even the Jacobins would not have been justified by it. In short, they found themselves obliged to take us at our word: and having sent over six thousand men under the command of General Sir Ralph Abercromby, to secure ourselves against any hostile attempts on our part, we took care not to make such an attempt; but left them quietly in England. \n",
      " The French have not been satisfied with this, and have ever since been plotting what they call the 'Second Revolution,' which was only a change of ministry; for after all they did very little more than turn out the Girondists and put the Jacobins into their places – two things which had often before happened to them, and nothing more. This being their object in coming to war, they declared against us without hesitation, though it was an affair of so great difficulty that no man expected that we could ever carry our point; and yet we did – as I hope to be able hereafter to convince the reader – but that is not my business at present. \n",
      " In the midst of this war, a change in the ministry came on: one of their old men was turned out by the French, another of whom was chosen instead; and the two, who were sent over to France after the peace had been made between us, being ordered to receive our envoys, did it with all possible kindness. The same thing happened in France: they received us without any coldness or disrespect – as indeed nothing can be more respectable than either of their courts; and even before we came away, one of the new ministers offered his congratulations to me on my return home.\n",
      "The Peace was made about a month after our arrival in Paris: we had time to finish all necessary business, but not quite so long as might have been wished. As soon as this important affair was settled between us and the French, I went up into Brittany again, and thence home with my family; having staid only till the peace was signed.\n",
      "The war, which lasted ten years (and at a great expense too), ended in this manner – that the two kings came to an agreement about their possessions by treaty: so that what England had been forced into by the wars before Charles's reign, they voluntarily abandoned by this peace; and France gave up everything which she had acquired after his accession. \n",
      "10  \n",
      " A PRECIOUS DIALOGUE OF DR MERIT AND MR SWINBURNE\n",
      "_S_.–I don't know that I have seen you for a good while now – how is the world?\n",
      "_M_.–Those that live in it are so, as usual; and I must confess to you, that since my last journey into France, I do not feel myself half so well pleased with the life we lead here.\n",
      "_S_.–No wonder after a ten years' war – I have been sorry for our old acquaintance Mr Hume ever since he has been out of his grave; and I must own it is a great pity that you did not keep him a little longer,\n",
      "llama_print_timings:        load time =   12660.36 ms\n",
      "llama_print_timings:      sample time =     155.49 ms /  1024 runs   (    0.15 ms per token,  6585.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1107.03 ms /   494 tokens (    2.24 ms per token,   446.24 tokens per second)\n",
      "llama_print_timings:        eval time =   27186.47 ms /  1023 runs   (   26.58 ms per token,    37.63 tokens per second)\n",
      "llama_print_timings:       total time =   28700.14 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70c07219-6f1a-4e38-9590-bb0fe7cea15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192045\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.13 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  250.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 12603.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England had been doing the same for some years before; but, from a variety of circumstances, had at length been brought to a stand: where France now beheld her, and where that countrys neighbouring people beheld each other, both flourishing as to persons, property, and prospects – upon paper. \n",
      " The reader will please not to misunderstand the above paragraph; for I am by no means inquisitive enough to pry into the private affairs of any gentleman. My information concerning his wealth was received from himself: not through any betrayal of secrets between us. In short, my statement is this – that a very large proportion (but how large I cannot say) of all the property in France and England consisted of paper money. It had never been legally established whether a sum stated on paper could be demanded in coin: consequently, no gentleman of any standing, or any otherwise distinguished person, would receive such a statement without strong suspicions that it was intended as a joke; though these suspicions were not generally put into words. \n",
      " The French Government, having found it convenient to make more paper money than they had copper and silver to back it with, discovered in their progress that the public distrusted it: consequently, that no man of consequence would receive it without a written promise – by which he meant a signature or signatures on a piece of paper – for an equal amount of the precious metals. As this did not increase their confidence, they went still further and issued a proclamation forbidding any person to refuse paper money on any account whatever; and this without the least reserve, and in as peremptory terms as if it were an edict from Popes or Emperors. \n",
      " The people very readily understood that such a proclamation was intended to frighten them into a compliance with its demands; but they had not come thus far by a rapid progress towards civilisation without learning wisdom of some kind, and this particular sort of wisdom they were resolved to have. So they went on quietly smiling at the Government, and refusing paper money upon all occasions. \n",
      " The consequence was, that in spite of all the proclamations, no man of consideration would receive it; and when a demand for payment of taxes became pressing, a considerable number of shopkeepers would not suffer themselves to be bullied into paying more than one-fourth of their taxes, if then. They could not but observe, too, that when they had paid in the smallest part, a large proportion of their debt was cancelled forthwith by an edict from Paris; so it began to dawn upon them that a sort of barter might be made between paper and metals, which might possibly prove very advantageous. \n",
      " The next thing was to find out what was the best medium for this purpose. Money is certainly very well as far as it goes – I mean coin. But people want something more than coin, if they can get anything else; that is to say, a certain number of pence worth more in money than other things are worth in coin: and they do not want them merely to have the means of paying taxes; but for purposes of ordinary trade – such as making a living – it is much better to be able to exchange an ounce or two of silver, which will purchase what you want, than an immense heap of paper. So this was another thing the people began to think of, and they could not help thinking that there were two ways in which this object might possibly be accomplished – the way of issuing paper money as well as gold; or the way of finding a metal more convenient for money than silver. \n",
      " They did not go very far into either of these theories yet. They had neither the time nor the leisure to do so; they were too busy with their old trade, which is no bad one – that of making all the wealth and power of France by fighting and looting foreign nations. But the next thing to come on was to look at this question from a new point of view: which was to ask themselves, whether it was not possible to have both gold and paper money without having more silver in circulation than at present; so that there might be plenty of silver for use when they wanted it; but that the value of every piece of coin should be about as high as now, while all the rest – namely, paper notes and bank-bills – should be worth nothing but what was due on them, unless backed up by gold. \n",
      " It will be seen at once that this would do away with any difficulty that might arise from there being too much silver in circulation; for no one would put up a single ounce of his silver coin as security for notes or bank-bills which he thought were not likely to be honoured if presented. The government, however, can always force men to pay up\n",
      "llama_print_timings:        load time =   12522.35 ms\n",
      "llama_print_timings:      sample time =     157.55 ms /  1024 runs   (    0.15 ms per token,  6499.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1145.00 ms /   494 tokens (    2.32 ms per token,   431.44 tokens per second)\n",
      "llama_print_timings:        eval time =   27098.68 ms /  1023 runs   (   26.49 ms per token,    37.75 tokens per second)\n",
      "llama_print_timings:       total time =   28642.69 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b6f4e-6235-47a1-8942-f8d04e6f1b29",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8088f7bc-2a09-43d7-8940-5721b1af6bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192103\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   88.03 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 6936.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is probable that, even with this terrible example constantly before their eyes, they would have soon forgotten his lesson, if they could have forgotten his name; but he was one of the unfortunate few who have been born to trouble as the sparks fly upward. They were all stocked and locked up in the jail, until such time as it should please their good Christian countrymen to remember the maxim that those who live by the sword shall perish by the sword, when after long imprisonment they were disposed of in short shrift. \n",
      "Here end the memoirs of Sidney Carton. He is not the hero of this book, nor any part of it; but he is one among many of whom it may be said that he was born to trouble as the sparks fly upward, and that he died with the light burning on his face in a distant city, where he had never been himself, for which he had always cared nothing, and whose name, until within a very few hours before he passed away, he could not even have properly pronounced. \n",
      "Book Three\n",
      "Chapter Eighteen\n",
      "In the morning, the whole court was still and dull like Sunday; there were no crowds and little business. The sun shone through the rain of yesterday, upon the stone pavement, which seemed to be strewn with dirty water, and tainted with a stale and smoky flavour of candles and oil-lamps, as if it had been rained on while night lasted.\n",
      "A woman, wrapped in a large shawl which half enveloped her, was sitting on the lowest step at the back entrance of the building, nursing an infant with whom she appeared to have suffered a long and obstinate illness: she could hardly have been prettier than she was then. There was no expression of dejection or fatigue upon her face; it had an air of patient endurance that it would not lose for all the world, which it never lost in its happiest hours, because it would have lost a dearer treasure—a quiet smile. She sat with her back to the wall and her face towards the damp stone steps on which she leant; the child lay upon her lap with his face turned towards her own. He was not crying or calling to her, as children will who are unwell and in pain, for he was better off than that, but only looking up into her face and smiling as if there were some joke between them. The woman saw nothing of the rain-washed flagstones where she sat, nor of the frowsy stains upon the old grey walls; the child on her lap seemed to make all those places and people fairer than they are in their sunniest hours, so that it was a very quiet street indeed.\n",
      "It happened about eleven o'clock that an officer of police passed by and saw them sitting there with such calm content in the midst of so much hardship; he took them to be husband and wife—as, for aught I know, they may have been—and, being one of a class which makes a point of showing its humanity as often as it can find opportunity to do so, he stopped and put this question: 'Pray what is the matter with you both?'\n",
      "The woman's face lighted up at the sound of his voice. She did not reply directly, but said to the child that he ought to know better than ask a question like that. It was such a good joke that the two of them laughed heartily together; and as they were still laughing when some one else stopped to inquire what was wrong with them, she found it necessary to explain matters for the stranger's sake: 'I am ill,' said the woman, pointing to her child—who was at the same time indicating his mother with a knowing look. 'He is hungry.'\n",
      "The officer of police listened with a face as full of compassion as if he really were human; he turned about and went back the way he had come, stopping at the end of every alley to ask for something which he might give them in charity—a penny worth of bread or milk. When he reached home again (and it was his wife who let him in), his first words were: 'What have I been doing all my life? What do other people live for if not to help those in need?'\n",
      "'\n",
      "llama_print_timings:        load time =    7893.32 ms\n",
      "llama_print_timings:      sample time =     173.96 ms /  1024 runs   (    0.17 ms per token,  5886.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1708.85 ms /   494 tokens (    3.46 ms per token,   289.08 tokens per second)\n",
      "llama_print_timings:        eval time =   33956.92 ms /  1023 runs   (   33.19 ms per token,    30.13 tokens per second)\n",
      "llama_print_timings:       total time =   36079.53 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2c6185c-75d7-4a75-914e-e198684b3c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192151\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   88.03 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 6936.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never killed any one; burning a youth, who had never plundered or murdered anybody; and flogging a woman to death, who had never stolen above a couple of handkerchiefs, and one or two bottles of rum. \n",
      "# – Chapter Four –\n",
      "The time is come the wanderer home   \n",
      "To the couch where first he lay,   \n",
      "And the dark ways he has gone   \n",
      "Lead back to the light of day:\n",
      "So shall the soul full satiate find   \n",
      "With her God, and in his mind;   \n",
      "The weary wings will fold, nor pine   \n",
      "To win the unattainable shine.\n",
      "– _Anonymous_.\n",
      "I.  \n",
      "Little Dorrit's Arrival\n",
      "THE place was London, and the time the first year of His present Majesty King George IV., then lately come to the throne. It is not necessary for understanding the story to know all about these things; so, from the moment when a gentleman in black clothes, with a white wig, and an umbrella, came into a certain court in Clifford's Inn, through a heavy rain-shower in the last week of August, 1786, and said to Mr. Merdle, the junior partner, 'What's this bill?' down to the time when the same gentleman in black clothes, with a white wig, and an umbrella, came into the same court in Clifford's Inn through another heavy rain-shower, in the first week of September, 1786, and said to Mr. Merdle, the senior partner, 'What's this bill?' there ensued great perplexity, and an immediate application was made by the junior partner on behalf of himself and his copartner, praying to be relieved from the payment of debts amounting in the whole to eight thousand six hundred and seventy pounds, six shillings, and nine-pence halfpenny, due to sundry persons at different times; which debts were supposed by all parties interested (who were a good many) to have been discharged by the decease of one Mr. Dorrit, formerly of Portsmouth, and afterwards of Chatham: but there were doubts about this; also there was an opposing creditor, who appeared to have some right to dispute the claims set up against the estate of that gentleman on behalf of himself and others. This creditor was a gentleman in black clothes, with a white wig, and no umbrella, who had been sent down expressly by certain other persons interested in this question: who were numerous also.\n",
      "In the meanwhile, there was a great deal of bustle about nothing; for the bill, which had first occasioned all the disturbance, was only an imaginary one. The cause, and indeed the whole business, never turned upon its being genuine or not, but upon the doubtful question whether it really was Mr. Dorrit who had died in the Marshalsea Prison. If so-and it was very much if-he had left no effects behind him: he had owed a great deal of money; but that was nothing to do with the business. The business was, Whether John Chivery, who had been bound with this fictitious Dorrit, in the year seventeen hundred and eighty-six (to save him from being committed to prison) by his own recognizance for five thousand pounds, and a few others for smaller sums, were obliged to pay their sureties; or whether Chivery was at liberty now to recover those several recognizances in full.\n",
      "In the Marshalsea Debtors' Prison, one November evening of the year seventeen hundred and ninety-one, John Chivery, standing at a barred window, looked down into the prison-yard below, where a man was pacing up and down with his arms folded on his breast. The man had been walking there since three in the afternoon; and as he walked he whistled to himself: a soft air of long ago which John Chivery knew, for it had often sounded at the old window when he sat opposite to it, watching and waiting for a glimpse of his mother.\n",
      "John Chivery was dressed in the clothes that had belonged to his father; for his own, which were ragged and worn, he never put on nowadays, but kept them for best occasions: he looked very little like the John Chivery of old time who used to pass his evenings in the workshop of a watch-maker. But as he\n",
      "llama_print_timings:        load time =    7217.41 ms\n",
      "llama_print_timings:      sample time =     155.42 ms /  1024 runs   (    0.15 ms per token,  6588.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1729.39 ms /   494 tokens (    3.50 ms per token,   285.65 tokens per second)\n",
      "llama_print_timings:        eval time =   33576.15 ms /  1023 runs   (   32.82 ms per token,    30.47 tokens per second)\n",
      "llama_print_timings:       total time =   35709.98 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0b39cd7-6046-4f03-ab13-50efe29377c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192198\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   88.03 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 6936.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands amputated, his nose slit, and his body covered with wounds, who had contumaciously objected to joining in a dance to honour the King’s coronation.\n",
      "It was a hot day in midsummer when the sun stood still upon Salisbury Plain; yet there were those in Wessex whose minds moved more slowly than the earth under their feet. Amongst such as these it would be hardly too strong an assertion, that in no country where the common tongue of humanity is spoken do men and women sit longer at dinner than in the proud land of Hengist and Horsa; and I think (without vanity be it spoken) there are few of them, even nowadays, who chew their meat less deliberately, or who talk with more circumlocutious volubility. The consequence is, that it may not unfrequently happen in England, as a gentleman once observed to me, that the manners of those at table may seem to have been modelled on those of the dead, and that there is an unconscious harking back to some bygone etiquette. Perhaps no people ever were more anxious than the English about their position in life; perhaps none are more sensitive in regard to trifles, or more easily discomposed: possibly, too, the race has been endowed with a nervous temperament; and all these things taken together have tended to create that strange moral atmosphere which makes such peculiarity of manners possible.\n",
      "‘Why! how is this?’ cried the old man, with his usual smile, ‘that you, who are so often at Bath and Bristol, should never yet have honoured me with a visit at my own door—though indeed I am not much better than a stranger to any of you; for what has been done by one member of this family seems to be forgotten by all the others. No wonder, perhaps, that such is the case, for it has always been the custom of our house never to stand upon ceremony; and as we were poor, no one ever cared about us, so we made few distinctions between ourselves and the lower classes: but since our fortunes have been a little mended we must not be quite so familiar with everyone—even with some relations.’\n",
      "‘If this be true,’ said Mrs. Cumnor, ‘then you are not to blame, as I am sure that I have never neglected any opportunity of showing my regard for you and your daughter; and in proof whereof it is a fact which I cannot help thinking strange—that since Miss Penelope left Bath I have scarcely seen her anywhere.’\n",
      "‘Not at all,’ said the old man, with great emphasis. ‘I do not mean that my daughter is too proud to make friends wherever she may find herself: but that she has been misled and deceived; for it is the custom in some families—the good family of Hurst, among others, and a very bad one, I am told, at this place, though I believe they have removed since I spoke of their shameful conduct to my sister-in-law—to treat all persons not belonging to themselves as their inferiors. Such a custom is utterly opposed to ours: we never considered ourselves superior to any people, unless perhaps the Dissenters, who are sometimes said to be even worse than Papists; and we always lived on terms of the greatest familiarity with those in our own station.’\n",
      "‘Your daughter’s situation,’ said the old lady, ‘has been a most unfortunate one. She is not so handsome as she might be, and her temper has always been of that kind which will not bear opposition; though I am bound to say that there have been few families with whom we were ever on terms who showed us more attention in return for our own good-will and good offices than the family of General Tynte.’\n",
      "‘Indeed,’ said Sir John, ‘they are a very amiable family. They have always treated my daughter’s father and mother with great kindness. I remember that when we went over to Ireland they showed us every sort of civil attention, and Mr. Tynte was extremely good-natured at a ball given by the ladies.’\n",
      "‘There was nothing, perhaps,’ said Lady Middleton, ‘more likely to engage Mr. Tynte’s heart than dancing.’\n",
      "The subject was here dropped, and Sir John was in the midst of some good-humoured raillery against Mr. Robinson for having taken a wife so much younger than himself—when Sir John’s man came into the room with two letters, one directed to ‘Mr. Tynte,’ and the other to ‘L\n",
      "llama_print_timings:        load time =   11714.43 ms\n",
      "llama_print_timings:      sample time =     156.28 ms /  1024 runs   (    0.15 ms per token,  6552.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1709.45 ms /   494 tokens (    3.46 ms per token,   288.98 tokens per second)\n",
      "llama_print_timings:        eval time =   34086.23 ms /  1023 runs   (   33.32 ms per token,    30.01 tokens per second)\n",
      "llama_print_timings:       total time =   36194.18 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb27f0-9bb9-4ee5-b7d6-617348a5e1d5",
   "metadata": {},
   "source": [
    "### 13B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d9de77b-77e4-4b90-b88e-a4d54857a2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192255\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.72 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  312.64 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 24514.08 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 24989.09 MiB (model: 24514.08 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honor to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history.\n",
      "It is in vain to summon any ghost of reasoning from such beginnings; or to present this monstrous incongruity of things. If it were possible for mankind to be made happy by one foolish opinion, the whole world might have been happy half a century ago by the doctrine of Universal Salvation: but that all men should have agreed in the same dream at the very same time, is not more incredible than the doctrine itself. \n",
      "II\n",
      "The Man with Two Left Feet  \n",
      "(1853)\n",
      "A MAN'S face – I don't mean merely a human face, nor a good face; but _the_ man's own face – as it were, the map of his character: the expression of his mood and disposition. Into this detail many people read further than is altogether safe – for one can be too curious about oneself in that way. There are times when the very pretences we make to others (our manners) reveal more truth than we care to acknowledge to ourselves; and it has been my observation, from long practice as an amateur detective, that our most serious errors – our serious mistakes of life or morals – generally have their roots in what is most amiable and loveliest about us. There are, for instance, some people who never seem to make a blunder or say the wrong thing; but the secret of it is just this: they have so much good sense that they are careful not to appear foolish in anything; therefore they never really commit themselves to any opinion until they are quite sure that they can defend it against all comers. Such a course as this, which I have known many women follow (for it is, by the way, far more common among women than men), would seem to be the surest guarantee of success; and yet nothing will prove to my mind that these people are not really foolish; for they only _pretend_ to be careful. I believe the fact is, their opinions have been so often approved by others, and especially by themselves, that they have become very confident and conceited in them – a disposition of the mind which may indeed seem praiseworthy at first sight; but it must surely have its drawbacks; for what confidence or faith can be placed in any conviction that we have not been able to defend? And yet the very best people often appear to act as if they believed in nothing – I don't mean the most _worldly_ , who are merely shrewd, and understand how far their interests go; but those who, like my dear wife, for instance, are so amiable, and kind, and good that they can make even themselves forget how good they really are!\n",
      "The man in whose case I am interested here is not one of these people: he has no particular merit or defect; and, though he is by nature a timid and modest person (as you will see if you listen to him), it seems as if he had somehow got the idea that everything depends upon his own opinion – an idea which, as I have reason to know, has done me infinite harm at various times in my life. I have observed, for instance, how people are apt to fall into some prejudiced way of looking at things which makes them imagine they know what a certain person would do or say in any given circumstances – just as I often suppose I _know_ what Mrs Bowater is going to say on the other side of the door – and the more strongly their own conviction strikes them, the less willing are they to believe that any other people could have different opinions. This man has a decided opinion on nearly every subject in which he thinks himself competent to judge; but in matters where his judgment is doubtful, or he does not think it necessary to form one at all, there is no end to the pains he will take to get another person's point of view – that of a woman, for instance, with whom he has fallen in love, though he admits frankly that they have been but very casually introduced.\n",
      "I\n",
      "llama_print_timings:        load time =   27882.37 ms\n",
      "llama_print_timings:      sample time =     157.31 ms /  1024 runs   (    0.15 ms per token,  6509.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1751.10 ms /   494 tokens (    3.54 ms per token,   282.11 tokens per second)\n",
      "llama_print_timings:        eval time =   34985.89 ms /  1023 runs   (   34.20 ms per token,    29.24 tokens per second)\n",
      "llama_print_timings:       total time =   37139.26 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "934d42d7-5274-4821-a1b5-6810d373887b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192326\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.72 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  312.64 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 24514.08 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 24989.09 MiB (model: 24514.08 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards; and because, additionally, he had doubly insulted these reverend fathers by laughing at their umbrellas. \n",
      "It was in such an age, and under such Christian teaching, that Jean Valjean appeared before the tribunal of judgment. \n",
      "BOOK FIRST  \n",
      "THE WHOLE ENTERPRISE OF JEAN VALJEAN\n",
      "CHAPTER I—HOW THE MENAGERIE CAME TO MARLYE-LE-ROI\n",
      "The convict was pronounced guilty, the penalty fixed at five years’ hard labour and a fine of four hundred francs. He was immediately transferred to the prison at Toulon; there were only three prisons in France in which the sentence could be executed with all its severities: Bicêtre, Rochefort and Toulon.\n",
      "Jean Valjean had never seen a galleysman or an galley-slave; he knew nothing about them but that they were condemned to work at oars until they died, that they were covered in irons on their feet like his own, only that these irons were riveted to the soles of the shoes by bolts through the soles and through the shoe-leather; that they wore a collar round the neck and another on the back which were joined by an iron bar; that they were chained hand to hand with six or eight of their comrades, like galley-slaves in the pictures. This was all he knew about the galleys. But what did not escape his penetration was a peculiarity connected with his own case. The prison doctor at Toulon had taken a fancy for him, and had kept him three months longer than usual in hospital. A year later, while he was still in prison, he heard that this doctor, M. Richard, had been made the governor of Bicêtre. It struck him as being peculiar: it seemed to him that if he had come out of his hole in order to make a man’s life miserable, there would have been no need for him to leave Toulon; he might just as well have gone on amusing himself at the expense of the convicts confided to his care. But perhaps he preferred Paris; he had not enjoyed being isolated in a little sea-port town. He did not take long in reaching Bicêtre: the post of governor was an appointment which, by custom, was given to young men fresh from school, and who were not too well informed about what they were about. As soon as M. Richard took possession of his new place, Jean Valjean became anxious again; he had not been wrong in connecting this man’s departure from Toulon with himself; he said to himself that the doctor wished to find him once more in a convenient situation for completing his design upon him; so that in order to get away from this second M. Richard, if he were condemned to remain at Bicêtre any length of time, he would be forced to submit to have the mark branded on his shoulder, and thus compromise himself. This was not an agreeable prospect.\n",
      "He made the best bargain he could with one of the turnkeys; he bought from him some linen shirts in which a convict is less conspicuous; but these were not sufficient to conceal his misfortune under every circumstance, for, as we have already said, Jean Valjean was an unusually well-built and strong man. It would seem that he had taken advantage of the unobserved hours which he enjoyed while doing his task in the galleys,—to repair all those deficiencies of bodily strength which are due to long confinement, to exercise with great pains those muscles which suffer from disuse. There was more than one way of escaping; it is probable that Jean Valjean knew several of these ways: but he chose the simplest and most difficult. He waited for a dark night, on some fine autumn evening when the moon did not shine, when there were no visitors at the prison, when all was tranquil in the cells, when the turnkey had finished his rounds, and when M. Bamatabois was drinking his café; then he got upon the roof of one of the wings which fronted the wall, and from this roof he slipped down to the ground outside the outer\n",
      "llama_print_timings:        load time =   41209.84 ms\n",
      "llama_print_timings:      sample time =     155.89 ms /  1024 runs   (    0.15 ms per token,  6568.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1845.07 ms /   494 tokens (    3.73 ms per token,   267.74 tokens per second)\n",
      "llama_print_timings:        eval time =   35274.27 ms /  1023 runs   (   34.48 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:       total time =   37519.49 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21add004-be6a-49ca-9d3e-ae21ae8ef335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192420\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.72 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  312.64 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 24514.08 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 24989.09 MiB (model: 24514.08 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down and called Blacks white in a matter of difference about some broken pots. It is likely enough that these virtuous pastors, who were so careful to save France from the fatal influence of heretics, would have denounced Marat as an atheist, and would have burned him alive too, if they had found it congenial with their mission to serve France and not themselves. \n",
      "Chapter 1\n",
      "On a fine autumn evening in the year eighteen hundred and five, Mr. Jarndyce, with some little difficulty upon his side and much ease upon mine as he good-humouredly assured me, escorted Miss Summerson home from our city-residence at Camberwell. We had been to see a play together, I recollect—one of those performances that are so like bad dreams in the reading that nothing can compensate for their being so in the reality: which are as ill imitated by report or description as they are vividly realised on the haggard and distressed stage.\n",
      "This was one of those plays, I believe. At all events it had such an effect upon Miss Summerson that she had remained silent during most of it; but when it was done, she said to me, 'Oh Esther! We were all so happy once!' The tears rolled down her face and dropped upon her dress without her seeming conscious of them.\n",
      "She soon dried them, however. I observed to Mr. Jarndyce that she was not like what I had known her; but I said no more until we arrived at our chambers in the Temple, for fear of embarrassing him or appearing to remind him of anything unpleasant to himself by allusion to the circumstances under which he had taken charge of Miss Summerson's interests.\n",
      "'She has grown thinner,' said Mr. Jarndyce as we turned into his chambers and went towards his table at the window. 'I have been thinking she looks more delicate than when I first knew her, though she is not fairer.'\n",
      "'No,' said I. 'But there was a time when she was so cheerful! She seemed then to have such confidence in everything!'\n",
      "'Ah!' cried Mr. Jarndyce as he took the key from his pocket and looked at it—'ah!' with a heavy sigh, 'it was not in my power to prevent her confidence being shaken.'\n",
      "I knew that some painful circumstances were connected with Miss Summerson's history which Mr. Jarndyce had always kept in mind, although he seemed never to have mentioned them to any one. I now remembered that it had been hinted by Lady Dedlock in conversation with me on the subject of a legacy—I think a yearly sum of money—which she was said to hold for Miss Summerson's benefit but which we knew she had never given her and had, as I believed, long since forgotten.\n",
      "Mr. Jarndyce laid his hand upon my arm as if he were going to tell me something more on the subject; but he checked himself and only observed that she was like a young creature in a dream—that she passed through everything, even the present time, with a face of sadness and anxiety and trouble—and so turned into his own room.\n",
      "It had become quite dark while we were talking, and when he came back with the light he seemed to have recovered himself. He was full of business again and was busy in making his arrangements for going down to Bleak House next day. I helped him as much as I could without tiring Miss Summerson; and we were all three quite happy together in our good work, when there came a knock at the door, and my guardian received a letter from Mr. Kenge's clerk that was directed for him.\n",
      "Mr. Jarndyce broke the seal and read it by the light of the candle. As he did so I saw his face change, and I felt Miss Summerson tremble in her seat. 'Oh!' said Richard, putting his hands before his eyes as if to shut out something that distressed him very much.\n",
      "'I will read it,' said my guardian presently.\n",
      "The letter was very short indeed. It only stated that Mr. Woodcourt had been attacked by a fever at Marseilles, whither he had gone on the business of his profession, and had died in great pain, recommending to all parties concerned their best consideration for Miss Summerson and her happiness; adding some words\n",
      "llama_print_timings:        load time =   38880.36 ms\n",
      "llama_print_timings:      sample time =     181.25 ms /  1024 runs   (    0.18 ms per token,  5649.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1846.88 ms /   494 tokens (    3.74 ms per token,   267.48 tokens per second)\n",
      "llama_print_timings:        eval time =   35235.83 ms /  1023 runs   (   34.44 ms per token,    29.03 tokens per second)\n",
      "llama_print_timings:       total time =   37508.40 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab51db-d528-440d-ad00-e9f76d2b4b5d",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c306d621-040a-44e7-bb85-f7e64fcd3785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192503\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.10 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  114.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 17390.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his lips (to punish him, one supposes, for not speaking through them); with ordering another youth to be put to death, for pulling the extinguisher, which was on top of a lighted candle, out of his master's hand; and with denying a priesthood to a turbulent ecclesiastic, who thereupon in good set terms gave his holiness to understand that he denied _that_ Pope's right to be one. \n",
      " But troubles arise in many shapes, and come upon us from quarters whence we look not for them. To the statesman in particular they come with accumulated force from such sources of perplexity; and it requires no common degree of sagacity thoroughly to search out those intricate mazes in which they involve his affairs. \n",
      " It happened on a Saturday night that a child was born to Monsieur and Madame Defarge – as Monsieur and Madame Defarge already had reason to know, it was not the first child who had ever been born to them. \n",
      " The season was exceeding cold, and in the height of the winter that same year, Madame Defarge was taken ill and died. \n",
      "A CALM IN PROVENCE\n",
      "IT WAS NOW THE TIME OF day when a man puts his tools away, hangs up his coat behind the door, and sits down to supper. I looked at my watch: half past six. I went inside. The room was cold – the house had not warmed up after the night of snow – and I lit the stove in the kitchen before going back into the sitting room and shutting the shutters against the night, which was getting colder by the minute. In spite of these measures there remained a draft; the shutters did not fit closely at either side, and the windows themselves had no insulating properties whatsoever. I put up a sheet of cardboard. Then I looked out through my handiwork at the mountains, which were getting whiter by the minute – if that were possible. The snow was beginning to fall more heavily; it had come on fast in the last half hour or so and now, as far as I could judge from my limited observation point, there wasn't a house for miles around that did not have an accumulation of at least four inches already.\n",
      "I looked into the larder – the English word seemed appropriate here – to see if I had enough bread in stock for a week of snowbound days, then decided that I was going to make my own. I put on the radio. The news began with a long discussion of the weather; I couldn't follow it all because the announcer spoke too quickly and his French was far from idiomatic, but it appeared that the entire region – the _département_ of Var, in which Provence is located – had been put on 'code orange', and the public were requested to stay at home. I knew what that meant: schools closed, offices shut down, hospitals overwhelmed with staff who couldn't get there by car or bus, roads impassable...\n",
      "I set about my bread-making – in Provence, you bake it at least once a week – and when the dough had risen I put a stew of lentils in the oven to cook while I went back upstairs with an English–French dictionary and worked out what the weather forecast meant. I didn't really have to bother, since by now it was clear that it was snowing quite heavily outside. I could see a couple of people making their way through the deepening drifts, but they had no trouble getting about; in fact they seemed to be walking quite briskly. When you live on the edge of the Mediterranean you are not prepared for cold weather. You might well have several lightweight coats and jackets, but the only footwear that comes up to a blizzard's standards is an anorak or – if you're really serious about going out in it – a pair of waders.\n",
      "The radio informed me that there were more than a thousand snowbound motorists somewhere near Gap. The emergency services had set up a 'field hospital', whatever that means, to tend their hypothermia cases. There was also some confusion at the Marseille airport; I didn't understand why but it seemed to involve frozen hydraulics and cancelled flights.\n",
      "The snow fell all day long in what the locals call _une tempête de neige_ (a snowstorm). It came down so thickly that when\n",
      "llama_print_timings:        load time =   31973.83 ms\n",
      "llama_print_timings:      sample time =     166.38 ms /  1024 runs   (    0.16 ms per token,  6154.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3189.12 ms /   494 tokens (    6.46 ms per token,   154.90 tokens per second)\n",
      "llama_print_timings:        eval time =   51453.57 ms /  1023 runs   (   50.30 ms per token,    19.88 tokens per second)\n",
      "llama_print_timings:       total time =   55073.75 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96cd60f7-7530-446a-8461-a099e73f3b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192855\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.10 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  114.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 17390.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his lips (to penalise him for not having denied, when denial was useless, his treaty with the enemy); bloodying the face of a mother, to cause her daughter to abandon a seducer; and committing twenty persons to the hospital for having spoiled some ladies' complexions by supplying them with an essence of lemon juice in place of one of vitriol. \n",
      " Thus was the second year of Louis XVI.\n",
      " CHAPTER II: The Mail\n",
      " IT WAS the Dover road that lay, on a Friday night late in November, before the first of the long array of mail coaches that were to arrive in London\n",
      " from Canterbury during the hour.  All the bags that had come down from town were made up at Dover for the afternoon mails to Paris and Brussels; and as they were handed in one by one to the general mail, and drew it slowly along the ground till they nearly touched its roof, some curious person outside might have fancied that an announcement of an earthquake was on its way to the continent, preceded by this ominous rumbling of the letters.  It rolled out of the yard in tow of one horse, at a walking pace ; but its great wheels and its round of ten horses rendered it, when they were put to, an exceedingly light and airy machine to steer along the highway.  Its usual pace was between seven and eight miles an hour, but sometimes when its load was heavy, and the roads bad, it crept along at little more than five.\n",
      "And yet, in spite of all this, and of some other drawbacks to punctuality, such as a now-and-then change of horses at a point too far ahead to break into the last gallop on the last stretch of road, the general mail was pretty well up to time.  It had been due in London at twenty minutes before eight, on Friday night; it was then twenty minutes past eight o'clock ; and the first of the two hundred and thirty bags of which its load consisted, had not yet been opened in the General Post Office, down St Martin's-le-Grand way.  It was like an overdue ship ; something must have happened to it on the road.\n",
      "The coach lumbered into the yard, and drew up at the gate.  There were two passengers inside, who both got out: one taking a small portmanteau from the roof, the other a smaller parcel from the seat opposite him, and leaving them in the hall.  A postal passenger had come over from Paris with the mail, but he remained outside; and, indeed, the guards had been discharged at Dover, on the previous side of the Channel.\n",
      "There were two people on the coach-box : the guard, and a ruddy little man in a snuff-coloured coat, whose face and general appearance it would be difficult to connect with any other occupation but that he followed.  He had a huge stick in his hand ; a hat on his head ; and an unaccountable amount of loose shag in his neckcloth.\n",
      "The guard had not got down, for he knew what was going to happen, and waited to see it done.  The coachman, who had been staring about him in all directions as his custom was, remained on the box ; so, did the two passengers who had arrived.\n",
      "'Why, Sammy!' said one of these latter. 'How are you ?'\n",
      "'Why, sir!' returned the guard, shaking hands with him.  'How are YOU ? I have seen you a good many times, but I haven't seen you lately.'\n",
      "'No.  I have been away,' returned the other.\n",
      "The coachman, who had a special pride in standing on his own roof, inflicted himself between them as Sam Weller said this, and lifted the lid of an immense red tub that stood beside him.\n",
      "'Come,' he said. 'Here we are ! Up with you, Mrs Pipchin, old girl !'  And the coachman pitched a heavy brown-paper parcel on to the roof of the coach.\n",
      "The guard turned back to meet his fare, and then stopped and looked round. The vehicle was empty.  Sam Weller had departed.\n",
      "The coachman waited until the fare was gone into the house, and then tumbling back into his own place, said to himself, with a chuckle, 'I think I did unlock that there little door, sir.'\n",
      "At this point\n",
      "llama_print_timings:        load time =   27475.72 ms\n",
      "llama_print_timings:      sample time =     155.01 ms /  1024 runs   (    0.15 ms per token,  6605.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.90 ms /   494 tokens (    6.43 ms per token,   155.45 tokens per second)\n",
      "llama_print_timings:        eval time =   47795.53 ms /  1023 runs   (   46.72 ms per token,    21.40 tokens per second)\n",
      "llama_print_timings:       total time =   51373.14 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffca4a61-421e-4071-8f3d-2b3bbff4796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703192948\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.10 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  114.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 17390.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. \n",
      " It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered until needed, rude implements which, only wanting to be sharpened, polished, and oiled, would become fit instruments to place in the hands of a bold man who might arise some day to do great deeds with them. It is likely enough that in stupid fond bosoms there was a slumbering spark, which had long lain asleep, waiting for the hand of Fate to be thrown down amongst a few sticks of stubble, and left to blaze. Dark woodman, dim-lighted match, blown hair and all, was not this destiny? Was not this Fate?\n",
      "The mender of roads who waylaid travellers as they pass along the Paris streets, with the object of extorting from them a pittance for the repair of the highway, flourishes his monthly stipend with his monthly journal, and delivers himself something like this: –\n",
      "\"Citizens, look here! Here's the paper of to-day. Read it if you can; if you can't, I will tell you what is in it.\n",
      "\" 'Death to the tyrants!' \"\n",
      "\"Which tyrant?\"\n",
      "\"The king.\"\n",
      "\"How about the other one, the emperor?\"\n",
      "\"There are two of them now. They are both tyrants. The emperor is bound to be one, by the new law.\"\n",
      "\"Why don't he come?\"\n",
      "\"Because it is so ordered.\"\n",
      "\"When will he come?\"\n",
      "\"To-morrow night. To-morrow is the day of his Coronation Feast.\"\n",
      "\"What, to-night!\"\n",
      "\"Yes. You may sleep securely in your beds to-night, for the new Emperor cannot possibly arrive before morning. But I say it again; when he does come, I hope you will remember what I have told you, and do your duty! Repression is the only way,\" the mender of roads added, throwing back his metal cap and smoothing his hair with both hands, \"to prevent this sort of thing from happening often. Adieu, citizens!\"\n",
      "With that final word, he put his cap on again (a much more formidable size than mine) and retraced his steps, with the Doyenné following him at a distance of half an hundred yards or so. I saw her turn into her shop doorway as the mender of roads turned into ours.\n",
      "\"Citizens,\" said Madame Defarge, laying down her knitting, and beginning in a lower voice, \"I fear we have a spy in our ranks to-night.\"\n",
      "\"On behalf of the Republic?\" asked the first speaker.\n",
      "\"Of whose Republic?\" demanded the second speaker.\n",
      "\"It may be,\" said the third, \"that this mender of roads is some good citizen who, seeing trouble in the air, has come hither for safety.\"\n",
      "\"This good citizen,\" pursued the first speaker, \"had better have a care that he is not recognised and challenged. If I saw him—multiple spy as he is—going home to sleep! I would knot his head under his arm. Think you, Citizen Evrémonde, that your brother's life would be safe in this quarter?\"\n",
      "\"In a time of Revolution,\" answered Defarge, \"all the honest men are on one side, and all the scoundrels and mischief-makers are on the other. It is always so now, and always has been.\"\n",
      "\"Tell me your thoughts on this head,\" said Madame Defarge to her old husband; \"for whenever I have seen my brother in your company, you have been very cold to him.\"\n",
      "\"Hah! You know of my history, then?\" demanded Defarge.\n",
      "\"I know,\" was Madame Defarge's reply,\n",
      "llama_print_timings:        load time =   27880.06 ms\n",
      "llama_print_timings:      sample time =     155.38 ms /  1024 runs   (    0.15 ms per token,  6590.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3241.24 ms /   494 tokens (    6.56 ms per token,   152.41 tokens per second)\n",
      "llama_print_timings:        eval time =   47585.97 ms /  1023 runs   (   46.52 ms per token,    21.50 tokens per second)\n",
      "llama_print_timings:       total time =   51226.06 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b20ad-6a54-458f-a85a-f3cc3002e935",
   "metadata": {},
   "source": [
    "### 30B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba52581-5544-4b97-85b8-457790a8b342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703193033\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.78 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  406.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 61639.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 62516.33 MiB (model: 61639.32 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the same Woodman had appointed years ago, to be his horses that should drag him and his load. But that Woodman, Fate, had a sound sleep, and heard no whispers, drove no chariot through a splendid and a guilty land, was employed by no such Great Rebellion, and in this present year of fifteen hundred and seventy nine, did not present himself to any man's imagination, in the character of the revolutionary Horseman.\n",
      "Social change is one thing; social upheaval is another. This was a time of upheavals. We have now come into the third period in the history of the world — a period which we may call the Social Period. It began in France, with the outbreak of the revolution of 1789. In this revolution, the power of the nobles and clergy was taken from them and given to the middle class (bourgeoisie). Then the bourgeoisie had control for a period of time. This led to great changes in France:\n",
      "The power of the king is broken. He becomes a figurehead — not really important or powerful anymore.\n",
      "People are free to worship any way they want. No one religion has legal superiority over another, and people can worship as they like, instead of being forced to worship the government-preferred religion. This leads to a lot of different denominations (branches) of Christianity.\n",
      "Slavery is ended in France.\n",
      "People get land because of where they live, not how rich their father was. This leads to the end of feudalism and makes way for capitalism — an economy based on private business instead of farming.\n",
      "But this also created a lot of problems:\n",
      "Many of the peasants, now free from serfdom, didn't know how to take care of their own land. So they ended up starving or having to sell their property to rich people who did know how to take care of it.\n",
      "The king, whose power had been weakened by the revolution, was executed. But in time, the people regretted that act. They decided they liked some parts of the old order — like the monarchy — and decided they could put together a constitutional monarchy. A constitution is just a set of rules for how the government will operate; this kind of system puts limits on what the monarch can do without violating these rules, and this limits his power.\n",
      "The church was broken up into many different groups. In doing so, it weakened its own influence in society — people had less respect for the clergy.\n",
      "The Revolution also brought war with other European countries, and eventually a dictatorship under Napoleon Bonaparte. He's sometimes seen as an enemy of Christianity because he made changes to laws and churches that favored him politically, but also tried to preserve the power of the Catholic Church in France. But we have more freedom than most of Europe today because of this Revolution, so in the end it did make a difference.\n",
      "But for now, let's return to England — and to William Wilberforce.\n",
      "He had become friends with Thomas Clarkson, who had begun traveling around England, trying to persuade people to oppose slavery. One day he spoke at Hull (a port city in northern England), where a young man named John Newton heard his speech.\n",
      "Newton was the son of an English sea captain; after the death of his father, he had joined the Royal Navy, but had given it up and gone into business instead. When he gave this up, he went to work as a servant on a ship that sailed from England to the West Indies. There, the slaves he saw inspired him to become more religious; he studied with Thomas Scott (the\n",
      "llama_print_timings:        load time =  102915.07 ms\n",
      "llama_print_timings:      sample time =     155.59 ms /  1024 runs   (    0.15 ms per token,  6581.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3276.71 ms /   494 tokens (    6.63 ms per token,   150.76 tokens per second)\n",
      "llama_print_timings:        eval time =   54037.03 ms /  1023 runs   (   52.82 ms per token,    18.93 tokens per second)\n",
      "llama_print_timings:       total time =   57717.80 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ee4085-91b4-44ce-9f66-ff1f04079686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703193207\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.78 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  406.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 61639.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 62516.33 MiB (model: 61639.32 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, \"in consequence of the failure of his ammunition:\" after which the mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in London gaols fought battles with their turnkeys, and the majesty of the law fired blunderbusses in among them, loaded with rounds of lead and ball; thieves snipped off diamond crosses from the necks of noble lords at Court drawing-rooms; musketeers went into St. Giles's, to search for contraband goods, and the mob fired on the musketeers, and the musketeers fired on the mob, and nobody thought any of these occurrences much out of the common way. In the midst of them, the hangman, ever busy and ever worse than useless, was in constant requisition; now, stringing up long rows of miscellaneous criminals; now, hanging a housebreaker on Saturday who had been taken on Tuesday; now, burning people in the hand at Newgate by the dozen, and now burning pamphlets at the door of Westminster Hall; to-day, taking the life of an atrocious murderer, and to-morrow of a wretched pilferer who had robbed a farmer's boy of sixpence.\n",
      "All these, and a thousand other things, came to pass in and close upon the dear old year one thousand seven hundred and seventy-five. Environed by them, while the Woodman and the Farmer worked unheeded, those two of the large jaws, and those other two of the plain and the fair faces, trod with stir enough, and carried their divine rights with a high hand. Thus did the year one thousand seven hundred and seventy-five conduct their Greatnesses, and myriads of small creatures—the creatures of this chronicle among the rest—along the roads that lay before them.\n",
      "CHAPTER V\n",
      "Born of good old stock, a better man was Mr. Bounderby, than most men are, perhaps; but, at heart, he was not particularly meek and quiet. Not that it would have been expected in him: his bringing up had not exactly tended in that direction, nor had early circumstances encouraged the soft development of his character.\n",
      "If a man lived on bread alone, that man would\n",
      "llama_print_timings:        load time =   98472.14 ms\n",
      "llama_print_timings:      sample time =     153.47 ms /  1024 runs   (    0.15 ms per token,  6672.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3607.03 ms /   494 tokens (    7.30 ms per token,   136.95 tokens per second)\n",
      "llama_print_timings:        eval time =   54117.18 ms /  1023 runs   (   52.90 ms per token,    18.90 tokens per second)\n",
      "llama_print_timings:       total time =   58125.08 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b35d80-2af5-498b-9b56-e5bd1f081c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703193385\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.78 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  406.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 61639.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 62516.33 MiB (model: 61639.32 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous. In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, \"in consequence of the failure of his ammunition\": after which the mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in custody were left unattended in back rooms at country inns, and disappeared in the night; servants were tithe-robbers; the whole system of Detection was as rotten as everything else, and there was consequently no such thing as a respectable witness; guns against personal aggression were bluntly laid on tables between parties and words, instead of being held secretly in the hand; Honorouble members of Parliament shewed themselves as barefacedly corrupt as Jack Ketch the hangman shewed himself frankly drunk; and the grandest recognition of great men was \"to take the freedom of the Treadwathle Street Spunging-house.\"\n",
      "If that Bleak House of a city had itself been put cautiously to bed, not all its beds would have made up the quantity of accommodation that was requisite. So, on the misty mornings when the wanderer who had no calling save that of morning walker in the streets—and there were many such then as the only alternative to the night cellar—found himself displaced by grand establishments and at a loss how to dispose himself until day, he strolled along the London Bridge Road and looked over the balustrades at the two melancholy ships always anchored off the marshes below, and whitening in the fog. The Grampus and the Orion made a touch of homely comfort in the dreary scene; being hospitable houses of entertainment to which London apprentices had for centuries resorted on Sundays and holidays, to eat hot rolls and drink cold gin-and-water by the jug.\n",
      "At first these ships presented no remarkable appearance except that they were strangely out of place in the river; but as the eye grew more accustomed to them in their own element, traits peculiar to each would be observed. In the Orion, for instance, there was an air of sober dullness and slow accumulation of wealth. The Grampus on the other hand was all vivacity and spirit. In short, they were (as any man who had ever beheld them in full contrast must have felt) a moral admonition; the one rebuking the\n",
      "llama_print_timings:        load time =  105021.22 ms\n",
      "llama_print_timings:      sample time =     156.25 ms /  1024 runs   (    0.15 ms per token,  6553.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3586.28 ms /   494 tokens (    7.26 ms per token,   137.75 tokens per second)\n",
      "llama_print_timings:        eval time =   54541.99 ms /  1023 runs   (   53.32 ms per token,    18.76 tokens per second)\n",
      "llama_print_timings:       total time =   58536.18 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b24fab-f886-498f-a9f5-d91bf3d5a189",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703193568\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.01 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  140.90 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 34950.11 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 36349.11 MiB (model: 34950.11 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution.\n",
      "But, that woodman and that farmer, though they work unceasingly, are very slow and very silent, and he must have a long patience who has to watch and wait for them.\n",
      "Of this period, too, it may be remarked; while every little court was teeming with intrigue, when the higher and the lower nobility were banded together in one common cause; while the word Liberty could make the highest chin in all France go down and the proudest back in all Europe bend its haughty head; a man disfigured by having had his ear chopped off close to his head, and branded with an iron hot enough to have sunk into and singed the soul out of every wicked Frenchman living, is standing on a scaffold at Toulon, with that same right hand cut off and tied up to his neck as lawlessly and barbarously as if he had been a dog. Here also, within a few months from this time, a young soldier is touched upon the sleeve in the streets of Paris: it opens; he has got the King's shilling in his coat. Mark that man for promotion.\n",
      "Let us see some more of our old friends before we get back to the bank parlor again.\n",
      "Madame Defarge still sat in her accustomed chair, knitting by the fire. Close at her side stood The Vengeance, knitting too. The old Jacobin sat opposite them, watching with furtive eye the proceedings of their bony fingers. A bottle of wine on a shelf flanking Madame Defarge's chair, was gradually getting low in that department, but it was not yet out.\n",
      "\"I have been wondering,\" said Madame Defarge, after silently considering and looking towards her husband for some length of time, \"I have been wondering which prisoners the Citizen Evremonde will obtain to-morrow?\"\n",
      "\"Eh! Who knows? Perhaps a good many.\"\n",
      "The Vengeance lifted her head and asked: \"To have their heads lopped off?\"\n",
      "Madame Defarge looked sternly at her, and went on with her work. The Vengeance subsided, and cowered down.\n",
      "\"It is not likely,\" said Madame Defarge, after a pause; and then added, slowly and thoughtfully: \"though it is possible.\"\n",
      "Defarge brought the wine from the shelf; poured three small glasses of it into as many tumblers; drew back to his corner by the hearth; and, composedly drinking, said in an easy tone: \"Three! Three, did you say?\"\n",
      "\"Three,\" replied Madame Defarge.\n",
      "Defarge went on sipping his wine, and looking at her with an obvious desire to express something in his face, if the cruel lines of that visage could have been softened for a moment.\n",
      "\"You will be content?\" said he. \"Three is not too much; three is enough.\"\n",
      "\"I tell you three is enough,\" returned madame defarge; \"three are as good as a hundred. Let us not lose time. If it must be done to-morrow, let it be done at once. Where is the Citizen?\"\n",
      "Here Defarge's wife laid her hand upon her husband's arm; and with an anxious look, and in a hurried, broken manner, said:\n",
      "\"I know what you have done, my husband. I know that you have had thousands of people killed to-day. Ah, good God!\"\n",
      "Defarge looked down at the floor on which his wife was\n",
      "llama_print_timings:        load time =   57004.31 ms\n",
      "llama_print_timings:      sample time =     156.33 ms /  1024 runs   (    0.15 ms per token,  6550.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5159.51 ms /   494 tokens (   10.44 ms per token,    95.75 tokens per second)\n",
      "llama_print_timings:        eval time =   64167.02 ms /  1023 runs   (   62.72 ms per token,    15.94 tokens per second)\n",
      "llama_print_timings:       total time =   69739.03 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a9efa8-70c8-444e-bb4f-ac2bc6d90b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703193737\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.01 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  140.90 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 34950.11 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 36349.11 MiB (model: 34950.11 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrels of the Revolution—the Prussians were not yet over their own frontier.\n",
      "But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the road-agents in the neighbourhood of Hampstead and Highgate would have considered themselves affronted, if challenged by less than five horsemen; on the Eastern Counties the mail was waylaid by a single highwayman, and the guard shot dead at his post. But we had supposed these to be the birth-throes of an old state of things which was in course of death, and the pangs of another new one that was coming into life. Great changes were daily taking place above us and around us—changes unseen and unfelt by us, but not therefore the less real and potent. The Circumlocution Office went round and round; but it turned a wheel of an office-making machine that ground men to atoms for its own use; and as certainly crushed them with its wheels when they were no longer wanted, as did any other revolution that ever was heard of in the annals of the world.\n",
      "My dear Mr. Copperfield, how could you ever think that story true of my being married to a bachelor of arts, a wine merchant, a factory operative, a man of independent fortune, and all the rest of it? I suppose it was the consequence of your knowing me in former times when I really was more giddy and thoughtless than I am now. The fact is that Mr. Spenlow fell in love with me last winter at my friend Miss Jenkyns’s house (she lives at Stowe in Buckinghamshire); and knowing all the world prefers money to beauty, he proposed bringing a certain great action for a client of his against our firm, as a proof that he had not married me for money. If I am ever to marry anybody I should like it to be you; but pray do not think about it at present, for Mr. Spenlow is in India on account of the suit, which will very likely last six years if they have to examine all the witnesses as they talk of doing; and when he comes back he has promised that we shall go and live in Switzerland, so that we shall not be under the tyranny of my aunt (who is rather exacting I allow) or his mother (who is the worst woman in the world, Mr. Spenlow says). In the meantime I have bought a box at Drury Lane theatre, which was never done by any young lady before; and I drive about London every day in my phaeton and pair; besides having a black man for my footman who has been married seven times: none of your low life black men with us! When the great suit is over we are all going to put on crape, and go into mourning for the two thousand pounds I had from papa.\n",
      "I am told that you are settled in some dull place near London; and though it will never do for me to associate with you while I am living in this gay but wicked\n",
      "llama_print_timings:        load time =   57912.80 ms\n",
      "llama_print_timings:      sample time =     155.95 ms /  1024 runs   (    0.15 ms per token,  6566.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5188.46 ms /   494 tokens (   10.50 ms per token,    95.21 tokens per second)\n",
      "llama_print_timings:        eval time =   64417.49 ms /  1023 runs   (   62.97 ms per token,    15.88 tokens per second)\n",
      "llama_print_timings:       total time =   70023.74 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc5a9f8-0193-4742-9e35-3e64bf49139f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703193874\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.01 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  140.90 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 34950.11 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 36349.11 MiB (model: 34950.11 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrels of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, are very slow and quiet, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the road-agents took up their station at Hyde Park Corner, within four miles of St. James’s Palace; the mail was robbed and the guard shot within six miles of Hicks’ Hall; a special act of Parliament had to be passed empowering the Hounslow Heath footpad to be brought to trial at the Old Bailey Sessions instead of the Middlesex, because otherwise his villainy might escape punishment altogether. The immediate and only consequence of Captain Swing’s activities was that, for a few months, it did not become quite so common in some parts as formerly, for a farmer to take a gun outside with him when he went to look at his grass fields, lest the tiles should be found to have fallen off the rick-yard by some ill-disposed person; or to let a man who was not known go up to the barn without somebody else following him.\n",
      "There were riots all over the kingdom on account of the scarcity of food and employment. The people rose in many places, and among other acts of violence, broke open prisons where debtors were confined; the idea being that there was some analogy between a person’s being unable to pay his private debts to another, and being unable to pay his public debt to the whole State. There were also riots against machinery—not so much perhaps because it did harm as because it was believed to do harm; but in those days it certainly did some harm: for there was very little education at that time, no compulsory national education then; ignorant men and women, with small children to support on their daily wages, could not see their way out of the difficulty caused by labour-saving machinery, nor could they be helped. There were riots also because the Reform Bill was passed too slowly; that is to say, there were many who thought it had gone far enough when it gave political power to every man having an income of £10 per year over his head rent and taxes, whereas others wanted it given to all men, no matter whether their wages amounted to as much in the course of a week or not.\n",
      "So there was discontent on various grounds, and though people did not then live so luxuriously as they have since done, when trade has increased, yet trade had decreased at that time, and therefore food was scarcer than usual. On these accounts it happened, as we are informed by Sir Thomas Crompe, who was Lord-Lieutenant of the county at this period, that he was compelled to issue a proclamation, enjoining all magistrates within twenty miles round about to be vigilant and active in repressing riots. He also begged them not to allow themselves to be intimidated by numbers; for, said he, ‘we have a right to the protection of our laws’; as if there was no right but his own.\n",
      "llama_print_timings:        load time =   57843.05 ms\n",
      "llama_print_timings:      sample time =     157.54 ms /  1024 runs   (    0.15 ms per token,  6499.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5184.20 ms /   494 tokens (   10.49 ms per token,    95.29 tokens per second)\n",
      "llama_print_timings:        eval time =   64771.38 ms /  1023 runs   (   63.32 ms per token,    15.79 tokens per second)\n",
      "llama_print_timings:       total time =   70363.97 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626f8b0-1d41-4802-b12d-d96fa0ba353e",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c8c8e6-c07d-4854-8206-b2f9321349c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703194030\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.31 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  500.28 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 124025.03 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 125424.04 MiB (model: 124025.03 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the road-agents, as they were called, made no more scruple of shooting a traveller than a fowler did of netting a bird; the mail was waylaid by the Great North Road, and raw country-boys fired at the coachman, and robbed the passengers; prisoners in London gaols, condemned to death, were rescued by their friends; Thurtell who killed Weare in his gig, had been seen a dozen times by the officer in whose custody he was upon the very night when he did the murder; nor was there, from first to last, that general maintaining of existing rules and order throughout society, so necessary to English comfort, that even this chronic disrepair of law and lawfulness grew intolerable with the least unusual addition to it.\n",
      "Nevertheless, we had our compensation. Lord George Gordon was a Protestant hero, who saved the land from Popery and Woodfall’s Register from destruction by Government; and his noble deeds were not less truly recorded in that paper than in the Gazette. It was a creditable circumstance to any man, to have been committed in Newgate under him as a rioter. If any man had ever had the courage to assert in public assembly, or even in private company after dinner, that Lord George Gordon had not headed an immense Protestant mob with the single-handed intention of preventing the passing of the Papists’ Bill, he would have been justly and rightly deemed a most impious traitor. The No-Popery spirit was abroad in England, and extended to all sorts of people, even to such as were not Catholics themselves. It was reported that there had lately been found out among the ancient family archives of some Catholic grandee (the Marquis of Blandford, I think), a document importing that he was only to enjoy his present vast estates on condition of their becoming forfeited to the Crown if any of his descendants ever became Protestant. To avert so frightful an impending calamity, it had been settled by the marquis and his friends that he should send all his sons abroad as Catholics; and it was currently reported that they were actually in a Catholic monastery in France with shaven crowns, taking orders! This story was so far credited as to be gravely debated at our dinner-table at Greenwich whether the boys might not be rescued and brought over to England as heirs to those estates. Mr. Pitt said that they certainly would not be, if the law were properly administered; but then, perhaps they might. My father thought they would, but was not quite certain; while Mr. Rogers opined that, whatever might have been decided on such a case in times gone by, there could be no doubt of its issue now, for Lord Thurlow’s influence and that of the Government was sufficient to carry any question at present before\n",
      "llama_print_timings:        load time =  206771.22 ms\n",
      "llama_print_timings:      sample time =     189.17 ms /  1024 runs   (    0.18 ms per token,  5413.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6005.45 ms /   494 tokens (   12.16 ms per token,    82.26 tokens per second)\n",
      "llama_print_timings:        eval time =   90446.86 ms /  1023 runs   (   88.41 ms per token,    11.31 tokens per second)\n",
      "llama_print_timings:       total time =   96948.54 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6700ddcc-24fa-4a8d-a917-a19792fbcade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703194364\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.31 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  500.28 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 124025.03 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 125424.04 MiB (model: 124025.03 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution.\n",
      "But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the road agency system was in full operation, and the mail was robbed in broad daylight by the very diligence guards.\n",
      "But when old clothes were so carefully preserved, it is not surprising that other things were, perhaps, less thought of than they ought to have been. Thus Mr. Lorry, who was a bachelor, lay under small suspicion of having any design in reserve, when he went out with the young lady, to whom he was betrothed; and was not unlikely to have his proposals accepted–when the moment for their avowal should arrive.\n",
      "Therefore, Mr. Lorry dined with the Manettes on this Sunday, and was very cheerful. And when the time came for him to leave them, it gave him sincere regret to part.\n",
      "“I shall look in again very soon,” said he, as he shook hands; “meanwhile I give you my best wishes, my dear man, and I hope you will believe that your having such a friend as I am is of some avail. I am a mere man of business, but I have a feeling heart; and if I had to choose again, I wouldn’t go into the Bank.”\n",
      "“My dear Mr. Lorry! And when will you say that to-morrow?”\n",
      "“It’s an idle thing to ask,” returned Mr. Lorry, “for we all lie by necessity: even our own mothers do not hear the truth from us on all matters. Be good enough to give my love to Lucie, and let her know that I will come again soon.”\n",
      "The touching affection he showed was more than she had expected or seen in him before; and remembering how recently she had heard him speak of his daughter, she regretted that he should have no one to whom he could at once pour out his heart.\n",
      "The Doctor dined with them that day also; he remained until the dark evening, when Darnay left them to return home. He was very thoughtful and full of anxiety: not for himself; his life was hardly of any value to him: but for Lucie and her father. They would have so much to bear, if these things that threatened should happen!\n",
      "That night he wrote the following letter to Gabelle at Evreux:\n",
      "“This is a warning to you, my dear friend. Do not return to your house in Paris; it will be searched and watched. I write a few lines by my servant; and charge him, if you are there, to place this unsealed letter in your own hands. You can leave the country at once (there is time) through any of the neighbouring ports, and go either to England or to America as you think best. Your hasty flight need cause you little uneasiness: your absence will easily be accounted for on the score of ill-health. I know that you are blameless in all this matter; and only desire your safety. My name is\n",
      "llama_print_timings:        load time =  219133.43 ms\n",
      "llama_print_timings:      sample time =     158.15 ms /  1024 runs   (    0.15 ms per token,  6474.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5747.48 ms /   494 tokens (   11.63 ms per token,    85.95 tokens per second)\n",
      "llama_print_timings:        eval time =   82020.25 ms /  1023 runs   (   80.18 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =   88194.58 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bcaaa50-a5a4-4180-b751-b7158b0e6959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703194701\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.31 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  500.28 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 124025.03 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 1280.00 MB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "llama_new_context_with_model: compute buffer total size = 122.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 119.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 125424.04 MiB (model: 124025.03 MiB, context: 1399.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the road-agents, as they were called, infested the woods round London, and the great roads leading to it; travellers were set upon and robbed in broad day, within a few miles of St Paul's; footpads, lurking in gardens, sprang out on ladies as they passed; small shop-keepers in the city, found it advisable to go well armed. Pickpockets of an atrocious type had but recently been taken red-handed, and were still under sentence of death though only teenagers yet, as hardened villains as ever stood in the Old Bailey as Sheppard and Wild, so romanticised by later writers like Daniel Defoe.\n",
      "Thieves broke into country houses, stripping them of all movables, down to the very lead from the roofs; brigands attacked coaches outright and killed the guards. The whole island, from Cornwall to Caithness, was in a state of ferment and turbulence; seditious speakers went freely up and down, inflaming audiences at cobbler's stalls and wayside public-houses; secret societies were formed, with oaths and passwords; discontent was openly avowed. The manufacturing towns were in perpetual tumult, the people there, pretending for a time to have another legislature of their own apart from Parliament, even issuing a counterfeit money under the name of shillings. At Sheffield, armed bands paraded the streets, and the carnage of birds and beasts that was perpetrated by these misguided wretches is not to be recapitulated here. In the Midland Counties the destruction of machinery went on to such an extent, that a special detachment of military was sent down from London to put it down; the weavers broke open and destroyed labor-saving stocking frames in the houses of manufacturers, attacked factories by night with firearms, forcibly stopped all working people whom they found at labour in them, compelled large numbers of men to take an oath against using frames under any circumstances, beat up the workmen's houses, and demolished their furniture. The state of these unhappy districts was pitiable in consequence; for they were visited with such a sudden loss of trade, that numbers of weavers were reduced to live upon charity in the streets.\n",
      "In this general ferment of disturbance, not only was all law at an end, and all life insecure, but no man knew how far his neighbour might go, or what he might do. People who had long been rich began to tremble for their property; people who had long been poor began to demand why they should be poor any longer. Lords of manors who had squeezed the last penny out of their peasants, now found themselves in danger from the starving\n",
      "llama_print_timings:        load time =  193548.75 ms\n",
      "llama_print_timings:      sample time =     157.38 ms /  1024 runs   (    0.15 ms per token,  6506.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5820.53 ms /   494 tokens (   11.78 ms per token,    84.87 tokens per second)\n",
      "llama_print_timings:        eval time =   81177.17 ms /  1023 runs   (   79.35 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   87408.22 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
