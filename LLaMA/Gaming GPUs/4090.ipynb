{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Fri Dec 22 01:20:56 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:25:00.0 Off |                  Off |\n",
      "| 30%   24C    P8              21W / 450W |      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "model name\t: AMD EPYC 7B13 64-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       1056617192 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 14321, done.\u001b[K\n",
      "remote: Counting objects: 100% (4097/4097), done.\u001b[K\n",
      "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
      "remote: Total 14321 (delta 4007), reused 3947 (delta 3927), pack-reused 10224\u001b[K\n",
      "Receiving objects: 100% (14321/14321), 16.27 MiB | 39.19 MiB/s, done.\n",
      "Resolving deltas: 100% (10045/10045), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f20a13-e11c-4e56-98b9-4a896510f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0  10913      0 --:--:-- --:--:-- --:--:-- 10963\n",
      "Downloading tokenizer\n",
      "--2023-12-21 18:45:56--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-12-21 18:45:56 (10.2 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-12-21 18:45:56--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 18:45:57 (36.1 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-12-21 18:45:57--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[===================>]  12.55G  26.8MB/s    in 6m 42s  \n",
      "\n",
      "2023-12-21 18:52:40 (31.9 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-12-21 18:52:40--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0.008s  \n",
      "\n",
      "2023-12-21 18:52:40 (12.7 KB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 18:52:40--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 18:52:40 (132 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-12-21 18:53:04--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  35.1MB/s    in 7m 0s   \n",
      "\n",
      "2023-12-21 19:00:04 (29.6 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-21 19:00:04--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  14.1MB/s    in 15m 42s \n",
      "\n",
      "2023-12-21 19:15:47 (13.2 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-21 19:15:47--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:15:47 (33.6 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 19:15:47--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:15:48 (188 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-12-21 19:16:36--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  20.2MB/s    in 19m 2s  \n",
      "\n",
      "2023-12-21 19:35:38 (13.6 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:35:38--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  81.7MB/s    in 4m 55s  \n",
      "\n",
      "2023-12-21 19:40:34 (52.5 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:40:34--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  84.6MB/s    in 3m 33s  \n",
      "\n",
      "2023-12-21 19:44:07 (72.9 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:44:07--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  83.9MB/s    in 3m 51s  \n",
      "\n",
      "2023-12-21 19:47:58 (67.3 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-21 19:47:58--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:47:58 (30.5 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 19:47:58--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 19:47:58 (269 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-12-21 19:50:08--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  25.2MB/s    in 4m 37s  \n",
      "\n",
      "2023-12-21 19:54:46 (56.2 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 19:54:46--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  78.4MB/s    in 8m 51s  \n",
      "\n",
      "2023-12-21 20:03:37 (29.3 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:03:37--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  67.0MB/s    in 4m 48s  \n",
      "\n",
      "2023-12-21 20:08:25 (54.1 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:08:25--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  46.0MB/s    in 3m 49s  \n",
      "\n",
      "2023-12-21 20:12:14 (68.1 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:12:14--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  53.5MB/s    in 5m 1s   \n",
      "\n",
      "2023-12-21 20:17:16 (51.7 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:17:16--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  88.7MB/s    in 3m 25s  \n",
      "\n",
      "2023-12-21 20:20:41 (76.0 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:20:41--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  61.6MB/s    in 4m 26s  \n",
      "\n",
      "2023-12-21 20:25:08 (58.5 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:25:08--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  91.7MB/s    in 3m 21s  \n",
      "\n",
      "2023-12-21 20:28:29 (77.5 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-21 20:28:29--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 20:28:29 (33.3 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-21 20:28:29--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-21 20:28:30 (485 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B\t7B-v2\t\t\t  ggml-vocab-mpt.gguf\n",
      "13B-v2\tggml-vocab-aquila.gguf\t  ggml-vocab-refact.gguf\n",
      "30B\tggml-vocab-baichuan.gguf  ggml-vocab-stablelm-3b-4e1t.gguf\n",
      "65B\tggml-vocab-falcon.gguf\t  ggml-vocab-starcoder.gguf\n",
      "70B-v2\tggml-vocab-gpt-neox.gguf  tokenizer.model\n",
      "7B\tggml-vocab-llama.gguf\t  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# Obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376847f6-3509-4a11-b211-ec1fb0d73ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13283  100 13283    0     0  75878      0 --:--:-- --:--:-- --:--:-- 76339\n"
     ]
    }
   ],
   "source": [
    "# If you encounter the error \"does not appear to have a file named config.json\" when converting the models to ggml FP16 format, try to convert the model to huggingface format to get the config.json file.\n",
    "!curl -o convert_llama_weights_to_hf.py https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bee9eb-b358-40f2-8582-f93dad231f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ff2a39-7f7d-4285-9652-a89b9503005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp tokenizer.model 7B/\n",
    "!cp tokenizer.model 13B/\n",
    "!cp tokenizer.model 30B/\n",
    "!cp tokenizer.model 65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc57513-b8fa-46ba-8315-9f118b998c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.36.2)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: protobuf>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall accelerate # If you have this package, uninstall it first, then use `convert to hf model` to get the config.json.\n",
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636f3946-8a2b-4a9d-a23a-9e880e96ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/7B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/13B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/30B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/65B/.\n"
     ]
    }
   ],
   "source": [
    "# We don't need these models actually. We only need this to figure out the config.json error.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/7B/ --model_size 7B --output_dir models/7B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/13B/ --model_size 13B --output_dir models/13B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/30B/ --model_size 30B --output_dir models/30B/ # Surprisingly, it still solves the problem although you can't find the config.json file.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/65B/ --model_size 65B --output_dir models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbf7dba-9d3e-4c40-95c8-58ccc63d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit your params.json file if the \"vocab_size\" mismatch\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/7B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/7B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/13B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/13B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/30B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/30B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/65B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/65B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/7B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [4096]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 4096]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "skipping tensor blk.0.attn_rot_embd\n",
      "skipping tensor blk.1.attn_rot_embd\n",
      "skipping tensor blk.2.attn_rot_embd\n",
      "skipping tensor blk.3.attn_rot_embd\n",
      "skipping tensor blk.4.attn_rot_embd\n",
      "skipping tensor blk.5.attn_rot_embd\n",
      "skipping tensor blk.6.attn_rot_embd\n",
      "skipping tensor blk.7.attn_rot_embd\n",
      "skipping tensor blk.8.attn_rot_embd\n",
      "skipping tensor blk.9.attn_rot_embd\n",
      "skipping tensor blk.10.attn_rot_embd\n",
      "skipping tensor blk.11.attn_rot_embd\n",
      "skipping tensor blk.12.attn_rot_embd\n",
      "skipping tensor blk.13.attn_rot_embd\n",
      "skipping tensor blk.14.attn_rot_embd\n",
      "skipping tensor blk.15.attn_rot_embd\n",
      "skipping tensor blk.16.attn_rot_embd\n",
      "skipping tensor blk.17.attn_rot_embd\n",
      "skipping tensor blk.18.attn_rot_embd\n",
      "skipping tensor blk.19.attn_rot_embd\n",
      "skipping tensor blk.20.attn_rot_embd\n",
      "skipping tensor blk.21.attn_rot_embd\n",
      "skipping tensor blk.22.attn_rot_embd\n",
      "skipping tensor blk.23.attn_rot_embd\n",
      "skipping tensor blk.24.attn_rot_embd\n",
      "skipping tensor blk.25.attn_rot_embd\n",
      "skipping tensor blk.26.attn_rot_embd\n",
      "skipping tensor blk.27.attn_rot_embd\n",
      "skipping tensor blk.28.attn_rot_embd\n",
      "skipping tensor blk.29.attn_rot_embd\n",
      "skipping tensor blk.30.attn_rot_embd\n",
      "skipping tensor blk.31.attn_rot_embd\n",
      "Writing models/7B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 11/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 12/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 13/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 14/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 15/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 16/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 17/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 18/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 19/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 20/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 21/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 22/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 23/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 24/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 25/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 26/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 28/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 29/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 31/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 32/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 33/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 34/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 35/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 37/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 38/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 40/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 41/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 42/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 43/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 44/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 46/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 47/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 49/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 50/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 51/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 52/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 53/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 55/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 56/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 58/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 59/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 60/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 61/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 62/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 64/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 65/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 67/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 68/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 69/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 70/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 71/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 73/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 74/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 76/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 77/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 78/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 79/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 80/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 82/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 83/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 85/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 86/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 87/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 88/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 89/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 91/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 92/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 94/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 95/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 96/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 97/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 98/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[100/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[101/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[102/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[103/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[104/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[105/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[106/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[108/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[109/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[110/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[111/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[112/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[113/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[114/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[115/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[116/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[117/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[118/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[119/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[120/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[121/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[122/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[123/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[124/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[125/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[126/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[127/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[128/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[129/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[130/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[131/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[132/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[133/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[134/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[135/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[136/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[137/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[138/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[139/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[140/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[141/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[142/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[143/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[144/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[145/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[146/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[147/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[148/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[149/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[150/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[151/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[152/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[153/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[154/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[155/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[156/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[158/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[159/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[160/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[162/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[163/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[164/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[165/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[166/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[167/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[168/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[169/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[170/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[171/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[172/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[173/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[174/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[175/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[176/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[177/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[178/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[179/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[180/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[181/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[182/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[183/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[184/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[185/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[186/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[187/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[188/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[189/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[190/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[191/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[192/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[193/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[194/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[195/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[196/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[197/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[198/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[199/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[200/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[201/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[203/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[204/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[205/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[206/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[207/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[209/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[210/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[211/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[212/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[213/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[214/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[215/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[216/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[217/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[218/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[219/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[220/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[221/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[222/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[223/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[224/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[225/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[226/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[227/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[228/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[229/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[230/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[231/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[232/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[233/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[234/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[235/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[236/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[237/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[238/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[239/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[240/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[241/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[242/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[243/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[244/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[245/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[246/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[247/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[248/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[249/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[250/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[251/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[252/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[253/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[254/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[255/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[256/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[257/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[258/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[259/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[260/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[261/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[262/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[263/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[264/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[265/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[266/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[267/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[268/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[269/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[270/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[271/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[272/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[273/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[274/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[275/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[276/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[277/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[278/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[279/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[280/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[281/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[282/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[283/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[284/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[285/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[286/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[287/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[288/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[289/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[290/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[291/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "Wrote models/7B/ggml-model-f16.gguf\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "params = Params(n_vocab=32000, n_embd=5120, n_layer=40, n_ctx=2048, n_ff=13824, n_head=40, n_head_kv=40, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/13B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 5120]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [5120]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 5120]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [5120]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [5120]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [5120]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [5120]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [5120]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [5120]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [5120]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [5120]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [5120]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [5120]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [5120]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [5120]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [5120]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [5120]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [5120]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [5120]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [5120]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [5120]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [5120]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [5120]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [5120]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [5120]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [5120]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [5120]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [5120]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [5120]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [5120]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [5120]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [5120]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [5120]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [5120]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [5120]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [5120]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [5120]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [5120]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [5120]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [5120]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [5120]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [5120]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [5120]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [5120]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/13B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/363] Writing tensor token_embd.weight                      | size  32000 x   5120  | type F16  | T+   0\n",
      "[  2/363] Writing tensor output_norm.weight                     | size   5120           | type F32  | T+   0\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type F16  | T+   1\n",
      "[  4/363] Writing tensor blk.0.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  5/363] Writing tensor blk.0.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  6/363] Writing tensor blk.0.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  7/363] Writing tensor blk.0.attn_output.weight               | size   5120 x   5120  | type F16  | T+   1\n",
      "[  8/363] Writing tensor blk.0.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[  9/363] Writing tensor blk.0.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   1\n",
      "[ 10/363] Writing tensor blk.0.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 11/363] Writing tensor blk.0.attn_norm.weight                 | size   5120           | type F32  | T+   1\n",
      "[ 12/363] Writing tensor blk.0.ffn_norm.weight                  | size   5120           | type F32  | T+   1\n",
      "[ 13/363] Writing tensor blk.1.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 14/363] Writing tensor blk.1.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 15/363] Writing tensor blk.1.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 16/363] Writing tensor blk.1.attn_output.weight               | size   5120 x   5120  | type F16  | T+   1\n",
      "[ 17/363] Writing tensor blk.1.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 18/363] Writing tensor blk.1.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   2\n",
      "[ 19/363] Writing tensor blk.1.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 20/363] Writing tensor blk.1.attn_norm.weight                 | size   5120           | type F32  | T+   2\n",
      "[ 21/363] Writing tensor blk.1.ffn_norm.weight                  | size   5120           | type F32  | T+   2\n",
      "[ 22/363] Writing tensor blk.2.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 23/363] Writing tensor blk.2.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 24/363] Writing tensor blk.2.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 25/363] Writing tensor blk.2.attn_output.weight               | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 26/363] Writing tensor blk.2.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 27/363] Writing tensor blk.2.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   3\n",
      "[ 28/363] Writing tensor blk.2.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 29/363] Writing tensor blk.2.attn_norm.weight                 | size   5120           | type F32  | T+   3\n",
      "[ 30/363] Writing tensor blk.2.ffn_norm.weight                  | size   5120           | type F32  | T+   3\n",
      "[ 31/363] Writing tensor blk.3.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 32/363] Writing tensor blk.3.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 33/363] Writing tensor blk.3.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 34/363] Writing tensor blk.3.attn_output.weight               | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 35/363] Writing tensor blk.3.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 36/363] Writing tensor blk.3.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   4\n",
      "[ 37/363] Writing tensor blk.3.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 38/363] Writing tensor blk.3.attn_norm.weight                 | size   5120           | type F32  | T+   4\n",
      "[ 39/363] Writing tensor blk.3.ffn_norm.weight                  | size   5120           | type F32  | T+   4\n",
      "[ 40/363] Writing tensor blk.4.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 41/363] Writing tensor blk.4.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 42/363] Writing tensor blk.4.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 43/363] Writing tensor blk.4.attn_output.weight               | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 44/363] Writing tensor blk.4.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 45/363] Writing tensor blk.4.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   5\n",
      "[ 46/363] Writing tensor blk.4.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 47/363] Writing tensor blk.4.attn_norm.weight                 | size   5120           | type F32  | T+   5\n",
      "[ 48/363] Writing tensor blk.4.ffn_norm.weight                  | size   5120           | type F32  | T+   5\n",
      "[ 49/363] Writing tensor blk.5.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 50/363] Writing tensor blk.5.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 51/363] Writing tensor blk.5.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 52/363] Writing tensor blk.5.attn_output.weight               | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 53/363] Writing tensor blk.5.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 54/363] Writing tensor blk.5.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   6\n",
      "[ 55/363] Writing tensor blk.5.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 56/363] Writing tensor blk.5.attn_norm.weight                 | size   5120           | type F32  | T+   6\n",
      "[ 57/363] Writing tensor blk.5.ffn_norm.weight                  | size   5120           | type F32  | T+   6\n",
      "[ 58/363] Writing tensor blk.6.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 59/363] Writing tensor blk.6.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 60/363] Writing tensor blk.6.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 61/363] Writing tensor blk.6.attn_output.weight               | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 62/363] Writing tensor blk.6.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 63/363] Writing tensor blk.6.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   7\n",
      "[ 64/363] Writing tensor blk.6.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 65/363] Writing tensor blk.6.attn_norm.weight                 | size   5120           | type F32  | T+   7\n",
      "[ 66/363] Writing tensor blk.6.ffn_norm.weight                  | size   5120           | type F32  | T+   7\n",
      "[ 67/363] Writing tensor blk.7.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 68/363] Writing tensor blk.7.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 69/363] Writing tensor blk.7.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 70/363] Writing tensor blk.7.attn_output.weight               | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 71/363] Writing tensor blk.7.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 72/363] Writing tensor blk.7.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   8\n",
      "[ 73/363] Writing tensor blk.7.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   8\n",
      "[ 74/363] Writing tensor blk.7.attn_norm.weight                 | size   5120           | type F32  | T+   8\n",
      "[ 75/363] Writing tensor blk.7.ffn_norm.weight                  | size   5120           | type F32  | T+   8\n",
      "[ 76/363] Writing tensor blk.8.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 77/363] Writing tensor blk.8.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 78/363] Writing tensor blk.8.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 79/363] Writing tensor blk.8.attn_output.weight               | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 80/363] Writing tensor blk.8.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   8\n",
      "[ 81/363] Writing tensor blk.8.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   8\n",
      "[ 82/363] Writing tensor blk.8.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   9\n",
      "[ 83/363] Writing tensor blk.8.attn_norm.weight                 | size   5120           | type F32  | T+   9\n",
      "[ 84/363] Writing tensor blk.8.ffn_norm.weight                  | size   5120           | type F32  | T+   9\n",
      "[ 85/363] Writing tensor blk.9.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 86/363] Writing tensor blk.9.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 87/363] Writing tensor blk.9.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 88/363] Writing tensor blk.9.attn_output.weight               | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 89/363] Writing tensor blk.9.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   9\n",
      "[ 90/363] Writing tensor blk.9.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   9\n",
      "[ 91/363] Writing tensor blk.9.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  10\n",
      "[ 92/363] Writing tensor blk.9.attn_norm.weight                 | size   5120           | type F32  | T+  10\n",
      "[ 93/363] Writing tensor blk.9.ffn_norm.weight                  | size   5120           | type F32  | T+  10\n",
      "[ 94/363] Writing tensor blk.10.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 95/363] Writing tensor blk.10.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 96/363] Writing tensor blk.10.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 97/363] Writing tensor blk.10.attn_output.weight              | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 98/363] Writing tensor blk.10.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  10\n",
      "[ 99/363] Writing tensor blk.10.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  10\n",
      "[100/363] Writing tensor blk.10.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  11\n",
      "[101/363] Writing tensor blk.10.attn_norm.weight                | size   5120           | type F32  | T+  11\n",
      "[102/363] Writing tensor blk.10.ffn_norm.weight                 | size   5120           | type F32  | T+  11\n",
      "[103/363] Writing tensor blk.11.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[104/363] Writing tensor blk.11.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[105/363] Writing tensor blk.11.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[106/363] Writing tensor blk.11.attn_output.weight              | size   5120 x   5120  | type F16  | T+  11\n",
      "[107/363] Writing tensor blk.11.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  11\n",
      "[108/363] Writing tensor blk.11.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  11\n",
      "[109/363] Writing tensor blk.11.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  12\n",
      "[110/363] Writing tensor blk.11.attn_norm.weight                | size   5120           | type F32  | T+  12\n",
      "[111/363] Writing tensor blk.11.ffn_norm.weight                 | size   5120           | type F32  | T+  12\n",
      "[112/363] Writing tensor blk.12.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[113/363] Writing tensor blk.12.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[114/363] Writing tensor blk.12.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[115/363] Writing tensor blk.12.attn_output.weight              | size   5120 x   5120  | type F16  | T+  12\n",
      "[116/363] Writing tensor blk.12.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  12\n",
      "[117/363] Writing tensor blk.12.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  12\n",
      "[118/363] Writing tensor blk.12.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  12\n",
      "[119/363] Writing tensor blk.12.attn_norm.weight                | size   5120           | type F32  | T+  13\n",
      "[120/363] Writing tensor blk.12.ffn_norm.weight                 | size   5120           | type F32  | T+  13\n",
      "[121/363] Writing tensor blk.13.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[122/363] Writing tensor blk.13.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[123/363] Writing tensor blk.13.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[124/363] Writing tensor blk.13.attn_output.weight              | size   5120 x   5120  | type F16  | T+  13\n",
      "[125/363] Writing tensor blk.13.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  13\n",
      "[126/363] Writing tensor blk.13.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  13\n",
      "[127/363] Writing tensor blk.13.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  13\n",
      "[128/363] Writing tensor blk.13.attn_norm.weight                | size   5120           | type F32  | T+  13\n",
      "[129/363] Writing tensor blk.13.ffn_norm.weight                 | size   5120           | type F32  | T+  13\n",
      "[130/363] Writing tensor blk.14.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[131/363] Writing tensor blk.14.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[132/363] Writing tensor blk.14.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[133/363] Writing tensor blk.14.attn_output.weight              | size   5120 x   5120  | type F16  | T+  14\n",
      "[134/363] Writing tensor blk.14.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  14\n",
      "[135/363] Writing tensor blk.14.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  14\n",
      "[136/363] Writing tensor blk.14.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  14\n",
      "[137/363] Writing tensor blk.14.attn_norm.weight                | size   5120           | type F32  | T+  14\n",
      "[138/363] Writing tensor blk.14.ffn_norm.weight                 | size   5120           | type F32  | T+  14\n",
      "[139/363] Writing tensor blk.15.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[140/363] Writing tensor blk.15.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[141/363] Writing tensor blk.15.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[142/363] Writing tensor blk.15.attn_output.weight              | size   5120 x   5120  | type F16  | T+  14\n",
      "[143/363] Writing tensor blk.15.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  15\n",
      "[144/363] Writing tensor blk.15.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  15\n",
      "[145/363] Writing tensor blk.15.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  15\n",
      "[146/363] Writing tensor blk.15.attn_norm.weight                | size   5120           | type F32  | T+  15\n",
      "[147/363] Writing tensor blk.15.ffn_norm.weight                 | size   5120           | type F32  | T+  15\n",
      "[148/363] Writing tensor blk.16.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[149/363] Writing tensor blk.16.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[150/363] Writing tensor blk.16.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[151/363] Writing tensor blk.16.attn_output.weight              | size   5120 x   5120  | type F16  | T+  15\n",
      "[152/363] Writing tensor blk.16.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  15\n",
      "[153/363] Writing tensor blk.16.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  16\n",
      "[154/363] Writing tensor blk.16.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  16\n",
      "[155/363] Writing tensor blk.16.attn_norm.weight                | size   5120           | type F32  | T+  16\n",
      "[156/363] Writing tensor blk.16.ffn_norm.weight                 | size   5120           | type F32  | T+  16\n",
      "[157/363] Writing tensor blk.17.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[158/363] Writing tensor blk.17.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[159/363] Writing tensor blk.17.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[160/363] Writing tensor blk.17.attn_output.weight              | size   5120 x   5120  | type F16  | T+  16\n",
      "[161/363] Writing tensor blk.17.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  16\n",
      "[162/363] Writing tensor blk.17.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  17\n",
      "[163/363] Writing tensor blk.17.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  17\n",
      "[164/363] Writing tensor blk.17.attn_norm.weight                | size   5120           | type F32  | T+  17\n",
      "[165/363] Writing tensor blk.17.ffn_norm.weight                 | size   5120           | type F32  | T+  17\n",
      "[166/363] Writing tensor blk.18.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[167/363] Writing tensor blk.18.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[168/363] Writing tensor blk.18.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[169/363] Writing tensor blk.18.attn_output.weight              | size   5120 x   5120  | type F16  | T+  17\n",
      "[170/363] Writing tensor blk.18.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  17\n",
      "[171/363] Writing tensor blk.18.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  18\n",
      "[172/363] Writing tensor blk.18.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  18\n",
      "[173/363] Writing tensor blk.18.attn_norm.weight                | size   5120           | type F32  | T+  18\n",
      "[174/363] Writing tensor blk.18.ffn_norm.weight                 | size   5120           | type F32  | T+  18\n",
      "[175/363] Writing tensor blk.19.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[176/363] Writing tensor blk.19.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[177/363] Writing tensor blk.19.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[178/363] Writing tensor blk.19.attn_output.weight              | size   5120 x   5120  | type F16  | T+  18\n",
      "[179/363] Writing tensor blk.19.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  18\n",
      "[180/363] Writing tensor blk.19.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  19\n",
      "[181/363] Writing tensor blk.19.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  19\n",
      "[182/363] Writing tensor blk.19.attn_norm.weight                | size   5120           | type F32  | T+  19\n",
      "[183/363] Writing tensor blk.19.ffn_norm.weight                 | size   5120           | type F32  | T+  19\n",
      "[184/363] Writing tensor blk.20.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[185/363] Writing tensor blk.20.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[186/363] Writing tensor blk.20.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[187/363] Writing tensor blk.20.attn_output.weight              | size   5120 x   5120  | type F16  | T+  19\n",
      "[188/363] Writing tensor blk.20.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  19\n",
      "[189/363] Writing tensor blk.20.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  20\n",
      "[190/363] Writing tensor blk.20.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  20\n",
      "[191/363] Writing tensor blk.20.attn_norm.weight                | size   5120           | type F32  | T+  20\n",
      "[192/363] Writing tensor blk.20.ffn_norm.weight                 | size   5120           | type F32  | T+  20\n",
      "[193/363] Writing tensor blk.21.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[194/363] Writing tensor blk.21.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[195/363] Writing tensor blk.21.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[196/363] Writing tensor blk.21.attn_output.weight              | size   5120 x   5120  | type F16  | T+  20\n",
      "[197/363] Writing tensor blk.21.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  20\n",
      "[198/363] Writing tensor blk.21.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  21\n",
      "[199/363] Writing tensor blk.21.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  21\n",
      "[200/363] Writing tensor blk.21.attn_norm.weight                | size   5120           | type F32  | T+  21\n",
      "[201/363] Writing tensor blk.21.ffn_norm.weight                 | size   5120           | type F32  | T+  21\n",
      "[202/363] Writing tensor blk.22.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[203/363] Writing tensor blk.22.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[204/363] Writing tensor blk.22.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[205/363] Writing tensor blk.22.attn_output.weight              | size   5120 x   5120  | type F16  | T+  21\n",
      "[206/363] Writing tensor blk.22.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  21\n",
      "[207/363] Writing tensor blk.22.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  22\n",
      "[208/363] Writing tensor blk.22.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  22\n",
      "[209/363] Writing tensor blk.22.attn_norm.weight                | size   5120           | type F32  | T+  22\n",
      "[210/363] Writing tensor blk.22.ffn_norm.weight                 | size   5120           | type F32  | T+  22\n",
      "[211/363] Writing tensor blk.23.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[212/363] Writing tensor blk.23.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[213/363] Writing tensor blk.23.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[214/363] Writing tensor blk.23.attn_output.weight              | size   5120 x   5120  | type F16  | T+  22\n",
      "[215/363] Writing tensor blk.23.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  22\n",
      "[216/363] Writing tensor blk.23.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  22\n",
      "[217/363] Writing tensor blk.23.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  23\n",
      "[218/363] Writing tensor blk.23.attn_norm.weight                | size   5120           | type F32  | T+  23\n",
      "[219/363] Writing tensor blk.23.ffn_norm.weight                 | size   5120           | type F32  | T+  23\n",
      "[220/363] Writing tensor blk.24.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[221/363] Writing tensor blk.24.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[222/363] Writing tensor blk.24.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[223/363] Writing tensor blk.24.attn_output.weight              | size   5120 x   5120  | type F16  | T+  23\n",
      "[224/363] Writing tensor blk.24.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  23\n",
      "[225/363] Writing tensor blk.24.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  23\n",
      "[226/363] Writing tensor blk.24.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  24\n",
      "[227/363] Writing tensor blk.24.attn_norm.weight                | size   5120           | type F32  | T+  24\n",
      "[228/363] Writing tensor blk.24.ffn_norm.weight                 | size   5120           | type F32  | T+  24\n",
      "[229/363] Writing tensor blk.25.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[230/363] Writing tensor blk.25.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[231/363] Writing tensor blk.25.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  24\n",
      "[232/363] Writing tensor blk.25.attn_output.weight              | size   5120 x   5120  | type F16  | T+  24\n",
      "[233/363] Writing tensor blk.25.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  24\n",
      "[234/363] Writing tensor blk.25.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  24\n",
      "[235/363] Writing tensor blk.25.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  25\n",
      "[236/363] Writing tensor blk.25.attn_norm.weight                | size   5120           | type F32  | T+  25\n",
      "[237/363] Writing tensor blk.25.ffn_norm.weight                 | size   5120           | type F32  | T+  25\n",
      "[238/363] Writing tensor blk.26.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[239/363] Writing tensor blk.26.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[240/363] Writing tensor blk.26.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[241/363] Writing tensor blk.26.attn_output.weight              | size   5120 x   5120  | type F16  | T+  25\n",
      "[242/363] Writing tensor blk.26.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  25\n",
      "[243/363] Writing tensor blk.26.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  25\n",
      "[244/363] Writing tensor blk.26.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  25\n",
      "[245/363] Writing tensor blk.26.attn_norm.weight                | size   5120           | type F32  | T+  26\n",
      "[246/363] Writing tensor blk.26.ffn_norm.weight                 | size   5120           | type F32  | T+  26\n",
      "[247/363] Writing tensor blk.27.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[248/363] Writing tensor blk.27.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[249/363] Writing tensor blk.27.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[250/363] Writing tensor blk.27.attn_output.weight              | size   5120 x   5120  | type F16  | T+  26\n",
      "[251/363] Writing tensor blk.27.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  26\n",
      "[252/363] Writing tensor blk.27.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  26\n",
      "[253/363] Writing tensor blk.27.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  26\n",
      "[254/363] Writing tensor blk.27.attn_norm.weight                | size   5120           | type F32  | T+  26\n",
      "[255/363] Writing tensor blk.27.ffn_norm.weight                 | size   5120           | type F32  | T+  26\n",
      "[256/363] Writing tensor blk.28.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[257/363] Writing tensor blk.28.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[258/363] Writing tensor blk.28.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[259/363] Writing tensor blk.28.attn_output.weight              | size   5120 x   5120  | type F16  | T+  26\n",
      "[260/363] Writing tensor blk.28.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  27\n",
      "[261/363] Writing tensor blk.28.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  27\n",
      "[262/363] Writing tensor blk.28.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  27\n",
      "[263/363] Writing tensor blk.28.attn_norm.weight                | size   5120           | type F32  | T+  27\n",
      "[264/363] Writing tensor blk.28.ffn_norm.weight                 | size   5120           | type F32  | T+  27\n",
      "[265/363] Writing tensor blk.29.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[266/363] Writing tensor blk.29.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[267/363] Writing tensor blk.29.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[268/363] Writing tensor blk.29.attn_output.weight              | size   5120 x   5120  | type F16  | T+  27\n",
      "[269/363] Writing tensor blk.29.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  28\n",
      "[270/363] Writing tensor blk.29.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  28\n",
      "[271/363] Writing tensor blk.29.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  28\n",
      "[272/363] Writing tensor blk.29.attn_norm.weight                | size   5120           | type F32  | T+  28\n",
      "[273/363] Writing tensor blk.29.ffn_norm.weight                 | size   5120           | type F32  | T+  28\n",
      "[274/363] Writing tensor blk.30.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[275/363] Writing tensor blk.30.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[276/363] Writing tensor blk.30.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[277/363] Writing tensor blk.30.attn_output.weight              | size   5120 x   5120  | type F16  | T+  28\n",
      "[278/363] Writing tensor blk.30.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  28\n",
      "[279/363] Writing tensor blk.30.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  29\n",
      "[280/363] Writing tensor blk.30.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  29\n",
      "[281/363] Writing tensor blk.30.attn_norm.weight                | size   5120           | type F32  | T+  29\n",
      "[282/363] Writing tensor blk.30.ffn_norm.weight                 | size   5120           | type F32  | T+  29\n",
      "[283/363] Writing tensor blk.31.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[284/363] Writing tensor blk.31.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[285/363] Writing tensor blk.31.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[286/363] Writing tensor blk.31.attn_output.weight              | size   5120 x   5120  | type F16  | T+  29\n",
      "[287/363] Writing tensor blk.31.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  29\n",
      "[288/363] Writing tensor blk.31.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  30\n",
      "[289/363] Writing tensor blk.31.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  30\n",
      "[290/363] Writing tensor blk.31.attn_norm.weight                | size   5120           | type F32  | T+  30\n",
      "[291/363] Writing tensor blk.31.ffn_norm.weight                 | size   5120           | type F32  | T+  30\n",
      "[292/363] Writing tensor blk.32.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[293/363] Writing tensor blk.32.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[294/363] Writing tensor blk.32.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[295/363] Writing tensor blk.32.attn_output.weight              | size   5120 x   5120  | type F16  | T+  30\n",
      "[296/363] Writing tensor blk.32.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  30\n",
      "[297/363] Writing tensor blk.32.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  31\n",
      "[298/363] Writing tensor blk.32.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  31\n",
      "[299/363] Writing tensor blk.32.attn_norm.weight                | size   5120           | type F32  | T+  31\n",
      "[300/363] Writing tensor blk.32.ffn_norm.weight                 | size   5120           | type F32  | T+  31\n",
      "[301/363] Writing tensor blk.33.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[302/363] Writing tensor blk.33.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[303/363] Writing tensor blk.33.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[304/363] Writing tensor blk.33.attn_output.weight              | size   5120 x   5120  | type F16  | T+  31\n",
      "[305/363] Writing tensor blk.33.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  31\n",
      "[306/363] Writing tensor blk.33.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  32\n",
      "[307/363] Writing tensor blk.33.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  32\n",
      "[308/363] Writing tensor blk.33.attn_norm.weight                | size   5120           | type F32  | T+  32\n",
      "[309/363] Writing tensor blk.33.ffn_norm.weight                 | size   5120           | type F32  | T+  32\n",
      "[310/363] Writing tensor blk.34.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[311/363] Writing tensor blk.34.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[312/363] Writing tensor blk.34.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[313/363] Writing tensor blk.34.attn_output.weight              | size   5120 x   5120  | type F16  | T+  32\n",
      "[314/363] Writing tensor blk.34.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  32\n",
      "[315/363] Writing tensor blk.34.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  33\n",
      "[316/363] Writing tensor blk.34.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  33\n",
      "[317/363] Writing tensor blk.34.attn_norm.weight                | size   5120           | type F32  | T+  33\n",
      "[318/363] Writing tensor blk.34.ffn_norm.weight                 | size   5120           | type F32  | T+  33\n",
      "[319/363] Writing tensor blk.35.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  33\n",
      "[320/363] Writing tensor blk.35.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  33\n",
      "[321/363] Writing tensor blk.35.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  33\n",
      "[322/363] Writing tensor blk.35.attn_output.weight              | size   5120 x   5120  | type F16  | T+  33\n",
      "[323/363] Writing tensor blk.35.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  33\n",
      "[324/363] Writing tensor blk.35.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  34\n",
      "[325/363] Writing tensor blk.35.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  34\n",
      "[326/363] Writing tensor blk.35.attn_norm.weight                | size   5120           | type F32  | T+  34\n",
      "[327/363] Writing tensor blk.35.ffn_norm.weight                 | size   5120           | type F32  | T+  34\n",
      "[328/363] Writing tensor blk.36.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  34\n",
      "[329/363] Writing tensor blk.36.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  34\n",
      "[330/363] Writing tensor blk.36.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  34\n",
      "[331/363] Writing tensor blk.36.attn_output.weight              | size   5120 x   5120  | type F16  | T+  34\n",
      "[332/363] Writing tensor blk.36.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  34\n",
      "[333/363] Writing tensor blk.36.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  35\n",
      "[334/363] Writing tensor blk.36.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  35\n",
      "[335/363] Writing tensor blk.36.attn_norm.weight                | size   5120           | type F32  | T+  35\n",
      "[336/363] Writing tensor blk.36.ffn_norm.weight                 | size   5120           | type F32  | T+  35\n",
      "[337/363] Writing tensor blk.37.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  35\n",
      "[338/363] Writing tensor blk.37.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  35\n",
      "[339/363] Writing tensor blk.37.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  35\n",
      "[340/363] Writing tensor blk.37.attn_output.weight              | size   5120 x   5120  | type F16  | T+  35\n",
      "[341/363] Writing tensor blk.37.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  35\n",
      "[342/363] Writing tensor blk.37.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  36\n",
      "[343/363] Writing tensor blk.37.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  36\n",
      "[344/363] Writing tensor blk.37.attn_norm.weight                | size   5120           | type F32  | T+  36\n",
      "[345/363] Writing tensor blk.37.ffn_norm.weight                 | size   5120           | type F32  | T+  36\n",
      "[346/363] Writing tensor blk.38.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  36\n",
      "[347/363] Writing tensor blk.38.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  36\n",
      "[348/363] Writing tensor blk.38.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  36\n",
      "[349/363] Writing tensor blk.38.attn_output.weight              | size   5120 x   5120  | type F16  | T+  36\n",
      "[350/363] Writing tensor blk.38.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  36\n",
      "[351/363] Writing tensor blk.38.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  37\n",
      "[352/363] Writing tensor blk.38.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  37\n",
      "[353/363] Writing tensor blk.38.attn_norm.weight                | size   5120           | type F32  | T+  37\n",
      "[354/363] Writing tensor blk.38.ffn_norm.weight                 | size   5120           | type F32  | T+  37\n",
      "[355/363] Writing tensor blk.39.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  37\n",
      "[356/363] Writing tensor blk.39.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  37\n",
      "[357/363] Writing tensor blk.39.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  37\n",
      "[358/363] Writing tensor blk.39.attn_output.weight              | size   5120 x   5120  | type F16  | T+  37\n",
      "[359/363] Writing tensor blk.39.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  37\n",
      "[360/363] Writing tensor blk.39.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  38\n",
      "[361/363] Writing tensor blk.39.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  38\n",
      "[362/363] Writing tensor blk.39.attn_norm.weight                | size   5120           | type F32  | T+  38\n",
      "[363/363] Writing tensor blk.39.ffn_norm.weight                 | size   5120           | type F32  | T+  38\n",
      "Wrote models/13B/ggml-model-f16.gguf\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "params = Params(n_vocab=32000, n_embd=6656, n_layer=60, n_ctx=2048, n_ff=17920, n_head=52, n_head_kv=52, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/30B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 6656]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [6656]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 6656]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [6656]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [6656]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [6656]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [6656]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [6656]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [6656]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [6656]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [6656]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [6656]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [6656]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [6656]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [6656]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [6656]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [6656]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [6656]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [6656]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [6656]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [6656]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [6656]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [6656]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [6656]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [6656]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [6656]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [6656]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [6656]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [6656]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [6656]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [6656]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [6656]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [6656]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [6656]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [6656]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [6656]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [6656]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [6656]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [6656]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [6656]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [6656]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [6656]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [6656]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [6656]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [6656]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [6656]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [6656]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [6656]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [6656]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [6656]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [6656]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [6656]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [6656]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [6656]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [6656]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [6656]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [6656]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [6656]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [6656]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [6656]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [6656]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [6656]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [6656]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [6656]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/30B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/543] Writing tensor token_embd.weight                      | size  32000 x   6656  | type F16  | T+   0\n",
      "[  2/543] Writing tensor output_norm.weight                     | size   6656           | type F32  | T+   0\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type F16  | T+   1\n",
      "[  4/543] Writing tensor blk.0.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   1\n",
      "[  5/543] Writing tensor blk.0.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   1\n",
      "[  6/543] Writing tensor blk.0.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   1\n",
      "[  7/543] Writing tensor blk.0.attn_output.weight               | size   6656 x   6656  | type F16  | T+   1\n",
      "[  8/543] Writing tensor blk.0.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   1\n",
      "[  9/543] Writing tensor blk.0.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   1\n",
      "[ 10/543] Writing tensor blk.0.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   2\n",
      "[ 11/543] Writing tensor blk.0.attn_norm.weight                 | size   6656           | type F32  | T+   2\n",
      "[ 12/543] Writing tensor blk.0.ffn_norm.weight                  | size   6656           | type F32  | T+   2\n",
      "[ 13/543] Writing tensor blk.1.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 14/543] Writing tensor blk.1.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 15/543] Writing tensor blk.1.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 16/543] Writing tensor blk.1.attn_output.weight               | size   6656 x   6656  | type F16  | T+   2\n",
      "[ 17/543] Writing tensor blk.1.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   2\n",
      "[ 18/543] Writing tensor blk.1.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   3\n",
      "[ 19/543] Writing tensor blk.1.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   3\n",
      "[ 20/543] Writing tensor blk.1.attn_norm.weight                 | size   6656           | type F32  | T+   3\n",
      "[ 21/543] Writing tensor blk.1.ffn_norm.weight                  | size   6656           | type F32  | T+   3\n",
      "[ 22/543] Writing tensor blk.2.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 23/543] Writing tensor blk.2.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 24/543] Writing tensor blk.2.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 25/543] Writing tensor blk.2.attn_output.weight               | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 26/543] Writing tensor blk.2.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   4\n",
      "[ 27/543] Writing tensor blk.2.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   4\n",
      "[ 28/543] Writing tensor blk.2.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   4\n",
      "[ 29/543] Writing tensor blk.2.attn_norm.weight                 | size   6656           | type F32  | T+   5\n",
      "[ 30/543] Writing tensor blk.2.ffn_norm.weight                  | size   6656           | type F32  | T+   5\n",
      "[ 31/543] Writing tensor blk.3.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 32/543] Writing tensor blk.3.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 33/543] Writing tensor blk.3.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 34/543] Writing tensor blk.3.attn_output.weight               | size   6656 x   6656  | type F16  | T+   5\n",
      "[ 35/543] Writing tensor blk.3.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   5\n",
      "[ 36/543] Writing tensor blk.3.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   6\n",
      "[ 37/543] Writing tensor blk.3.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   6\n",
      "[ 38/543] Writing tensor blk.3.attn_norm.weight                 | size   6656           | type F32  | T+   6\n",
      "[ 39/543] Writing tensor blk.3.ffn_norm.weight                  | size   6656           | type F32  | T+   6\n",
      "[ 40/543] Writing tensor blk.4.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 41/543] Writing tensor blk.4.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 42/543] Writing tensor blk.4.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 43/543] Writing tensor blk.4.attn_output.weight               | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 44/543] Writing tensor blk.4.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   6\n",
      "[ 45/543] Writing tensor blk.4.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   7\n",
      "[ 46/543] Writing tensor blk.4.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   7\n",
      "[ 47/543] Writing tensor blk.4.attn_norm.weight                 | size   6656           | type F32  | T+   7\n",
      "[ 48/543] Writing tensor blk.4.ffn_norm.weight                  | size   6656           | type F32  | T+   7\n",
      "[ 49/543] Writing tensor blk.5.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 50/543] Writing tensor blk.5.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 51/543] Writing tensor blk.5.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 52/543] Writing tensor blk.5.attn_output.weight               | size   6656 x   6656  | type F16  | T+   7\n",
      "[ 53/543] Writing tensor blk.5.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   8\n",
      "[ 54/543] Writing tensor blk.5.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   8\n",
      "[ 55/543] Writing tensor blk.5.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   9\n",
      "[ 56/543] Writing tensor blk.5.attn_norm.weight                 | size   6656           | type F32  | T+   9\n",
      "[ 57/543] Writing tensor blk.5.ffn_norm.weight                  | size   6656           | type F32  | T+   9\n",
      "[ 58/543] Writing tensor blk.6.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 59/543] Writing tensor blk.6.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 60/543] Writing tensor blk.6.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 61/543] Writing tensor blk.6.attn_output.weight               | size   6656 x   6656  | type F16  | T+   9\n",
      "[ 62/543] Writing tensor blk.6.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   9\n",
      "[ 63/543] Writing tensor blk.6.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  10\n",
      "[ 64/543] Writing tensor blk.6.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  10\n",
      "[ 65/543] Writing tensor blk.6.attn_norm.weight                 | size   6656           | type F32  | T+  11\n",
      "[ 66/543] Writing tensor blk.6.ffn_norm.weight                  | size   6656           | type F32  | T+  11\n",
      "[ 67/543] Writing tensor blk.7.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 68/543] Writing tensor blk.7.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 69/543] Writing tensor blk.7.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 70/543] Writing tensor blk.7.attn_output.weight               | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 71/543] Writing tensor blk.7.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  11\n",
      "[ 72/543] Writing tensor blk.7.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  12\n",
      "[ 73/543] Writing tensor blk.7.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 74/543] Writing tensor blk.7.attn_norm.weight                 | size   6656           | type F32  | T+  12\n",
      "[ 75/543] Writing tensor blk.7.ffn_norm.weight                  | size   6656           | type F32  | T+  12\n",
      "[ 76/543] Writing tensor blk.8.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 77/543] Writing tensor blk.8.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 78/543] Writing tensor blk.8.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 79/543] Writing tensor blk.8.attn_output.weight               | size   6656 x   6656  | type F16  | T+  12\n",
      "[ 80/543] Writing tensor blk.8.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 81/543] Writing tensor blk.8.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  13\n",
      "[ 82/543] Writing tensor blk.8.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  13\n",
      "[ 83/543] Writing tensor blk.8.attn_norm.weight                 | size   6656           | type F32  | T+  13\n",
      "[ 84/543] Writing tensor blk.8.ffn_norm.weight                  | size   6656           | type F32  | T+  13\n",
      "[ 85/543] Writing tensor blk.9.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 86/543] Writing tensor blk.9.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 87/543] Writing tensor blk.9.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 88/543] Writing tensor blk.9.attn_output.weight               | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 89/543] Writing tensor blk.9.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  14\n",
      "[ 90/543] Writing tensor blk.9.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  15\n",
      "[ 91/543] Writing tensor blk.9.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  15\n",
      "[ 92/543] Writing tensor blk.9.attn_norm.weight                 | size   6656           | type F32  | T+  15\n",
      "[ 93/543] Writing tensor blk.9.ffn_norm.weight                  | size   6656           | type F32  | T+  15\n",
      "[ 94/543] Writing tensor blk.10.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 95/543] Writing tensor blk.10.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 96/543] Writing tensor blk.10.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 97/543] Writing tensor blk.10.attn_output.weight              | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 98/543] Writing tensor blk.10.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  15\n",
      "[ 99/543] Writing tensor blk.10.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  16\n",
      "[100/543] Writing tensor blk.10.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  16\n",
      "[101/543] Writing tensor blk.10.attn_norm.weight                | size   6656           | type F32  | T+  17\n",
      "[102/543] Writing tensor blk.10.ffn_norm.weight                 | size   6656           | type F32  | T+  17\n",
      "[103/543] Writing tensor blk.11.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  17\n",
      "[104/543] Writing tensor blk.11.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  17\n",
      "[105/543] Writing tensor blk.11.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  17\n",
      "[106/543] Writing tensor blk.11.attn_output.weight              | size   6656 x   6656  | type F16  | T+  17\n",
      "[107/543] Writing tensor blk.11.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  17\n",
      "[108/543] Writing tensor blk.11.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  17\n",
      "[109/543] Writing tensor blk.11.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  18\n",
      "[110/543] Writing tensor blk.11.attn_norm.weight                | size   6656           | type F32  | T+  18\n",
      "[111/543] Writing tensor blk.11.ffn_norm.weight                 | size   6656           | type F32  | T+  18\n",
      "[112/543] Writing tensor blk.12.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[113/543] Writing tensor blk.12.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[114/543] Writing tensor blk.12.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[115/543] Writing tensor blk.12.attn_output.weight              | size   6656 x   6656  | type F16  | T+  18\n",
      "[116/543] Writing tensor blk.12.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  18\n",
      "[117/543] Writing tensor blk.12.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  19\n",
      "[118/543] Writing tensor blk.12.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  19\n",
      "[119/543] Writing tensor blk.12.attn_norm.weight                | size   6656           | type F32  | T+  19\n",
      "[120/543] Writing tensor blk.12.ffn_norm.weight                 | size   6656           | type F32  | T+  19\n",
      "[121/543] Writing tensor blk.13.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[122/543] Writing tensor blk.13.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[123/543] Writing tensor blk.13.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[124/543] Writing tensor blk.13.attn_output.weight              | size   6656 x   6656  | type F16  | T+  19\n",
      "[125/543] Writing tensor blk.13.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  20\n",
      "[126/543] Writing tensor blk.13.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  20\n",
      "[127/543] Writing tensor blk.13.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  21\n",
      "[128/543] Writing tensor blk.13.attn_norm.weight                | size   6656           | type F32  | T+  21\n",
      "[129/543] Writing tensor blk.13.ffn_norm.weight                 | size   6656           | type F32  | T+  21\n",
      "[130/543] Writing tensor blk.14.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  21\n",
      "[131/543] Writing tensor blk.14.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  21\n",
      "[132/543] Writing tensor blk.14.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  21\n",
      "[133/543] Writing tensor blk.14.attn_output.weight              | size   6656 x   6656  | type F16  | T+  21\n",
      "[134/543] Writing tensor blk.14.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  21\n",
      "[135/543] Writing tensor blk.14.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  22\n",
      "[136/543] Writing tensor blk.14.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  22\n",
      "[137/543] Writing tensor blk.14.attn_norm.weight                | size   6656           | type F32  | T+  22\n",
      "[138/543] Writing tensor blk.14.ffn_norm.weight                 | size   6656           | type F32  | T+  22\n",
      "[139/543] Writing tensor blk.15.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[140/543] Writing tensor blk.15.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[141/543] Writing tensor blk.15.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[142/543] Writing tensor blk.15.attn_output.weight              | size   6656 x   6656  | type F16  | T+  22\n",
      "[143/543] Writing tensor blk.15.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  23\n",
      "[144/543] Writing tensor blk.15.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  23\n",
      "[145/543] Writing tensor blk.15.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  24\n",
      "[146/543] Writing tensor blk.15.attn_norm.weight                | size   6656           | type F32  | T+  24\n",
      "[147/543] Writing tensor blk.15.ffn_norm.weight                 | size   6656           | type F32  | T+  24\n",
      "[148/543] Writing tensor blk.16.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  24\n",
      "[149/543] Writing tensor blk.16.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  24\n",
      "[150/543] Writing tensor blk.16.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  24\n",
      "[151/543] Writing tensor blk.16.attn_output.weight              | size   6656 x   6656  | type F16  | T+  24\n",
      "[152/543] Writing tensor blk.16.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  24\n",
      "[153/543] Writing tensor blk.16.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  25\n",
      "[154/543] Writing tensor blk.16.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  25\n",
      "[155/543] Writing tensor blk.16.attn_norm.weight                | size   6656           | type F32  | T+  25\n",
      "[156/543] Writing tensor blk.16.ffn_norm.weight                 | size   6656           | type F32  | T+  25\n",
      "[157/543] Writing tensor blk.17.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[158/543] Writing tensor blk.17.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[159/543] Writing tensor blk.17.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[160/543] Writing tensor blk.17.attn_output.weight              | size   6656 x   6656  | type F16  | T+  25\n",
      "[161/543] Writing tensor blk.17.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  25\n",
      "[162/543] Writing tensor blk.17.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  26\n",
      "[163/543] Writing tensor blk.17.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  26\n",
      "[164/543] Writing tensor blk.17.attn_norm.weight                | size   6656           | type F32  | T+  26\n",
      "[165/543] Writing tensor blk.17.ffn_norm.weight                 | size   6656           | type F32  | T+  26\n",
      "[166/543] Writing tensor blk.18.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  26\n",
      "[167/543] Writing tensor blk.18.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[168/543] Writing tensor blk.18.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[169/543] Writing tensor blk.18.attn_output.weight              | size   6656 x   6656  | type F16  | T+  27\n",
      "[170/543] Writing tensor blk.18.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  27\n",
      "[171/543] Writing tensor blk.18.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  27\n",
      "[172/543] Writing tensor blk.18.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  28\n",
      "[173/543] Writing tensor blk.18.attn_norm.weight                | size   6656           | type F32  | T+  28\n",
      "[174/543] Writing tensor blk.18.ffn_norm.weight                 | size   6656           | type F32  | T+  28\n",
      "[175/543] Writing tensor blk.19.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  28\n",
      "[176/543] Writing tensor blk.19.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  28\n",
      "[177/543] Writing tensor blk.19.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  28\n",
      "[178/543] Writing tensor blk.19.attn_output.weight              | size   6656 x   6656  | type F16  | T+  28\n",
      "[179/543] Writing tensor blk.19.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  28\n",
      "[180/543] Writing tensor blk.19.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  29\n",
      "[181/543] Writing tensor blk.19.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  29\n",
      "[182/543] Writing tensor blk.19.attn_norm.weight                | size   6656           | type F32  | T+  29\n",
      "[183/543] Writing tensor blk.19.ffn_norm.weight                 | size   6656           | type F32  | T+  29\n",
      "[184/543] Writing tensor blk.20.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[185/543] Writing tensor blk.20.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[186/543] Writing tensor blk.20.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[187/543] Writing tensor blk.20.attn_output.weight              | size   6656 x   6656  | type F16  | T+  29\n",
      "[188/543] Writing tensor blk.20.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  30\n",
      "[189/543] Writing tensor blk.20.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  30\n",
      "[190/543] Writing tensor blk.20.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  31\n",
      "[191/543] Writing tensor blk.20.attn_norm.weight                | size   6656           | type F32  | T+  31\n",
      "[192/543] Writing tensor blk.20.ffn_norm.weight                 | size   6656           | type F32  | T+  31\n",
      "[193/543] Writing tensor blk.21.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  31\n",
      "[194/543] Writing tensor blk.21.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  31\n",
      "[195/543] Writing tensor blk.21.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  31\n",
      "[196/543] Writing tensor blk.21.attn_output.weight              | size   6656 x   6656  | type F16  | T+  31\n",
      "[197/543] Writing tensor blk.21.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  31\n",
      "[198/543] Writing tensor blk.21.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  32\n",
      "[199/543] Writing tensor blk.21.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  32\n",
      "[200/543] Writing tensor blk.21.attn_norm.weight                | size   6656           | type F32  | T+  32\n",
      "[201/543] Writing tensor blk.21.ffn_norm.weight                 | size   6656           | type F32  | T+  32\n",
      "[202/543] Writing tensor blk.22.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  32\n",
      "[203/543] Writing tensor blk.22.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[204/543] Writing tensor blk.22.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[205/543] Writing tensor blk.22.attn_output.weight              | size   6656 x   6656  | type F16  | T+  33\n",
      "[206/543] Writing tensor blk.22.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  33\n",
      "[207/543] Writing tensor blk.22.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  33\n",
      "[208/543] Writing tensor blk.22.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  34\n",
      "[209/543] Writing tensor blk.22.attn_norm.weight                | size   6656           | type F32  | T+  34\n",
      "[210/543] Writing tensor blk.22.ffn_norm.weight                 | size   6656           | type F32  | T+  34\n",
      "[211/543] Writing tensor blk.23.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[212/543] Writing tensor blk.23.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[213/543] Writing tensor blk.23.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  34\n",
      "[214/543] Writing tensor blk.23.attn_output.weight              | size   6656 x   6656  | type F16  | T+  34\n",
      "[215/543] Writing tensor blk.23.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  34\n",
      "[216/543] Writing tensor blk.23.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  35\n",
      "[217/543] Writing tensor blk.23.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  35\n",
      "[218/543] Writing tensor blk.23.attn_norm.weight                | size   6656           | type F32  | T+  36\n",
      "[219/543] Writing tensor blk.23.ffn_norm.weight                 | size   6656           | type F32  | T+  36\n",
      "[220/543] Writing tensor blk.24.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  36\n",
      "[221/543] Writing tensor blk.24.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  36\n",
      "[222/543] Writing tensor blk.24.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  36\n",
      "[223/543] Writing tensor blk.24.attn_output.weight              | size   6656 x   6656  | type F16  | T+  36\n",
      "[224/543] Writing tensor blk.24.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  36\n",
      "[225/543] Writing tensor blk.24.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  37\n",
      "[226/543] Writing tensor blk.24.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  37\n",
      "[227/543] Writing tensor blk.24.attn_norm.weight                | size   6656           | type F32  | T+  37\n",
      "[228/543] Writing tensor blk.24.ffn_norm.weight                 | size   6656           | type F32  | T+  37\n",
      "[229/543] Writing tensor blk.25.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[230/543] Writing tensor blk.25.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[231/543] Writing tensor blk.25.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[232/543] Writing tensor blk.25.attn_output.weight              | size   6656 x   6656  | type F16  | T+  37\n",
      "[233/543] Writing tensor blk.25.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  38\n",
      "[234/543] Writing tensor blk.25.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  38\n",
      "[235/543] Writing tensor blk.25.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  39\n",
      "[236/543] Writing tensor blk.25.attn_norm.weight                | size   6656           | type F32  | T+  39\n",
      "[237/543] Writing tensor blk.25.ffn_norm.weight                 | size   6656           | type F32  | T+  39\n",
      "[238/543] Writing tensor blk.26.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  39\n",
      "[239/543] Writing tensor blk.26.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  39\n",
      "[240/543] Writing tensor blk.26.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  39\n",
      "[241/543] Writing tensor blk.26.attn_output.weight              | size   6656 x   6656  | type F16  | T+  39\n",
      "[242/543] Writing tensor blk.26.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  39\n",
      "[243/543] Writing tensor blk.26.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  40\n",
      "[244/543] Writing tensor blk.26.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  41\n",
      "[245/543] Writing tensor blk.26.attn_norm.weight                | size   6656           | type F32  | T+  41\n",
      "[246/543] Writing tensor blk.26.ffn_norm.weight                 | size   6656           | type F32  | T+  41\n",
      "[247/543] Writing tensor blk.27.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[248/543] Writing tensor blk.27.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[249/543] Writing tensor blk.27.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[250/543] Writing tensor blk.27.attn_output.weight              | size   6656 x   6656  | type F16  | T+  41\n",
      "[251/543] Writing tensor blk.27.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  41\n",
      "[252/543] Writing tensor blk.27.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  42\n",
      "[253/543] Writing tensor blk.27.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  42\n",
      "[254/543] Writing tensor blk.27.attn_norm.weight                | size   6656           | type F32  | T+  42\n",
      "[255/543] Writing tensor blk.27.ffn_norm.weight                 | size   6656           | type F32  | T+  42\n",
      "[256/543] Writing tensor blk.28.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  42\n",
      "[257/543] Writing tensor blk.28.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  42\n",
      "[258/543] Writing tensor blk.28.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  43\n",
      "[259/543] Writing tensor blk.28.attn_output.weight              | size   6656 x   6656  | type F16  | T+  43\n",
      "[260/543] Writing tensor blk.28.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  43\n",
      "[261/543] Writing tensor blk.28.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  43\n",
      "[262/543] Writing tensor blk.28.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  44\n",
      "[263/543] Writing tensor blk.28.attn_norm.weight                | size   6656           | type F32  | T+  44\n",
      "[264/543] Writing tensor blk.28.ffn_norm.weight                 | size   6656           | type F32  | T+  44\n",
      "[265/543] Writing tensor blk.29.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[266/543] Writing tensor blk.29.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[267/543] Writing tensor blk.29.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[268/543] Writing tensor blk.29.attn_output.weight              | size   6656 x   6656  | type F16  | T+  44\n",
      "[269/543] Writing tensor blk.29.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  44\n",
      "[270/543] Writing tensor blk.29.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  45\n",
      "[271/543] Writing tensor blk.29.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  46\n",
      "[272/543] Writing tensor blk.29.attn_norm.weight                | size   6656           | type F32  | T+  46\n",
      "[273/543] Writing tensor blk.29.ffn_norm.weight                 | size   6656           | type F32  | T+  46\n",
      "[274/543] Writing tensor blk.30.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[275/543] Writing tensor blk.30.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[276/543] Writing tensor blk.30.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[277/543] Writing tensor blk.30.attn_output.weight              | size   6656 x   6656  | type F16  | T+  46\n",
      "[278/543] Writing tensor blk.30.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  46\n",
      "[279/543] Writing tensor blk.30.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  47\n",
      "[280/543] Writing tensor blk.30.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  47\n",
      "[281/543] Writing tensor blk.30.attn_norm.weight                | size   6656           | type F32  | T+  47\n",
      "[282/543] Writing tensor blk.30.ffn_norm.weight                 | size   6656           | type F32  | T+  47\n",
      "[283/543] Writing tensor blk.31.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  47\n",
      "[284/543] Writing tensor blk.31.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  47\n",
      "[285/543] Writing tensor blk.31.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  47\n",
      "[286/543] Writing tensor blk.31.attn_output.weight              | size   6656 x   6656  | type F16  | T+  47\n",
      "[287/543] Writing tensor blk.31.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  48\n",
      "[288/543] Writing tensor blk.31.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  48\n",
      "[289/543] Writing tensor blk.31.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  49\n",
      "[290/543] Writing tensor blk.31.attn_norm.weight                | size   6656           | type F32  | T+  49\n",
      "[291/543] Writing tensor blk.31.ffn_norm.weight                 | size   6656           | type F32  | T+  49\n",
      "[292/543] Writing tensor blk.32.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[293/543] Writing tensor blk.32.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[294/543] Writing tensor blk.32.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[295/543] Writing tensor blk.32.attn_output.weight              | size   6656 x   6656  | type F16  | T+  49\n",
      "[296/543] Writing tensor blk.32.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  49\n",
      "[297/543] Writing tensor blk.32.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  50\n",
      "[298/543] Writing tensor blk.32.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  50\n",
      "[299/543] Writing tensor blk.32.attn_norm.weight                | size   6656           | type F32  | T+  51\n",
      "[300/543] Writing tensor blk.32.ffn_norm.weight                 | size   6656           | type F32  | T+  51\n",
      "[301/543] Writing tensor blk.33.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[302/543] Writing tensor blk.33.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[303/543] Writing tensor blk.33.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[304/543] Writing tensor blk.33.attn_output.weight              | size   6656 x   6656  | type F16  | T+  51\n",
      "[305/543] Writing tensor blk.33.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  51\n",
      "[306/543] Writing tensor blk.33.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  52\n",
      "[307/543] Writing tensor blk.33.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  52\n",
      "[308/543] Writing tensor blk.33.attn_norm.weight                | size   6656           | type F32  | T+  52\n",
      "[309/543] Writing tensor blk.33.ffn_norm.weight                 | size   6656           | type F32  | T+  52\n",
      "[310/543] Writing tensor blk.34.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  52\n",
      "[311/543] Writing tensor blk.34.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  52\n",
      "[312/543] Writing tensor blk.34.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  52\n",
      "[313/543] Writing tensor blk.34.attn_output.weight              | size   6656 x   6656  | type F16  | T+  52\n",
      "[314/543] Writing tensor blk.34.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  53\n",
      "[315/543] Writing tensor blk.34.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  53\n",
      "[316/543] Writing tensor blk.34.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  54\n",
      "[317/543] Writing tensor blk.34.attn_norm.weight                | size   6656           | type F32  | T+  54\n",
      "[318/543] Writing tensor blk.34.ffn_norm.weight                 | size   6656           | type F32  | T+  54\n",
      "[319/543] Writing tensor blk.35.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[320/543] Writing tensor blk.35.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[321/543] Writing tensor blk.35.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[322/543] Writing tensor blk.35.attn_output.weight              | size   6656 x   6656  | type F16  | T+  54\n",
      "[323/543] Writing tensor blk.35.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  54\n",
      "[324/543] Writing tensor blk.35.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  55\n",
      "[325/543] Writing tensor blk.35.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  56\n",
      "[326/543] Writing tensor blk.35.attn_norm.weight                | size   6656           | type F32  | T+  56\n",
      "[327/543] Writing tensor blk.35.ffn_norm.weight                 | size   6656           | type F32  | T+  56\n",
      "[328/543] Writing tensor blk.36.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[329/543] Writing tensor blk.36.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[330/543] Writing tensor blk.36.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[331/543] Writing tensor blk.36.attn_output.weight              | size   6656 x   6656  | type F16  | T+  56\n",
      "[332/543] Writing tensor blk.36.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  56\n",
      "[333/543] Writing tensor blk.36.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  57\n",
      "[334/543] Writing tensor blk.36.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  57\n",
      "[335/543] Writing tensor blk.36.attn_norm.weight                | size   6656           | type F32  | T+  57\n",
      "[336/543] Writing tensor blk.36.ffn_norm.weight                 | size   6656           | type F32  | T+  57\n",
      "[337/543] Writing tensor blk.37.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[338/543] Writing tensor blk.37.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[339/543] Writing tensor blk.37.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  57\n",
      "[340/543] Writing tensor blk.37.attn_output.weight              | size   6656 x   6656  | type F16  | T+  57\n",
      "[341/543] Writing tensor blk.37.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  58\n",
      "[342/543] Writing tensor blk.37.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  59\n",
      "[343/543] Writing tensor blk.37.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  59\n",
      "[344/543] Writing tensor blk.37.attn_norm.weight                | size   6656           | type F32  | T+  59\n",
      "[345/543] Writing tensor blk.37.ffn_norm.weight                 | size   6656           | type F32  | T+  59\n",
      "[346/543] Writing tensor blk.38.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[347/543] Writing tensor blk.38.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[348/543] Writing tensor blk.38.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[349/543] Writing tensor blk.38.attn_output.weight              | size   6656 x   6656  | type F16  | T+  59\n",
      "[350/543] Writing tensor blk.38.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  59\n",
      "[351/543] Writing tensor blk.38.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  60\n",
      "[352/543] Writing tensor blk.38.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  60\n",
      "[353/543] Writing tensor blk.38.attn_norm.weight                | size   6656           | type F32  | T+  60\n",
      "[354/543] Writing tensor blk.38.ffn_norm.weight                 | size   6656           | type F32  | T+  60\n",
      "[355/543] Writing tensor blk.39.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  60\n",
      "[356/543] Writing tensor blk.39.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  61\n",
      "[357/543] Writing tensor blk.39.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  61\n",
      "[358/543] Writing tensor blk.39.attn_output.weight              | size   6656 x   6656  | type F16  | T+  61\n",
      "[359/543] Writing tensor blk.39.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  61\n",
      "[360/543] Writing tensor blk.39.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  62\n",
      "[361/543] Writing tensor blk.39.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  62\n",
      "[362/543] Writing tensor blk.39.attn_norm.weight                | size   6656           | type F32  | T+  62\n",
      "[363/543] Writing tensor blk.39.ffn_norm.weight                 | size   6656           | type F32  | T+  62\n",
      "[364/543] Writing tensor blk.40.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[365/543] Writing tensor blk.40.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[366/543] Writing tensor blk.40.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[367/543] Writing tensor blk.40.attn_output.weight              | size   6656 x   6656  | type F16  | T+  62\n",
      "[368/543] Writing tensor blk.40.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  63\n",
      "[369/543] Writing tensor blk.40.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  63\n",
      "[370/543] Writing tensor blk.40.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  64\n",
      "[371/543] Writing tensor blk.40.attn_norm.weight                | size   6656           | type F32  | T+  64\n",
      "[372/543] Writing tensor blk.40.ffn_norm.weight                 | size   6656           | type F32  | T+  64\n",
      "[373/543] Writing tensor blk.41.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[374/543] Writing tensor blk.41.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[375/543] Writing tensor blk.41.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[376/543] Writing tensor blk.41.attn_output.weight              | size   6656 x   6656  | type F16  | T+  64\n",
      "[377/543] Writing tensor blk.41.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  64\n",
      "[378/543] Writing tensor blk.41.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  65\n",
      "[379/543] Writing tensor blk.41.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  65\n",
      "[380/543] Writing tensor blk.41.attn_norm.weight                | size   6656           | type F32  | T+  66\n",
      "[381/543] Writing tensor blk.41.ffn_norm.weight                 | size   6656           | type F32  | T+  66\n",
      "[382/543] Writing tensor blk.42.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[383/543] Writing tensor blk.42.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[384/543] Writing tensor blk.42.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  66\n",
      "[385/543] Writing tensor blk.42.attn_output.weight              | size   6656 x   6656  | type F16  | T+  66\n",
      "[386/543] Writing tensor blk.42.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  66\n",
      "[387/543] Writing tensor blk.42.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  67\n",
      "[388/543] Writing tensor blk.42.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  67\n",
      "[389/543] Writing tensor blk.42.attn_norm.weight                | size   6656           | type F32  | T+  67\n",
      "[390/543] Writing tensor blk.42.ffn_norm.weight                 | size   6656           | type F32  | T+  67\n",
      "[391/543] Writing tensor blk.43.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[392/543] Writing tensor blk.43.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[393/543] Writing tensor blk.43.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[394/543] Writing tensor blk.43.attn_output.weight              | size   6656 x   6656  | type F16  | T+  67\n",
      "[395/543] Writing tensor blk.43.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  68\n",
      "[396/543] Writing tensor blk.43.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  68\n",
      "[397/543] Writing tensor blk.43.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  69\n",
      "[398/543] Writing tensor blk.43.attn_norm.weight                | size   6656           | type F32  | T+  69\n",
      "[399/543] Writing tensor blk.43.ffn_norm.weight                 | size   6656           | type F32  | T+  69\n",
      "[400/543] Writing tensor blk.44.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[401/543] Writing tensor blk.44.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[402/543] Writing tensor blk.44.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[403/543] Writing tensor blk.44.attn_output.weight              | size   6656 x   6656  | type F16  | T+  69\n",
      "[404/543] Writing tensor blk.44.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  69\n",
      "[405/543] Writing tensor blk.44.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  70\n",
      "[406/543] Writing tensor blk.44.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  70\n",
      "[407/543] Writing tensor blk.44.attn_norm.weight                | size   6656           | type F32  | T+  70\n",
      "[408/543] Writing tensor blk.44.ffn_norm.weight                 | size   6656           | type F32  | T+  70\n",
      "[409/543] Writing tensor blk.45.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[410/543] Writing tensor blk.45.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[411/543] Writing tensor blk.45.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[412/543] Writing tensor blk.45.attn_output.weight              | size   6656 x   6656  | type F16  | T+  70\n",
      "[413/543] Writing tensor blk.45.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  71\n",
      "[414/543] Writing tensor blk.45.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  71\n",
      "[415/543] Writing tensor blk.45.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  72\n",
      "[416/543] Writing tensor blk.45.attn_norm.weight                | size   6656           | type F32  | T+  72\n",
      "[417/543] Writing tensor blk.45.ffn_norm.weight                 | size   6656           | type F32  | T+  72\n",
      "[418/543] Writing tensor blk.46.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  72\n",
      "[419/543] Writing tensor blk.46.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  72\n",
      "[420/543] Writing tensor blk.46.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  72\n",
      "[421/543] Writing tensor blk.46.attn_output.weight              | size   6656 x   6656  | type F16  | T+  72\n",
      "[422/543] Writing tensor blk.46.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  72\n",
      "[423/543] Writing tensor blk.46.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  73\n",
      "[424/543] Writing tensor blk.46.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  73\n",
      "[425/543] Writing tensor blk.46.attn_norm.weight                | size   6656           | type F32  | T+  74\n",
      "[426/543] Writing tensor blk.46.ffn_norm.weight                 | size   6656           | type F32  | T+  74\n",
      "[427/543] Writing tensor blk.47.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  74\n",
      "[428/543] Writing tensor blk.47.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  74\n",
      "[429/543] Writing tensor blk.47.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  74\n",
      "[430/543] Writing tensor blk.47.attn_output.weight              | size   6656 x   6656  | type F16  | T+  74\n",
      "[431/543] Writing tensor blk.47.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  74\n",
      "[432/543] Writing tensor blk.47.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  75\n",
      "[433/543] Writing tensor blk.47.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  75\n",
      "[434/543] Writing tensor blk.47.attn_norm.weight                | size   6656           | type F32  | T+  76\n",
      "[435/543] Writing tensor blk.47.ffn_norm.weight                 | size   6656           | type F32  | T+  76\n",
      "[436/543] Writing tensor blk.48.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[437/543] Writing tensor blk.48.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[438/543] Writing tensor blk.48.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  76\n",
      "[439/543] Writing tensor blk.48.attn_output.weight              | size   6656 x   6656  | type F16  | T+  76\n",
      "[440/543] Writing tensor blk.48.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  76\n",
      "[441/543] Writing tensor blk.48.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  77\n",
      "[442/543] Writing tensor blk.48.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  77\n",
      "[443/543] Writing tensor blk.48.attn_norm.weight                | size   6656           | type F32  | T+  77\n",
      "[444/543] Writing tensor blk.48.ffn_norm.weight                 | size   6656           | type F32  | T+  77\n",
      "[445/543] Writing tensor blk.49.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  77\n",
      "[446/543] Writing tensor blk.49.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  77\n",
      "[447/543] Writing tensor blk.49.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  77\n",
      "[448/543] Writing tensor blk.49.attn_output.weight              | size   6656 x   6656  | type F16  | T+  77\n",
      "[449/543] Writing tensor blk.49.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  77\n",
      "[450/543] Writing tensor blk.49.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  78\n",
      "[451/543] Writing tensor blk.49.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  79\n",
      "[452/543] Writing tensor blk.49.attn_norm.weight                | size   6656           | type F32  | T+  79\n",
      "[453/543] Writing tensor blk.49.ffn_norm.weight                 | size   6656           | type F32  | T+  79\n",
      "[454/543] Writing tensor blk.50.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[455/543] Writing tensor blk.50.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[456/543] Writing tensor blk.50.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[457/543] Writing tensor blk.50.attn_output.weight              | size   6656 x   6656  | type F16  | T+  79\n",
      "[458/543] Writing tensor blk.50.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  79\n",
      "[459/543] Writing tensor blk.50.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  80\n",
      "[460/543] Writing tensor blk.50.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  80\n",
      "[461/543] Writing tensor blk.50.attn_norm.weight                | size   6656           | type F32  | T+  80\n",
      "[462/543] Writing tensor blk.50.ffn_norm.weight                 | size   6656           | type F32  | T+  80\n",
      "[463/543] Writing tensor blk.51.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[464/543] Writing tensor blk.51.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[465/543] Writing tensor blk.51.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  80\n",
      "[466/543] Writing tensor blk.51.attn_output.weight              | size   6656 x   6656  | type F16  | T+  80\n",
      "[467/543] Writing tensor blk.51.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  81\n",
      "[468/543] Writing tensor blk.51.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  81\n",
      "[469/543] Writing tensor blk.51.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  82\n",
      "[470/543] Writing tensor blk.51.attn_norm.weight                | size   6656           | type F32  | T+  82\n",
      "[471/543] Writing tensor blk.51.ffn_norm.weight                 | size   6656           | type F32  | T+  82\n",
      "[472/543] Writing tensor blk.52.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[473/543] Writing tensor blk.52.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[474/543] Writing tensor blk.52.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  82\n",
      "[475/543] Writing tensor blk.52.attn_output.weight              | size   6656 x   6656  | type F16  | T+  82\n",
      "[476/543] Writing tensor blk.52.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  82\n",
      "[477/543] Writing tensor blk.52.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  83\n",
      "[478/543] Writing tensor blk.52.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  84\n",
      "[479/543] Writing tensor blk.52.attn_norm.weight                | size   6656           | type F32  | T+  84\n",
      "[480/543] Writing tensor blk.52.ffn_norm.weight                 | size   6656           | type F32  | T+  84\n",
      "[481/543] Writing tensor blk.53.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  84\n",
      "[482/543] Writing tensor blk.53.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  84\n",
      "[483/543] Writing tensor blk.53.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  84\n",
      "[484/543] Writing tensor blk.53.attn_output.weight              | size   6656 x   6656  | type F16  | T+  84\n",
      "[485/543] Writing tensor blk.53.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  84\n",
      "[486/543] Writing tensor blk.53.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  85\n",
      "[487/543] Writing tensor blk.53.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  86\n",
      "[488/543] Writing tensor blk.53.attn_norm.weight                | size   6656           | type F32  | T+  86\n",
      "[489/543] Writing tensor blk.53.ffn_norm.weight                 | size   6656           | type F32  | T+  86\n",
      "[490/543] Writing tensor blk.54.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  86\n",
      "[491/543] Writing tensor blk.54.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  86\n",
      "[492/543] Writing tensor blk.54.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  86\n",
      "[493/543] Writing tensor blk.54.attn_output.weight              | size   6656 x   6656  | type F16  | T+  86\n",
      "[494/543] Writing tensor blk.54.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  86\n",
      "[495/543] Writing tensor blk.54.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  87\n",
      "[496/543] Writing tensor blk.54.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  87\n",
      "[497/543] Writing tensor blk.54.attn_norm.weight                | size   6656           | type F32  | T+  87\n",
      "[498/543] Writing tensor blk.54.ffn_norm.weight                 | size   6656           | type F32  | T+  87\n",
      "[499/543] Writing tensor blk.55.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  87\n",
      "[500/543] Writing tensor blk.55.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  87\n",
      "[501/543] Writing tensor blk.55.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  87\n",
      "[502/543] Writing tensor blk.55.attn_output.weight              | size   6656 x   6656  | type F16  | T+  87\n",
      "[503/543] Writing tensor blk.55.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  88\n",
      "[504/543] Writing tensor blk.55.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  88\n",
      "[505/543] Writing tensor blk.55.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  89\n",
      "[506/543] Writing tensor blk.55.attn_norm.weight                | size   6656           | type F32  | T+  89\n",
      "[507/543] Writing tensor blk.55.ffn_norm.weight                 | size   6656           | type F32  | T+  89\n",
      "[508/543] Writing tensor blk.56.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  89\n",
      "[509/543] Writing tensor blk.56.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  89\n",
      "[510/543] Writing tensor blk.56.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  89\n",
      "[511/543] Writing tensor blk.56.attn_output.weight              | size   6656 x   6656  | type F16  | T+  89\n",
      "[512/543] Writing tensor blk.56.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  89\n",
      "[513/543] Writing tensor blk.56.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  90\n",
      "[514/543] Writing tensor blk.56.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  90\n",
      "[515/543] Writing tensor blk.56.attn_norm.weight                | size   6656           | type F32  | T+  90\n",
      "[516/543] Writing tensor blk.56.ffn_norm.weight                 | size   6656           | type F32  | T+  90\n",
      "[517/543] Writing tensor blk.57.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[518/543] Writing tensor blk.57.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[519/543] Writing tensor blk.57.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[520/543] Writing tensor blk.57.attn_output.weight              | size   6656 x   6656  | type F16  | T+  90\n",
      "[521/543] Writing tensor blk.57.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  91\n",
      "[522/543] Writing tensor blk.57.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  92\n",
      "[523/543] Writing tensor blk.57.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  92\n",
      "[524/543] Writing tensor blk.57.attn_norm.weight                | size   6656           | type F32  | T+  92\n",
      "[525/543] Writing tensor blk.57.ffn_norm.weight                 | size   6656           | type F32  | T+  92\n",
      "[526/543] Writing tensor blk.58.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  92\n",
      "[527/543] Writing tensor blk.58.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  92\n",
      "[528/543] Writing tensor blk.58.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  92\n",
      "[529/543] Writing tensor blk.58.attn_output.weight              | size   6656 x   6656  | type F16  | T+  92\n",
      "[530/543] Writing tensor blk.58.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  92\n",
      "[531/543] Writing tensor blk.58.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  93\n",
      "[532/543] Writing tensor blk.58.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  93\n",
      "[533/543] Writing tensor blk.58.attn_norm.weight                | size   6656           | type F32  | T+  93\n",
      "[534/543] Writing tensor blk.58.ffn_norm.weight                 | size   6656           | type F32  | T+  93\n",
      "[535/543] Writing tensor blk.59.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  93\n",
      "[536/543] Writing tensor blk.59.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  93\n",
      "[537/543] Writing tensor blk.59.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  94\n",
      "[538/543] Writing tensor blk.59.attn_output.weight              | size   6656 x   6656  | type F16  | T+  94\n",
      "[539/543] Writing tensor blk.59.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  94\n",
      "[540/543] Writing tensor blk.59.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  95\n",
      "[541/543] Writing tensor blk.59.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  95\n",
      "[542/543] Writing tensor blk.59.attn_norm.weight                | size   6656           | type F32  | T+  95\n",
      "[543/543] Writing tensor blk.59.ffn_norm.weight                 | size   6656           | type F32  | T+  95\n",
      "Wrote models/30B/ggml-model-f16.gguf\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "params = Params(n_vocab=32000, n_embd=8192, n_layer=80, n_ctx=4096, n_ff=22016, n_head=64, n_head_kv=64, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/65B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 8192]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [8192]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 8192]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [8192]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [8192]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [8192]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [8192]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [8192]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [8192]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [8192]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [8192]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [8192]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [8192]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [8192]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [8192]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [8192]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [8192]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [8192]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [8192]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [8192]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [8192]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [8192]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [8192]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [8192]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [8192]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [8192]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [8192]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [8192]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [8192]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [8192]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [8192]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [8192]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [8192]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [8192]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [8192]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [8192]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [8192]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [8192]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [8192]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [8192]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [8192]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [8192]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [8192]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [8192]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [8192]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [8192]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [8192]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [8192]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [8192]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [8192]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [8192]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [8192]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [8192]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [8192]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [8192]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [8192]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [8192]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [8192]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [8192]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [8192]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [8192]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [8192]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [8192]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.60.attention.wq.weight                    -> blk.60.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wk.weight                    -> blk.60.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wv.weight                    -> blk.60.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wo.weight                    -> blk.60.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.60.feed_forward.w1.weight                 -> blk.60.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.60.feed_forward.w2.weight                 -> blk.60.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.60.feed_forward.w3.weight                 -> blk.60.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.60.attention_norm.weight                  -> blk.60.attn_norm.weight                  | F16    | [8192]\n",
      "layers.60.ffn_norm.weight                        -> blk.60.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.61.attention.wq.weight                    -> blk.61.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wk.weight                    -> blk.61.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wv.weight                    -> blk.61.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wo.weight                    -> blk.61.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.61.feed_forward.w1.weight                 -> blk.61.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.61.feed_forward.w2.weight                 -> blk.61.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.61.feed_forward.w3.weight                 -> blk.61.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.61.attention_norm.weight                  -> blk.61.attn_norm.weight                  | F16    | [8192]\n",
      "layers.61.ffn_norm.weight                        -> blk.61.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.62.attention.wq.weight                    -> blk.62.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wk.weight                    -> blk.62.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wv.weight                    -> blk.62.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wo.weight                    -> blk.62.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.62.feed_forward.w1.weight                 -> blk.62.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.62.feed_forward.w2.weight                 -> blk.62.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.62.feed_forward.w3.weight                 -> blk.62.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.62.attention_norm.weight                  -> blk.62.attn_norm.weight                  | F16    | [8192]\n",
      "layers.62.ffn_norm.weight                        -> blk.62.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.63.attention.wq.weight                    -> blk.63.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wk.weight                    -> blk.63.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wv.weight                    -> blk.63.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wo.weight                    -> blk.63.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.63.feed_forward.w1.weight                 -> blk.63.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.63.feed_forward.w2.weight                 -> blk.63.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.63.feed_forward.w3.weight                 -> blk.63.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.63.attention_norm.weight                  -> blk.63.attn_norm.weight                  | F16    | [8192]\n",
      "layers.63.ffn_norm.weight                        -> blk.63.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.64.attention.wq.weight                    -> blk.64.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wk.weight                    -> blk.64.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wv.weight                    -> blk.64.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wo.weight                    -> blk.64.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.64.feed_forward.w1.weight                 -> blk.64.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.64.feed_forward.w2.weight                 -> blk.64.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.64.feed_forward.w3.weight                 -> blk.64.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.64.attention_norm.weight                  -> blk.64.attn_norm.weight                  | F16    | [8192]\n",
      "layers.64.ffn_norm.weight                        -> blk.64.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.65.attention.wq.weight                    -> blk.65.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wk.weight                    -> blk.65.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wv.weight                    -> blk.65.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wo.weight                    -> blk.65.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.65.feed_forward.w1.weight                 -> blk.65.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.65.feed_forward.w2.weight                 -> blk.65.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.65.feed_forward.w3.weight                 -> blk.65.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.65.attention_norm.weight                  -> blk.65.attn_norm.weight                  | F16    | [8192]\n",
      "layers.65.ffn_norm.weight                        -> blk.65.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.66.attention.wq.weight                    -> blk.66.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wk.weight                    -> blk.66.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wv.weight                    -> blk.66.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wo.weight                    -> blk.66.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.66.feed_forward.w1.weight                 -> blk.66.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.66.feed_forward.w2.weight                 -> blk.66.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.66.feed_forward.w3.weight                 -> blk.66.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.66.attention_norm.weight                  -> blk.66.attn_norm.weight                  | F16    | [8192]\n",
      "layers.66.ffn_norm.weight                        -> blk.66.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.67.attention.wq.weight                    -> blk.67.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wk.weight                    -> blk.67.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wv.weight                    -> blk.67.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wo.weight                    -> blk.67.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.67.feed_forward.w1.weight                 -> blk.67.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.67.feed_forward.w2.weight                 -> blk.67.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.67.feed_forward.w3.weight                 -> blk.67.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.67.attention_norm.weight                  -> blk.67.attn_norm.weight                  | F16    | [8192]\n",
      "layers.67.ffn_norm.weight                        -> blk.67.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.68.attention.wq.weight                    -> blk.68.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wk.weight                    -> blk.68.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wv.weight                    -> blk.68.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wo.weight                    -> blk.68.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.68.feed_forward.w1.weight                 -> blk.68.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.68.feed_forward.w2.weight                 -> blk.68.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.68.feed_forward.w3.weight                 -> blk.68.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.68.attention_norm.weight                  -> blk.68.attn_norm.weight                  | F16    | [8192]\n",
      "layers.68.ffn_norm.weight                        -> blk.68.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.69.attention.wq.weight                    -> blk.69.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wk.weight                    -> blk.69.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wv.weight                    -> blk.69.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wo.weight                    -> blk.69.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.69.feed_forward.w1.weight                 -> blk.69.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.69.feed_forward.w2.weight                 -> blk.69.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.69.feed_forward.w3.weight                 -> blk.69.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.69.attention_norm.weight                  -> blk.69.attn_norm.weight                  | F16    | [8192]\n",
      "layers.69.ffn_norm.weight                        -> blk.69.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.70.attention.wq.weight                    -> blk.70.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wk.weight                    -> blk.70.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wv.weight                    -> blk.70.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wo.weight                    -> blk.70.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.70.feed_forward.w1.weight                 -> blk.70.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.70.feed_forward.w2.weight                 -> blk.70.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.70.feed_forward.w3.weight                 -> blk.70.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.70.attention_norm.weight                  -> blk.70.attn_norm.weight                  | F16    | [8192]\n",
      "layers.70.ffn_norm.weight                        -> blk.70.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.71.attention.wq.weight                    -> blk.71.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wk.weight                    -> blk.71.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wv.weight                    -> blk.71.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wo.weight                    -> blk.71.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.71.feed_forward.w1.weight                 -> blk.71.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.71.feed_forward.w2.weight                 -> blk.71.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.71.feed_forward.w3.weight                 -> blk.71.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.71.attention_norm.weight                  -> blk.71.attn_norm.weight                  | F16    | [8192]\n",
      "layers.71.ffn_norm.weight                        -> blk.71.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.72.attention.wq.weight                    -> blk.72.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wk.weight                    -> blk.72.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wv.weight                    -> blk.72.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wo.weight                    -> blk.72.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.72.feed_forward.w1.weight                 -> blk.72.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.72.feed_forward.w2.weight                 -> blk.72.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.72.feed_forward.w3.weight                 -> blk.72.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.72.attention_norm.weight                  -> blk.72.attn_norm.weight                  | F16    | [8192]\n",
      "layers.72.ffn_norm.weight                        -> blk.72.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.73.attention.wq.weight                    -> blk.73.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wk.weight                    -> blk.73.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wv.weight                    -> blk.73.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wo.weight                    -> blk.73.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.73.feed_forward.w1.weight                 -> blk.73.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.73.feed_forward.w2.weight                 -> blk.73.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.73.feed_forward.w3.weight                 -> blk.73.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.73.attention_norm.weight                  -> blk.73.attn_norm.weight                  | F16    | [8192]\n",
      "layers.73.ffn_norm.weight                        -> blk.73.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.74.attention.wq.weight                    -> blk.74.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wk.weight                    -> blk.74.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wv.weight                    -> blk.74.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wo.weight                    -> blk.74.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.74.feed_forward.w1.weight                 -> blk.74.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.74.feed_forward.w2.weight                 -> blk.74.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.74.feed_forward.w3.weight                 -> blk.74.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.74.attention_norm.weight                  -> blk.74.attn_norm.weight                  | F16    | [8192]\n",
      "layers.74.ffn_norm.weight                        -> blk.74.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.75.attention.wq.weight                    -> blk.75.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wk.weight                    -> blk.75.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wv.weight                    -> blk.75.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wo.weight                    -> blk.75.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.75.feed_forward.w1.weight                 -> blk.75.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.75.feed_forward.w2.weight                 -> blk.75.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.75.feed_forward.w3.weight                 -> blk.75.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.75.attention_norm.weight                  -> blk.75.attn_norm.weight                  | F16    | [8192]\n",
      "layers.75.ffn_norm.weight                        -> blk.75.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.76.attention.wq.weight                    -> blk.76.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wk.weight                    -> blk.76.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wv.weight                    -> blk.76.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wo.weight                    -> blk.76.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.76.feed_forward.w1.weight                 -> blk.76.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.76.feed_forward.w2.weight                 -> blk.76.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.76.feed_forward.w3.weight                 -> blk.76.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.76.attention_norm.weight                  -> blk.76.attn_norm.weight                  | F16    | [8192]\n",
      "layers.76.ffn_norm.weight                        -> blk.76.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.77.attention.wq.weight                    -> blk.77.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wk.weight                    -> blk.77.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wv.weight                    -> blk.77.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wo.weight                    -> blk.77.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.77.feed_forward.w1.weight                 -> blk.77.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.77.feed_forward.w2.weight                 -> blk.77.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.77.feed_forward.w3.weight                 -> blk.77.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.77.attention_norm.weight                  -> blk.77.attn_norm.weight                  | F16    | [8192]\n",
      "layers.77.ffn_norm.weight                        -> blk.77.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.78.attention.wq.weight                    -> blk.78.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wk.weight                    -> blk.78.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wv.weight                    -> blk.78.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wo.weight                    -> blk.78.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.78.feed_forward.w1.weight                 -> blk.78.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.78.feed_forward.w2.weight                 -> blk.78.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.78.feed_forward.w3.weight                 -> blk.78.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.78.attention_norm.weight                  -> blk.78.attn_norm.weight                  | F16    | [8192]\n",
      "layers.78.ffn_norm.weight                        -> blk.78.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.79.attention.wq.weight                    -> blk.79.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wk.weight                    -> blk.79.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wv.weight                    -> blk.79.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wo.weight                    -> blk.79.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.79.feed_forward.w1.weight                 -> blk.79.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.79.feed_forward.w2.weight                 -> blk.79.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.79.feed_forward.w3.weight                 -> blk.79.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.79.attention_norm.weight                  -> blk.79.attn_norm.weight                  | F16    | [8192]\n",
      "layers.79.ffn_norm.weight                        -> blk.79.ffn_norm.weight                   | F16    | [8192]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/65B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/723] Writing tensor token_embd.weight                      | size  32000 x   8192  | type F16  | T+   1\n",
      "[  2/723] Writing tensor output_norm.weight                     | size   8192           | type F32  | T+   1\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type F16  | T+   1\n",
      "[  4/723] Writing tensor blk.0.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  5/723] Writing tensor blk.0.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  6/723] Writing tensor blk.0.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  7/723] Writing tensor blk.0.attn_output.weight               | size   8192 x   8192  | type F16  | T+   1\n",
      "[  8/723] Writing tensor blk.0.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   2\n",
      "[  9/723] Writing tensor blk.0.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   2\n",
      "[ 10/723] Writing tensor blk.0.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   2\n",
      "[ 11/723] Writing tensor blk.0.attn_norm.weight                 | size   8192           | type F32  | T+   2\n",
      "[ 12/723] Writing tensor blk.0.ffn_norm.weight                  | size   8192           | type F32  | T+   2\n",
      "[ 13/723] Writing tensor blk.1.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 14/723] Writing tensor blk.1.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 15/723] Writing tensor blk.1.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 16/723] Writing tensor blk.1.attn_output.weight               | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 17/723] Writing tensor blk.1.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   3\n",
      "[ 18/723] Writing tensor blk.1.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   3\n",
      "[ 19/723] Writing tensor blk.1.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   3\n",
      "[ 20/723] Writing tensor blk.1.attn_norm.weight                 | size   8192           | type F32  | T+   3\n",
      "[ 21/723] Writing tensor blk.1.ffn_norm.weight                  | size   8192           | type F32  | T+   3\n",
      "[ 22/723] Writing tensor blk.2.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 23/723] Writing tensor blk.2.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   3\n",
      "[ 24/723] Writing tensor blk.2.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   4\n",
      "[ 25/723] Writing tensor blk.2.attn_output.weight               | size   8192 x   8192  | type F16  | T+   4\n",
      "[ 26/723] Writing tensor blk.2.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   4\n",
      "[ 27/723] Writing tensor blk.2.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   4\n",
      "[ 28/723] Writing tensor blk.2.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   4\n",
      "[ 29/723] Writing tensor blk.2.attn_norm.weight                 | size   8192           | type F32  | T+   5\n",
      "[ 30/723] Writing tensor blk.2.ffn_norm.weight                  | size   8192           | type F32  | T+   5\n",
      "[ 31/723] Writing tensor blk.3.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 32/723] Writing tensor blk.3.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 33/723] Writing tensor blk.3.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 34/723] Writing tensor blk.3.attn_output.weight               | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 35/723] Writing tensor blk.3.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   5\n",
      "[ 36/723] Writing tensor blk.3.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   6\n",
      "[ 37/723] Writing tensor blk.3.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   6\n",
      "[ 38/723] Writing tensor blk.3.attn_norm.weight                 | size   8192           | type F32  | T+   6\n",
      "[ 39/723] Writing tensor blk.3.ffn_norm.weight                  | size   8192           | type F32  | T+   6\n",
      "[ 40/723] Writing tensor blk.4.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 41/723] Writing tensor blk.4.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 42/723] Writing tensor blk.4.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 43/723] Writing tensor blk.4.attn_output.weight               | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 44/723] Writing tensor blk.4.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   7\n",
      "[ 45/723] Writing tensor blk.4.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   7\n",
      "[ 46/723] Writing tensor blk.4.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   7\n",
      "[ 47/723] Writing tensor blk.4.attn_norm.weight                 | size   8192           | type F32  | T+   8\n",
      "[ 48/723] Writing tensor blk.4.ffn_norm.weight                  | size   8192           | type F32  | T+   8\n",
      "[ 49/723] Writing tensor blk.5.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 50/723] Writing tensor blk.5.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 51/723] Writing tensor blk.5.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 52/723] Writing tensor blk.5.attn_output.weight               | size   8192 x   8192  | type F16  | T+   8\n",
      "[ 53/723] Writing tensor blk.5.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   9\n",
      "[ 54/723] Writing tensor blk.5.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   9\n",
      "[ 55/723] Writing tensor blk.5.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   9\n",
      "[ 56/723] Writing tensor blk.5.attn_norm.weight                 | size   8192           | type F32  | T+  10\n",
      "[ 57/723] Writing tensor blk.5.ffn_norm.weight                  | size   8192           | type F32  | T+  10\n",
      "[ 58/723] Writing tensor blk.6.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  10\n",
      "[ 59/723] Writing tensor blk.6.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  10\n",
      "[ 60/723] Writing tensor blk.6.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  10\n",
      "[ 61/723] Writing tensor blk.6.attn_output.weight               | size   8192 x   8192  | type F16  | T+  10\n",
      "[ 62/723] Writing tensor blk.6.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  10\n",
      "[ 63/723] Writing tensor blk.6.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  10\n",
      "[ 64/723] Writing tensor blk.6.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  10\n",
      "[ 65/723] Writing tensor blk.6.attn_norm.weight                 | size   8192           | type F32  | T+  11\n",
      "[ 66/723] Writing tensor blk.6.ffn_norm.weight                  | size   8192           | type F32  | T+  11\n",
      "[ 67/723] Writing tensor blk.7.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 68/723] Writing tensor blk.7.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 69/723] Writing tensor blk.7.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 70/723] Writing tensor blk.7.attn_output.weight               | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 71/723] Writing tensor blk.7.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  11\n",
      "[ 72/723] Writing tensor blk.7.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  11\n",
      "[ 73/723] Writing tensor blk.7.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 74/723] Writing tensor blk.7.attn_norm.weight                 | size   8192           | type F32  | T+  12\n",
      "[ 75/723] Writing tensor blk.7.ffn_norm.weight                  | size   8192           | type F32  | T+  12\n",
      "[ 76/723] Writing tensor blk.8.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 77/723] Writing tensor blk.8.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 78/723] Writing tensor blk.8.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 79/723] Writing tensor blk.8.attn_output.weight               | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 80/723] Writing tensor blk.8.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 81/723] Writing tensor blk.8.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  13\n",
      "[ 82/723] Writing tensor blk.8.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  13\n",
      "[ 83/723] Writing tensor blk.8.attn_norm.weight                 | size   8192           | type F32  | T+  13\n",
      "[ 84/723] Writing tensor blk.8.ffn_norm.weight                  | size   8192           | type F32  | T+  13\n",
      "[ 85/723] Writing tensor blk.9.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 86/723] Writing tensor blk.9.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 87/723] Writing tensor blk.9.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 88/723] Writing tensor blk.9.attn_output.weight               | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 89/723] Writing tensor blk.9.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  14\n",
      "[ 90/723] Writing tensor blk.9.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  14\n",
      "[ 91/723] Writing tensor blk.9.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  14\n",
      "[ 92/723] Writing tensor blk.9.attn_norm.weight                 | size   8192           | type F32  | T+  14\n",
      "[ 93/723] Writing tensor blk.9.ffn_norm.weight                  | size   8192           | type F32  | T+  14\n",
      "[ 94/723] Writing tensor blk.10.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 95/723] Writing tensor blk.10.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 96/723] Writing tensor blk.10.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 97/723] Writing tensor blk.10.attn_output.weight              | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 98/723] Writing tensor blk.10.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  15\n",
      "[ 99/723] Writing tensor blk.10.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  15\n",
      "[100/723] Writing tensor blk.10.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  16\n",
      "[101/723] Writing tensor blk.10.attn_norm.weight                | size   8192           | type F32  | T+  16\n",
      "[102/723] Writing tensor blk.10.ffn_norm.weight                 | size   8192           | type F32  | T+  16\n",
      "[103/723] Writing tensor blk.11.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  16\n",
      "[104/723] Writing tensor blk.11.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  16\n",
      "[105/723] Writing tensor blk.11.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  16\n",
      "[106/723] Writing tensor blk.11.attn_output.weight              | size   8192 x   8192  | type F16  | T+  16\n",
      "[107/723] Writing tensor blk.11.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  16\n",
      "[108/723] Writing tensor blk.11.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  17\n",
      "[109/723] Writing tensor blk.11.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  17\n",
      "[110/723] Writing tensor blk.11.attn_norm.weight                | size   8192           | type F32  | T+  17\n",
      "[111/723] Writing tensor blk.11.ffn_norm.weight                 | size   8192           | type F32  | T+  17\n",
      "[112/723] Writing tensor blk.12.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[113/723] Writing tensor blk.12.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[114/723] Writing tensor blk.12.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[115/723] Writing tensor blk.12.attn_output.weight              | size   8192 x   8192  | type F16  | T+  18\n",
      "[116/723] Writing tensor blk.12.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  18\n",
      "[117/723] Writing tensor blk.12.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  18\n",
      "[118/723] Writing tensor blk.12.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  18\n",
      "[119/723] Writing tensor blk.12.attn_norm.weight                | size   8192           | type F32  | T+  19\n",
      "[120/723] Writing tensor blk.12.ffn_norm.weight                 | size   8192           | type F32  | T+  19\n",
      "[121/723] Writing tensor blk.13.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[122/723] Writing tensor blk.13.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[123/723] Writing tensor blk.13.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[124/723] Writing tensor blk.13.attn_output.weight              | size   8192 x   8192  | type F16  | T+  19\n",
      "[125/723] Writing tensor blk.13.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  19\n",
      "[126/723] Writing tensor blk.13.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  19\n",
      "[127/723] Writing tensor blk.13.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  20\n",
      "[128/723] Writing tensor blk.13.attn_norm.weight                | size   8192           | type F32  | T+  20\n",
      "[129/723] Writing tensor blk.13.ffn_norm.weight                 | size   8192           | type F32  | T+  20\n",
      "[130/723] Writing tensor blk.14.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  20\n",
      "[131/723] Writing tensor blk.14.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  20\n",
      "[132/723] Writing tensor blk.14.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  20\n",
      "[133/723] Writing tensor blk.14.attn_output.weight              | size   8192 x   8192  | type F16  | T+  20\n",
      "[134/723] Writing tensor blk.14.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  21\n",
      "[135/723] Writing tensor blk.14.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  21\n",
      "[136/723] Writing tensor blk.14.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  21\n",
      "[137/723] Writing tensor blk.14.attn_norm.weight                | size   8192           | type F32  | T+  21\n",
      "[138/723] Writing tensor blk.14.ffn_norm.weight                 | size   8192           | type F32  | T+  21\n",
      "[139/723] Writing tensor blk.15.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[140/723] Writing tensor blk.15.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[141/723] Writing tensor blk.15.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[142/723] Writing tensor blk.15.attn_output.weight              | size   8192 x   8192  | type F16  | T+  21\n",
      "[143/723] Writing tensor blk.15.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  22\n",
      "[144/723] Writing tensor blk.15.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  22\n",
      "[145/723] Writing tensor blk.15.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  22\n",
      "[146/723] Writing tensor blk.15.attn_norm.weight                | size   8192           | type F32  | T+  22\n",
      "[147/723] Writing tensor blk.15.ffn_norm.weight                 | size   8192           | type F32  | T+  22\n",
      "[148/723] Writing tensor blk.16.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  22\n",
      "[149/723] Writing tensor blk.16.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  22\n",
      "[150/723] Writing tensor blk.16.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  22\n",
      "[151/723] Writing tensor blk.16.attn_output.weight              | size   8192 x   8192  | type F16  | T+  22\n",
      "[152/723] Writing tensor blk.16.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  23\n",
      "[153/723] Writing tensor blk.16.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  23\n",
      "[154/723] Writing tensor blk.16.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  24\n",
      "[155/723] Writing tensor blk.16.attn_norm.weight                | size   8192           | type F32  | T+  24\n",
      "[156/723] Writing tensor blk.16.ffn_norm.weight                 | size   8192           | type F32  | T+  24\n",
      "[157/723] Writing tensor blk.17.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[158/723] Writing tensor blk.17.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[159/723] Writing tensor blk.17.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[160/723] Writing tensor blk.17.attn_output.weight              | size   8192 x   8192  | type F16  | T+  24\n",
      "[161/723] Writing tensor blk.17.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  24\n",
      "[162/723] Writing tensor blk.17.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  25\n",
      "[163/723] Writing tensor blk.17.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  25\n",
      "[164/723] Writing tensor blk.17.attn_norm.weight                | size   8192           | type F32  | T+  25\n",
      "[165/723] Writing tensor blk.17.ffn_norm.weight                 | size   8192           | type F32  | T+  25\n",
      "[166/723] Writing tensor blk.18.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[167/723] Writing tensor blk.18.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[168/723] Writing tensor blk.18.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[169/723] Writing tensor blk.18.attn_output.weight              | size   8192 x   8192  | type F16  | T+  25\n",
      "[170/723] Writing tensor blk.18.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  26\n",
      "[171/723] Writing tensor blk.18.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  26\n",
      "[172/723] Writing tensor blk.18.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  26\n",
      "[173/723] Writing tensor blk.18.attn_norm.weight                | size   8192           | type F32  | T+  26\n",
      "[174/723] Writing tensor blk.18.ffn_norm.weight                 | size   8192           | type F32  | T+  26\n",
      "[175/723] Writing tensor blk.19.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[176/723] Writing tensor blk.19.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[177/723] Writing tensor blk.19.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  27\n",
      "[178/723] Writing tensor blk.19.attn_output.weight              | size   8192 x   8192  | type F16  | T+  27\n",
      "[179/723] Writing tensor blk.19.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  27\n",
      "[180/723] Writing tensor blk.19.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  27\n",
      "[181/723] Writing tensor blk.19.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  27\n",
      "[182/723] Writing tensor blk.19.attn_norm.weight                | size   8192           | type F32  | T+  28\n",
      "[183/723] Writing tensor blk.19.ffn_norm.weight                 | size   8192           | type F32  | T+  28\n",
      "[184/723] Writing tensor blk.20.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[185/723] Writing tensor blk.20.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[186/723] Writing tensor blk.20.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[187/723] Writing tensor blk.20.attn_output.weight              | size   8192 x   8192  | type F16  | T+  28\n",
      "[188/723] Writing tensor blk.20.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  28\n",
      "[189/723] Writing tensor blk.20.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  28\n",
      "[190/723] Writing tensor blk.20.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  29\n",
      "[191/723] Writing tensor blk.20.attn_norm.weight                | size   8192           | type F32  | T+  29\n",
      "[192/723] Writing tensor blk.20.ffn_norm.weight                 | size   8192           | type F32  | T+  29\n",
      "[193/723] Writing tensor blk.21.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  29\n",
      "[194/723] Writing tensor blk.21.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  29\n",
      "[195/723] Writing tensor blk.21.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  29\n",
      "[196/723] Writing tensor blk.21.attn_output.weight              | size   8192 x   8192  | type F16  | T+  29\n",
      "[197/723] Writing tensor blk.21.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  29\n",
      "[198/723] Writing tensor blk.21.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  30\n",
      "[199/723] Writing tensor blk.21.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  30\n",
      "[200/723] Writing tensor blk.21.attn_norm.weight                | size   8192           | type F32  | T+  30\n",
      "[201/723] Writing tensor blk.21.ffn_norm.weight                 | size   8192           | type F32  | T+  30\n",
      "[202/723] Writing tensor blk.22.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[203/723] Writing tensor blk.22.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[204/723] Writing tensor blk.22.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[205/723] Writing tensor blk.22.attn_output.weight              | size   8192 x   8192  | type F16  | T+  30\n",
      "[206/723] Writing tensor blk.22.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  31\n",
      "[207/723] Writing tensor blk.22.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  31\n",
      "[208/723] Writing tensor blk.22.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  31\n",
      "[209/723] Writing tensor blk.22.attn_norm.weight                | size   8192           | type F32  | T+  31\n",
      "[210/723] Writing tensor blk.22.ffn_norm.weight                 | size   8192           | type F32  | T+  31\n",
      "[211/723] Writing tensor blk.23.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[212/723] Writing tensor blk.23.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[213/723] Writing tensor blk.23.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[214/723] Writing tensor blk.23.attn_output.weight              | size   8192 x   8192  | type F16  | T+  31\n",
      "[215/723] Writing tensor blk.23.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  32\n",
      "[216/723] Writing tensor blk.23.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  32\n",
      "[217/723] Writing tensor blk.23.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  32\n",
      "[218/723] Writing tensor blk.23.attn_norm.weight                | size   8192           | type F32  | T+  32\n",
      "[219/723] Writing tensor blk.23.ffn_norm.weight                 | size   8192           | type F32  | T+  32\n",
      "[220/723] Writing tensor blk.24.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  32\n",
      "[221/723] Writing tensor blk.24.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  32\n",
      "[222/723] Writing tensor blk.24.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  32\n",
      "[223/723] Writing tensor blk.24.attn_output.weight              | size   8192 x   8192  | type F16  | T+  33\n",
      "[224/723] Writing tensor blk.24.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  33\n",
      "[225/723] Writing tensor blk.24.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  33\n",
      "[226/723] Writing tensor blk.24.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  33\n",
      "[227/723] Writing tensor blk.24.attn_norm.weight                | size   8192           | type F32  | T+  34\n",
      "[228/723] Writing tensor blk.24.ffn_norm.weight                 | size   8192           | type F32  | T+  34\n",
      "[229/723] Writing tensor blk.25.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[230/723] Writing tensor blk.25.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[231/723] Writing tensor blk.25.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[232/723] Writing tensor blk.25.attn_output.weight              | size   8192 x   8192  | type F16  | T+  34\n",
      "[233/723] Writing tensor blk.25.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  34\n",
      "[234/723] Writing tensor blk.25.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  35\n",
      "[235/723] Writing tensor blk.25.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  35\n",
      "[236/723] Writing tensor blk.25.attn_norm.weight                | size   8192           | type F32  | T+  35\n",
      "[237/723] Writing tensor blk.25.ffn_norm.weight                 | size   8192           | type F32  | T+  35\n",
      "[238/723] Writing tensor blk.26.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  35\n",
      "[239/723] Writing tensor blk.26.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  35\n",
      "[240/723] Writing tensor blk.26.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  35\n",
      "[241/723] Writing tensor blk.26.attn_output.weight              | size   8192 x   8192  | type F16  | T+  35\n",
      "[242/723] Writing tensor blk.26.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  36\n",
      "[243/723] Writing tensor blk.26.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  36\n",
      "[244/723] Writing tensor blk.26.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  36\n",
      "[245/723] Writing tensor blk.26.attn_norm.weight                | size   8192           | type F32  | T+  36\n",
      "[246/723] Writing tensor blk.26.ffn_norm.weight                 | size   8192           | type F32  | T+  36\n",
      "[247/723] Writing tensor blk.27.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[248/723] Writing tensor blk.27.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[249/723] Writing tensor blk.27.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[250/723] Writing tensor blk.27.attn_output.weight              | size   8192 x   8192  | type F16  | T+  36\n",
      "[251/723] Writing tensor blk.27.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  37\n",
      "[252/723] Writing tensor blk.27.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  37\n",
      "[253/723] Writing tensor blk.27.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  37\n",
      "[254/723] Writing tensor blk.27.attn_norm.weight                | size   8192           | type F32  | T+  37\n",
      "[255/723] Writing tensor blk.27.ffn_norm.weight                 | size   8192           | type F32  | T+  37\n",
      "[256/723] Writing tensor blk.28.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[257/723] Writing tensor blk.28.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[258/723] Writing tensor blk.28.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[259/723] Writing tensor blk.28.attn_output.weight              | size   8192 x   8192  | type F16  | T+  38\n",
      "[260/723] Writing tensor blk.28.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  38\n",
      "[261/723] Writing tensor blk.28.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  38\n",
      "[262/723] Writing tensor blk.28.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  38\n",
      "[263/723] Writing tensor blk.28.attn_norm.weight                | size   8192           | type F32  | T+  39\n",
      "[264/723] Writing tensor blk.28.ffn_norm.weight                 | size   8192           | type F32  | T+  39\n",
      "[265/723] Writing tensor blk.29.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[266/723] Writing tensor blk.29.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[267/723] Writing tensor blk.29.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[268/723] Writing tensor blk.29.attn_output.weight              | size   8192 x   8192  | type F16  | T+  39\n",
      "[269/723] Writing tensor blk.29.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  39\n",
      "[270/723] Writing tensor blk.29.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  39\n",
      "[271/723] Writing tensor blk.29.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  39\n",
      "[272/723] Writing tensor blk.29.attn_norm.weight                | size   8192           | type F32  | T+  40\n",
      "[273/723] Writing tensor blk.29.ffn_norm.weight                 | size   8192           | type F32  | T+  40\n",
      "[274/723] Writing tensor blk.30.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[275/723] Writing tensor blk.30.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[276/723] Writing tensor blk.30.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  40\n",
      "[277/723] Writing tensor blk.30.attn_output.weight              | size   8192 x   8192  | type F16  | T+  40\n",
      "[278/723] Writing tensor blk.30.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  40\n",
      "[279/723] Writing tensor blk.30.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  40\n",
      "[280/723] Writing tensor blk.30.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  41\n",
      "[281/723] Writing tensor blk.30.attn_norm.weight                | size   8192           | type F32  | T+  41\n",
      "[282/723] Writing tensor blk.30.ffn_norm.weight                 | size   8192           | type F32  | T+  41\n",
      "[283/723] Writing tensor blk.31.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  41\n",
      "[284/723] Writing tensor blk.31.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  41\n",
      "[285/723] Writing tensor blk.31.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  41\n",
      "[286/723] Writing tensor blk.31.attn_output.weight              | size   8192 x   8192  | type F16  | T+  41\n",
      "[287/723] Writing tensor blk.31.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  42\n",
      "[288/723] Writing tensor blk.31.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  42\n",
      "[289/723] Writing tensor blk.31.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  42\n",
      "[290/723] Writing tensor blk.31.attn_norm.weight                | size   8192           | type F32  | T+  42\n",
      "[291/723] Writing tensor blk.31.ffn_norm.weight                 | size   8192           | type F32  | T+  42\n",
      "[292/723] Writing tensor blk.32.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  42\n",
      "[293/723] Writing tensor blk.32.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  42\n",
      "[294/723] Writing tensor blk.32.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  42\n",
      "[295/723] Writing tensor blk.32.attn_output.weight              | size   8192 x   8192  | type F16  | T+  42\n",
      "[296/723] Writing tensor blk.32.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  43\n",
      "[297/723] Writing tensor blk.32.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  43\n",
      "[298/723] Writing tensor blk.32.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  43\n",
      "[299/723] Writing tensor blk.32.attn_norm.weight                | size   8192           | type F32  | T+  43\n",
      "[300/723] Writing tensor blk.32.ffn_norm.weight                 | size   8192           | type F32  | T+  43\n",
      "[301/723] Writing tensor blk.33.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[302/723] Writing tensor blk.33.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[303/723] Writing tensor blk.33.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[304/723] Writing tensor blk.33.attn_output.weight              | size   8192 x   8192  | type F16  | T+  43\n",
      "[305/723] Writing tensor blk.33.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  44\n",
      "[306/723] Writing tensor blk.33.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  44\n",
      "[307/723] Writing tensor blk.33.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  44\n",
      "[308/723] Writing tensor blk.33.attn_norm.weight                | size   8192           | type F32  | T+  44\n",
      "[309/723] Writing tensor blk.33.ffn_norm.weight                 | size   8192           | type F32  | T+  44\n",
      "[310/723] Writing tensor blk.34.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[311/723] Writing tensor blk.34.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[312/723] Writing tensor blk.34.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[313/723] Writing tensor blk.34.attn_output.weight              | size   8192 x   8192  | type F16  | T+  45\n",
      "[314/723] Writing tensor blk.34.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  45\n",
      "[315/723] Writing tensor blk.34.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  46\n",
      "[316/723] Writing tensor blk.34.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  46\n",
      "[317/723] Writing tensor blk.34.attn_norm.weight                | size   8192           | type F32  | T+  47\n",
      "[318/723] Writing tensor blk.34.ffn_norm.weight                 | size   8192           | type F32  | T+  47\n",
      "[319/723] Writing tensor blk.35.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  47\n",
      "[320/723] Writing tensor blk.35.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  47\n",
      "[321/723] Writing tensor blk.35.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  47\n",
      "[322/723] Writing tensor blk.35.attn_output.weight              | size   8192 x   8192  | type F16  | T+  47\n",
      "[323/723] Writing tensor blk.35.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  47\n",
      "[324/723] Writing tensor blk.35.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  48\n",
      "[325/723] Writing tensor blk.35.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  48\n",
      "[326/723] Writing tensor blk.35.attn_norm.weight                | size   8192           | type F32  | T+  48\n",
      "[327/723] Writing tensor blk.35.ffn_norm.weight                 | size   8192           | type F32  | T+  48\n",
      "[328/723] Writing tensor blk.36.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[329/723] Writing tensor blk.36.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[330/723] Writing tensor blk.36.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[331/723] Writing tensor blk.36.attn_output.weight              | size   8192 x   8192  | type F16  | T+  49\n",
      "[332/723] Writing tensor blk.36.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  49\n",
      "[333/723] Writing tensor blk.36.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  50\n",
      "[334/723] Writing tensor blk.36.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  50\n",
      "[335/723] Writing tensor blk.36.attn_norm.weight                | size   8192           | type F32  | T+  51\n",
      "[336/723] Writing tensor blk.36.ffn_norm.weight                 | size   8192           | type F32  | T+  51\n",
      "[337/723] Writing tensor blk.37.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[338/723] Writing tensor blk.37.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[339/723] Writing tensor blk.37.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[340/723] Writing tensor blk.37.attn_output.weight              | size   8192 x   8192  | type F16  | T+  51\n",
      "[341/723] Writing tensor blk.37.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  51\n",
      "[342/723] Writing tensor blk.37.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  52\n",
      "[343/723] Writing tensor blk.37.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  52\n",
      "[344/723] Writing tensor blk.37.attn_norm.weight                | size   8192           | type F32  | T+  53\n",
      "[345/723] Writing tensor blk.37.ffn_norm.weight                 | size   8192           | type F32  | T+  53\n",
      "[346/723] Writing tensor blk.38.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  53\n",
      "[347/723] Writing tensor blk.38.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  53\n",
      "[348/723] Writing tensor blk.38.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  53\n",
      "[349/723] Writing tensor blk.38.attn_output.weight              | size   8192 x   8192  | type F16  | T+  53\n",
      "[350/723] Writing tensor blk.38.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  54\n",
      "[351/723] Writing tensor blk.38.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  54\n",
      "[352/723] Writing tensor blk.38.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  55\n",
      "[353/723] Writing tensor blk.38.attn_norm.weight                | size   8192           | type F32  | T+  55\n",
      "[354/723] Writing tensor blk.38.ffn_norm.weight                 | size   8192           | type F32  | T+  55\n",
      "[355/723] Writing tensor blk.39.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  55\n",
      "[356/723] Writing tensor blk.39.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  56\n",
      "[357/723] Writing tensor blk.39.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  56\n",
      "[358/723] Writing tensor blk.39.attn_output.weight              | size   8192 x   8192  | type F16  | T+  56\n",
      "[359/723] Writing tensor blk.39.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  56\n",
      "[360/723] Writing tensor blk.39.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  56\n",
      "[361/723] Writing tensor blk.39.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  57\n",
      "[362/723] Writing tensor blk.39.attn_norm.weight                | size   8192           | type F32  | T+  58\n",
      "[363/723] Writing tensor blk.39.ffn_norm.weight                 | size   8192           | type F32  | T+  58\n",
      "[364/723] Writing tensor blk.40.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[365/723] Writing tensor blk.40.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[366/723] Writing tensor blk.40.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[367/723] Writing tensor blk.40.attn_output.weight              | size   8192 x   8192  | type F16  | T+  58\n",
      "[368/723] Writing tensor blk.40.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  58\n",
      "[369/723] Writing tensor blk.40.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  58\n",
      "[370/723] Writing tensor blk.40.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  59\n",
      "[371/723] Writing tensor blk.40.attn_norm.weight                | size   8192           | type F32  | T+  59\n",
      "[372/723] Writing tensor blk.40.ffn_norm.weight                 | size   8192           | type F32  | T+  59\n",
      "[373/723] Writing tensor blk.41.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  59\n",
      "[374/723] Writing tensor blk.41.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  59\n",
      "[375/723] Writing tensor blk.41.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  59\n",
      "[376/723] Writing tensor blk.41.attn_output.weight              | size   8192 x   8192  | type F16  | T+  59\n",
      "[377/723] Writing tensor blk.41.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  59\n",
      "[378/723] Writing tensor blk.41.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  60\n",
      "[379/723] Writing tensor blk.41.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  60\n",
      "[380/723] Writing tensor blk.41.attn_norm.weight                | size   8192           | type F32  | T+  60\n",
      "[381/723] Writing tensor blk.41.ffn_norm.weight                 | size   8192           | type F32  | T+  60\n",
      "[382/723] Writing tensor blk.42.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[383/723] Writing tensor blk.42.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[384/723] Writing tensor blk.42.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[385/723] Writing tensor blk.42.attn_output.weight              | size   8192 x   8192  | type F16  | T+  60\n",
      "[386/723] Writing tensor blk.42.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  61\n",
      "[387/723] Writing tensor blk.42.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  61\n",
      "[388/723] Writing tensor blk.42.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  61\n",
      "[389/723] Writing tensor blk.42.attn_norm.weight                | size   8192           | type F32  | T+  61\n",
      "[390/723] Writing tensor blk.42.ffn_norm.weight                 | size   8192           | type F32  | T+  61\n",
      "[391/723] Writing tensor blk.43.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  61\n",
      "[392/723] Writing tensor blk.43.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  61\n",
      "[393/723] Writing tensor blk.43.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  61\n",
      "[394/723] Writing tensor blk.43.attn_output.weight              | size   8192 x   8192  | type F16  | T+  61\n",
      "[395/723] Writing tensor blk.43.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  62\n",
      "[396/723] Writing tensor blk.43.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  62\n",
      "[397/723] Writing tensor blk.43.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  62\n",
      "[398/723] Writing tensor blk.43.attn_norm.weight                | size   8192           | type F32  | T+  62\n",
      "[399/723] Writing tensor blk.43.ffn_norm.weight                 | size   8192           | type F32  | T+  62\n",
      "[400/723] Writing tensor blk.44.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[401/723] Writing tensor blk.44.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[402/723] Writing tensor blk.44.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[403/723] Writing tensor blk.44.attn_output.weight              | size   8192 x   8192  | type F16  | T+  63\n",
      "[404/723] Writing tensor blk.44.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  63\n",
      "[405/723] Writing tensor blk.44.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  63\n",
      "[406/723] Writing tensor blk.44.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  63\n",
      "[407/723] Writing tensor blk.44.attn_norm.weight                | size   8192           | type F32  | T+  64\n",
      "[408/723] Writing tensor blk.44.ffn_norm.weight                 | size   8192           | type F32  | T+  64\n",
      "[409/723] Writing tensor blk.45.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[410/723] Writing tensor blk.45.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[411/723] Writing tensor blk.45.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  64\n",
      "[412/723] Writing tensor blk.45.attn_output.weight              | size   8192 x   8192  | type F16  | T+  64\n",
      "[413/723] Writing tensor blk.45.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  64\n",
      "[414/723] Writing tensor blk.45.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  64\n",
      "[415/723] Writing tensor blk.45.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  65\n",
      "[416/723] Writing tensor blk.45.attn_norm.weight                | size   8192           | type F32  | T+  65\n",
      "[417/723] Writing tensor blk.45.ffn_norm.weight                 | size   8192           | type F32  | T+  65\n",
      "[418/723] Writing tensor blk.46.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[419/723] Writing tensor blk.46.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[420/723] Writing tensor blk.46.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[421/723] Writing tensor blk.46.attn_output.weight              | size   8192 x   8192  | type F16  | T+  65\n",
      "[422/723] Writing tensor blk.46.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  65\n",
      "[423/723] Writing tensor blk.46.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  66\n",
      "[424/723] Writing tensor blk.46.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  66\n",
      "[425/723] Writing tensor blk.46.attn_norm.weight                | size   8192           | type F32  | T+  66\n",
      "[426/723] Writing tensor blk.46.ffn_norm.weight                 | size   8192           | type F32  | T+  66\n",
      "[427/723] Writing tensor blk.47.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[428/723] Writing tensor blk.47.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[429/723] Writing tensor blk.47.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[430/723] Writing tensor blk.47.attn_output.weight              | size   8192 x   8192  | type F16  | T+  66\n",
      "[431/723] Writing tensor blk.47.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  67\n",
      "[432/723] Writing tensor blk.47.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  67\n",
      "[433/723] Writing tensor blk.47.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  67\n",
      "[434/723] Writing tensor blk.47.attn_norm.weight                | size   8192           | type F32  | T+  67\n",
      "[435/723] Writing tensor blk.47.ffn_norm.weight                 | size   8192           | type F32  | T+  67\n",
      "[436/723] Writing tensor blk.48.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  67\n",
      "[437/723] Writing tensor blk.48.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  67\n",
      "[438/723] Writing tensor blk.48.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  67\n",
      "[439/723] Writing tensor blk.48.attn_output.weight              | size   8192 x   8192  | type F16  | T+  67\n",
      "[440/723] Writing tensor blk.48.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  68\n",
      "[441/723] Writing tensor blk.48.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  68\n",
      "[442/723] Writing tensor blk.48.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  68\n",
      "[443/723] Writing tensor blk.48.attn_norm.weight                | size   8192           | type F32  | T+  68\n",
      "[444/723] Writing tensor blk.48.ffn_norm.weight                 | size   8192           | type F32  | T+  68\n",
      "[445/723] Writing tensor blk.49.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[446/723] Writing tensor blk.49.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[447/723] Writing tensor blk.49.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[448/723] Writing tensor blk.49.attn_output.weight              | size   8192 x   8192  | type F16  | T+  69\n",
      "[449/723] Writing tensor blk.49.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  69\n",
      "[450/723] Writing tensor blk.49.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  69\n",
      "[451/723] Writing tensor blk.49.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  69\n",
      "[452/723] Writing tensor blk.49.attn_norm.weight                | size   8192           | type F32  | T+  70\n",
      "[453/723] Writing tensor blk.49.ffn_norm.weight                 | size   8192           | type F32  | T+  70\n",
      "[454/723] Writing tensor blk.50.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[455/723] Writing tensor blk.50.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[456/723] Writing tensor blk.50.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[457/723] Writing tensor blk.50.attn_output.weight              | size   8192 x   8192  | type F16  | T+  70\n",
      "[458/723] Writing tensor blk.50.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  70\n",
      "[459/723] Writing tensor blk.50.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  71\n",
      "[460/723] Writing tensor blk.50.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  71\n",
      "[461/723] Writing tensor blk.50.attn_norm.weight                | size   8192           | type F32  | T+  71\n",
      "[462/723] Writing tensor blk.50.ffn_norm.weight                 | size   8192           | type F32  | T+  71\n",
      "[463/723] Writing tensor blk.51.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[464/723] Writing tensor blk.51.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[465/723] Writing tensor blk.51.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[466/723] Writing tensor blk.51.attn_output.weight              | size   8192 x   8192  | type F16  | T+  71\n",
      "[467/723] Writing tensor blk.51.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  72\n",
      "[468/723] Writing tensor blk.51.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  72\n",
      "[469/723] Writing tensor blk.51.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  72\n",
      "[470/723] Writing tensor blk.51.attn_norm.weight                | size   8192           | type F32  | T+  72\n",
      "[471/723] Writing tensor blk.51.ffn_norm.weight                 | size   8192           | type F32  | T+  72\n",
      "[472/723] Writing tensor blk.52.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  72\n",
      "[473/723] Writing tensor blk.52.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  72\n",
      "[474/723] Writing tensor blk.52.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  72\n",
      "[475/723] Writing tensor blk.52.attn_output.weight              | size   8192 x   8192  | type F16  | T+  72\n",
      "[476/723] Writing tensor blk.52.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  73\n",
      "[477/723] Writing tensor blk.52.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  73\n",
      "[478/723] Writing tensor blk.52.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  73\n",
      "[479/723] Writing tensor blk.52.attn_norm.weight                | size   8192           | type F32  | T+  73\n",
      "[480/723] Writing tensor blk.52.ffn_norm.weight                 | size   8192           | type F32  | T+  73\n",
      "[481/723] Writing tensor blk.53.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  73\n",
      "[482/723] Writing tensor blk.53.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  73\n",
      "[483/723] Writing tensor blk.53.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  73\n",
      "[484/723] Writing tensor blk.53.attn_output.weight              | size   8192 x   8192  | type F16  | T+  73\n",
      "[485/723] Writing tensor blk.53.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  74\n",
      "[486/723] Writing tensor blk.53.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  74\n",
      "[487/723] Writing tensor blk.53.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  74\n",
      "[488/723] Writing tensor blk.53.attn_norm.weight                | size   8192           | type F32  | T+  74\n",
      "[489/723] Writing tensor blk.53.ffn_norm.weight                 | size   8192           | type F32  | T+  74\n",
      "[490/723] Writing tensor blk.54.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[491/723] Writing tensor blk.54.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  74\n",
      "[492/723] Writing tensor blk.54.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[493/723] Writing tensor blk.54.attn_output.weight              | size   8192 x   8192  | type F16  | T+  75\n",
      "[494/723] Writing tensor blk.54.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  75\n",
      "[495/723] Writing tensor blk.54.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  75\n",
      "[496/723] Writing tensor blk.54.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  75\n",
      "[497/723] Writing tensor blk.54.attn_norm.weight                | size   8192           | type F32  | T+  76\n",
      "[498/723] Writing tensor blk.54.ffn_norm.weight                 | size   8192           | type F32  | T+  76\n",
      "[499/723] Writing tensor blk.55.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  76\n",
      "[500/723] Writing tensor blk.55.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  76\n",
      "[501/723] Writing tensor blk.55.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  76\n",
      "[502/723] Writing tensor blk.55.attn_output.weight              | size   8192 x   8192  | type F16  | T+  76\n",
      "[503/723] Writing tensor blk.55.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  76\n",
      "[504/723] Writing tensor blk.55.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  76\n",
      "[505/723] Writing tensor blk.55.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  77\n",
      "[506/723] Writing tensor blk.55.attn_norm.weight                | size   8192           | type F32  | T+  77\n",
      "[507/723] Writing tensor blk.55.ffn_norm.weight                 | size   8192           | type F32  | T+  77\n",
      "[508/723] Writing tensor blk.56.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[509/723] Writing tensor blk.56.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[510/723] Writing tensor blk.56.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[511/723] Writing tensor blk.56.attn_output.weight              | size   8192 x   8192  | type F16  | T+  77\n",
      "[512/723] Writing tensor blk.56.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  77\n",
      "[513/723] Writing tensor blk.56.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  78\n",
      "[514/723] Writing tensor blk.56.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  78\n",
      "[515/723] Writing tensor blk.56.attn_norm.weight                | size   8192           | type F32  | T+  78\n",
      "[516/723] Writing tensor blk.56.ffn_norm.weight                 | size   8192           | type F32  | T+  78\n",
      "[517/723] Writing tensor blk.57.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  78\n",
      "[518/723] Writing tensor blk.57.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  78\n",
      "[519/723] Writing tensor blk.57.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  78\n",
      "[520/723] Writing tensor blk.57.attn_output.weight              | size   8192 x   8192  | type F16  | T+  78\n",
      "[521/723] Writing tensor blk.57.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  79\n",
      "[522/723] Writing tensor blk.57.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  79\n",
      "[523/723] Writing tensor blk.57.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  80\n",
      "[524/723] Writing tensor blk.57.attn_norm.weight                | size   8192           | type F32  | T+  80\n",
      "[525/723] Writing tensor blk.57.ffn_norm.weight                 | size   8192           | type F32  | T+  80\n",
      "[526/723] Writing tensor blk.58.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[527/723] Writing tensor blk.58.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[528/723] Writing tensor blk.58.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  80\n",
      "[529/723] Writing tensor blk.58.attn_output.weight              | size   8192 x   8192  | type F16  | T+  81\n",
      "[530/723] Writing tensor blk.58.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  81\n",
      "[531/723] Writing tensor blk.58.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  81\n",
      "[532/723] Writing tensor blk.58.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  82\n",
      "[533/723] Writing tensor blk.58.attn_norm.weight                | size   8192           | type F32  | T+  83\n",
      "[534/723] Writing tensor blk.58.ffn_norm.weight                 | size   8192           | type F32  | T+  83\n",
      "[535/723] Writing tensor blk.59.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[536/723] Writing tensor blk.59.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[537/723] Writing tensor blk.59.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[538/723] Writing tensor blk.59.attn_output.weight              | size   8192 x   8192  | type F16  | T+  83\n",
      "[539/723] Writing tensor blk.59.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  83\n",
      "[540/723] Writing tensor blk.59.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  84\n",
      "[541/723] Writing tensor blk.59.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  84\n",
      "[542/723] Writing tensor blk.59.attn_norm.weight                | size   8192           | type F32  | T+  84\n",
      "[543/723] Writing tensor blk.59.ffn_norm.weight                 | size   8192           | type F32  | T+  84\n",
      "[544/723] Writing tensor blk.60.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  84\n",
      "[545/723] Writing tensor blk.60.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[546/723] Writing tensor blk.60.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[547/723] Writing tensor blk.60.attn_output.weight              | size   8192 x   8192  | type F16  | T+  85\n",
      "[548/723] Writing tensor blk.60.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  86\n",
      "[549/723] Writing tensor blk.60.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  86\n",
      "[550/723] Writing tensor blk.60.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  86\n",
      "[551/723] Writing tensor blk.60.attn_norm.weight                | size   8192           | type F32  | T+  87\n",
      "[552/723] Writing tensor blk.60.ffn_norm.weight                 | size   8192           | type F32  | T+  87\n",
      "[553/723] Writing tensor blk.61.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[554/723] Writing tensor blk.61.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[555/723] Writing tensor blk.61.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[556/723] Writing tensor blk.61.attn_output.weight              | size   8192 x   8192  | type F16  | T+  87\n",
      "[557/723] Writing tensor blk.61.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  88\n",
      "[558/723] Writing tensor blk.61.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  88\n",
      "[559/723] Writing tensor blk.61.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  89\n",
      "[560/723] Writing tensor blk.61.attn_norm.weight                | size   8192           | type F32  | T+  89\n",
      "[561/723] Writing tensor blk.61.ffn_norm.weight                 | size   8192           | type F32  | T+  89\n",
      "[562/723] Writing tensor blk.62.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[563/723] Writing tensor blk.62.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[564/723] Writing tensor blk.62.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  89\n",
      "[565/723] Writing tensor blk.62.attn_output.weight              | size   8192 x   8192  | type F16  | T+  90\n",
      "[566/723] Writing tensor blk.62.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  90\n",
      "[567/723] Writing tensor blk.62.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  90\n",
      "[568/723] Writing tensor blk.62.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  91\n",
      "[569/723] Writing tensor blk.62.attn_norm.weight                | size   8192           | type F32  | T+  91\n",
      "[570/723] Writing tensor blk.62.ffn_norm.weight                 | size   8192           | type F32  | T+  91\n",
      "[571/723] Writing tensor blk.63.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  91\n",
      "[572/723] Writing tensor blk.63.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  91\n",
      "[573/723] Writing tensor blk.63.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  91\n",
      "[574/723] Writing tensor blk.63.attn_output.weight              | size   8192 x   8192  | type F16  | T+  92\n",
      "[575/723] Writing tensor blk.63.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  92\n",
      "[576/723] Writing tensor blk.63.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  92\n",
      "[577/723] Writing tensor blk.63.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  93\n",
      "[578/723] Writing tensor blk.63.attn_norm.weight                | size   8192           | type F32  | T+  93\n",
      "[579/723] Writing tensor blk.63.ffn_norm.weight                 | size   8192           | type F32  | T+  93\n",
      "[580/723] Writing tensor blk.64.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[581/723] Writing tensor blk.64.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[582/723] Writing tensor blk.64.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[583/723] Writing tensor blk.64.attn_output.weight              | size   8192 x   8192  | type F16  | T+  93\n",
      "[584/723] Writing tensor blk.64.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  93\n",
      "[585/723] Writing tensor blk.64.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  94\n",
      "[586/723] Writing tensor blk.64.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  94\n",
      "[587/723] Writing tensor blk.64.attn_norm.weight                | size   8192           | type F32  | T+  94\n",
      "[588/723] Writing tensor blk.64.ffn_norm.weight                 | size   8192           | type F32  | T+  94\n",
      "[589/723] Writing tensor blk.65.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[590/723] Writing tensor blk.65.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[591/723] Writing tensor blk.65.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[592/723] Writing tensor blk.65.attn_output.weight              | size   8192 x   8192  | type F16  | T+  94\n",
      "[593/723] Writing tensor blk.65.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  95\n",
      "[594/723] Writing tensor blk.65.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  95\n",
      "[595/723] Writing tensor blk.65.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  95\n",
      "[596/723] Writing tensor blk.65.attn_norm.weight                | size   8192           | type F32  | T+  95\n",
      "[597/723] Writing tensor blk.65.ffn_norm.weight                 | size   8192           | type F32  | T+  95\n",
      "[598/723] Writing tensor blk.66.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  95\n",
      "[599/723] Writing tensor blk.66.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  95\n",
      "[600/723] Writing tensor blk.66.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  95\n",
      "[601/723] Writing tensor blk.66.attn_output.weight              | size   8192 x   8192  | type F16  | T+  95\n",
      "[602/723] Writing tensor blk.66.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  96\n",
      "[603/723] Writing tensor blk.66.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  96\n",
      "[604/723] Writing tensor blk.66.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  96\n",
      "[605/723] Writing tensor blk.66.attn_norm.weight                | size   8192           | type F32  | T+  96\n",
      "[606/723] Writing tensor blk.66.ffn_norm.weight                 | size   8192           | type F32  | T+  96\n",
      "[607/723] Writing tensor blk.67.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[608/723] Writing tensor blk.67.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[609/723] Writing tensor blk.67.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[610/723] Writing tensor blk.67.attn_output.weight              | size   8192 x   8192  | type F16  | T+  96\n",
      "[611/723] Writing tensor blk.67.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  97\n",
      "[612/723] Writing tensor blk.67.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  97\n",
      "[613/723] Writing tensor blk.67.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  97\n",
      "[614/723] Writing tensor blk.67.attn_norm.weight                | size   8192           | type F32  | T+  97\n",
      "[615/723] Writing tensor blk.67.ffn_norm.weight                 | size   8192           | type F32  | T+  97\n",
      "[616/723] Writing tensor blk.68.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  97\n",
      "[617/723] Writing tensor blk.68.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  97\n",
      "[618/723] Writing tensor blk.68.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  97\n",
      "[619/723] Writing tensor blk.68.attn_output.weight              | size   8192 x   8192  | type F16  | T+  98\n",
      "[620/723] Writing tensor blk.68.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  98\n",
      "[621/723] Writing tensor blk.68.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  99\n",
      "[622/723] Writing tensor blk.68.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  99\n",
      "[623/723] Writing tensor blk.68.attn_norm.weight                | size   8192           | type F32  | T+ 100\n",
      "[624/723] Writing tensor blk.68.ffn_norm.weight                 | size   8192           | type F32  | T+ 100\n",
      "[625/723] Writing tensor blk.69.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[626/723] Writing tensor blk.69.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[627/723] Writing tensor blk.69.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[628/723] Writing tensor blk.69.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 100\n",
      "[629/723] Writing tensor blk.69.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 100\n",
      "[630/723] Writing tensor blk.69.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 101\n",
      "[631/723] Writing tensor blk.69.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 101\n",
      "[632/723] Writing tensor blk.69.attn_norm.weight                | size   8192           | type F32  | T+ 101\n",
      "[633/723] Writing tensor blk.69.ffn_norm.weight                 | size   8192           | type F32  | T+ 101\n",
      "[634/723] Writing tensor blk.70.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[635/723] Writing tensor blk.70.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[636/723] Writing tensor blk.70.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[637/723] Writing tensor blk.70.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 101\n",
      "[638/723] Writing tensor blk.70.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 102\n",
      "[639/723] Writing tensor blk.70.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 102\n",
      "[640/723] Writing tensor blk.70.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 102\n",
      "[641/723] Writing tensor blk.70.attn_norm.weight                | size   8192           | type F32  | T+ 102\n",
      "[642/723] Writing tensor blk.70.ffn_norm.weight                 | size   8192           | type F32  | T+ 102\n",
      "[643/723] Writing tensor blk.71.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[644/723] Writing tensor blk.71.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 103\n",
      "[645/723] Writing tensor blk.71.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 103\n",
      "[646/723] Writing tensor blk.71.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 103\n",
      "[647/723] Writing tensor blk.71.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 103\n",
      "[648/723] Writing tensor blk.71.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 103\n",
      "[649/723] Writing tensor blk.71.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 103\n",
      "[650/723] Writing tensor blk.71.attn_norm.weight                | size   8192           | type F32  | T+ 104\n",
      "[651/723] Writing tensor blk.71.ffn_norm.weight                 | size   8192           | type F32  | T+ 104\n",
      "[652/723] Writing tensor blk.72.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[653/723] Writing tensor blk.72.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[654/723] Writing tensor blk.72.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[655/723] Writing tensor blk.72.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 104\n",
      "[656/723] Writing tensor blk.72.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 104\n",
      "[657/723] Writing tensor blk.72.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 104\n",
      "[658/723] Writing tensor blk.72.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 105\n",
      "[659/723] Writing tensor blk.72.attn_norm.weight                | size   8192           | type F32  | T+ 105\n",
      "[660/723] Writing tensor blk.72.ffn_norm.weight                 | size   8192           | type F32  | T+ 105\n",
      "[661/723] Writing tensor blk.73.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 105\n",
      "[662/723] Writing tensor blk.73.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 105\n",
      "[663/723] Writing tensor blk.73.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 105\n",
      "[664/723] Writing tensor blk.73.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 105\n",
      "[665/723] Writing tensor blk.73.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 106\n",
      "[666/723] Writing tensor blk.73.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 106\n",
      "[667/723] Writing tensor blk.73.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 106\n",
      "[668/723] Writing tensor blk.73.attn_norm.weight                | size   8192           | type F32  | T+ 106\n",
      "[669/723] Writing tensor blk.73.ffn_norm.weight                 | size   8192           | type F32  | T+ 106\n",
      "[670/723] Writing tensor blk.74.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[671/723] Writing tensor blk.74.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[672/723] Writing tensor blk.74.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[673/723] Writing tensor blk.74.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 106\n",
      "[674/723] Writing tensor blk.74.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 107\n",
      "[675/723] Writing tensor blk.74.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 107\n",
      "[676/723] Writing tensor blk.74.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 108\n",
      "[677/723] Writing tensor blk.74.attn_norm.weight                | size   8192           | type F32  | T+ 108\n",
      "[678/723] Writing tensor blk.74.ffn_norm.weight                 | size   8192           | type F32  | T+ 108\n",
      "[679/723] Writing tensor blk.75.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 108\n",
      "[680/723] Writing tensor blk.75.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 108\n",
      "[681/723] Writing tensor blk.75.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 108\n",
      "[682/723] Writing tensor blk.75.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 109\n",
      "[683/723] Writing tensor blk.75.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 109\n",
      "[684/723] Writing tensor blk.75.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 109\n",
      "[685/723] Writing tensor blk.75.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 109\n",
      "[686/723] Writing tensor blk.75.attn_norm.weight                | size   8192           | type F32  | T+ 109\n",
      "[687/723] Writing tensor blk.75.ffn_norm.weight                 | size   8192           | type F32  | T+ 109\n",
      "[688/723] Writing tensor blk.76.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[689/723] Writing tensor blk.76.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[690/723] Writing tensor blk.76.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[691/723] Writing tensor blk.76.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 110\n",
      "[692/723] Writing tensor blk.76.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 110\n",
      "[693/723] Writing tensor blk.76.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 110\n",
      "[694/723] Writing tensor blk.76.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 110\n",
      "[695/723] Writing tensor blk.76.attn_norm.weight                | size   8192           | type F32  | T+ 110\n",
      "[696/723] Writing tensor blk.76.ffn_norm.weight                 | size   8192           | type F32  | T+ 110\n",
      "[697/723] Writing tensor blk.77.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 110\n",
      "[698/723] Writing tensor blk.77.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 111\n",
      "[699/723] Writing tensor blk.77.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 111\n",
      "[700/723] Writing tensor blk.77.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 111\n",
      "[701/723] Writing tensor blk.77.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 111\n",
      "[702/723] Writing tensor blk.77.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 112\n",
      "[703/723] Writing tensor blk.77.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 112\n",
      "[704/723] Writing tensor blk.77.attn_norm.weight                | size   8192           | type F32  | T+ 112\n",
      "[705/723] Writing tensor blk.77.ffn_norm.weight                 | size   8192           | type F32  | T+ 112\n",
      "[706/723] Writing tensor blk.78.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[707/723] Writing tensor blk.78.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[708/723] Writing tensor blk.78.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[709/723] Writing tensor blk.78.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 112\n",
      "[710/723] Writing tensor blk.78.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 113\n",
      "[711/723] Writing tensor blk.78.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 113\n",
      "[712/723] Writing tensor blk.78.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 113\n",
      "[713/723] Writing tensor blk.78.attn_norm.weight                | size   8192           | type F32  | T+ 113\n",
      "[714/723] Writing tensor blk.78.ffn_norm.weight                 | size   8192           | type F32  | T+ 113\n",
      "[715/723] Writing tensor blk.79.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 113\n",
      "[716/723] Writing tensor blk.79.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 113\n",
      "[717/723] Writing tensor blk.79.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 113\n",
      "[718/723] Writing tensor blk.79.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 113\n",
      "[719/723] Writing tensor blk.79.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 114\n",
      "[720/723] Writing tensor blk.79.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 114\n",
      "[721/723] Writing tensor blk.79.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 114\n",
      "[722/723] Writing tensor blk.79.attn_norm.weight                | size   8192           | type F32  | T+ 114\n",
      "[723/723] Writing tensor blk.79.ffn_norm.weight                 | size   8192           | type F32  | T+ 114\n",
      "Wrote models/65B/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/7B/ggml-model-f16.gguf' to './models/7B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 1714336 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
      "[   4/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   9/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  22/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  82/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 109/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 262/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 271/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 280/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 282/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 290/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 24128.97 ms\n",
      "main:    total time = 24128.97 ms\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/13B/ggml-model-f16.gguf' to './models/13B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_quantize_internal: meta size = 1718656 bytes\n",
      "[   1/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   312.50 MiB ->    87.89 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   312.50 MiB ->   128.17 MiB | hist: \n",
      "[   4/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  12/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  13/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  21/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  22/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  30/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  31/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  39/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  40/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  48/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  49/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  57/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  58/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  66/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  67/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  75/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  76/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  84/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  85/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  93/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  94/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 102/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 109/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 111/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 120/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 129/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 138/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 147/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 156/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 165/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 174/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 183/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 192/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 201/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 210/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 219/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 228/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 237/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 246/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 255/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 264/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 273/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 282/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 291/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 300/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 309/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 318/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 327/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 334/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 336/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 343/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 345/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 352/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 354/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 361/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 362/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 363/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 36137.95 ms\n",
      "main:    total time = 36137.95 ms\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/30B/ggml-model-f16.gguf' to './models/30B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llama_model_quantize_internal: meta size = 1729408 bytes\n",
      "[   1/ 543]                    token_embd.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   406.25 MiB ->   114.26 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                   output_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   406.25 MiB ->   166.63 MiB | hist: \n",
      "[   4/ 543]                  blk.0.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]                  blk.0.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]                  blk.0.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]             blk.0.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]                blk.0.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 543]                blk.0.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 543]                  blk.0.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 543]               blk.0.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  12/ 543]                blk.0.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  13/ 543]                  blk.1.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]                  blk.1.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]                  blk.1.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]             blk.1.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]                blk.1.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 543]                blk.1.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]                  blk.1.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]               blk.1.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  21/ 543]                blk.1.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  22/ 543]                  blk.2.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]                  blk.2.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]                  blk.2.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]             blk.2.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]                blk.2.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 543]                blk.2.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]                  blk.2.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]               blk.2.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  30/ 543]                blk.2.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  31/ 543]                  blk.3.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]                  blk.3.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]                  blk.3.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]             blk.3.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]                blk.3.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 543]                blk.3.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  37/ 543]                  blk.3.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 543]               blk.3.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  39/ 543]                blk.3.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  40/ 543]                  blk.4.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]                  blk.4.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]                  blk.4.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]             blk.4.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]                blk.4.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 543]                blk.4.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]                  blk.4.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]               blk.4.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  48/ 543]                blk.4.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  49/ 543]                  blk.5.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]                  blk.5.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]                  blk.5.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]             blk.5.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]                blk.5.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 543]                blk.5.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]                  blk.5.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]               blk.5.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  57/ 543]                blk.5.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  58/ 543]                  blk.6.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]                  blk.6.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]                  blk.6.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]             blk.6.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]                blk.6.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 543]                blk.6.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]                  blk.6.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]               blk.6.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  66/ 543]                blk.6.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  67/ 543]                  blk.7.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]                  blk.7.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]                  blk.7.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]             blk.7.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]                blk.7.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 543]                blk.7.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]                  blk.7.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]               blk.7.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  75/ 543]                blk.7.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  76/ 543]                  blk.8.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]                  blk.8.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]                  blk.8.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]             blk.8.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]                blk.8.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 543]                blk.8.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]                  blk.8.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]               blk.8.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  84/ 543]                blk.8.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  85/ 543]                  blk.9.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]                  blk.9.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]                  blk.9.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]             blk.9.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]                blk.9.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 543]                blk.9.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]                  blk.9.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]               blk.9.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  93/ 543]                blk.9.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  94/ 543]                 blk.10.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]                 blk.10.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]                 blk.10.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]            blk.10.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]               blk.10.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 543]               blk.10.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]                 blk.10.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]              blk.10.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 102/ 543]               blk.10.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]                 blk.11.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]                 blk.11.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]                 blk.11.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]            blk.11.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]               blk.11.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 543]               blk.11.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]                 blk.11.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]              blk.11.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 111/ 543]               blk.11.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]                 blk.12.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]                 blk.12.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]                 blk.12.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]            blk.12.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]               blk.12.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 543]               blk.12.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]                 blk.12.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]              blk.12.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 120/ 543]               blk.12.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]                 blk.13.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]                 blk.13.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]                 blk.13.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]            blk.13.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]               blk.13.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 543]               blk.13.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]                 blk.13.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]              blk.13.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 129/ 543]               blk.13.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]                 blk.14.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]                 blk.14.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]                 blk.14.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]            blk.14.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]               blk.14.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 543]               blk.14.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 543]                 blk.14.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 543]              blk.14.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 138/ 543]               blk.14.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]                 blk.15.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]                 blk.15.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]                 blk.15.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]            blk.15.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]               blk.15.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 543]               blk.15.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]                 blk.15.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]              blk.15.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 147/ 543]               blk.15.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]                 blk.16.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]                 blk.16.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]                 blk.16.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]            blk.16.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]               blk.16.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 543]               blk.16.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]                 blk.16.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]              blk.16.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 156/ 543]               blk.16.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]                 blk.17.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]                 blk.17.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]                 blk.17.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]            blk.17.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]               blk.17.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 543]               blk.17.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]                 blk.17.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]              blk.17.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 165/ 543]               blk.17.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]                 blk.18.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]                 blk.18.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]                 blk.18.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]            blk.18.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]               blk.18.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 543]               blk.18.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]                 blk.18.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]              blk.18.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 174/ 543]               blk.18.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]                 blk.19.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]                 blk.19.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]                 blk.19.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]            blk.19.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]               blk.19.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 543]               blk.19.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 181/ 543]                 blk.19.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 543]              blk.19.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 183/ 543]               blk.19.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]                 blk.20.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]                 blk.20.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]                 blk.20.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]            blk.20.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]               blk.20.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 543]               blk.20.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 190/ 543]                 blk.20.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 543]              blk.20.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 192/ 543]               blk.20.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]                 blk.21.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]                 blk.21.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]                 blk.21.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]            blk.21.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]               blk.21.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 543]               blk.21.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]                 blk.21.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]              blk.21.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 201/ 543]               blk.21.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]                 blk.22.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]                 blk.22.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]                 blk.22.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]            blk.22.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]               blk.22.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 543]               blk.22.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]                 blk.22.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]              blk.22.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 210/ 543]               blk.22.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]                 blk.23.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]                 blk.23.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]                 blk.23.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]            blk.23.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]               blk.23.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 543]               blk.23.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]                 blk.23.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]              blk.23.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 219/ 543]               blk.23.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]                 blk.24.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]                 blk.24.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]                 blk.24.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]            blk.24.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]               blk.24.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 543]               blk.24.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]                 blk.24.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]              blk.24.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 228/ 543]               blk.24.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]                 blk.25.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]                 blk.25.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]                 blk.25.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]            blk.25.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]               blk.25.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 543]               blk.25.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]                 blk.25.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]              blk.25.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 237/ 543]               blk.25.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]                 blk.26.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]                 blk.26.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]                 blk.26.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]            blk.26.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]               blk.26.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 543]               blk.26.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]                 blk.26.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]              blk.26.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 246/ 543]               blk.26.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]                 blk.27.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]                 blk.27.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]                 blk.27.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]            blk.27.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]               blk.27.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 543]               blk.27.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]                 blk.27.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]              blk.27.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 255/ 543]               blk.27.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]                 blk.28.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]                 blk.28.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]                 blk.28.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]            blk.28.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]               blk.28.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 543]               blk.28.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]                 blk.28.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]              blk.28.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 264/ 543]               blk.28.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]                 blk.29.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]                 blk.29.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]                 blk.29.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]            blk.29.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]               blk.29.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 543]               blk.29.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]                 blk.29.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]              blk.29.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 273/ 543]               blk.29.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]                 blk.30.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]                 blk.30.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]                 blk.30.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]            blk.30.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]               blk.30.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 543]               blk.30.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]                 blk.30.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]              blk.30.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 282/ 543]               blk.30.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]                 blk.31.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]                 blk.31.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]                 blk.31.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]            blk.31.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]               blk.31.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 543]               blk.31.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]                 blk.31.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]              blk.31.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 291/ 543]               blk.31.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]                 blk.32.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]                 blk.32.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]                 blk.32.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]            blk.32.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]               blk.32.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 543]               blk.32.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]                 blk.32.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]              blk.32.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 300/ 543]               blk.32.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]                 blk.33.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]                 blk.33.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]                 blk.33.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]            blk.33.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]               blk.33.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 543]               blk.33.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]                 blk.33.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]              blk.33.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 309/ 543]               blk.33.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]                 blk.34.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]                 blk.34.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]                 blk.34.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]            blk.34.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]               blk.34.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 543]               blk.34.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]                 blk.34.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]              blk.34.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 318/ 543]               blk.34.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]                 blk.35.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]                 blk.35.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]                 blk.35.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]            blk.35.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]               blk.35.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 543]               blk.35.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]                 blk.35.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]              blk.35.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 327/ 543]               blk.35.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]                 blk.36.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]                 blk.36.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]                 blk.36.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]            blk.36.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]               blk.36.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 543]               blk.36.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]                 blk.36.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]              blk.36.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 336/ 543]               blk.36.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]                 blk.37.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]                 blk.37.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]                 blk.37.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]            blk.37.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]               blk.37.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 543]               blk.37.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]                 blk.37.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]              blk.37.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 345/ 543]               blk.37.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]                 blk.38.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]                 blk.38.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]                 blk.38.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]            blk.38.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]               blk.38.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 543]               blk.38.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]                 blk.38.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]              blk.38.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 354/ 543]               blk.38.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]                 blk.39.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]                 blk.39.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]                 blk.39.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]            blk.39.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]               blk.39.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 543]               blk.39.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]                 blk.39.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]              blk.39.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 363/ 543]               blk.39.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]                 blk.40.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]                 blk.40.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]                 blk.40.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]            blk.40.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]               blk.40.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 543]               blk.40.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]                 blk.40.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]              blk.40.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 372/ 543]               blk.40.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]                 blk.41.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]                 blk.41.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]                 blk.41.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]            blk.41.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]               blk.41.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 543]               blk.41.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]                 blk.41.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]              blk.41.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 381/ 543]               blk.41.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]                 blk.42.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]                 blk.42.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]                 blk.42.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]            blk.42.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]               blk.42.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 543]               blk.42.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]                 blk.42.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]              blk.42.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 390/ 543]               blk.42.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]                 blk.43.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]                 blk.43.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]                 blk.43.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]            blk.43.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]               blk.43.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 543]               blk.43.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]                 blk.43.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]              blk.43.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 399/ 543]               blk.43.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]                 blk.44.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]                 blk.44.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]                 blk.44.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]            blk.44.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]               blk.44.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 543]               blk.44.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]                 blk.44.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]              blk.44.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 408/ 543]               blk.44.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]                 blk.45.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]                 blk.45.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]                 blk.45.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]            blk.45.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]               blk.45.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 543]               blk.45.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]                 blk.45.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]              blk.45.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 417/ 543]               blk.45.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]                 blk.46.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]                 blk.46.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]                 blk.46.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]            blk.46.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]               blk.46.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 543]               blk.46.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]                 blk.46.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]              blk.46.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 426/ 543]               blk.46.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]                 blk.47.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]                 blk.47.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]                 blk.47.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]            blk.47.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]               blk.47.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 543]               blk.47.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]                 blk.47.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]              blk.47.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 435/ 543]               blk.47.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]                 blk.48.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]                 blk.48.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]                 blk.48.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]            blk.48.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]               blk.48.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 543]               blk.48.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]                 blk.48.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]              blk.48.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 444/ 543]               blk.48.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]                 blk.49.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]                 blk.49.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]                 blk.49.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]            blk.49.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]               blk.49.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 543]               blk.49.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]                 blk.49.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]              blk.49.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 453/ 543]               blk.49.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]                 blk.50.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]                 blk.50.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]                 blk.50.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]            blk.50.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]               blk.50.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 543]               blk.50.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]                 blk.50.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]              blk.50.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 462/ 543]               blk.50.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]                 blk.51.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]                 blk.51.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]                 blk.51.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]            blk.51.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]               blk.51.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 543]               blk.51.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]                 blk.51.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]              blk.51.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 471/ 543]               blk.51.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]                 blk.52.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]                 blk.52.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]                 blk.52.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]            blk.52.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]               blk.52.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 543]               blk.52.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]                 blk.52.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]              blk.52.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 480/ 543]               blk.52.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]                 blk.53.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]                 blk.53.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]                 blk.53.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]            blk.53.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]               blk.53.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 543]               blk.53.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]                 blk.53.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]              blk.53.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 489/ 543]               blk.53.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]                 blk.54.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]                 blk.54.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]                 blk.54.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]            blk.54.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]               blk.54.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 543]               blk.54.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]                 blk.54.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]              blk.54.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 498/ 543]               blk.54.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]                 blk.55.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]                 blk.55.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]                 blk.55.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]            blk.55.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]               blk.55.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 543]               blk.55.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]                 blk.55.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]              blk.55.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 507/ 543]               blk.55.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]                 blk.56.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]                 blk.56.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]                 blk.56.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]            blk.56.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]               blk.56.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 543]               blk.56.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 514/ 543]                 blk.56.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 543]              blk.56.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 516/ 543]               blk.56.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]                 blk.57.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]                 blk.57.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]                 blk.57.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]            blk.57.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]               blk.57.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 543]               blk.57.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 523/ 543]                 blk.57.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 543]              blk.57.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 525/ 543]               blk.57.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]                 blk.58.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]                 blk.58.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]                 blk.58.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]            blk.58.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]               blk.58.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 543]               blk.58.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 532/ 543]                 blk.58.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 543]              blk.58.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 534/ 543]               blk.58.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]                 blk.59.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]                 blk.59.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]                 blk.59.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]            blk.59.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]               blk.59.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 540/ 543]               blk.59.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 541/ 543]                 blk.59.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 542/ 543]              blk.59.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 543/ 543]               blk.59.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 75597.39 ms\n",
      "main:    total time = 75597.39 ms\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/65B/ggml-model-f16.gguf' to './models/65B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llama_model_quantize_internal: meta size = 1740160 bytes\n",
      "[   1/ 723]                    token_embd.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   500.00 MiB ->   140.62 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                   output_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   500.00 MiB ->   205.08 MiB | hist: \n",
      "[   4/ 723]                  blk.0.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]                  blk.0.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]                  blk.0.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]             blk.0.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]                blk.0.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[   9/ 723]                blk.0.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 723]                  blk.0.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  11/ 723]               blk.0.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  12/ 723]                blk.0.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  13/ 723]                  blk.1.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]                  blk.1.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]                  blk.1.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]             blk.1.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]                blk.1.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 723]                blk.1.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]                  blk.1.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]               blk.1.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  21/ 723]                blk.1.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  22/ 723]                  blk.2.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]                  blk.2.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]                  blk.2.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]             blk.2.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]                blk.2.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 723]                blk.2.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]                  blk.2.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]               blk.2.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  30/ 723]                blk.2.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  31/ 723]                  blk.3.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]                  blk.3.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]                  blk.3.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]             blk.3.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]                blk.3.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 723]                blk.3.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]                  blk.3.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]               blk.3.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  39/ 723]                blk.3.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  40/ 723]                  blk.4.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]                  blk.4.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]                  blk.4.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]             blk.4.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]                blk.4.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 723]                blk.4.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]                  blk.4.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]               blk.4.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  48/ 723]                blk.4.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  49/ 723]                  blk.5.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]                  blk.5.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]                  blk.5.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]             blk.5.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]                blk.5.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 723]                blk.5.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]                  blk.5.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]               blk.5.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  57/ 723]                blk.5.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  58/ 723]                  blk.6.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]                  blk.6.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]                  blk.6.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]             blk.6.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]                blk.6.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 723]                blk.6.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]                  blk.6.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]               blk.6.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  66/ 723]                blk.6.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  67/ 723]                  blk.7.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]                  blk.7.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]                  blk.7.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]             blk.7.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]                blk.7.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 723]                blk.7.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]                  blk.7.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]               blk.7.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  75/ 723]                blk.7.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  76/ 723]                  blk.8.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]                  blk.8.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]                  blk.8.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]             blk.8.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]                blk.8.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 723]                blk.8.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]                  blk.8.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]               blk.8.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  84/ 723]                blk.8.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  85/ 723]                  blk.9.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]                  blk.9.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]                  blk.9.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]             blk.9.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]                blk.9.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 723]                blk.9.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 723]                  blk.9.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 723]               blk.9.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  93/ 723]                blk.9.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  94/ 723]                 blk.10.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]                 blk.10.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]                 blk.10.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]            blk.10.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]               blk.10.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 723]               blk.10.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 723]                 blk.10.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 723]              blk.10.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 102/ 723]               blk.10.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]                 blk.11.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]                 blk.11.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]                 blk.11.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]            blk.11.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]               blk.11.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 723]               blk.11.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]                 blk.11.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]              blk.11.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 111/ 723]               blk.11.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]                 blk.12.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]                 blk.12.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]                 blk.12.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]            blk.12.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]               blk.12.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 723]               blk.12.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]                 blk.12.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]              blk.12.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 120/ 723]               blk.12.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]                 blk.13.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]                 blk.13.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]                 blk.13.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]            blk.13.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]               blk.13.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 723]               blk.13.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]                 blk.13.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]              blk.13.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 129/ 723]               blk.13.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]                 blk.14.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]                 blk.14.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]                 blk.14.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]            blk.14.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]               blk.14.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 723]               blk.14.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]                 blk.14.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]              blk.14.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 138/ 723]               blk.14.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]                 blk.15.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]                 blk.15.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]                 blk.15.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]            blk.15.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]               blk.15.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 723]               blk.15.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 723]                 blk.15.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 723]              blk.15.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 147/ 723]               blk.15.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]                 blk.16.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]                 blk.16.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]                 blk.16.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]            blk.16.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]               blk.16.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 723]               blk.16.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]                 blk.16.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]              blk.16.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 156/ 723]               blk.16.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]                 blk.17.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]                 blk.17.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]                 blk.17.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]            blk.17.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]               blk.17.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 723]               blk.17.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]                 blk.17.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]              blk.17.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 165/ 723]               blk.17.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]                 blk.18.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]                 blk.18.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]                 blk.18.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]            blk.18.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]               blk.18.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 723]               blk.18.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]                 blk.18.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]              blk.18.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 174/ 723]               blk.18.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]                 blk.19.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]                 blk.19.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]                 blk.19.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]            blk.19.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]               blk.19.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 723]               blk.19.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]                 blk.19.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]              blk.19.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 183/ 723]               blk.19.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]                 blk.20.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]                 blk.20.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]                 blk.20.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]            blk.20.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]               blk.20.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 723]               blk.20.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]                 blk.20.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]              blk.20.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 192/ 723]               blk.20.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]                 blk.21.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]                 blk.21.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]                 blk.21.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]            blk.21.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]               blk.21.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 723]               blk.21.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]                 blk.21.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]              blk.21.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 201/ 723]               blk.21.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]                 blk.22.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]                 blk.22.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]                 blk.22.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]            blk.22.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]               blk.22.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 723]               blk.22.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]                 blk.22.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]              blk.22.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 210/ 723]               blk.22.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]                 blk.23.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]                 blk.23.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]                 blk.23.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]            blk.23.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]               blk.23.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 723]               blk.23.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]                 blk.23.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]              blk.23.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 219/ 723]               blk.23.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]                 blk.24.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]                 blk.24.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]                 blk.24.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]            blk.24.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]               blk.24.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 723]               blk.24.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]                 blk.24.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]              blk.24.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 228/ 723]               blk.24.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]                 blk.25.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]                 blk.25.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]                 blk.25.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]            blk.25.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]               blk.25.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 723]               blk.25.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]                 blk.25.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]              blk.25.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 237/ 723]               blk.25.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]                 blk.26.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]                 blk.26.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]                 blk.26.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]            blk.26.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]               blk.26.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 723]               blk.26.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]                 blk.26.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]              blk.26.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 246/ 723]               blk.26.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]                 blk.27.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]                 blk.27.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]                 blk.27.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]            blk.27.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]               blk.27.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 723]               blk.27.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]                 blk.27.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]              blk.27.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 255/ 723]               blk.27.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]                 blk.28.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]                 blk.28.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]                 blk.28.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]            blk.28.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]               blk.28.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 723]               blk.28.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]                 blk.28.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]              blk.28.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 264/ 723]               blk.28.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]                 blk.29.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]                 blk.29.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]                 blk.29.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]            blk.29.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]               blk.29.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 723]               blk.29.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]                 blk.29.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]              blk.29.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 273/ 723]               blk.29.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]                 blk.30.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]                 blk.30.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]                 blk.30.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]            blk.30.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]               blk.30.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 723]               blk.30.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]                 blk.30.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]              blk.30.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 282/ 723]               blk.30.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]                 blk.31.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]                 blk.31.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]                 blk.31.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]            blk.31.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]               blk.31.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 723]               blk.31.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]                 blk.31.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]              blk.31.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 291/ 723]               blk.31.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]                 blk.32.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]                 blk.32.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]                 blk.32.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]            blk.32.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]               blk.32.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 723]               blk.32.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]                 blk.32.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]              blk.32.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 300/ 723]               blk.32.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]                 blk.33.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]                 blk.33.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]                 blk.33.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]            blk.33.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]               blk.33.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 723]               blk.33.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]                 blk.33.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]              blk.33.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 309/ 723]               blk.33.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]                 blk.34.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]                 blk.34.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]                 blk.34.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]            blk.34.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]               blk.34.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 723]               blk.34.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]                 blk.34.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]              blk.34.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 318/ 723]               blk.34.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]                 blk.35.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]                 blk.35.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]                 blk.35.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]            blk.35.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]               blk.35.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 723]               blk.35.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]                 blk.35.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]              blk.35.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 327/ 723]               blk.35.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]                 blk.36.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]                 blk.36.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]                 blk.36.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]            blk.36.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]               blk.36.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 723]               blk.36.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]                 blk.36.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]              blk.36.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 336/ 723]               blk.36.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]                 blk.37.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]                 blk.37.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]                 blk.37.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]            blk.37.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]               blk.37.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 723]               blk.37.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]                 blk.37.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]              blk.37.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 345/ 723]               blk.37.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]                 blk.38.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]                 blk.38.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]                 blk.38.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]            blk.38.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]               blk.38.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 723]               blk.38.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]                 blk.38.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]              blk.38.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 354/ 723]               blk.38.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]                 blk.39.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]                 blk.39.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]                 blk.39.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]            blk.39.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]               blk.39.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 723]               blk.39.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]                 blk.39.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]              blk.39.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 363/ 723]               blk.39.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]                 blk.40.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]                 blk.40.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]                 blk.40.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]            blk.40.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]               blk.40.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 723]               blk.40.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]                 blk.40.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]              blk.40.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 372/ 723]               blk.40.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]                 blk.41.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]                 blk.41.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]                 blk.41.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]            blk.41.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]               blk.41.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 723]               blk.41.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]                 blk.41.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]              blk.41.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 381/ 723]               blk.41.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]                 blk.42.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]                 blk.42.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]                 blk.42.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]            blk.42.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]               blk.42.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 723]               blk.42.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]                 blk.42.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]              blk.42.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 390/ 723]               blk.42.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]                 blk.43.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]                 blk.43.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]                 blk.43.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]            blk.43.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]               blk.43.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 723]               blk.43.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]                 blk.43.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]              blk.43.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 399/ 723]               blk.43.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]                 blk.44.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]                 blk.44.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]                 blk.44.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]            blk.44.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]               blk.44.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 723]               blk.44.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]                 blk.44.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]              blk.44.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 408/ 723]               blk.44.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]                 blk.45.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]                 blk.45.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]                 blk.45.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]            blk.45.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]               blk.45.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 723]               blk.45.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]                 blk.45.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]              blk.45.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 417/ 723]               blk.45.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]                 blk.46.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]                 blk.46.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]                 blk.46.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]            blk.46.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]               blk.46.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 723]               blk.46.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]                 blk.46.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]              blk.46.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 426/ 723]               blk.46.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]                 blk.47.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]                 blk.47.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]                 blk.47.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]            blk.47.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]               blk.47.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 723]               blk.47.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]                 blk.47.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]              blk.47.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 435/ 723]               blk.47.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]                 blk.48.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]                 blk.48.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]                 blk.48.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]            blk.48.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]               blk.48.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 723]               blk.48.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]                 blk.48.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]              blk.48.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 444/ 723]               blk.48.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]                 blk.49.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]                 blk.49.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]                 blk.49.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]            blk.49.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]               blk.49.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 723]               blk.49.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]                 blk.49.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]              blk.49.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 453/ 723]               blk.49.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]                 blk.50.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]                 blk.50.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]                 blk.50.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]            blk.50.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]               blk.50.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 723]               blk.50.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]                 blk.50.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]              blk.50.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 462/ 723]               blk.50.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]                 blk.51.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]                 blk.51.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]                 blk.51.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]            blk.51.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]               blk.51.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 723]               blk.51.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]                 blk.51.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]              blk.51.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 471/ 723]               blk.51.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]                 blk.52.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]                 blk.52.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]                 blk.52.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]            blk.52.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]               blk.52.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 723]               blk.52.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]                 blk.52.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]              blk.52.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 480/ 723]               blk.52.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]                 blk.53.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]                 blk.53.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]                 blk.53.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]            blk.53.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]               blk.53.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 723]               blk.53.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]                 blk.53.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]              blk.53.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 489/ 723]               blk.53.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]                 blk.54.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]                 blk.54.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]                 blk.54.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]            blk.54.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]               blk.54.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 723]               blk.54.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]                 blk.54.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]              blk.54.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 498/ 723]               blk.54.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]                 blk.55.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]                 blk.55.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]                 blk.55.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]            blk.55.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]               blk.55.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 723]               blk.55.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]                 blk.55.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]              blk.55.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 507/ 723]               blk.55.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]                 blk.56.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]                 blk.56.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]                 blk.56.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]            blk.56.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]               blk.56.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 723]               blk.56.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]                 blk.56.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]              blk.56.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 516/ 723]               blk.56.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]                 blk.57.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]                 blk.57.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]                 blk.57.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]            blk.57.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]               blk.57.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 723]               blk.57.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]                 blk.57.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]              blk.57.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 525/ 723]               blk.57.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]                 blk.58.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]                 blk.58.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]                 blk.58.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]            blk.58.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]               blk.58.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 723]               blk.58.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]                 blk.58.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]              blk.58.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 534/ 723]               blk.58.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]                 blk.59.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]                 blk.59.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]                 blk.59.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]            blk.59.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]               blk.59.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 540/ 723]               blk.59.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]                 blk.59.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]              blk.59.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 543/ 723]               blk.59.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]                 blk.60.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]                 blk.60.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]                 blk.60.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]            blk.60.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]               blk.60.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 549/ 723]               blk.60.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]                 blk.60.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]              blk.60.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 552/ 723]               blk.60.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]                 blk.61.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]                 blk.61.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]                 blk.61.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]            blk.61.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]               blk.61.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 558/ 723]               blk.61.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]                 blk.61.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]              blk.61.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 561/ 723]               blk.61.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]                 blk.62.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]                 blk.62.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]                 blk.62.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]            blk.62.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]               blk.62.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 567/ 723]               blk.62.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]                 blk.62.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]              blk.62.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 570/ 723]               blk.62.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]                 blk.63.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]                 blk.63.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]                 blk.63.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]            blk.63.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]               blk.63.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 576/ 723]               blk.63.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]                 blk.63.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]              blk.63.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 579/ 723]               blk.63.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]                 blk.64.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]                 blk.64.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]                 blk.64.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]            blk.64.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]               blk.64.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 585/ 723]               blk.64.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]                 blk.64.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]              blk.64.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 588/ 723]               blk.64.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]                 blk.65.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]                 blk.65.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]                 blk.65.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]            blk.65.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]               blk.65.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 594/ 723]               blk.65.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]                 blk.65.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]              blk.65.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 597/ 723]               blk.65.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]                 blk.66.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]                 blk.66.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]                 blk.66.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]            blk.66.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]               blk.66.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 603/ 723]               blk.66.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]                 blk.66.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]              blk.66.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 606/ 723]               blk.66.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]                 blk.67.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]                 blk.67.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]                 blk.67.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]            blk.67.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]               blk.67.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 612/ 723]               blk.67.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]                 blk.67.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]              blk.67.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 615/ 723]               blk.67.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]                 blk.68.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]                 blk.68.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]                 blk.68.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]            blk.68.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]               blk.68.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 621/ 723]               blk.68.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]                 blk.68.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]              blk.68.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 624/ 723]               blk.68.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]                 blk.69.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]                 blk.69.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]                 blk.69.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]            blk.69.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]               blk.69.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 630/ 723]               blk.69.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]                 blk.69.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]              blk.69.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 633/ 723]               blk.69.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]                 blk.70.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]                 blk.70.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]                 blk.70.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]            blk.70.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]               blk.70.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 639/ 723]               blk.70.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]                 blk.70.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]              blk.70.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 642/ 723]               blk.70.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]                 blk.71.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]                 blk.71.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]                 blk.71.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]            blk.71.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]               blk.71.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 648/ 723]               blk.71.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]                 blk.71.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]              blk.71.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 651/ 723]               blk.71.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]                 blk.72.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]                 blk.72.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]                 blk.72.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]            blk.72.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]               blk.72.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 657/ 723]               blk.72.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]                 blk.72.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]              blk.72.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 660/ 723]               blk.72.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]                 blk.73.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]                 blk.73.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]                 blk.73.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]            blk.73.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]               blk.73.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 666/ 723]               blk.73.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]                 blk.73.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]              blk.73.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 669/ 723]               blk.73.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]                 blk.74.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]                 blk.74.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]                 blk.74.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]            blk.74.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]               blk.74.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 675/ 723]               blk.74.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]                 blk.74.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]              blk.74.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 678/ 723]               blk.74.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]                 blk.75.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]                 blk.75.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]                 blk.75.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]            blk.75.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]               blk.75.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 684/ 723]               blk.75.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]                 blk.75.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]              blk.75.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 687/ 723]               blk.75.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]                 blk.76.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]                 blk.76.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]                 blk.76.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]            blk.76.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]               blk.76.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 693/ 723]               blk.76.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 694/ 723]                 blk.76.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 695/ 723]              blk.76.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 696/ 723]               blk.76.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]                 blk.77.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]                 blk.77.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]                 blk.77.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]            blk.77.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]               blk.77.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 702/ 723]               blk.77.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 703/ 723]                 blk.77.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 704/ 723]              blk.77.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 705/ 723]               blk.77.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]                 blk.78.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]                 blk.78.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]                 blk.78.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]            blk.78.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]               blk.78.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 711/ 723]               blk.78.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 712/ 723]                 blk.78.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 713/ 723]              blk.78.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 714/ 723]               blk.78.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]                 blk.79.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]                 blk.79.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]                 blk.79.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]            blk.79.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]               blk.79.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 720/ 723]               blk.79.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 721/ 723]                 blk.79.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 722/ 723]              blk.79.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 723/ 723]               blk.79.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 145286.29 ms\n",
      "main:    total time = 145286.29 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.gguf ./models/13B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.gguf ./models/30B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.gguf ./models/65B/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS:  \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
      "removed 'build-info.o'\n",
      "removed 'common.o'\n",
      "removed 'console.o'\n",
      "removed 'ggml-alloc.o'\n",
      "removed 'ggml-backend.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml-quants.o'\n",
      "removed 'ggml.o'\n",
      "removed 'grammar-parser.o'\n",
      "removed 'llama.o'\n",
      "removed 'sampling.o'\n",
      "removed 'train.o'\n",
      "removed 'tests/test-c.o'\n",
      "removed 'benchmark-matmult'\n",
      "removed 'common/build-info.cpp'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'vdot'\n",
      "removed 'q8dot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'convert-llama2c-to-ggml'\n",
      "removed 'simple'\n",
      "removed 'batched'\n",
      "removed 'batched-bench'\n",
      "removed 'save-load-state'\n",
      "removed 'server'\n",
      "removed 'gguf'\n",
      "removed 'llama-bench'\n",
      "removed 'libllava.a'\n",
      "removed 'llava-cli'\n",
      "removed 'baby-llama'\n",
      "removed 'beam-search'\n",
      "removed 'speculative'\n",
      "removed 'infill'\n",
      "removed 'tokenize'\n",
      "removed 'parallel'\n",
      "removed 'finetune'\n",
      "removed 'export-lora'\n",
      "removed 'lookahead'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
      "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
      "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib   -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib  -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "/usr/bin/ld: cannot find llama.o: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "make: *** [Makefile:607: server] Error 1\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88ef08-68f5-4655-ac49-d0368c11b004",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33724f57-0378-4652-86b1-2b4858d56528",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db5a686-a0d6-4af2-9753-fd4e6b42a706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208177\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.98 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   70.42 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 3577.55 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.06 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, after suffering much iniquity under a Management which had lost touch with its fellows, rallied and set up again in honourable independence: thereby gaining new laurels for itself and its exalted Head, and honour of a sort to the remaining Powers who were parties in the contest. \n",
      " The peace established between France and England, by the Treaty of Amiens, was but of short duration; and hostilities recommenced between these two countries in 1803: at which period we enter upon our present narrative. \n",
      "# 15\n",
      "It was a fine evening in June, and the tide was flowing down the river Thames toward the sea. In the centre of its breadth – that is to say, near the middle point between the shores of the United Kingdom and France (which divide the channel) – floated the British ship of war called the _Zephyr_. She had a few hours before weighed anchor from Portsmouth harbour: the home port of her captain, Lord Cochrane.\n",
      "On this night there was no moon in the sky; so that it was not to be expected any one could see very far around him. But yet a person who was well acquainted with the locality – or even a stranger unfamiliar with it – might have detected some peculiarities of form and aspect in the vessel which floated past, at this moment. These consisted of the fact that she had no foretop-sail; and (strange to say) no main-sail either. Her whole canvas consisted of two topsails on either side, with a jib and mizzen, besides a fore-topmast stay-sail.\n",
      "She was also peculiar in her rigging, which consisted entirely of braces; there being no halyards – ropes used for hoisting and lowering the sails – to be seen upon deck anywhere, and no gaffs to be observed either, on account of those two fore-topmast stay-sail sheets (which were attached by means of a block at the mizzen-mast backstay) running directly through the fore topmast-head. She had also only one mainyard. These peculiarities – as well as others which are too numerous to mention here, and of no consequence – might have suggested to any observer who was not on board the _Zephyr_ , that she was a _gun_ brig; for such were all the vessels in her class when this incident occurred; though (it is true) they could hardly be known by their appearance alone.\n",
      "This vessel, with the exception of some ten or twelve guns and some small arms – which did not exceed half a dozen muskets (the whole number), including a few pistols and swords, as well as two carbines, and one blunderbuss, both belonging to the captain – was manned by men. The crew consisted of forty-five persons; fourteen of whom were landsmen or inlanders; for such was the phrase applied to those who did not come from the sea; and twenty-one were seamen and marines; with a doctor aboard, called Peter Pounce, a person whom our readers may have met before, though he is here introduced for the first time.\n",
      "'Ahoy! what ship is that?' said the doctor to the captain at this moment; who had just entered the cabin, from the deck above, where they were about to set sail, with the purpose of sailing into some port or other on the coast, where they would remain a day or two for refreshment and provisions, before continuing their voyage.\n",
      "'That's the _Ranger_ , Doctor Pounce,' replied Captain Finnerty; 'and that is the brig we are going to take. They have already got a prize, as I hear.'\n",
      "'Prizes are of no use in your line of business, captain,' said Peter; 'that is not my profession either: though I think if I were to sail on this coast for another three or four years, I should do very well – better than many, that is; so they tell me. And yet what would become of poor Mistress Finnerty, and her family – with no more money to support them in the world?'\n",
      "'I shall have enough, Doctor,' replied Captain Finnerty: 'they will bring us plenty before we have done. I mean to plunder every craft that falls into my hands, as long as there is a chance of doing it; and the _Ranger_ was bound out from this port in ballast.'\n",
      "'Oh! I don't know,' said Doctor Pounce: 'I hope you won't do that. The _Ranger_ may be quite private property – perhaps a letter of marque, or\n",
      "llama_print_timings:        load time =    2979.20 ms\n",
      "llama_print_timings:      sample time =     156.14 ms /  1024 runs   (    0.15 ms per token,  6558.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.10 ms /   494 tokens (    0.24 ms per token,  4113.14 tokens per second)\n",
      "llama_print_timings:        eval time =    6834.97 ms /  1023 runs   (    6.68 ms per token,   149.67 tokens per second)\n",
      "llama_print_timings:       total time =    7349.09 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9324415-e69d-4ed2-8b56-2094995fbe8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208198\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.98 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   70.42 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 3577.55 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.06 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, stiffer in her joints and less nimble in her limbs, was trying an experiment with her paper: which is called backing (as if a horse were backed), and which has been since successfully tried on several other occasions. The paper had hitherto kept its value; but the experiment now to be made by the National Debt Commissioners would bring out clearly, whether that would always be found to be the case. \n",
      " This document is currently under review (2018). If you'd like to provide feedback please email your comments to [email protected] \n",
      "# LIST OF IMAGES\n",
      "Foreword\n",
      "Title page\n",
      "Preface: Acknowledgements, Dedication, and Note on Text\n",
      "[Chapter 1  \n",
      " **The** _**Titus**_ **Afternoon, Tuesday May the Sixteenth, 1796.**](ch00_fm03a_cont.html#ch-2)\n",
      "[Chapter 2  \n",
      " **Catherine Morland, at the Age of Eleven Years and a Half.**](ch00_fm03a_cont.html#ch-3)\n",
      "[Chapter 3  \n",
      " **The Clergyman's Visit.**](ch00_fm03a_cont.html#ch-4)\n",
      "[Chapter 4  \n",
      " **Mansfield Park.**](ch00_fm03a_cont.html#ch-5)\n",
      "[Chapter 5  \n",
      " **Lady Russell's Story of the Clergyman in the Highlands and his Successive Families at Mansfield, Thornton Lacey &c.&c.**](ch00_fm03a_cont.html#ch-6)\n",
      "[Chapter 6  \n",
      " **Lady Bertram's Illness, and Edmund's Return to Mansfield Park. The Sick-Room.**](ch00_fm03a_cont.html#ch-7)\n",
      "[Chapter 7  \n",
      " **Fanny at Home in Portsmouth.—The Picture Gallery.—Betty. A Letter. The Morlands come to Mansfield Park. Mrs Norris's Complaints, and Sir Thomas's Comforts.**](ch00_fm03a_cont.html#ch-8)\n",
      "[Chapter 8  \n",
      " **Mrs Norris's Scheme.—Fanny and Edmund together at Mansfield Park—Their first Walk.—Susan comes to visit her Sister.—Her Letter to Fanny.—Mr Rushworth calls at Mansfield Park.**](ch00_fm03a_cont.html#ch-9)\n",
      "[Chapter 9  \n",
      " **Fanny at home in Portsmouth again—A Morning Call, a Walk with Sir Thomas, and a Painful Conversation.—The Morlands leave Mansfield Park for the Continent, and some Days pass without their being heard of.—Fanny's Thoughts. A Ball. Mrs Grant's Return. Fanny at Work in her Mother's Drawing-room—Mrs Norris takes a Walk by herself, and is very unwell on her Return.—She tells Lady Bertram to have Fanny up early the next Morning, for she does not think she will be able to attend them to the Shrimpings.—Their Visit to Mrs Price.—A Letter from Edmund.—Fanny goes with Edmund to visit his Father. Their Visit at home is interrupted by a Call from Sir Thomas and Mr Woodhouse. Lady Bertram, Fanny, and Mrs Norris go out for a Drive in the Carriage. Edmund proposes to join them—Their Meeting at the Cottage.—Miss Lee and Miss Murray come to Mansfield Park to pay their Visit.—Fanny finds herself able to attend her Aunt and Uncle when they call on Mrs Price, as she is engaged to go to a Concert with her Cousins. The Concert.—She is sitting near Edmund in the Upper Box.—A Conversation between Miss Bertram and Fanny about Mr Rushworth—He arrives.—Fanny returns Home.**](68-toc.html#d5e23)\n",
      "[Chapter I  \n",
      " _Mansfield Park_](68-toc.html#d5e24)\n",
      "My mother died when I was two years old, and never saw Edmund or me.—'Oh! yes, she did!' said Fanny to herself; 'she was with us only the week before we went away from Mansfield.'—For so had she heard it represented to\n",
      "llama_print_timings:        load time =    5575.80 ms\n",
      "llama_print_timings:      sample time =     152.97 ms /  1024 runs   (    0.15 ms per token,  6693.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =      91.02 ms /   494 tokens (    0.18 ms per token,  5427.62 tokens per second)\n",
      "llama_print_timings:        eval time =    6855.88 ms /  1023 runs   (    6.70 ms per token,   149.21 tokens per second)\n",
      "llama_print_timings:       total time =    7334.90 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0cf40d6-f822-4ba8-a315-c3819dd2af9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208213\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.98 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   70.42 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 3577.55 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.06 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, on her part, went uphill without friction; issuing nothing but notes payable to bearer (thereby enriching many honest men) and spending nothing (thereby impoverishing many more). The former process filled the pockets of a great number of people with bank-notes; the latter brought theirs back empty. But at this point an incident took place which, in its consequences, may be compared to that memorable event in the history of our own country, when the tea of the East India Company was thrown into Boston harbour. It is not necessary to repeat here the story of what occurred on the memorable 13th Vendémiaire (5th October), 1795, in consequence of the faction called _The Mountain_ becoming paramount at Paris. Suffice it to say, that this event has ever since that period governed the history of both countries; and that at the moment when Mr. Percy, editor of the Morning Post, wrote his last number for that paper, the consequences produced by 13th Vendémiaire (5th October), 1795, were still daily felt in Britain: though more than two years had elapsed since 14th July (25th Germinal), 1796, when a counter-revolution of a different sort had again altered the government at Paris.\n",
      "At this moment (late October) Mr. Percy had no foreseeing powers in him: he was, indeed, like many other persons whose business it is to speak their thoughts aloud without knowing that they are thinking them—a man who has, as a great statesman said of himself when a little younger than Percy, \"nothing on his mind.\" His brain was quite clear; but he had not a word to say upon the subject. He could not even think whether it would be better for him or his paper to write against this government which now reigned at Paris (as opposed to that which had reigned there two years before), inasmuch as his own views were altogether unenlightened upon such a delicate matter. For though he was himself one of the very few men who, in those days, did not sympathise with _la réaction_ —a thing that must have been very trying to him as an honest man—he felt it impossible to say anything against that government: because all the friends and associates whom he would be most likely to address upon such a subject were precisely of the same opinion. He was therefore, at this time, without words; for his mind was like a garden that had been unoccupied since midsummer, and consequently produced very little fruit. In such circumstances—when the head is as clear as in the morning, and the heart as heavy as evening—there are few things more likely to happen than what happened now: namely, an abrupt interruption, or rather a sudden rush, of unconnected matter into one's mind: in this case into Mr. Percy's.\n",
      "But when I say that he had no foreseeing powers, I do not mean to be understood as implying that his thoughts were merely the result of association; for such an interpretation would be a gross misstatement. What I mean is, simply and plainly, that if there was anything in Mr. Percy's brain besides mere words or sentences at that moment, it was not at all in his power to express what those things might have been.\n",
      "It had occurred to him only half an hour before he heard the voice outside which now called upon him as a master of ceremonies for an evening party: though whether the thoughts of which I speak were already there or merely rising up like so many children's faces on the schoolmaster at the sound of the bell, was not certain to Mr. Percy; indeed, he had forgotten them, and they seemed almost to have disappeared by that time, when his attention became suddenly arrested by a ringing voice calling upon him to come out. It came from below in the passage, which ran along the whole length of the building, as was necessary for ventilation: it was the voice of one who had been summoned to a ball, and who now, like the master of ceremonies on such occasions, waited patiently by the door; not at all impatient in expectation that Mr. Percy would open it, and go out with him, or rather for him, into the garden.\n",
      "He had no objection: indeed he was already standing at his study-window, looking down upon a scene which seemed very different from what it had been when he went to bed at night; for the sky was not overcast as then. Now there were stars shining through it, and two or three of them were of the first magnitude.\n",
      "\"All right,\" said Mr. Percy—or rather,\n",
      "llama_print_timings:        load time =    2153.07 ms\n",
      "llama_print_timings:      sample time =     156.06 ms /  1024 runs   (    0.15 ms per token,  6561.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.82 ms /   494 tokens (    0.43 ms per token,  2343.25 tokens per second)\n",
      "llama_print_timings:        eval time =    6849.83 ms /  1023 runs   (    6.70 ms per token,   149.35 tokens per second)\n",
      "llama_print_timings:       total time =    7460.72 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a124de-0e22-49fc-8ea3-d324d831a961",
   "metadata": {},
   "source": [
    "### 7B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518024e7-d24d-4701-a788-6445d04b3ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208233\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.13 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  250.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 12603.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, having undertaken to be the banker’s agent in this respect, was already getting both ends into a fine state of mutual exhaustion. \n",
      " The French nation (for there are two nations in France) were boiling over with an exasperated feeling against their government, and particularly against its chief. They had taken an affectionate leave of Louis, as the man who held out to them the brightest hopes of liberty: but now that his coronation was at hand, they began to reconsider their sentiments; and to repent a little, perhaps, of the enthusiasm they had shewn on his behalf. The most sagacious among them were not without misgivings as to the stability of the constitution. They saw a young king; but an old queen regnant. The Duke d’Orleans, the first minister, was the son of a duchess, the daughter of Henry IV.: and they knew that royalist blood ran in his veins. His father had been beheaded by the people of Paris: his mother had saved herself from destruction by flying to Vienna: Louis Philippe himself, when under forty, had twice been brought back from the scaffold. It was plain that this minister, who had so many reasons for hating kings and courts, was not likely to show much fondness for a royalist dynasty. In vain they called themselves the constitutional party, and represented their king as merely a symbol of national liberty: no one believed them but those among whom he was born, and whom it was his interest to flatter. The king, moreover, did all that could be expected from him, in order to disarm jealousy: he received none of the courtiers who had been at war with his predecessor; he treated Madame Elizabeth with open contempt; he went as little as possible into the Louvre; he put no restraint upon his wife’s vices; and, by thus making her odious to the nation, made himself odious also.\n",
      "There were some in France who feared that he had not so much changed his principles as altered the form of them: that he was still a constitutionalist at heart, but a royalist by instinct; and that he would take care of his own interests, and let the nation go hang. Louis Philippe, moreover, had other qualities which disposed men to suspicion, and gave occasion for uneasiness. He was very good-looking: there is no doubt that this helped him forward in life; but it has been found by experience that good looks are a great nuisance to any government whatever: they do not give it the same authority as a bad countenance. Then he had no real education, and showed his want of one; so that men could not help saying to each other, “What will become of us?”\n",
      "But there was another circumstance which made the new dynasty doubly formidable: its wealth.\n",
      "Louis Philippe’s father-in-law, Louis XVI., had left him no patrimony whatever: the king himself was so poor that he could hardly have provided for his mother and sisters without selling what remained of his wife’s jewels; but in France money is more than ever a sort of personal right, and those who had made fortunes before the revolution found themselves in an excellent position. They were the only people who still had anything to lose, and they had always thought that there was a kind of natural alliance between riches and the monarchy.\n",
      "Money is not as dangerous in France as it ought to be: it has no place at the court; it is always considered disgraceful if you have too much of it; and one does not hear of anyone being ruined there except for political reasons, which means by losing money. But this is a misfortune peculiar to France alone in Europe; in all other countries people are obliged to make their fortunes, and do so very often in a manner that gives no offence.\n",
      "The men of the revolution had made great mistakes in the way they treated money: instead of making it an object of derision among the upper classes like kings did elsewhere, they made it almost an object of worship, to the detriment of the higher virtues and to their own misfortune, for we may observe that since then, the monarchy has always been poor, though not destitute. The revolutionary government was jealous of those who had amassed wealth during the period of tyranny: it confiscated it all on one pretext or another; and it did so in a way which proved its lack of any practical ideas about political economy.\n",
      "When a revolution is over, money ought to be allowed to come back again into circulation; for there are still the same number of men, as before, who require a livelihood just as much as before;\n",
      "llama_print_timings:        load time =   17299.66 ms\n",
      "llama_print_timings:      sample time =     165.68 ms /  1024 runs   (    0.16 ms per token,  6180.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =      68.82 ms /   494 tokens (    0.14 ms per token,  7178.04 tokens per second)\n",
      "llama_print_timings:        eval time =   16810.01 ms /  1023 runs   (   16.43 ms per token,    60.86 tokens per second)\n",
      "llama_print_timings:       total time =   17281.36 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec4b98bb-a046-4be5-9f98-cc5f64724a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208271\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.13 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  250.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 12603.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, after long wallowing in a mire of barbarism and ignorance, was beginning to come out, like Pharaoh’s daughter. France, nevertheless, is said to have had some good writers at this time: as, indeed, she has often done since, and as she still continues to do. The names of Voltaire, Rousseau, and Beaumarchais are familiar to us; we know them also by their works. Voltaire died a few years after this tale was written; Rousseau some years before. \n",
      " Chapter II \n",
      " London in the latter end of August: a week or two after the Prince’s death. The streets, for the most part, quiet and empty; though in the evening crowds resort to Westminster Abbey. In one street a little knot of people were talking together under a lamp-post, among whom Charles Prossick had found a chance of hearing something of what was going on, while he waited at a shop door. A woman was telling them that she and another person had gone in the evening to the house of Mr. Barsad (who had been pointed out by the spies as one to be watched), in Golden Square; and that they had there found several people assembled: among whom, on being asked who were present, her friend replied: “All here.” \n",
      " “What’s that?” demanded a man, stepping from among those standing. The woman made no answer; the man looked round as if to find out from whom the question came. It was addressed, however, by another, who had stepped forward too: so that there was nothing for it but an immediate reply. \n",
      " “‘All here?’” repeated Charles Prossick to himself; and walked on with a more decided step than he had hitherto displayed. A few words from the people whom he met—some of them strangers to him, some only acquaintances—a word or two about this affair: nothing definite; no direct information; but all pointing in the direction in which he wished to go. \n",
      " “All here,” repeated Charles Prossick to himself again as he walked on. The people had told him what they knew; and he was satisfied that it would be useless for them further to waste their time and his by standing together talking in the dark street: while he, if he were determined and resolute enough, might probably have better success elsewhere. \n",
      " And so, with a brisk step, he turned out of the quiet streets wherein he had been walking, into those that were more noisy and populous, as is often the case in great cities; and found his way through crowds to the spot which he sought. He made towards the door at the end of the narrow passage by which it was entered, without seeing any person who seemed likely to stop him: but just as he put his hand upon it, a man stood before it, looking out into the street with a countenance of extreme weariness and distress—a man who might be twenty-two or twenty-three. \n",
      " “Are you alone?” asked Charles Prossick of the person in the doorway: meaning by that expression whether there was anybody behind him in the passage; for, as far as he knew, there were but few other doors to open into it from either way. \n",
      " The man turned his head, and looked at Charles Prossick for a moment before he replied. “I think there’s some one coming,” said he, “but I don’t know who ’tis.” \n",
      " “Will you let me in?” demanded Charles Prossick, who now saw that the man was a little intoxicated, and looked to be not at all strong. \n",
      " “No; why should I? There ain’t nothing for you here,” said he. \n",
      " “You don’t know what there is here!” answered Charles Prossick, quickly. \n",
      " The other laughed in the same kind of quiet, thoughtful manner as before: saying, as if to himself, “Don’t he?” and looking down at the ground for a few moments. Then he took his hand off from the door-handle which he held, and said, “Well, come in then; I dare say there ’s something here.” \n",
      " The other went up into the passage with him, and then they both stood in front of it, looking about them: Charles Prossick meanwhile saying to himself,—“Here is a man who has been out at work all day, and has been drinking too; he will do whatever I like, unless he is drunk enough. And this must be done before midnight.” \n",
      " He looked down the passage, but saw nobody coming. He then went into the room at its end, in which he found a number of men seated or standing about: some, as if they\n",
      "llama_print_timings:        load time =   11339.92 ms\n",
      "llama_print_timings:      sample time =     156.04 ms /  1024 runs   (    0.15 ms per token,  6562.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =      67.96 ms /   494 tokens (    0.14 ms per token,  7268.87 tokens per second)\n",
      "llama_print_timings:        eval time =   16760.30 ms /  1023 runs   (   16.38 ms per token,    61.04 tokens per second)\n",
      "llama_print_timings:       total time =   17219.83 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c07219-6f1a-4e38-9590-bb0fe7cea15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208303\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.13 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  250.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 12603.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, who had begun by taking the lead in the art of paying away promissory notes which were not worth their engraving, kept by the same process abreast with the French nation up to the very moment when that unfortunate people, accustomed to the hard conditions of struggle and self-dependence, found themselves suddenly called to carry on a prolonged and expensive war without money in their purses. England, who had lent millions upon millions, for purposes of paper speculation at home and of military adventure abroad, was in imminent peril of not being able to redeem her engagements. The national credit was at stake; nothing less than the independence of the country was involved. The Bank of England issued notes for two millions more than it had funds in its coffers; but finding that these could not be negotiated, made advances on its own private account, to the amount of another million, which it hoped to make good out of the national credit at a future day. In the midst of all this difficulty and danger, an event occurred which disturbed the tranquillity of Europe, and threatened, in some degree, to embarrass the exertions necessary for saving the country from ruin. \n",
      " There was a Frenchman on board ship who did not speak English. He had come over to try if he could pick up some money in his own way; that is, by making bets with any one whom he chanced to meet. The other people on board, knowing something of the man's nationality and of his profession, took care to keep away from him, for they were all in a position of great danger—that of having their word of honour to lose. They could not very well avoid making themselves acquainted with what was going on in France; and this made them uneasy. If the Frenchman were to discover that there had been some disaster in his own country, which would prevent him from backing his bets when he wanted to do so—the man's word of honour being pledged for the payment of any sum upon demand, down to any amount; a thing not at all uncommon on such occasions as this.\n",
      "The Frenchman was very cautious in what he said and did. He took no part in conversation; and if anyone were to ask him something about himself or his country, he always answered very briefly. No one ventured upon asking him questions without reason. A rumour came aboard the vessel which seemed to concern only Paris, and not France as a whole. The Frenchman, on hearing it, changed colour; but no further notice was taken of what he said. After some days, however, he grew more talkative, and appeared rather gay: it was as if he had made up his mind to face the worst; and this appearance was increased by his having a purse on him, from which he pulled out a handful of gold pieces occasionally, for no particular purpose apparently. Some persons in the ship who had formed an acquaintance with him asked him what money meant: he gave them a very short answer to that question: it was not worth while to ask more than once.\n",
      "When they came into the Bay of Biscay, a young lady on board made the Frenchman's acquaintance. She began by asking him for news about Paris—or rather, she asked if there had been any change since he left it? He replied that everything was going on as usual; and at that very moment she saw in his hand a paper containing a statement of what had happened in France during the time that elapsed since his departure. She asked him to give her the paper—and he handed it over to her at once.\n",
      "She read it, and found that there was an insurrection on foot: Paris was in the hands of the people; and all other places were threatened with similar changes. She returned to the Frenchman, who had resumed his former tone of cheerfulness, and asked him how he could have such a look? He replied that everything was going right: the king was safe—and the army would be soon on the spot to restore order. The young lady read aloud what she thought of the paper in her hands; and at first the Frenchman appeared astonished by this unseasonable curiosity; but as he saw nothing was gained by denying that there really was an insurrection, he answered: \"Yes—the poor people have had enough to bear! You see how they are getting their revenge.\" He made a low bow with his hand on his heart; and then turning towards the ship's side went out upon the deck.\n",
      "As soon as she could speak to any one of her friends in safety, the young lady told them what had happened. She was asked if it were true that they were going to France? She replied: \"Yes.\" Her answer caused a general dismay; and\n",
      "llama_print_timings:        load time =   11204.34 ms\n",
      "llama_print_timings:      sample time =     155.26 ms /  1024 runs   (    0.15 ms per token,  6595.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =      67.79 ms /   494 tokens (    0.14 ms per token,  7286.89 tokens per second)\n",
      "llama_print_timings:        eval time =   16806.65 ms /  1023 runs   (   16.43 ms per token,    60.87 tokens per second)\n",
      "llama_print_timings:       total time =   17265.94 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b6f4e-6235-47a1-8942-f8d04e6f1b29",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8088f7bc-2a09-43d7-8940-5721b1af6bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208346\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   88.03 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 6936.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never killed any one; burning a woman, who had never harmed anybody; flogging a chain-gang (of infants!) so unmercifully with fiendish flags that the offscourings of the streets in after times pleaded for mercy, compared with the treatment endured by these poor children; and working off on them a cruelly remunerative trade, in the best holiday humours. It was a happy time, a gracious time, a season of much peace. Oh, Crimes of England!\n",
      "Happy Britain! Happy France! Where are now, or why not here, the good old times?\n",
      "And what does she hold in her right hand? Not an HAMMER for the blow that has fallen on us, but a ROLL OF PAPER MONEY. The only symbol we have left to work with is a piece of paper! And all we can do is sit there, with this blank paper, and hope...\n",
      "- 451 Magazine -\n",
      "\n",
      "*  Datos: Q27079663\n",
      "*  Multimedia: La Charte de la Charbonnière (illustration by Honoré Daumier)\n",
      "\n",
      "## Véase también\n",
      "\n",
      "* Anexo:Cronología de la Revolución francesa de 1848\n",
      "\n",
      "## Notas y referencias\n",
      "\n",
      "1.  Cf. La Révolution Française de 1848 et de 1968 (en francés), por Pierre Barlcays, en la revista Les Temps Modernes, nº 370, febrero de 1975; págs. 222-244.\n",
      "2.  «Charte du travail». Consultado el 15 de mayo de 2021.\n",
      "\n",
      "## Bibliografía\n",
      "\n",
      "* L'Univers des droits de l'Homme, La Charte de 1830. Textes fondateurs d'un nouveau monde politique (en francés), editorial La Découverte, coll. Repères, París, 2005. ISBN 978-2-707-14697-6.\n",
      "* L'Univers des droits de l'Homme, La Charte de 1830 - Le débat, un nouveau monde politique (en francés), editorial La Découverte, coll. Repères, París, 2005. ISBN 978-2-707-14686-2.\n",
      "* Mousnier, René (dir.), L’Europe au XIXe siècle (1789–1914), tomo I de la Histoire générale de l'Europe depuis 1550: Europe en révolution (1789-1848), La Découverte/Fayard, 2001.\n",
      "* Philippe Vigier, L’Europe de Napoléon - Histoire de l'Europe entre 1800 et 1815, Éditions Perrin, París, 2009.\n",
      "* Poujol, Louis (dir.), Les années révolutionnaires, 1789-1804, tome II de la Histoire générale de l'Europe depuis 1550: Europe en révolution (1789-1848), La Découverte/Fayard, 2013.\n",
      "* Prévost-Paradol, Adrien, Napoléon et sa mère, Michel Lévy frères, París, 1865 (edición actual: Tallandier, 2002).\n",
      "* Puech, Emile, Le Premier Empire à l'heure actuelle - Débats d'aujourd'hui sur la France napoléonienne, Fayard, París, 2007.\n",
      "* Schama, Simon, La Révolution française, Perrin, París, 1989 (edición actual: Gallimard, « Folio Histoire ») .\n",
      "* Séguier, René, L'Europe à Napoléon - De la bataille d'Austerlitz à la Campagne de Pologne, 1805-1807, tome I de la Histoire générale de l'Europe depuis 1550: Europe en révolution (1789-1848), La Découverte/Fayard, 2012.\n",
      "* Stourm, Georges, Le Premier Empire, \n",
      "llama_print_timings:        load time =    5787.19 ms\n",
      "llama_print_timings:      sample time =     148.93 ms /  1024 runs   (    0.15 ms per token,  6875.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.26 ms /   494 tokens (    0.32 ms per token,  3141.23 tokens per second)\n",
      "llama_print_timings:        eval time =   11524.05 ms /  1023 runs   (   11.26 ms per token,    88.77 tokens per second)\n",
      "llama_print_timings:       total time =   12087.61 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c6185c-75d7-4a75-914e-e198684b3c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208380\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   88.03 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 6936.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never killed any one, or impeded the traffic in any legally recognised highway; who, indeed, had hawked rice pudding and milk through the streets (which he continued to do after his conviction), but whose habit it was to carry some blood about with him in a bag. The whole history of France under the old _regime_ , is little else than a history of what is called by courtesy of comity, a system of administration. \n",
      "# **PART TWO**  \n",
      " A Tale of Two Cities\n",
      "# CHAPTER I\n",
      "I\n",
      "In the last century and a half, Paris had undergone many changes in her features, but there are not wanting people to whom she is as beautiful as ever. It may be questioned whether this attraction does not arise as much from the general aspect of the city, which has certainly lost none of its peculiar physiognomic expression, as from the local impressions connected with individual spots; but certain it is, that though some old favourite places have departed for ever, others have grown up in their room to occupy a similar position, or even a higher one. This phenomenon does not exist in other capitals of Europe; but in Paris, as we know well, there are always plenty of people eager to take the place of those who are tired of it: nay more – for it is the first time that such has happened with us – new comers prefer Paris to all her sister capitals. Thus is preserved a perpetual youthfulness and animation in this metropolis; but we will leave to our successors the task of describing its actual state, and proceed to treat of its aspect in days long departed.\n",
      "We will go back, therefore, three-quarters of a century, – say to 1792. This epoch is still so recent that the memory of it lingers among us; but it has receded far enough for all personal impressions of the past to be effaced from our minds, and to allow us to view it with serenity and impartiality. The Revolution had not yet begun – the old regime was still in its vigour and freshness. Paris, then, was a city of the Middle Ages; for its ancient monuments, its narrow streets and lanes, its churches, palaces and halls of justice, are all that can be found there.\n",
      "There were two towns: – one of brick houses, such as were built in those times; the other of timber houses, and not so large or populous. The former occupied only the centre and lower part of Paris; the latter covered a greater extent towards the north. There was a gate, through which you entered either city at your will. This gate – the Porte Saint-Denis – has now been walled up for more than half a century past, but there were still living persons who remembered that this gate, when open, was formed of four great doors, placed in front of each other and closing on each side, so as to form an enclosure. This gate opened upon the market-place which had taken its name from it; and to go out, you descended into a deep ditch by two staircases called the Petit and Grande Saint-Denis, through which the water of the old moat ran along.\n",
      "The lower town consisted almost exclusively of houses covered with tiles or thatched, of three stories only, and of such narrowness in frontage as to seem like one immense wall: this was the Rue Saint Denis, with its cross streets; the Rue Tailleur-aux-Laines, the Rue des Chantre, the Rue de la Calandriere, the Rue du Pas, etc., – names which have still retained their ancient forms. In some of these streets, especially the rue Saint Denis, were found great numbers of small tradesmen living under the same roof with their workshops: and even as late as 1789, there was a family of weavers at No. 126, in the Rue des Chantre, which consisted of twenty-seven persons.\n",
      "The houses were covered with tiles; those which have been repaired are still seen to be so, but it has become necessary to replace the tiled roofs by slates, for want of the proper material: this has been done at No. 287, in the Rue Saint Denis – the last remaining house covered with tiles that we could find. In some parts of these streets was seen a great quantity of timber houses built on piles and raised from ten to twenty feet above the ground. These\n",
      "llama_print_timings:        load time =   11822.31 ms\n",
      "llama_print_timings:      sample time =     163.76 ms /  1024 runs   (    0.16 ms per token,  6253.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.16 ms /   494 tokens (    0.32 ms per token,  3143.35 tokens per second)\n",
      "llama_print_timings:        eval time =   11515.21 ms /  1023 runs   (   11.26 ms per token,    88.84 tokens per second)\n",
      "llama_print_timings:       total time =   12074.49 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b39cd7-6046-4f03-ab13-50efe29377c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208407\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   88.03 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 6936.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the auspices of Monseigneur, she was increasing the expense of government, to reduce the revenue; and in order that these pecuniary regulations might not be hampered by her unfortunate commerce, she was revoking at a prodigious rate the edicts designed to encourage it.\n",
      "Sweeping thus, in an easy course, downhill all the perils of life, the chances of fortune, in their external aspects, might seem to have been sufficiently allowed for; and there might be some reason in supposing that the human creature born in the year one thousand seven hundred and seventy-five, had little on his hands from the first hour of his foreshortened existence, but to enjoy himself.\n",
      "But was there nothing behind all these things? \n",
      "Was it possible, if they were taken together, not to imagine a something which might pervade them, as a reed gives music to the wind that passes through it; and yet be invisible to mortal eyes? It is but a metaphor: let us go on.\n",
      "There was nothing else to be done by Mr. Scoresby or his wife, but to put their little girl in long clothes at once; which being attended to, they prepared to start for home, as the weather cleared up, and afforded better travelling opportunities every day. And this being determined on, Mrs. Scoresby made no secret of her resolution to Mr. Westgate, who had been a daily visitor ever since their arrival in town: but he, as usual, took the subject in quite another light; and was so positive that they were mistaken in what they said about it, and could not do it; and that they would be more comfortable where they were than going home on any account, if Mr. Scoresby chose to stay away, why—!\n",
      "\"Mr. Westgate,\" said Mrs. Scoresby with great determination (and the weather had so changed for the worse that there seemed now no chance of her having him at her fireside again), \"you needn't be afraid of our changing our minds, and going home. We shall go tomorrow, if you will come along to see us settled in London.\"\n",
      "\"I think I ought not,\" said Mr. Westgate. \"But we are all going to the country soon—Sir Leicester Dedlock and his friends. There'll be a very pleasant house-party. It's quite right that you should go back, but I shall stay where I am.\"\n",
      "Mrs. Scoresby made no rejoinder; but she said to herself with some irritation as she closed her reticule: \"That's all nonsense!\"—and, as it were, discharged her feelings on the subject in a kind of murmur. For, if Mrs. Rouncewell had known it, Mrs. Scoresby thought so much more of Mr. Westgate than of any other man that she would have given half-a-crown for his company to town, though she were not going back herself at all!\n",
      "All that day, as the coach went along the road in a muddy spring atmosphere of general gloom and depression, Mrs. Scoresby's mind was so occupied with thoughts of Mr. Westgate, and what he would say if he knew how much she thought about him, that her husband could not help remarking her silence—nor had she any reply to make when he addressed his wife in a tone between that of remonstrance and expostulation, and said, \"Why, my dear, you have quite forgotten your usual gaiety and brightness!\"\n",
      "\"I hope not,\" returned Mrs. Scoresby; but she sighed as she made the reply.\n",
      "There was no cause for complaint in Mrs. Chick, who had come to town for the day and taken her place inside the coach. It happened that Mr. Tulkinghorn, now a venerable-looking man of nearly eighty years old, had never been married; he was as single as ever, still kept on his own little chambers in Lincoln's Inn Fields (and if you had asked him why and how came they to be always the same little chambers?—he would have told you that he knew no other), and took such an interest in Mrs. Chick, for some reason or other, that he seldom failed of going over to Chickney on a day when business did not take him down into Lincolnshire.\n",
      "On the present occasion he came down with a young man of four-and-twenty or so—the Warden's son, Mr. Richard Carstone by name—whose countenance wore an unnaturally grave and thoughtful expression, which had become habitual to him in his father's lifetime\n",
      "llama_print_timings:        load time =   11667.47 ms\n",
      "llama_print_timings:      sample time =     157.80 ms /  1024 runs   (    0.15 ms per token,  6489.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.29 ms /   494 tokens (    0.32 ms per token,  3140.72 tokens per second)\n",
      "llama_print_timings:        eval time =   11491.36 ms /  1023 runs   (   11.23 ms per token,    89.02 tokens per second)\n",
      "llama_print_timings:       total time =   12043.06 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb27f0-9bb9-4ee5-b7d6-617348a5e1d5",
   "metadata": {},
   "source": [
    "### 13B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d9de77b-77e4-4b90-b88e-a4d54857a2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208438\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.72 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  312.64 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors: VRAM used: 24514.08 MiB\n",
      "................................................................................................\n",
      "CUDA error 2 at ggml-cuda.cu:9077: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9077: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab51db-d528-440d-ad00-e9f76d2b4b5d",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c306d621-040a-44e7-bb85-f7e64fcd3785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208489\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.10 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  114.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 17390.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his lips, for speaking disrespectfully of a Sister of Charity, – with frying to death a poor hair-dresser, for having dispersed seditious libels, at the request of _monsieur le procurateur générale,_ – and with committing to the devil half a dozen harmless artisans, for having inquired, of their own accord, into the condition of the king's finances. \n",
      "I\n",
      "Sydney Carton, idler, and Lady Dedlock, idler, compare notes at Tellson's Bank. They find that they are both wasting life wretchedly. The gentleman has lost his love because of a lack of money and is in the process of squandering what money he does have on drinking himself into an early grave. The lady suffers from ennui, and wishes she could leave her husband for the one man who has ever touched her heart, but feels trapped in her marriage by duty and custom. They console each other.\n",
      "Two other people entered the bank while these three were sitting there; no more than two, although Miss Pross and Jerry might have been entitled to make up the company, in virtue of their combined opinion of Solomon's judgement.\n",
      "One was Mr Cruncher, late of Tellson's; he of the iron hand. A honest, bluff, red-faced, pitifully earnest man, he was, as intelligent and thoughtful people say, a leading character in these histories. The other was a much younger man, whom Mr Cruncher took in charge, as representative of young men in general.\n",
      "'Where,' said the gentleman, after observing Mr Cruncher's evident pleasure at sight of him, 'where is our late chief?'\n",
      "'_My_ late Chief,' echoed Jerry. 'He was my late Chief too, though I never see him, which I don't think likely to do, any ways.'\n",
      "Mr Cruncher resigned his seat to the honoured guest, and exchanged it for one opposite to him, over against Tellson's iron railings. As he took his seat, he thus delivered himself:\n",
      "'Well, you see, Mum, at first I was a bit lonehanded with being discharged so short; but, you see, Miss Pross she kept me company; an' finally Old Orlick he got hold of me, an' then we went in for drinking. An', blest you,' said Mr Cruncher, taking a resigned sip out of his can, 'I should never ha' left off!'\n",
      "'_You_ would have been much better without it.'\n",
      "It was the unanimous opinion of the rest, expressed by Miss Pross, Jerry, and even Monseigneur, that Mr Cruncher had made no such remark in a more expressive form.\n",
      "'It did me good,' said Mr Cruncher. 'It did me real good, that stuff did.'\n",
      "Miss Pross could not persuade him to the least disrespect of it; neither would he hear a word in favour of abstinence.\n",
      "'While I have been a honest tradesman in a small way,' said Mr Cruncher, 'I earned money, you see; and while I go on like that, you can't say I don't get to know the flavour of my beer. But it must come from good ingredients; none of your brewing in the witches' caldrons here.'\n",
      "By which Mr Cruncher meant to express his disapprobation of the manufacture of porter and other kinds of liquid which are usually sold in general depositories of liquids in London.\n",
      "'Into every profound caldron, too,' said Jerry, 'something foul has got out of it, I have heard say. Mayhap it's not the fault of the caldron neither; it may be the fault of them that brews the stuff.'\n",
      "Mr Cruncher and his friends highly approved of this sentiment. They agreed to smoke Mr Cruncher's pipe, as a testimony of the unanimity with which they fell in with it. In order to raise their spirits, somebody (Jerry, I think) proposed Mrs Cruncher's health. Nobody was better qualified than Jerry to do honour to that proposition, and he did it like a man. When he had discharged his obligations to Mrs Cruncher in a strain of eloquence that held the company enthralled\n",
      "llama_print_timings:        load time =   25055.09 ms\n",
      "llama_print_timings:      sample time =     155.52 ms /  1024 runs   (    0.15 ms per token,  6584.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     348.92 ms /   494 tokens (    0.71 ms per token,  1415.81 tokens per second)\n",
      "llama_print_timings:        eval time =   25490.69 ms /  1023 runs   (   24.92 ms per token,    40.13 tokens per second)\n",
      "llama_print_timings:       total time =   26234.34 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96cd60f7-7530-446a-8461-a099e73f3b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208553\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.10 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  114.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 17390.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the protection of the cartooning facility of a barber, and the hard words of a horse-professor, liberty and equality began to make themselves at home in the state. Atheism, after suspending one or two ancient gods, flew away with a dancing motive against the culture of grass and potatoes, and was seen no more. \n",
      "Chapter I\n",
      "The year 1764 had been marked by many marvels and signs, foretelling great events that were soon to happen on earth. In the course of his life it may sometimes occur to a man that he has met with people who had no business wherever they happened to be; that they were community errors, walking blunders, but which he knew nothing about when they were before him. As the years went round, this man's conviction that there were such unaccountable phenomena in his existence gradually deepened and strengthened; he came to believe more and more firmly in the supernatural, and in the omnipresence of the supernatural. He saw with clearer eyes than most men ever have bestowed upon them that many strangely-wonderful things are every day passing unexplained before us; not originating in such an enigmatical way as to be at once and for ever illegible even by the sharpest wits, but fraught with such meaning, were we endowed with the power of reading them, that earth would be heaven at such a revelation.\n",
      "To begin with, it had been no small matter in itself that this man should be walking along Piccadilly at the time when all the fashionable world was collected there; and that the object and intention of his walking there himself should have been to take a general survey of the fashionable world which was collected there. The street at that hour—between four and six in the afternoon—was one blaze of light: the mansions were illuminated from top to bottom by rows of wax candles placed in sconces against the window-glass; the shops were alight with tapers, waving over their goods like flags; the waiters at the famous coffee-houses (much more plentiful than now) held great flambeaux, made of twigs of wood skewered together, which they carried like censers: and even without this strong reinforcement from the neighbouring establishments, each coffee-house itself shone forth resplendently in the intensity of its own separate conviction that to be worth anything it must be all alight, inside and out.\n",
      "Mr Tulkinghorn had been making himself one of the usual crowd for some time before he became aware of having attracted the notice of any single individual in it but the people who usually went there for entertainment. He was not a man to be easily detached from his thoughts, or induced to shed his occupations and amusements; neither did he find himself at any loss to employ himself while walking along the streets. There was plenty to think of in London as well as at his country residence.\n",
      "There are few more pleasant places for meditation than a crowded street; and Mr Tulkinghorn, undoubtedly, had found it so that afternoon, when he was recalled from his reverie by having his hat deliberately knocked off his head into the road. Starting at this unusual occurrence, he turned sharply round and seized the arm of a young woman who instantly coloured very red while she held up her hand in a frightened manner as if to prevent his seizing her in that way.\n",
      "“Here! Don’t you?” said Mr Tulkinghorn, rather sternly. “Don’t be afraid! It may save you from being run over.”\n",
      "The young lady was so much disconcerted by the loss of her presence of mind as to break into tears on the spot and so far fail in that best of all silences—a silence perfectly understood.\n",
      "“Hush, hush, my dear!” said Mr Tulkinghorn, alarmed by the sight of tears, “I didn’t mean to startle you, I am sure. Come, come! You must not give way like this.”\n",
      "The young lady had honesty enough to blush again and say with great humility, “Yes, sir,” though the apology came in rather inconveniently after she had turned her back upon him and was gone.\n",
      "“Bless my heart!” said Mr Tulkinghorn to himself, looking at nothing. “What could this young woman be thinking of? Dear me, dear me! We must get on.” And he walked forward again, but slowly and ponderingly—and it may be observed that he put his hand to his chin, a characteristic action with him when anything was on his mind—till he came to the end\n",
      "llama_print_timings:        load time =   17626.06 ms\n",
      "llama_print_timings:      sample time =     157.27 ms /  1024 runs   (    0.15 ms per token,  6511.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     349.17 ms /   494 tokens (    0.71 ms per token,  1414.77 tokens per second)\n",
      "llama_print_timings:        eval time =   25548.15 ms /  1023 runs   (   24.97 ms per token,    40.04 tokens per second)\n",
      "llama_print_timings:       total time =   26293.02 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffca4a61-421e-4071-8f3d-2b3bbff4796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208601\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.10 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  114.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 17390.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 780.00 MB\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "llama_new_context_with_model: compute buffer total size = 100.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 97.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 18267.64 MiB (model: 17390.64 MiB, context: 877.00 MiB)\n",
      "\n",
      "system_info: n_threads = 127 / 255 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his body in eighty-seven places, because he had not kneeled down in the rain within one hour after a harmless edict against public processions. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is not improbable that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there was sheltering for the night a man, who with such unspeakable horror felt a dream pass over him as he slept in the lodging which his labour provided, that he awoke in the morning trembling. \n",
      " II  \n",
      "The Sea Still Rises\n",
      "Early in the afternoon of a certain day in November, the London train was going westward through the wintry weather; frosty and foggy, with hard white flakes of snow which were being whirled about by unceasing wind. The guard in his little padded cell at the end of the train, looking out at the dismal prospect upon the plunging engines and tumultuous wheels, could not have been more sheltered from the weather than the two passengers in the compartment he was watching.\n",
      "A gentleman and lady had taken their places in it shortly after the train had left the Blackfriars Road station, and the gentleman had put before him the evening paper, with his overcoat upon the back of the seat, and his silk handkerchief spread out upon his knees. But though he held the paper so as to hide his face, it was evident to the guard that he did not read. He seemed to be waiting for something to happen; his manner expressed this; and the more the train plunged into tunnels, the more the wind and snow assailed it, the more his expectation grew.\n",
      "At last the guard, looking at him attentively, thought he discerned upon his forehead three words formed in the dirt and soot which were generally there, as if they had been placed Expressly on his brow, and particularly to be revealed to the guard. They were: Wait for her!\n",
      "The guard watched the gentleman and read these words — Wait for her! — until a lighted station disclosed them, brightly written, to every one. At last when it did come, the guard saw the three words form themselves, as if they had sparkled out of fire, in letters of flame on the outside of the carriage window. The guard was so impressed by this spectacle (he afterwards pointed out where the writing had appeared to have remained a long time), that he was fain, when they came out of the tunnel in which this took place, and it was daylight, not only to ask the gentleman if anything were the matter, but further to hint that his charge was being watched over.\n",
      "To which the gentleman replied: \"Yes. I know.\" But appeared to have no more to say on the subject.\n",
      "The guard bade him good-night, and the two travellers had not gone very far from the station when they alighted. They got out upon a little piece of frail wooden platform, approached by a footway of trestles that left the slush at which it terminated to spirt about the white nightgown in which Mrs Lirriper's housemaid had been suddenly taken ill and carried out, a perfectly new brown straw bonnet on her head. But no one took much notice of this sickness. All attention was concentrated on the gentleman who, being found to have left a bundle behind him after all the passengers were gone, came back alone to fetch it: in the dead of night.\n",
      "He was not at all disconcerted by this incident, but went through the ceremony of opening the bundle with great composure and then sat down on it to wait for some one who was to join him there. He made himself very comfortable, as if he were going to be a long time about it, pulled his hat over his nose to screen his eyes from the wind, thrust his chin into the breast of his top-coat, folded his gloved hands on his stick, and sat in that posture looking towards London.\n",
      "The dead stillness that reigned at this remote place, so near to a great city: the occasional sounds made by some restless workman who had not gone home: the distant r\n",
      "llama_print_timings:        load time =   13865.22 ms\n",
      "llama_print_timings:      sample time =     166.76 ms /  1024 runs   (    0.16 ms per token,  6140.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     349.63 ms /   494 tokens (    0.71 ms per token,  1412.93 tokens per second)\n",
      "llama_print_timings:        eval time =   25513.67 ms /  1023 runs   (   24.94 ms per token,    40.10 tokens per second)\n",
      "llama_print_timings:       total time =   26282.86 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b20ad-6a54-458f-a85a-f3cc3002e935",
   "metadata": {},
   "source": [
    "### 30B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba52581-5544-4b97-85b8-457790a8b342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208654\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.78 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  406.46 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "llm_load_tensors: VRAM used: 61639.32 MiB\n",
      "......................................\n",
      "CUDA error 2 at ggml-cuda.cu:9077: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9077: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b24fab-f886-498f-a9f5-d91bf3d5a189",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1671 (8fe03ff)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703208772\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.01 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =  140.90 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors: VRAM used: 34950.11 MiB\n",
      "...................................................................\n",
      "CUDA error 2 at ggml-cuda.cu:9077: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9077: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
