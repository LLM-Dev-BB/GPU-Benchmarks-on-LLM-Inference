{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Sat Dec 23 05:46:10 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080         On | 00000000:45:00.0 Off |                  N/A |\n",
      "|  0%   22C    P8               17W / 320W|      1MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "model name\t: AMD EPYC 7452 32-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       263805516 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 14501, done.\u001b[K\n",
      "remote: Counting objects: 100% (4554/4554), done.\u001b[K\n",
      "remote: Compressing objects: 100% (240/240), done.\u001b[K\n",
      "remote: Total 14501 (delta 4438), reused 4335 (delta 4314), pack-reused 9947\u001b[K\n",
      "Receiving objects: 100% (14501/14501), 16.53 MiB | 19.11 MiB/s, done.\n",
      "Resolving deltas: 100% (10148/10148), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f20a13-e11c-4e56-98b9-4a896510f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   7837      0 --:--:-- --:--:-- --:--:--  7848\n",
      "Downloading tokenizer\n",
      "--2023-12-23 05:46:14--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  1.68MB/s    in 0.3s    \n",
      "\n",
      "2023-12-23 05:46:16 (1.68 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-12-23 05:46:16--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 05:46:16 (67.2 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-12-23 05:46:16--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[===================>]  12.55G  18.2MB/s    in 11m 16s \n",
      "\n",
      "2023-12-23 05:57:33 (19.0 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-12-23 05:57:33--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 05:57:34 (25.0 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 05:57:34--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 05:57:34 (127 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-12-23 05:57:57--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  60.1MB/s    in 3m 22s  \n",
      "\n",
      "2023-12-23 06:01:20 (61.5 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-23 06:01:20--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  22.8MB/s    in 10m 19s \n",
      "\n",
      "2023-12-23 06:11:40 (20.0 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-12-23 06:11:40--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 06:11:40 (34.5 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 06:11:40--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 06:11:42 (216 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-12-23 06:12:29--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  22.0MB/s    in 32m 48s \n",
      "\n",
      "2023-12-23 06:45:18 (7.88 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 06:45:18--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  44.5MB/s    in 5m 36s  \n",
      "\n",
      "2023-12-23 06:50:55 (46.1 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 06:50:55--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  58.9MB/s    in 5m 8s   \n",
      "\n",
      "2023-12-23 06:56:05 (50.3 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 06:56:05--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  64.4MB/s    in 4m 15s  \n",
      "\n",
      "2023-12-23 07:00:22 (60.8 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-12-23 07:00:22--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 07:00:22 (36.2 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 07:00:22--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 07:00:23 (278 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-12-23 07:02:33--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  50.5MB/s    in 4m 27s  \n",
      "\n",
      "2023-12-23 07:07:00 (58.3 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:07:00--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  49.5MB/s    in 5m 47s  \n",
      "\n",
      "2023-12-23 07:12:48 (44.9 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:12:48--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated  80%[===============>    ]  12.21G  --.-KB/s    in 6m 5s   \n",
      "\n",
      "2023-12-23 07:18:55 (34.2 MB/s) - Connection closed at byte 13107331072. Retrying.\n",
      "\n",
      "--2023-12-23 07:18:56--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16323959449 (15G), 3216628377 (3.0G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[++++++++++++++++===>]  15.20G  41.0MB/s    in 78s     \n",
      "\n",
      "2023-12-23 07:20:15 (39.2 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:20:15--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  92.1MB/s    in 2m 57s  \n",
      "\n",
      "2023-12-23 07:23:12 (88.1 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:23:12--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  60.1MB/s    in 4m 20s  \n",
      "\n",
      "2023-12-23 07:27:32 (59.9 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:27:32--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  38.6MB/s    in 5m 26s  \n",
      "\n",
      "2023-12-23 07:32:59 (47.7 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:32:59--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  44.0MB/s    in 7m 58s  \n",
      "\n",
      "2023-12-23 07:40:58 (32.6 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:40:58--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  73.7MB/s    in 5m 8s   \n",
      "\n",
      "2023-12-23 07:46:07 (50.5 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-12-23 07:46:07--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 07:46:07 (32.0 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-12-23 07:46:07--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-23 07:46:08 (633 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B\t7B-v2\t\t\t  ggml-vocab-mpt.gguf\n",
      "13B-v2\tggml-vocab-aquila.gguf\t  ggml-vocab-refact.gguf\n",
      "30B\tggml-vocab-baichuan.gguf  ggml-vocab-stablelm-3b-4e1t.gguf\n",
      "65B\tggml-vocab-falcon.gguf\t  ggml-vocab-starcoder.gguf\n",
      "70B-v2\tggml-vocab-gpt-neox.gguf  tokenizer.model\n",
      "7B\tggml-vocab-llama.gguf\t  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# Obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376847f6-3509-4a11-b211-ec1fb0d73ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13283  100 13283    0     0  47015      0 --:--:-- --:--:-- --:--:-- 46936\n"
     ]
    }
   ],
   "source": [
    "# If you encounter the error \"does not appear to have a file named config.json\" when converting the models to ggml FP16 format, try to convert the model to huggingface format to get the config.json file.\n",
    "!curl -o convert_llama_weights_to_hf.py https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bee9eb-b358-40f2-8582-f93dad231f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ff2a39-7f7d-4285-9652-a89b9503005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp tokenizer.model 7B/\n",
    "!cp tokenizer.model 13B/\n",
    "!cp tokenizer.model 30B/\n",
    "!cp tokenizer.model 65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc57513-b8fa-46ba-8315-9f118b998c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.36.2)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: protobuf>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.34.0->-r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall accelerate # If you have this package, uninstall it first, then use `convert to hf model` to get the config.json.\n",
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636f3946-8a2b-4a9d-a23a-9e880e96ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/7B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/13B/.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 319, in <module>\n",
      "    main()\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 307, in main\n",
      "    write_model(\n",
      "  File \"/workspace/llama.cpp/convert_llama_weights_to_hf.py\", line 271, in write_model\n",
      "    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2863, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/30B/.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at models/65B/.\n"
     ]
    }
   ],
   "source": [
    "# We don't need these models actually. We only need this to figure out the config.json error.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/7B/ --model_size 7B --output_dir models/7B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/13B/ --model_size 13B --output_dir models/13B/\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/30B/ --model_size 30B --output_dir models/30B/ # Surprisingly, it still solves the problem although you can't find the config.json file.\n",
    "!python3 convert_llama_weights_to_hf.py --input_dir models/65B/ --model_size 65B --output_dir models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbf7dba-9d3e-4c40-95c8-58ccc63d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit your params.json file if the \"vocab_size\" mismatch\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/7B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/7B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/13B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/13B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/30B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/30B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('models/65B/params.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'vocab_size' key\n",
    "data['vocab_size'] = 32000\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open('models/65B/params.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/7B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [4096]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 4096]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "skipping tensor blk.0.attn_rot_embd\n",
      "skipping tensor blk.1.attn_rot_embd\n",
      "skipping tensor blk.2.attn_rot_embd\n",
      "skipping tensor blk.3.attn_rot_embd\n",
      "skipping tensor blk.4.attn_rot_embd\n",
      "skipping tensor blk.5.attn_rot_embd\n",
      "skipping tensor blk.6.attn_rot_embd\n",
      "skipping tensor blk.7.attn_rot_embd\n",
      "skipping tensor blk.8.attn_rot_embd\n",
      "skipping tensor blk.9.attn_rot_embd\n",
      "skipping tensor blk.10.attn_rot_embd\n",
      "skipping tensor blk.11.attn_rot_embd\n",
      "skipping tensor blk.12.attn_rot_embd\n",
      "skipping tensor blk.13.attn_rot_embd\n",
      "skipping tensor blk.14.attn_rot_embd\n",
      "skipping tensor blk.15.attn_rot_embd\n",
      "skipping tensor blk.16.attn_rot_embd\n",
      "skipping tensor blk.17.attn_rot_embd\n",
      "skipping tensor blk.18.attn_rot_embd\n",
      "skipping tensor blk.19.attn_rot_embd\n",
      "skipping tensor blk.20.attn_rot_embd\n",
      "skipping tensor blk.21.attn_rot_embd\n",
      "skipping tensor blk.22.attn_rot_embd\n",
      "skipping tensor blk.23.attn_rot_embd\n",
      "skipping tensor blk.24.attn_rot_embd\n",
      "skipping tensor blk.25.attn_rot_embd\n",
      "skipping tensor blk.26.attn_rot_embd\n",
      "skipping tensor blk.27.attn_rot_embd\n",
      "skipping tensor blk.28.attn_rot_embd\n",
      "skipping tensor blk.29.attn_rot_embd\n",
      "skipping tensor blk.30.attn_rot_embd\n",
      "skipping tensor blk.31.attn_rot_embd\n",
      "Writing models/7B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 11/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 12/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 13/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 14/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 15/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 16/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 17/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 18/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 19/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 20/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 21/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 22/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 23/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 24/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 25/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 26/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 28/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 29/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 31/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 32/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 33/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 34/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 35/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 37/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 38/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 40/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 41/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 42/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 43/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 44/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 46/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 47/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 49/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 50/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 51/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 52/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 53/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 55/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 56/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 58/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 59/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 60/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 61/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 62/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 64/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 65/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 67/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 68/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 69/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 70/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 71/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3\n",
      "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3\n",
      "[ 73/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   4\n",
      "[ 74/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[ 76/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 77/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 78/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 79/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 80/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   4\n",
      "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   4\n",
      "[ 82/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   4\n",
      "[ 83/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[ 85/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 86/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 87/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 88/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 89/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   4\n",
      "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   4\n",
      "[ 91/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   5\n",
      "[ 92/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n",
      "[ 94/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 95/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 96/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 97/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 98/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[100/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[101/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[102/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[103/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[104/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[105/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[106/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[108/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[109/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[110/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[111/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[112/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[113/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[114/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[115/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[116/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[117/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[118/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[119/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[120/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[121/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[122/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[123/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[124/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[125/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[126/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[127/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[128/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[129/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[130/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[131/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[132/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[133/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[134/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[135/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[136/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[137/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[138/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[139/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[140/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[141/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[142/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[143/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[144/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[145/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[146/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[147/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[148/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[149/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[150/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[151/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[152/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[153/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[154/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[155/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[156/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[158/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[159/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[160/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[162/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[163/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[164/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[165/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[166/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[167/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[168/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[169/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[170/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[171/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[172/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[173/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[174/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[175/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[176/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[177/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[178/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[179/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[180/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[181/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[182/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[183/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[184/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[185/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[186/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[187/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[188/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9\n",
      "[189/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9\n",
      "[190/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9\n",
      "[191/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  10\n",
      "[192/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[193/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[194/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[195/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[196/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11\n",
      "[197/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  11\n",
      "[198/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  11\n",
      "[199/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  12\n",
      "[200/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  12\n",
      "[201/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[203/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[204/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[205/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  12\n",
      "[206/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  12\n",
      "[207/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  12\n",
      "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  12\n",
      "[209/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  12\n",
      "[210/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[211/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[212/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[213/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[214/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  12\n",
      "[215/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  12\n",
      "[216/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  12\n",
      "[217/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  12\n",
      "[218/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  12\n",
      "[219/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[220/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[221/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[222/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  12\n",
      "[223/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  12\n",
      "[224/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  13\n",
      "[225/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  13\n",
      "[226/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  13\n",
      "[227/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  13\n",
      "[228/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[229/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[230/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[231/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[232/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  13\n",
      "[233/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  13\n",
      "[234/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  13\n",
      "[235/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  13\n",
      "[236/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  13\n",
      "[237/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[238/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[239/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[240/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[241/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  13\n",
      "[242/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  13\n",
      "[243/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  14\n",
      "[244/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  14\n",
      "[245/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  14\n",
      "[246/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[247/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[248/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[249/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[250/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  14\n",
      "[251/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  14\n",
      "[252/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  14\n",
      "[253/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  14\n",
      "[254/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  14\n",
      "[255/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[256/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[257/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[258/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[259/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  14\n",
      "[260/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  14\n",
      "[261/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  14\n",
      "[262/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  14\n",
      "[263/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  14\n",
      "[264/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[265/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[266/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[267/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[268/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
      "[269/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  15\n",
      "[270/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  15\n",
      "[271/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  15\n",
      "[272/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  15\n",
      "[273/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[274/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[275/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[276/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[277/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
      "[278/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  15\n",
      "[279/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  15\n",
      "[280/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  15\n",
      "[281/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  15\n",
      "[282/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[283/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[284/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[285/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[286/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
      "[287/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  15\n",
      "[288/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  16\n",
      "[289/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  16\n",
      "[290/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  16\n",
      "[291/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  16\n",
      "Wrote models/7B/ggml-model-f16.gguf\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "params = Params(n_vocab=32000, n_embd=5120, n_layer=40, n_ctx=2048, n_ff=13824, n_head=40, n_head_kv=40, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/13B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 5120]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [5120]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 5120]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [5120]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [5120]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [5120]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [5120]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [5120]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [5120]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [5120]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [5120]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [5120]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [5120, 5120]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [5120, 5120]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [13824, 5120]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [5120, 13824]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [13824, 5120]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [5120]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [5120]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [5120]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [5120]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [5120]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [5120]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [5120]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [5120]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [5120]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [5120]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [5120]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [5120]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [5120]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [5120]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [5120]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [5120]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [5120]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [5120]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [5120]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [5120]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [5120]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [5120]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [5120]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [5120]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [5120]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [5120]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [5120]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [5120]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [5120]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [5120]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [5120]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [5120]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [5120, 5120]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [5120, 5120]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [13824, 5120]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [5120, 13824]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [13824, 5120]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [5120]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [5120]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/13B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/363] Writing tensor token_embd.weight                      | size  32000 x   5120  | type F16  | T+   0\n",
      "[  2/363] Writing tensor output_norm.weight                     | size   5120           | type F32  | T+   0\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type F16  | T+   1\n",
      "[  4/363] Writing tensor blk.0.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  5/363] Writing tensor blk.0.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  6/363] Writing tensor blk.0.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   1\n",
      "[  7/363] Writing tensor blk.0.attn_output.weight               | size   5120 x   5120  | type F16  | T+   1\n",
      "[  8/363] Writing tensor blk.0.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   1\n",
      "[  9/363] Writing tensor blk.0.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   1\n",
      "[ 10/363] Writing tensor blk.0.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   1\n",
      "[ 11/363] Writing tensor blk.0.attn_norm.weight                 | size   5120           | type F32  | T+   1\n",
      "[ 12/363] Writing tensor blk.0.ffn_norm.weight                  | size   5120           | type F32  | T+   2\n",
      "[ 13/363] Writing tensor blk.1.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 14/363] Writing tensor blk.1.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 15/363] Writing tensor blk.1.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 16/363] Writing tensor blk.1.attn_output.weight               | size   5120 x   5120  | type F16  | T+   2\n",
      "[ 17/363] Writing tensor blk.1.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 18/363] Writing tensor blk.1.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   2\n",
      "[ 19/363] Writing tensor blk.1.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   2\n",
      "[ 20/363] Writing tensor blk.1.attn_norm.weight                 | size   5120           | type F32  | T+   3\n",
      "[ 21/363] Writing tensor blk.1.ffn_norm.weight                  | size   5120           | type F32  | T+   3\n",
      "[ 22/363] Writing tensor blk.2.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 23/363] Writing tensor blk.2.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 24/363] Writing tensor blk.2.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 25/363] Writing tensor blk.2.attn_output.weight               | size   5120 x   5120  | type F16  | T+   3\n",
      "[ 26/363] Writing tensor blk.2.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   3\n",
      "[ 27/363] Writing tensor blk.2.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   3\n",
      "[ 28/363] Writing tensor blk.2.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 29/363] Writing tensor blk.2.attn_norm.weight                 | size   5120           | type F32  | T+   4\n",
      "[ 30/363] Writing tensor blk.2.ffn_norm.weight                  | size   5120           | type F32  | T+   4\n",
      "[ 31/363] Writing tensor blk.3.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 32/363] Writing tensor blk.3.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 33/363] Writing tensor blk.3.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 34/363] Writing tensor blk.3.attn_output.weight               | size   5120 x   5120  | type F16  | T+   4\n",
      "[ 35/363] Writing tensor blk.3.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   4\n",
      "[ 36/363] Writing tensor blk.3.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   4\n",
      "[ 37/363] Writing tensor blk.3.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 38/363] Writing tensor blk.3.attn_norm.weight                 | size   5120           | type F32  | T+   5\n",
      "[ 39/363] Writing tensor blk.3.ffn_norm.weight                  | size   5120           | type F32  | T+   5\n",
      "[ 40/363] Writing tensor blk.4.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 41/363] Writing tensor blk.4.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 42/363] Writing tensor blk.4.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 43/363] Writing tensor blk.4.attn_output.weight               | size   5120 x   5120  | type F16  | T+   5\n",
      "[ 44/363] Writing tensor blk.4.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   5\n",
      "[ 45/363] Writing tensor blk.4.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   5\n",
      "[ 46/363] Writing tensor blk.4.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 47/363] Writing tensor blk.4.attn_norm.weight                 | size   5120           | type F32  | T+   6\n",
      "[ 48/363] Writing tensor blk.4.ffn_norm.weight                  | size   5120           | type F32  | T+   6\n",
      "[ 49/363] Writing tensor blk.5.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 50/363] Writing tensor blk.5.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 51/363] Writing tensor blk.5.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 52/363] Writing tensor blk.5.attn_output.weight               | size   5120 x   5120  | type F16  | T+   6\n",
      "[ 53/363] Writing tensor blk.5.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   6\n",
      "[ 54/363] Writing tensor blk.5.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   7\n",
      "[ 55/363] Writing tensor blk.5.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 56/363] Writing tensor blk.5.attn_norm.weight                 | size   5120           | type F32  | T+   7\n",
      "[ 57/363] Writing tensor blk.5.ffn_norm.weight                  | size   5120           | type F32  | T+   7\n",
      "[ 58/363] Writing tensor blk.6.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 59/363] Writing tensor blk.6.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 60/363] Writing tensor blk.6.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 61/363] Writing tensor blk.6.attn_output.weight               | size   5120 x   5120  | type F16  | T+   7\n",
      "[ 62/363] Writing tensor blk.6.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   7\n",
      "[ 63/363] Writing tensor blk.6.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   8\n",
      "[ 64/363] Writing tensor blk.6.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   8\n",
      "[ 65/363] Writing tensor blk.6.attn_norm.weight                 | size   5120           | type F32  | T+   8\n",
      "[ 66/363] Writing tensor blk.6.ffn_norm.weight                  | size   5120           | type F32  | T+   8\n",
      "[ 67/363] Writing tensor blk.7.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 68/363] Writing tensor blk.7.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 69/363] Writing tensor blk.7.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 70/363] Writing tensor blk.7.attn_output.weight               | size   5120 x   5120  | type F16  | T+   8\n",
      "[ 71/363] Writing tensor blk.7.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   8\n",
      "[ 72/363] Writing tensor blk.7.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+   9\n",
      "[ 73/363] Writing tensor blk.7.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+   9\n",
      "[ 74/363] Writing tensor blk.7.attn_norm.weight                 | size   5120           | type F32  | T+   9\n",
      "[ 75/363] Writing tensor blk.7.ffn_norm.weight                  | size   5120           | type F32  | T+   9\n",
      "[ 76/363] Writing tensor blk.8.attn_q.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 77/363] Writing tensor blk.8.attn_k.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 78/363] Writing tensor blk.8.attn_v.weight                    | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 79/363] Writing tensor blk.8.attn_output.weight               | size   5120 x   5120  | type F16  | T+   9\n",
      "[ 80/363] Writing tensor blk.8.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+   9\n",
      "[ 81/363] Writing tensor blk.8.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+  10\n",
      "[ 82/363] Writing tensor blk.8.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  10\n",
      "[ 83/363] Writing tensor blk.8.attn_norm.weight                 | size   5120           | type F32  | T+  10\n",
      "[ 84/363] Writing tensor blk.8.ffn_norm.weight                  | size   5120           | type F32  | T+  10\n",
      "[ 85/363] Writing tensor blk.9.attn_q.weight                    | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 86/363] Writing tensor blk.9.attn_k.weight                    | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 87/363] Writing tensor blk.9.attn_v.weight                    | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 88/363] Writing tensor blk.9.attn_output.weight               | size   5120 x   5120  | type F16  | T+  10\n",
      "[ 89/363] Writing tensor blk.9.ffn_gate.weight                  | size  13824 x   5120  | type F16  | T+  10\n",
      "[ 90/363] Writing tensor blk.9.ffn_down.weight                  | size   5120 x  13824  | type F16  | T+  11\n",
      "[ 91/363] Writing tensor blk.9.ffn_up.weight                    | size  13824 x   5120  | type F16  | T+  11\n",
      "[ 92/363] Writing tensor blk.9.attn_norm.weight                 | size   5120           | type F32  | T+  11\n",
      "[ 93/363] Writing tensor blk.9.ffn_norm.weight                  | size   5120           | type F32  | T+  11\n",
      "[ 94/363] Writing tensor blk.10.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[ 95/363] Writing tensor blk.10.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[ 96/363] Writing tensor blk.10.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  11\n",
      "[ 97/363] Writing tensor blk.10.attn_output.weight              | size   5120 x   5120  | type F16  | T+  11\n",
      "[ 98/363] Writing tensor blk.10.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  11\n",
      "[ 99/363] Writing tensor blk.10.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  12\n",
      "[100/363] Writing tensor blk.10.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  12\n",
      "[101/363] Writing tensor blk.10.attn_norm.weight                | size   5120           | type F32  | T+  12\n",
      "[102/363] Writing tensor blk.10.ffn_norm.weight                 | size   5120           | type F32  | T+  12\n",
      "[103/363] Writing tensor blk.11.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[104/363] Writing tensor blk.11.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[105/363] Writing tensor blk.11.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  12\n",
      "[106/363] Writing tensor blk.11.attn_output.weight              | size   5120 x   5120  | type F16  | T+  12\n",
      "[107/363] Writing tensor blk.11.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  12\n",
      "[108/363] Writing tensor blk.11.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  13\n",
      "[109/363] Writing tensor blk.11.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  13\n",
      "[110/363] Writing tensor blk.11.attn_norm.weight                | size   5120           | type F32  | T+  13\n",
      "[111/363] Writing tensor blk.11.ffn_norm.weight                 | size   5120           | type F32  | T+  13\n",
      "[112/363] Writing tensor blk.12.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[113/363] Writing tensor blk.12.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[114/363] Writing tensor blk.12.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  13\n",
      "[115/363] Writing tensor blk.12.attn_output.weight              | size   5120 x   5120  | type F16  | T+  13\n",
      "[116/363] Writing tensor blk.12.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  13\n",
      "[117/363] Writing tensor blk.12.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  14\n",
      "[118/363] Writing tensor blk.12.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  14\n",
      "[119/363] Writing tensor blk.12.attn_norm.weight                | size   5120           | type F32  | T+  14\n",
      "[120/363] Writing tensor blk.12.ffn_norm.weight                 | size   5120           | type F32  | T+  14\n",
      "[121/363] Writing tensor blk.13.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[122/363] Writing tensor blk.13.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[123/363] Writing tensor blk.13.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  14\n",
      "[124/363] Writing tensor blk.13.attn_output.weight              | size   5120 x   5120  | type F16  | T+  14\n",
      "[125/363] Writing tensor blk.13.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  15\n",
      "[126/363] Writing tensor blk.13.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  15\n",
      "[127/363] Writing tensor blk.13.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  15\n",
      "[128/363] Writing tensor blk.13.attn_norm.weight                | size   5120           | type F32  | T+  15\n",
      "[129/363] Writing tensor blk.13.ffn_norm.weight                 | size   5120           | type F32  | T+  15\n",
      "[130/363] Writing tensor blk.14.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[131/363] Writing tensor blk.14.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[132/363] Writing tensor blk.14.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  15\n",
      "[133/363] Writing tensor blk.14.attn_output.weight              | size   5120 x   5120  | type F16  | T+  15\n",
      "[134/363] Writing tensor blk.14.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  16\n",
      "[135/363] Writing tensor blk.14.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  16\n",
      "[136/363] Writing tensor blk.14.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  16\n",
      "[137/363] Writing tensor blk.14.attn_norm.weight                | size   5120           | type F32  | T+  16\n",
      "[138/363] Writing tensor blk.14.ffn_norm.weight                 | size   5120           | type F32  | T+  16\n",
      "[139/363] Writing tensor blk.15.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[140/363] Writing tensor blk.15.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[141/363] Writing tensor blk.15.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  16\n",
      "[142/363] Writing tensor blk.15.attn_output.weight              | size   5120 x   5120  | type F16  | T+  16\n",
      "[143/363] Writing tensor blk.15.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  17\n",
      "[144/363] Writing tensor blk.15.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  17\n",
      "[145/363] Writing tensor blk.15.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  17\n",
      "[146/363] Writing tensor blk.15.attn_norm.weight                | size   5120           | type F32  | T+  17\n",
      "[147/363] Writing tensor blk.15.ffn_norm.weight                 | size   5120           | type F32  | T+  17\n",
      "[148/363] Writing tensor blk.16.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  17\n",
      "[149/363] Writing tensor blk.16.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[150/363] Writing tensor blk.16.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  18\n",
      "[151/363] Writing tensor blk.16.attn_output.weight              | size   5120 x   5120  | type F16  | T+  18\n",
      "[152/363] Writing tensor blk.16.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  18\n",
      "[153/363] Writing tensor blk.16.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  18\n",
      "[154/363] Writing tensor blk.16.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  19\n",
      "[155/363] Writing tensor blk.16.attn_norm.weight                | size   5120           | type F32  | T+  19\n",
      "[156/363] Writing tensor blk.16.ffn_norm.weight                 | size   5120           | type F32  | T+  19\n",
      "[157/363] Writing tensor blk.17.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[158/363] Writing tensor blk.17.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[159/363] Writing tensor blk.17.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  19\n",
      "[160/363] Writing tensor blk.17.attn_output.weight              | size   5120 x   5120  | type F16  | T+  19\n",
      "[161/363] Writing tensor blk.17.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  19\n",
      "[162/363] Writing tensor blk.17.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  19\n",
      "[163/363] Writing tensor blk.17.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  20\n",
      "[164/363] Writing tensor blk.17.attn_norm.weight                | size   5120           | type F32  | T+  20\n",
      "[165/363] Writing tensor blk.17.ffn_norm.weight                 | size   5120           | type F32  | T+  20\n",
      "[166/363] Writing tensor blk.18.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[167/363] Writing tensor blk.18.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[168/363] Writing tensor blk.18.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  20\n",
      "[169/363] Writing tensor blk.18.attn_output.weight              | size   5120 x   5120  | type F16  | T+  20\n",
      "[170/363] Writing tensor blk.18.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  20\n",
      "[171/363] Writing tensor blk.18.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  20\n",
      "[172/363] Writing tensor blk.18.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  21\n",
      "[173/363] Writing tensor blk.18.attn_norm.weight                | size   5120           | type F32  | T+  21\n",
      "[174/363] Writing tensor blk.18.ffn_norm.weight                 | size   5120           | type F32  | T+  21\n",
      "[175/363] Writing tensor blk.19.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[176/363] Writing tensor blk.19.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[177/363] Writing tensor blk.19.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  21\n",
      "[178/363] Writing tensor blk.19.attn_output.weight              | size   5120 x   5120  | type F16  | T+  21\n",
      "[179/363] Writing tensor blk.19.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  21\n",
      "[180/363] Writing tensor blk.19.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  21\n",
      "[181/363] Writing tensor blk.19.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  22\n",
      "[182/363] Writing tensor blk.19.attn_norm.weight                | size   5120           | type F32  | T+  22\n",
      "[183/363] Writing tensor blk.19.ffn_norm.weight                 | size   5120           | type F32  | T+  22\n",
      "[184/363] Writing tensor blk.20.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[185/363] Writing tensor blk.20.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[186/363] Writing tensor blk.20.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  22\n",
      "[187/363] Writing tensor blk.20.attn_output.weight              | size   5120 x   5120  | type F16  | T+  22\n",
      "[188/363] Writing tensor blk.20.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  22\n",
      "[189/363] Writing tensor blk.20.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  22\n",
      "[190/363] Writing tensor blk.20.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  23\n",
      "[191/363] Writing tensor blk.20.attn_norm.weight                | size   5120           | type F32  | T+  23\n",
      "[192/363] Writing tensor blk.20.ffn_norm.weight                 | size   5120           | type F32  | T+  23\n",
      "[193/363] Writing tensor blk.21.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[194/363] Writing tensor blk.21.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[195/363] Writing tensor blk.21.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  23\n",
      "[196/363] Writing tensor blk.21.attn_output.weight              | size   5120 x   5120  | type F16  | T+  23\n",
      "[197/363] Writing tensor blk.21.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  23\n",
      "[198/363] Writing tensor blk.21.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  24\n",
      "[199/363] Writing tensor blk.21.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  25\n",
      "[200/363] Writing tensor blk.21.attn_norm.weight                | size   5120           | type F32  | T+  25\n",
      "[201/363] Writing tensor blk.21.ffn_norm.weight                 | size   5120           | type F32  | T+  25\n",
      "[202/363] Writing tensor blk.22.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[203/363] Writing tensor blk.22.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[204/363] Writing tensor blk.22.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  25\n",
      "[205/363] Writing tensor blk.22.attn_output.weight              | size   5120 x   5120  | type F16  | T+  25\n",
      "[206/363] Writing tensor blk.22.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  25\n",
      "[207/363] Writing tensor blk.22.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  26\n",
      "[208/363] Writing tensor blk.22.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  26\n",
      "[209/363] Writing tensor blk.22.attn_norm.weight                | size   5120           | type F32  | T+  26\n",
      "[210/363] Writing tensor blk.22.ffn_norm.weight                 | size   5120           | type F32  | T+  26\n",
      "[211/363] Writing tensor blk.23.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[212/363] Writing tensor blk.23.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[213/363] Writing tensor blk.23.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  26\n",
      "[214/363] Writing tensor blk.23.attn_output.weight              | size   5120 x   5120  | type F16  | T+  26\n",
      "[215/363] Writing tensor blk.23.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  26\n",
      "[216/363] Writing tensor blk.23.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  27\n",
      "[217/363] Writing tensor blk.23.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  27\n",
      "[218/363] Writing tensor blk.23.attn_norm.weight                | size   5120           | type F32  | T+  27\n",
      "[219/363] Writing tensor blk.23.ffn_norm.weight                 | size   5120           | type F32  | T+  27\n",
      "[220/363] Writing tensor blk.24.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[221/363] Writing tensor blk.24.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[222/363] Writing tensor blk.24.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  27\n",
      "[223/363] Writing tensor blk.24.attn_output.weight              | size   5120 x   5120  | type F16  | T+  27\n",
      "[224/363] Writing tensor blk.24.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  27\n",
      "[225/363] Writing tensor blk.24.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  28\n",
      "[226/363] Writing tensor blk.24.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  28\n",
      "[227/363] Writing tensor blk.24.attn_norm.weight                | size   5120           | type F32  | T+  28\n",
      "[228/363] Writing tensor blk.24.ffn_norm.weight                 | size   5120           | type F32  | T+  28\n",
      "[229/363] Writing tensor blk.25.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[230/363] Writing tensor blk.25.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[231/363] Writing tensor blk.25.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  28\n",
      "[232/363] Writing tensor blk.25.attn_output.weight              | size   5120 x   5120  | type F16  | T+  28\n",
      "[233/363] Writing tensor blk.25.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  28\n",
      "[234/363] Writing tensor blk.25.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  29\n",
      "[235/363] Writing tensor blk.25.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  29\n",
      "[236/363] Writing tensor blk.25.attn_norm.weight                | size   5120           | type F32  | T+  29\n",
      "[237/363] Writing tensor blk.25.ffn_norm.weight                 | size   5120           | type F32  | T+  29\n",
      "[238/363] Writing tensor blk.26.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[239/363] Writing tensor blk.26.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[240/363] Writing tensor blk.26.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  29\n",
      "[241/363] Writing tensor blk.26.attn_output.weight              | size   5120 x   5120  | type F16  | T+  29\n",
      "[242/363] Writing tensor blk.26.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  29\n",
      "[243/363] Writing tensor blk.26.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  30\n",
      "[244/363] Writing tensor blk.26.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  30\n",
      "[245/363] Writing tensor blk.26.attn_norm.weight                | size   5120           | type F32  | T+  30\n",
      "[246/363] Writing tensor blk.26.ffn_norm.weight                 | size   5120           | type F32  | T+  30\n",
      "[247/363] Writing tensor blk.27.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[248/363] Writing tensor blk.27.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[249/363] Writing tensor blk.27.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  30\n",
      "[250/363] Writing tensor blk.27.attn_output.weight              | size   5120 x   5120  | type F16  | T+  30\n",
      "[251/363] Writing tensor blk.27.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  30\n",
      "[252/363] Writing tensor blk.27.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  31\n",
      "[253/363] Writing tensor blk.27.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  31\n",
      "[254/363] Writing tensor blk.27.attn_norm.weight                | size   5120           | type F32  | T+  31\n",
      "[255/363] Writing tensor blk.27.ffn_norm.weight                 | size   5120           | type F32  | T+  31\n",
      "[256/363] Writing tensor blk.28.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[257/363] Writing tensor blk.28.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[258/363] Writing tensor blk.28.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  31\n",
      "[259/363] Writing tensor blk.28.attn_output.weight              | size   5120 x   5120  | type F16  | T+  31\n",
      "[260/363] Writing tensor blk.28.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  31\n",
      "[261/363] Writing tensor blk.28.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  32\n",
      "[262/363] Writing tensor blk.28.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  32\n",
      "[263/363] Writing tensor blk.28.attn_norm.weight                | size   5120           | type F32  | T+  32\n",
      "[264/363] Writing tensor blk.28.ffn_norm.weight                 | size   5120           | type F32  | T+  32\n",
      "[265/363] Writing tensor blk.29.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[266/363] Writing tensor blk.29.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[267/363] Writing tensor blk.29.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  32\n",
      "[268/363] Writing tensor blk.29.attn_output.weight              | size   5120 x   5120  | type F16  | T+  32\n",
      "[269/363] Writing tensor blk.29.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  33\n",
      "[270/363] Writing tensor blk.29.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  33\n",
      "[271/363] Writing tensor blk.29.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  33\n",
      "[272/363] Writing tensor blk.29.attn_norm.weight                | size   5120           | type F32  | T+  33\n",
      "[273/363] Writing tensor blk.29.ffn_norm.weight                 | size   5120           | type F32  | T+  33\n",
      "[274/363] Writing tensor blk.30.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  33\n",
      "[275/363] Writing tensor blk.30.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  33\n",
      "[276/363] Writing tensor blk.30.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  33\n",
      "[277/363] Writing tensor blk.30.attn_output.weight              | size   5120 x   5120  | type F16  | T+  33\n",
      "[278/363] Writing tensor blk.30.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  34\n",
      "[279/363] Writing tensor blk.30.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  34\n",
      "[280/363] Writing tensor blk.30.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  35\n",
      "[281/363] Writing tensor blk.30.attn_norm.weight                | size   5120           | type F32  | T+  35\n",
      "[282/363] Writing tensor blk.30.ffn_norm.weight                 | size   5120           | type F32  | T+  35\n",
      "[283/363] Writing tensor blk.31.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  35\n",
      "[284/363] Writing tensor blk.31.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  35\n",
      "[285/363] Writing tensor blk.31.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  35\n",
      "[286/363] Writing tensor blk.31.attn_output.weight              | size   5120 x   5120  | type F16  | T+  35\n",
      "[287/363] Writing tensor blk.31.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  35\n",
      "[288/363] Writing tensor blk.31.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  37\n",
      "[289/363] Writing tensor blk.31.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  38\n",
      "[290/363] Writing tensor blk.31.attn_norm.weight                | size   5120           | type F32  | T+  38\n",
      "[291/363] Writing tensor blk.31.ffn_norm.weight                 | size   5120           | type F32  | T+  38\n",
      "[292/363] Writing tensor blk.32.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  38\n",
      "[293/363] Writing tensor blk.32.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  38\n",
      "[294/363] Writing tensor blk.32.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  38\n",
      "[295/363] Writing tensor blk.32.attn_output.weight              | size   5120 x   5120  | type F16  | T+  38\n",
      "[296/363] Writing tensor blk.32.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  38\n",
      "[297/363] Writing tensor blk.32.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  39\n",
      "[298/363] Writing tensor blk.32.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  41\n",
      "[299/363] Writing tensor blk.32.attn_norm.weight                | size   5120           | type F32  | T+  41\n",
      "[300/363] Writing tensor blk.32.ffn_norm.weight                 | size   5120           | type F32  | T+  41\n",
      "[301/363] Writing tensor blk.33.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  41\n",
      "[302/363] Writing tensor blk.33.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  41\n",
      "[303/363] Writing tensor blk.33.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  41\n",
      "[304/363] Writing tensor blk.33.attn_output.weight              | size   5120 x   5120  | type F16  | T+  41\n",
      "[305/363] Writing tensor blk.33.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  41\n",
      "[306/363] Writing tensor blk.33.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  42\n",
      "[307/363] Writing tensor blk.33.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  42\n",
      "[308/363] Writing tensor blk.33.attn_norm.weight                | size   5120           | type F32  | T+  42\n",
      "[309/363] Writing tensor blk.33.ffn_norm.weight                 | size   5120           | type F32  | T+  42\n",
      "[310/363] Writing tensor blk.34.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  42\n",
      "[311/363] Writing tensor blk.34.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  42\n",
      "[312/363] Writing tensor blk.34.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  42\n",
      "[313/363] Writing tensor blk.34.attn_output.weight              | size   5120 x   5120  | type F16  | T+  42\n",
      "[314/363] Writing tensor blk.34.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  42\n",
      "[315/363] Writing tensor blk.34.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  43\n",
      "[316/363] Writing tensor blk.34.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  43\n",
      "[317/363] Writing tensor blk.34.attn_norm.weight                | size   5120           | type F32  | T+  43\n",
      "[318/363] Writing tensor blk.34.ffn_norm.weight                 | size   5120           | type F32  | T+  43\n",
      "[319/363] Writing tensor blk.35.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  43\n",
      "[320/363] Writing tensor blk.35.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  43\n",
      "[321/363] Writing tensor blk.35.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  43\n",
      "[322/363] Writing tensor blk.35.attn_output.weight              | size   5120 x   5120  | type F16  | T+  43\n",
      "[323/363] Writing tensor blk.35.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  43\n",
      "[324/363] Writing tensor blk.35.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  44\n",
      "[325/363] Writing tensor blk.35.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  44\n",
      "[326/363] Writing tensor blk.35.attn_norm.weight                | size   5120           | type F32  | T+  44\n",
      "[327/363] Writing tensor blk.35.ffn_norm.weight                 | size   5120           | type F32  | T+  44\n",
      "[328/363] Writing tensor blk.36.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  44\n",
      "[329/363] Writing tensor blk.36.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  44\n",
      "[330/363] Writing tensor blk.36.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  44\n",
      "[331/363] Writing tensor blk.36.attn_output.weight              | size   5120 x   5120  | type F16  | T+  44\n",
      "[332/363] Writing tensor blk.36.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  44\n",
      "[333/363] Writing tensor blk.36.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  45\n",
      "[334/363] Writing tensor blk.36.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  45\n",
      "[335/363] Writing tensor blk.36.attn_norm.weight                | size   5120           | type F32  | T+  45\n",
      "[336/363] Writing tensor blk.36.ffn_norm.weight                 | size   5120           | type F32  | T+  45\n",
      "[337/363] Writing tensor blk.37.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  45\n",
      "[338/363] Writing tensor blk.37.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  45\n",
      "[339/363] Writing tensor blk.37.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  45\n",
      "[340/363] Writing tensor blk.37.attn_output.weight              | size   5120 x   5120  | type F16  | T+  45\n",
      "[341/363] Writing tensor blk.37.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  46\n",
      "[342/363] Writing tensor blk.37.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  46\n",
      "[343/363] Writing tensor blk.37.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  46\n",
      "[344/363] Writing tensor blk.37.attn_norm.weight                | size   5120           | type F32  | T+  46\n",
      "[345/363] Writing tensor blk.37.ffn_norm.weight                 | size   5120           | type F32  | T+  46\n",
      "[346/363] Writing tensor blk.38.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  46\n",
      "[347/363] Writing tensor blk.38.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  46\n",
      "[348/363] Writing tensor blk.38.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  46\n",
      "[349/363] Writing tensor blk.38.attn_output.weight              | size   5120 x   5120  | type F16  | T+  47\n",
      "[350/363] Writing tensor blk.38.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  47\n",
      "[351/363] Writing tensor blk.38.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  47\n",
      "[352/363] Writing tensor blk.38.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  48\n",
      "[353/363] Writing tensor blk.38.attn_norm.weight                | size   5120           | type F32  | T+  48\n",
      "[354/363] Writing tensor blk.38.ffn_norm.weight                 | size   5120           | type F32  | T+  48\n",
      "[355/363] Writing tensor blk.39.attn_q.weight                   | size   5120 x   5120  | type F16  | T+  48\n",
      "[356/363] Writing tensor blk.39.attn_k.weight                   | size   5120 x   5120  | type F16  | T+  48\n",
      "[357/363] Writing tensor blk.39.attn_v.weight                   | size   5120 x   5120  | type F16  | T+  48\n",
      "[358/363] Writing tensor blk.39.attn_output.weight              | size   5120 x   5120  | type F16  | T+  48\n",
      "[359/363] Writing tensor blk.39.ffn_gate.weight                 | size  13824 x   5120  | type F16  | T+  48\n",
      "[360/363] Writing tensor blk.39.ffn_down.weight                 | size   5120 x  13824  | type F16  | T+  48\n",
      "[361/363] Writing tensor blk.39.ffn_up.weight                   | size  13824 x   5120  | type F16  | T+  49\n",
      "[362/363] Writing tensor blk.39.attn_norm.weight                | size   5120           | type F32  | T+  49\n",
      "[363/363] Writing tensor blk.39.ffn_norm.weight                 | size   5120           | type F32  | T+  49\n",
      "Wrote models/13B/ggml-model-f16.gguf\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "params = Params(n_vocab=32000, n_embd=6656, n_layer=60, n_ctx=2048, n_ff=17920, n_head=52, n_head_kv=52, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/30B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 6656]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [6656]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 6656]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [6656]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [6656]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [6656]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [6656]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [6656]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [6656]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [6656]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [6656]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [6656]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [6656, 6656]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [6656, 6656]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [17920, 6656]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [6656, 17920]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [17920, 6656]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [6656]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [6656]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [6656]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [6656]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [6656]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [6656]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [6656]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [6656]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [6656]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [6656]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [6656]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [6656]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [6656]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [6656]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [6656]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [6656]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [6656]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [6656]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [6656]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [6656]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [6656]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [6656]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [6656]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [6656]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [6656]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [6656]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [6656]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [6656]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [6656]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [6656]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [6656]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [6656]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [6656]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [6656]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [6656]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [6656]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [6656]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [6656]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [6656]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [6656]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [6656]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [6656]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [6656]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [6656]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [6656]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [6656]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [6656]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [6656]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [6656]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [6656]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [6656]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [6656]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [6656, 6656]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [6656, 6656]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [17920, 6656]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [6656, 17920]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [17920, 6656]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [6656]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [6656]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/30B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/543] Writing tensor token_embd.weight                      | size  32000 x   6656  | type F16  | T+   1\n",
      "[  2/543] Writing tensor output_norm.weight                     | size   6656           | type F32  | T+   1\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type F16  | T+   2\n",
      "[  4/543] Writing tensor blk.0.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[  5/543] Writing tensor blk.0.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[  6/543] Writing tensor blk.0.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   2\n",
      "[  7/543] Writing tensor blk.0.attn_output.weight               | size   6656 x   6656  | type F16  | T+   2\n",
      "[  8/543] Writing tensor blk.0.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   2\n",
      "[  9/543] Writing tensor blk.0.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   2\n",
      "[ 10/543] Writing tensor blk.0.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   3\n",
      "[ 11/543] Writing tensor blk.0.attn_norm.weight                 | size   6656           | type F32  | T+   3\n",
      "[ 12/543] Writing tensor blk.0.ffn_norm.weight                  | size   6656           | type F32  | T+   3\n",
      "[ 13/543] Writing tensor blk.1.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 14/543] Writing tensor blk.1.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 15/543] Writing tensor blk.1.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 16/543] Writing tensor blk.1.attn_output.weight               | size   6656 x   6656  | type F16  | T+   3\n",
      "[ 17/543] Writing tensor blk.1.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   3\n",
      "[ 18/543] Writing tensor blk.1.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   4\n",
      "[ 19/543] Writing tensor blk.1.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   4\n",
      "[ 20/543] Writing tensor blk.1.attn_norm.weight                 | size   6656           | type F32  | T+   4\n",
      "[ 21/543] Writing tensor blk.1.ffn_norm.weight                  | size   6656           | type F32  | T+   4\n",
      "[ 22/543] Writing tensor blk.2.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 23/543] Writing tensor blk.2.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 24/543] Writing tensor blk.2.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 25/543] Writing tensor blk.2.attn_output.weight               | size   6656 x   6656  | type F16  | T+   4\n",
      "[ 26/543] Writing tensor blk.2.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   5\n",
      "[ 27/543] Writing tensor blk.2.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   6\n",
      "[ 28/543] Writing tensor blk.2.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   6\n",
      "[ 29/543] Writing tensor blk.2.attn_norm.weight                 | size   6656           | type F32  | T+   6\n",
      "[ 30/543] Writing tensor blk.2.ffn_norm.weight                  | size   6656           | type F32  | T+   6\n",
      "[ 31/543] Writing tensor blk.3.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 32/543] Writing tensor blk.3.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 33/543] Writing tensor blk.3.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 34/543] Writing tensor blk.3.attn_output.weight               | size   6656 x   6656  | type F16  | T+   6\n",
      "[ 35/543] Writing tensor blk.3.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   7\n",
      "[ 36/543] Writing tensor blk.3.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   7\n",
      "[ 37/543] Writing tensor blk.3.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   8\n",
      "[ 38/543] Writing tensor blk.3.attn_norm.weight                 | size   6656           | type F32  | T+   8\n",
      "[ 39/543] Writing tensor blk.3.ffn_norm.weight                  | size   6656           | type F32  | T+   8\n",
      "[ 40/543] Writing tensor blk.4.attn_q.weight                    | size   6656 x   6656  | type F16  | T+   8\n",
      "[ 41/543] Writing tensor blk.4.attn_k.weight                    | size   6656 x   6656  | type F16  | T+   8\n",
      "[ 42/543] Writing tensor blk.4.attn_v.weight                    | size   6656 x   6656  | type F16  | T+   8\n",
      "[ 43/543] Writing tensor blk.4.attn_output.weight               | size   6656 x   6656  | type F16  | T+   8\n",
      "[ 44/543] Writing tensor blk.4.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+   8\n",
      "[ 45/543] Writing tensor blk.4.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+   9\n",
      "[ 46/543] Writing tensor blk.4.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+   9\n",
      "[ 47/543] Writing tensor blk.4.attn_norm.weight                 | size   6656           | type F32  | T+  10\n",
      "[ 48/543] Writing tensor blk.4.ffn_norm.weight                  | size   6656           | type F32  | T+  10\n",
      "[ 49/543] Writing tensor blk.5.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 50/543] Writing tensor blk.5.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 51/543] Writing tensor blk.5.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 52/543] Writing tensor blk.5.attn_output.weight               | size   6656 x   6656  | type F16  | T+  10\n",
      "[ 53/543] Writing tensor blk.5.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  10\n",
      "[ 54/543] Writing tensor blk.5.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  11\n",
      "[ 55/543] Writing tensor blk.5.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  11\n",
      "[ 56/543] Writing tensor blk.5.attn_norm.weight                 | size   6656           | type F32  | T+  11\n",
      "[ 57/543] Writing tensor blk.5.ffn_norm.weight                  | size   6656           | type F32  | T+  11\n",
      "[ 58/543] Writing tensor blk.6.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 59/543] Writing tensor blk.6.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 60/543] Writing tensor blk.6.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 61/543] Writing tensor blk.6.attn_output.weight               | size   6656 x   6656  | type F16  | T+  11\n",
      "[ 62/543] Writing tensor blk.6.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  12\n",
      "[ 63/543] Writing tensor blk.6.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  12\n",
      "[ 64/543] Writing tensor blk.6.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  13\n",
      "[ 65/543] Writing tensor blk.6.attn_norm.weight                 | size   6656           | type F32  | T+  13\n",
      "[ 66/543] Writing tensor blk.6.ffn_norm.weight                  | size   6656           | type F32  | T+  13\n",
      "[ 67/543] Writing tensor blk.7.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 68/543] Writing tensor blk.7.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 69/543] Writing tensor blk.7.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 70/543] Writing tensor blk.7.attn_output.weight               | size   6656 x   6656  | type F16  | T+  13\n",
      "[ 71/543] Writing tensor blk.7.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  13\n",
      "[ 72/543] Writing tensor blk.7.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  14\n",
      "[ 73/543] Writing tensor blk.7.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  14\n",
      "[ 74/543] Writing tensor blk.7.attn_norm.weight                 | size   6656           | type F32  | T+  14\n",
      "[ 75/543] Writing tensor blk.7.ffn_norm.weight                  | size   6656           | type F32  | T+  14\n",
      "[ 76/543] Writing tensor blk.8.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 77/543] Writing tensor blk.8.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 78/543] Writing tensor blk.8.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  14\n",
      "[ 79/543] Writing tensor blk.8.attn_output.weight               | size   6656 x   6656  | type F16  | T+  15\n",
      "[ 80/543] Writing tensor blk.8.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  15\n",
      "[ 81/543] Writing tensor blk.8.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  16\n",
      "[ 82/543] Writing tensor blk.8.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  16\n",
      "[ 83/543] Writing tensor blk.8.attn_norm.weight                 | size   6656           | type F32  | T+  16\n",
      "[ 84/543] Writing tensor blk.8.ffn_norm.weight                  | size   6656           | type F32  | T+  16\n",
      "[ 85/543] Writing tensor blk.9.attn_q.weight                    | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 86/543] Writing tensor blk.9.attn_k.weight                    | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 87/543] Writing tensor blk.9.attn_v.weight                    | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 88/543] Writing tensor blk.9.attn_output.weight               | size   6656 x   6656  | type F16  | T+  16\n",
      "[ 89/543] Writing tensor blk.9.ffn_gate.weight                  | size  17920 x   6656  | type F16  | T+  16\n",
      "[ 90/543] Writing tensor blk.9.ffn_down.weight                  | size   6656 x  17920  | type F16  | T+  17\n",
      "[ 91/543] Writing tensor blk.9.ffn_up.weight                    | size  17920 x   6656  | type F16  | T+  18\n",
      "[ 92/543] Writing tensor blk.9.attn_norm.weight                 | size   6656           | type F32  | T+  18\n",
      "[ 93/543] Writing tensor blk.9.ffn_norm.weight                  | size   6656           | type F32  | T+  18\n",
      "[ 94/543] Writing tensor blk.10.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[ 95/543] Writing tensor blk.10.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[ 96/543] Writing tensor blk.10.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  18\n",
      "[ 97/543] Writing tensor blk.10.attn_output.weight              | size   6656 x   6656  | type F16  | T+  18\n",
      "[ 98/543] Writing tensor blk.10.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  18\n",
      "[ 99/543] Writing tensor blk.10.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  19\n",
      "[100/543] Writing tensor blk.10.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  19\n",
      "[101/543] Writing tensor blk.10.attn_norm.weight                | size   6656           | type F32  | T+  19\n",
      "[102/543] Writing tensor blk.10.ffn_norm.weight                 | size   6656           | type F32  | T+  19\n",
      "[103/543] Writing tensor blk.11.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[104/543] Writing tensor blk.11.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  19\n",
      "[105/543] Writing tensor blk.11.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  20\n",
      "[106/543] Writing tensor blk.11.attn_output.weight              | size   6656 x   6656  | type F16  | T+  20\n",
      "[107/543] Writing tensor blk.11.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  20\n",
      "[108/543] Writing tensor blk.11.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  21\n",
      "[109/543] Writing tensor blk.11.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  22\n",
      "[110/543] Writing tensor blk.11.attn_norm.weight                | size   6656           | type F32  | T+  22\n",
      "[111/543] Writing tensor blk.11.ffn_norm.weight                 | size   6656           | type F32  | T+  22\n",
      "[112/543] Writing tensor blk.12.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[113/543] Writing tensor blk.12.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[114/543] Writing tensor blk.12.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  22\n",
      "[115/543] Writing tensor blk.12.attn_output.weight              | size   6656 x   6656  | type F16  | T+  22\n",
      "[116/543] Writing tensor blk.12.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  22\n",
      "[117/543] Writing tensor blk.12.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  23\n",
      "[118/543] Writing tensor blk.12.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  24\n",
      "[119/543] Writing tensor blk.12.attn_norm.weight                | size   6656           | type F32  | T+  24\n",
      "[120/543] Writing tensor blk.12.ffn_norm.weight                 | size   6656           | type F32  | T+  24\n",
      "[121/543] Writing tensor blk.13.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  24\n",
      "[122/543] Writing tensor blk.13.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  24\n",
      "[123/543] Writing tensor blk.13.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  24\n",
      "[124/543] Writing tensor blk.13.attn_output.weight              | size   6656 x   6656  | type F16  | T+  24\n",
      "[125/543] Writing tensor blk.13.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  24\n",
      "[126/543] Writing tensor blk.13.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  25\n",
      "[127/543] Writing tensor blk.13.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  25\n",
      "[128/543] Writing tensor blk.13.attn_norm.weight                | size   6656           | type F32  | T+  25\n",
      "[129/543] Writing tensor blk.13.ffn_norm.weight                 | size   6656           | type F32  | T+  25\n",
      "[130/543] Writing tensor blk.14.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[131/543] Writing tensor blk.14.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[132/543] Writing tensor blk.14.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  25\n",
      "[133/543] Writing tensor blk.14.attn_output.weight              | size   6656 x   6656  | type F16  | T+  25\n",
      "[134/543] Writing tensor blk.14.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  26\n",
      "[135/543] Writing tensor blk.14.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  27\n",
      "[136/543] Writing tensor blk.14.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  27\n",
      "[137/543] Writing tensor blk.14.attn_norm.weight                | size   6656           | type F32  | T+  27\n",
      "[138/543] Writing tensor blk.14.ffn_norm.weight                 | size   6656           | type F32  | T+  27\n",
      "[139/543] Writing tensor blk.15.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[140/543] Writing tensor blk.15.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[141/543] Writing tensor blk.15.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  27\n",
      "[142/543] Writing tensor blk.15.attn_output.weight              | size   6656 x   6656  | type F16  | T+  27\n",
      "[143/543] Writing tensor blk.15.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  28\n",
      "[144/543] Writing tensor blk.15.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  28\n",
      "[145/543] Writing tensor blk.15.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  29\n",
      "[146/543] Writing tensor blk.15.attn_norm.weight                | size   6656           | type F32  | T+  29\n",
      "[147/543] Writing tensor blk.15.ffn_norm.weight                 | size   6656           | type F32  | T+  29\n",
      "[148/543] Writing tensor blk.16.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[149/543] Writing tensor blk.16.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[150/543] Writing tensor blk.16.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  29\n",
      "[151/543] Writing tensor blk.16.attn_output.weight              | size   6656 x   6656  | type F16  | T+  29\n",
      "[152/543] Writing tensor blk.16.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  29\n",
      "[153/543] Writing tensor blk.16.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  30\n",
      "[154/543] Writing tensor blk.16.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  30\n",
      "[155/543] Writing tensor blk.16.attn_norm.weight                | size   6656           | type F32  | T+  30\n",
      "[156/543] Writing tensor blk.16.ffn_norm.weight                 | size   6656           | type F32  | T+  30\n",
      "[157/543] Writing tensor blk.17.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[158/543] Writing tensor blk.17.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[159/543] Writing tensor blk.17.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  30\n",
      "[160/543] Writing tensor blk.17.attn_output.weight              | size   6656 x   6656  | type F16  | T+  30\n",
      "[161/543] Writing tensor blk.17.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  31\n",
      "[162/543] Writing tensor blk.17.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  32\n",
      "[163/543] Writing tensor blk.17.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  33\n",
      "[164/543] Writing tensor blk.17.attn_norm.weight                | size   6656           | type F32  | T+  33\n",
      "[165/543] Writing tensor blk.17.ffn_norm.weight                 | size   6656           | type F32  | T+  33\n",
      "[166/543] Writing tensor blk.18.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[167/543] Writing tensor blk.18.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[168/543] Writing tensor blk.18.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  33\n",
      "[169/543] Writing tensor blk.18.attn_output.weight              | size   6656 x   6656  | type F16  | T+  33\n",
      "[170/543] Writing tensor blk.18.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  34\n",
      "[171/543] Writing tensor blk.18.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  35\n",
      "[172/543] Writing tensor blk.18.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  36\n",
      "[173/543] Writing tensor blk.18.attn_norm.weight                | size   6656           | type F32  | T+  37\n",
      "[174/543] Writing tensor blk.18.ffn_norm.weight                 | size   6656           | type F32  | T+  37\n",
      "[175/543] Writing tensor blk.19.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[176/543] Writing tensor blk.19.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[177/543] Writing tensor blk.19.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  37\n",
      "[178/543] Writing tensor blk.19.attn_output.weight              | size   6656 x   6656  | type F16  | T+  37\n",
      "[179/543] Writing tensor blk.19.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  38\n",
      "[180/543] Writing tensor blk.19.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  38\n",
      "[181/543] Writing tensor blk.19.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  39\n",
      "[182/543] Writing tensor blk.19.attn_norm.weight                | size   6656           | type F32  | T+  39\n",
      "[183/543] Writing tensor blk.19.ffn_norm.weight                 | size   6656           | type F32  | T+  39\n",
      "[184/543] Writing tensor blk.20.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  39\n",
      "[185/543] Writing tensor blk.20.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  39\n",
      "[186/543] Writing tensor blk.20.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  39\n",
      "[187/543] Writing tensor blk.20.attn_output.weight              | size   6656 x   6656  | type F16  | T+  39\n",
      "[188/543] Writing tensor blk.20.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  39\n",
      "[189/543] Writing tensor blk.20.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  40\n",
      "[190/543] Writing tensor blk.20.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  41\n",
      "[191/543] Writing tensor blk.20.attn_norm.weight                | size   6656           | type F32  | T+  41\n",
      "[192/543] Writing tensor blk.20.ffn_norm.weight                 | size   6656           | type F32  | T+  41\n",
      "[193/543] Writing tensor blk.21.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[194/543] Writing tensor blk.21.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[195/543] Writing tensor blk.21.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  41\n",
      "[196/543] Writing tensor blk.21.attn_output.weight              | size   6656 x   6656  | type F16  | T+  41\n",
      "[197/543] Writing tensor blk.21.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  41\n",
      "[198/543] Writing tensor blk.21.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  42\n",
      "[199/543] Writing tensor blk.21.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  42\n",
      "[200/543] Writing tensor blk.21.attn_norm.weight                | size   6656           | type F32  | T+  43\n",
      "[201/543] Writing tensor blk.21.ffn_norm.weight                 | size   6656           | type F32  | T+  43\n",
      "[202/543] Writing tensor blk.22.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  43\n",
      "[203/543] Writing tensor blk.22.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  43\n",
      "[204/543] Writing tensor blk.22.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  43\n",
      "[205/543] Writing tensor blk.22.attn_output.weight              | size   6656 x   6656  | type F16  | T+  43\n",
      "[206/543] Writing tensor blk.22.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  43\n",
      "[207/543] Writing tensor blk.22.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  44\n",
      "[208/543] Writing tensor blk.22.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  44\n",
      "[209/543] Writing tensor blk.22.attn_norm.weight                | size   6656           | type F32  | T+  44\n",
      "[210/543] Writing tensor blk.22.ffn_norm.weight                 | size   6656           | type F32  | T+  44\n",
      "[211/543] Writing tensor blk.23.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[212/543] Writing tensor blk.23.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[213/543] Writing tensor blk.23.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  44\n",
      "[214/543] Writing tensor blk.23.attn_output.weight              | size   6656 x   6656  | type F16  | T+  44\n",
      "[215/543] Writing tensor blk.23.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  45\n",
      "[216/543] Writing tensor blk.23.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  45\n",
      "[217/543] Writing tensor blk.23.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  46\n",
      "[218/543] Writing tensor blk.23.attn_norm.weight                | size   6656           | type F32  | T+  46\n",
      "[219/543] Writing tensor blk.23.ffn_norm.weight                 | size   6656           | type F32  | T+  46\n",
      "[220/543] Writing tensor blk.24.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[221/543] Writing tensor blk.24.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[222/543] Writing tensor blk.24.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  46\n",
      "[223/543] Writing tensor blk.24.attn_output.weight              | size   6656 x   6656  | type F16  | T+  46\n",
      "[224/543] Writing tensor blk.24.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  46\n",
      "[225/543] Writing tensor blk.24.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  47\n",
      "[226/543] Writing tensor blk.24.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  48\n",
      "[227/543] Writing tensor blk.24.attn_norm.weight                | size   6656           | type F32  | T+  48\n",
      "[228/543] Writing tensor blk.24.ffn_norm.weight                 | size   6656           | type F32  | T+  48\n",
      "[229/543] Writing tensor blk.25.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[230/543] Writing tensor blk.25.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[231/543] Writing tensor blk.25.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  48\n",
      "[232/543] Writing tensor blk.25.attn_output.weight              | size   6656 x   6656  | type F16  | T+  48\n",
      "[233/543] Writing tensor blk.25.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  48\n",
      "[234/543] Writing tensor blk.25.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  49\n",
      "[235/543] Writing tensor blk.25.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  49\n",
      "[236/543] Writing tensor blk.25.attn_norm.weight                | size   6656           | type F32  | T+  49\n",
      "[237/543] Writing tensor blk.25.ffn_norm.weight                 | size   6656           | type F32  | T+  49\n",
      "[238/543] Writing tensor blk.26.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[239/543] Writing tensor blk.26.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[240/543] Writing tensor blk.26.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  49\n",
      "[241/543] Writing tensor blk.26.attn_output.weight              | size   6656 x   6656  | type F16  | T+  49\n",
      "[242/543] Writing tensor blk.26.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  50\n",
      "[243/543] Writing tensor blk.26.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  51\n",
      "[244/543] Writing tensor blk.26.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  51\n",
      "[245/543] Writing tensor blk.26.attn_norm.weight                | size   6656           | type F32  | T+  51\n",
      "[246/543] Writing tensor blk.26.ffn_norm.weight                 | size   6656           | type F32  | T+  51\n",
      "[247/543] Writing tensor blk.27.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[248/543] Writing tensor blk.27.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[249/543] Writing tensor blk.27.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  51\n",
      "[250/543] Writing tensor blk.27.attn_output.weight              | size   6656 x   6656  | type F16  | T+  51\n",
      "[251/543] Writing tensor blk.27.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  51\n",
      "[252/543] Writing tensor blk.27.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  52\n",
      "[253/543] Writing tensor blk.27.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  53\n",
      "[254/543] Writing tensor blk.27.attn_norm.weight                | size   6656           | type F32  | T+  53\n",
      "[255/543] Writing tensor blk.27.ffn_norm.weight                 | size   6656           | type F32  | T+  53\n",
      "[256/543] Writing tensor blk.28.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  53\n",
      "[257/543] Writing tensor blk.28.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  53\n",
      "[258/543] Writing tensor blk.28.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  53\n",
      "[259/543] Writing tensor blk.28.attn_output.weight              | size   6656 x   6656  | type F16  | T+  53\n",
      "[260/543] Writing tensor blk.28.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  53\n",
      "[261/543] Writing tensor blk.28.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  54\n",
      "[262/543] Writing tensor blk.28.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  54\n",
      "[263/543] Writing tensor blk.28.attn_norm.weight                | size   6656           | type F32  | T+  54\n",
      "[264/543] Writing tensor blk.28.ffn_norm.weight                 | size   6656           | type F32  | T+  54\n",
      "[265/543] Writing tensor blk.29.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[266/543] Writing tensor blk.29.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  54\n",
      "[267/543] Writing tensor blk.29.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  55\n",
      "[268/543] Writing tensor blk.29.attn_output.weight              | size   6656 x   6656  | type F16  | T+  55\n",
      "[269/543] Writing tensor blk.29.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  55\n",
      "[270/543] Writing tensor blk.29.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  56\n",
      "[271/543] Writing tensor blk.29.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  56\n",
      "[272/543] Writing tensor blk.29.attn_norm.weight                | size   6656           | type F32  | T+  56\n",
      "[273/543] Writing tensor blk.29.ffn_norm.weight                 | size   6656           | type F32  | T+  56\n",
      "[274/543] Writing tensor blk.30.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[275/543] Writing tensor blk.30.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[276/543] Writing tensor blk.30.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  56\n",
      "[277/543] Writing tensor blk.30.attn_output.weight              | size   6656 x   6656  | type F16  | T+  56\n",
      "[278/543] Writing tensor blk.30.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  57\n",
      "[279/543] Writing tensor blk.30.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  59\n",
      "[280/543] Writing tensor blk.30.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  59\n",
      "[281/543] Writing tensor blk.30.attn_norm.weight                | size   6656           | type F32  | T+  59\n",
      "[282/543] Writing tensor blk.30.ffn_norm.weight                 | size   6656           | type F32  | T+  59\n",
      "[283/543] Writing tensor blk.31.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[284/543] Writing tensor blk.31.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[285/543] Writing tensor blk.31.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  59\n",
      "[286/543] Writing tensor blk.31.attn_output.weight              | size   6656 x   6656  | type F16  | T+  60\n",
      "[287/543] Writing tensor blk.31.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  61\n",
      "[288/543] Writing tensor blk.31.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  61\n",
      "[289/543] Writing tensor blk.31.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  62\n",
      "[290/543] Writing tensor blk.31.attn_norm.weight                | size   6656           | type F32  | T+  62\n",
      "[291/543] Writing tensor blk.31.ffn_norm.weight                 | size   6656           | type F32  | T+  62\n",
      "[292/543] Writing tensor blk.32.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[293/543] Writing tensor blk.32.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[294/543] Writing tensor blk.32.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  62\n",
      "[295/543] Writing tensor blk.32.attn_output.weight              | size   6656 x   6656  | type F16  | T+  62\n",
      "[296/543] Writing tensor blk.32.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  62\n",
      "[297/543] Writing tensor blk.32.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  63\n",
      "[298/543] Writing tensor blk.32.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  63\n",
      "[299/543] Writing tensor blk.32.attn_norm.weight                | size   6656           | type F32  | T+  63\n",
      "[300/543] Writing tensor blk.32.ffn_norm.weight                 | size   6656           | type F32  | T+  63\n",
      "[301/543] Writing tensor blk.33.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  63\n",
      "[302/543] Writing tensor blk.33.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[303/543] Writing tensor blk.33.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  64\n",
      "[304/543] Writing tensor blk.33.attn_output.weight              | size   6656 x   6656  | type F16  | T+  64\n",
      "[305/543] Writing tensor blk.33.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  64\n",
      "[306/543] Writing tensor blk.33.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  65\n",
      "[307/543] Writing tensor blk.33.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  65\n",
      "[308/543] Writing tensor blk.33.attn_norm.weight                | size   6656           | type F32  | T+  65\n",
      "[309/543] Writing tensor blk.33.ffn_norm.weight                 | size   6656           | type F32  | T+  65\n",
      "[310/543] Writing tensor blk.34.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  65\n",
      "[311/543] Writing tensor blk.34.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  65\n",
      "[312/543] Writing tensor blk.34.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  65\n",
      "[313/543] Writing tensor blk.34.attn_output.weight              | size   6656 x   6656  | type F16  | T+  65\n",
      "[314/543] Writing tensor blk.34.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  66\n",
      "[315/543] Writing tensor blk.34.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  66\n",
      "[316/543] Writing tensor blk.34.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  67\n",
      "[317/543] Writing tensor blk.34.attn_norm.weight                | size   6656           | type F32  | T+  67\n",
      "[318/543] Writing tensor blk.34.ffn_norm.weight                 | size   6656           | type F32  | T+  67\n",
      "[319/543] Writing tensor blk.35.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[320/543] Writing tensor blk.35.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[321/543] Writing tensor blk.35.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  67\n",
      "[322/543] Writing tensor blk.35.attn_output.weight              | size   6656 x   6656  | type F16  | T+  67\n",
      "[323/543] Writing tensor blk.35.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  67\n",
      "[324/543] Writing tensor blk.35.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  68\n",
      "[325/543] Writing tensor blk.35.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  69\n",
      "[326/543] Writing tensor blk.35.attn_norm.weight                | size   6656           | type F32  | T+  69\n",
      "[327/543] Writing tensor blk.35.ffn_norm.weight                 | size   6656           | type F32  | T+  69\n",
      "[328/543] Writing tensor blk.36.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[329/543] Writing tensor blk.36.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[330/543] Writing tensor blk.36.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  69\n",
      "[331/543] Writing tensor blk.36.attn_output.weight              | size   6656 x   6656  | type F16  | T+  69\n",
      "[332/543] Writing tensor blk.36.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  69\n",
      "[333/543] Writing tensor blk.36.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  70\n",
      "[334/543] Writing tensor blk.36.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  70\n",
      "[335/543] Writing tensor blk.36.attn_norm.weight                | size   6656           | type F32  | T+  70\n",
      "[336/543] Writing tensor blk.36.ffn_norm.weight                 | size   6656           | type F32  | T+  70\n",
      "[337/543] Writing tensor blk.37.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[338/543] Writing tensor blk.37.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[339/543] Writing tensor blk.37.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  70\n",
      "[340/543] Writing tensor blk.37.attn_output.weight              | size   6656 x   6656  | type F16  | T+  70\n",
      "[341/543] Writing tensor blk.37.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  71\n",
      "[342/543] Writing tensor blk.37.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  73\n",
      "[343/543] Writing tensor blk.37.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  73\n",
      "[344/543] Writing tensor blk.37.attn_norm.weight                | size   6656           | type F32  | T+  73\n",
      "[345/543] Writing tensor blk.37.ffn_norm.weight                 | size   6656           | type F32  | T+  73\n",
      "[346/543] Writing tensor blk.38.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  73\n",
      "[347/543] Writing tensor blk.38.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  73\n",
      "[348/543] Writing tensor blk.38.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  73\n",
      "[349/543] Writing tensor blk.38.attn_output.weight              | size   6656 x   6656  | type F16  | T+  73\n",
      "[350/543] Writing tensor blk.38.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  74\n",
      "[351/543] Writing tensor blk.38.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  74\n",
      "[352/543] Writing tensor blk.38.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  75\n",
      "[353/543] Writing tensor blk.38.attn_norm.weight                | size   6656           | type F32  | T+  75\n",
      "[354/543] Writing tensor blk.38.ffn_norm.weight                 | size   6656           | type F32  | T+  75\n",
      "[355/543] Writing tensor blk.39.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  75\n",
      "[356/543] Writing tensor blk.39.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  75\n",
      "[357/543] Writing tensor blk.39.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  75\n",
      "[358/543] Writing tensor blk.39.attn_output.weight              | size   6656 x   6656  | type F16  | T+  75\n",
      "[359/543] Writing tensor blk.39.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  75\n",
      "[360/543] Writing tensor blk.39.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  76\n",
      "[361/543] Writing tensor blk.39.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  76\n",
      "[362/543] Writing tensor blk.39.attn_norm.weight                | size   6656           | type F32  | T+  77\n",
      "[363/543] Writing tensor blk.39.ffn_norm.weight                 | size   6656           | type F32  | T+  77\n",
      "[364/543] Writing tensor blk.40.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  77\n",
      "[365/543] Writing tensor blk.40.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  77\n",
      "[366/543] Writing tensor blk.40.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  77\n",
      "[367/543] Writing tensor blk.40.attn_output.weight              | size   6656 x   6656  | type F16  | T+  77\n",
      "[368/543] Writing tensor blk.40.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  77\n",
      "[369/543] Writing tensor blk.40.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  78\n",
      "[370/543] Writing tensor blk.40.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  79\n",
      "[371/543] Writing tensor blk.40.attn_norm.weight                | size   6656           | type F32  | T+  79\n",
      "[372/543] Writing tensor blk.40.ffn_norm.weight                 | size   6656           | type F32  | T+  79\n",
      "[373/543] Writing tensor blk.41.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[374/543] Writing tensor blk.41.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[375/543] Writing tensor blk.41.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  79\n",
      "[376/543] Writing tensor blk.41.attn_output.weight              | size   6656 x   6656  | type F16  | T+  79\n",
      "[377/543] Writing tensor blk.41.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  79\n",
      "[378/543] Writing tensor blk.41.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  80\n",
      "[379/543] Writing tensor blk.41.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  81\n",
      "[380/543] Writing tensor blk.41.attn_norm.weight                | size   6656           | type F32  | T+  81\n",
      "[381/543] Writing tensor blk.41.ffn_norm.weight                 | size   6656           | type F32  | T+  81\n",
      "[382/543] Writing tensor blk.42.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  81\n",
      "[383/543] Writing tensor blk.42.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  81\n",
      "[384/543] Writing tensor blk.42.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  81\n",
      "[385/543] Writing tensor blk.42.attn_output.weight              | size   6656 x   6656  | type F16  | T+  81\n",
      "[386/543] Writing tensor blk.42.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  81\n",
      "[387/543] Writing tensor blk.42.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  82\n",
      "[388/543] Writing tensor blk.42.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  83\n",
      "[389/543] Writing tensor blk.42.attn_norm.weight                | size   6656           | type F32  | T+  83\n",
      "[390/543] Writing tensor blk.42.ffn_norm.weight                 | size   6656           | type F32  | T+  83\n",
      "[391/543] Writing tensor blk.43.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  83\n",
      "[392/543] Writing tensor blk.43.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  83\n",
      "[393/543] Writing tensor blk.43.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  83\n",
      "[394/543] Writing tensor blk.43.attn_output.weight              | size   6656 x   6656  | type F16  | T+  83\n",
      "[395/543] Writing tensor blk.43.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  83\n",
      "[396/543] Writing tensor blk.43.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  84\n",
      "[397/543] Writing tensor blk.43.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  85\n",
      "[398/543] Writing tensor blk.43.attn_norm.weight                | size   6656           | type F32  | T+  85\n",
      "[399/543] Writing tensor blk.43.ffn_norm.weight                 | size   6656           | type F32  | T+  85\n",
      "[400/543] Writing tensor blk.44.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  85\n",
      "[401/543] Writing tensor blk.44.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  85\n",
      "[402/543] Writing tensor blk.44.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  85\n",
      "[403/543] Writing tensor blk.44.attn_output.weight              | size   6656 x   6656  | type F16  | T+  85\n",
      "[404/543] Writing tensor blk.44.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  85\n",
      "[405/543] Writing tensor blk.44.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  86\n",
      "[406/543] Writing tensor blk.44.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  87\n",
      "[407/543] Writing tensor blk.44.attn_norm.weight                | size   6656           | type F32  | T+  87\n",
      "[408/543] Writing tensor blk.44.ffn_norm.weight                 | size   6656           | type F32  | T+  87\n",
      "[409/543] Writing tensor blk.45.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  87\n",
      "[410/543] Writing tensor blk.45.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  87\n",
      "[411/543] Writing tensor blk.45.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  87\n",
      "[412/543] Writing tensor blk.45.attn_output.weight              | size   6656 x   6656  | type F16  | T+  87\n",
      "[413/543] Writing tensor blk.45.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  87\n",
      "[414/543] Writing tensor blk.45.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  88\n",
      "[415/543] Writing tensor blk.45.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  88\n",
      "[416/543] Writing tensor blk.45.attn_norm.weight                | size   6656           | type F32  | T+  88\n",
      "[417/543] Writing tensor blk.45.ffn_norm.weight                 | size   6656           | type F32  | T+  88\n",
      "[418/543] Writing tensor blk.46.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  88\n",
      "[419/543] Writing tensor blk.46.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  88\n",
      "[420/543] Writing tensor blk.46.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  88\n",
      "[421/543] Writing tensor blk.46.attn_output.weight              | size   6656 x   6656  | type F16  | T+  88\n",
      "[422/543] Writing tensor blk.46.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  89\n",
      "[423/543] Writing tensor blk.46.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  90\n",
      "[424/543] Writing tensor blk.46.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  90\n",
      "[425/543] Writing tensor blk.46.attn_norm.weight                | size   6656           | type F32  | T+  90\n",
      "[426/543] Writing tensor blk.46.ffn_norm.weight                 | size   6656           | type F32  | T+  90\n",
      "[427/543] Writing tensor blk.47.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[428/543] Writing tensor blk.47.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[429/543] Writing tensor blk.47.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  90\n",
      "[430/543] Writing tensor blk.47.attn_output.weight              | size   6656 x   6656  | type F16  | T+  90\n",
      "[431/543] Writing tensor blk.47.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  91\n",
      "[432/543] Writing tensor blk.47.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  92\n",
      "[433/543] Writing tensor blk.47.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  92\n",
      "[434/543] Writing tensor blk.47.attn_norm.weight                | size   6656           | type F32  | T+  92\n",
      "[435/543] Writing tensor blk.47.ffn_norm.weight                 | size   6656           | type F32  | T+  92\n",
      "[436/543] Writing tensor blk.48.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  92\n",
      "[437/543] Writing tensor blk.48.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  92\n",
      "[438/543] Writing tensor blk.48.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  93\n",
      "[439/543] Writing tensor blk.48.attn_output.weight              | size   6656 x   6656  | type F16  | T+  93\n",
      "[440/543] Writing tensor blk.48.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  93\n",
      "[441/543] Writing tensor blk.48.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  94\n",
      "[442/543] Writing tensor blk.48.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  94\n",
      "[443/543] Writing tensor blk.48.attn_norm.weight                | size   6656           | type F32  | T+  94\n",
      "[444/543] Writing tensor blk.48.ffn_norm.weight                 | size   6656           | type F32  | T+  94\n",
      "[445/543] Writing tensor blk.49.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  94\n",
      "[446/543] Writing tensor blk.49.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  94\n",
      "[447/543] Writing tensor blk.49.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  94\n",
      "[448/543] Writing tensor blk.49.attn_output.weight              | size   6656 x   6656  | type F16  | T+  94\n",
      "[449/543] Writing tensor blk.49.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  95\n",
      "[450/543] Writing tensor blk.49.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  95\n",
      "[451/543] Writing tensor blk.49.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  96\n",
      "[452/543] Writing tensor blk.49.attn_norm.weight                | size   6656           | type F32  | T+  96\n",
      "[453/543] Writing tensor blk.49.ffn_norm.weight                 | size   6656           | type F32  | T+  96\n",
      "[454/543] Writing tensor blk.50.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  96\n",
      "[455/543] Writing tensor blk.50.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  96\n",
      "[456/543] Writing tensor blk.50.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  96\n",
      "[457/543] Writing tensor blk.50.attn_output.weight              | size   6656 x   6656  | type F16  | T+  96\n",
      "[458/543] Writing tensor blk.50.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  97\n",
      "[459/543] Writing tensor blk.50.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  97\n",
      "[460/543] Writing tensor blk.50.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+  98\n",
      "[461/543] Writing tensor blk.50.attn_norm.weight                | size   6656           | type F32  | T+  98\n",
      "[462/543] Writing tensor blk.50.ffn_norm.weight                 | size   6656           | type F32  | T+  98\n",
      "[463/543] Writing tensor blk.51.attn_q.weight                   | size   6656 x   6656  | type F16  | T+  98\n",
      "[464/543] Writing tensor blk.51.attn_k.weight                   | size   6656 x   6656  | type F16  | T+  98\n",
      "[465/543] Writing tensor blk.51.attn_v.weight                   | size   6656 x   6656  | type F16  | T+  98\n",
      "[466/543] Writing tensor blk.51.attn_output.weight              | size   6656 x   6656  | type F16  | T+  98\n",
      "[467/543] Writing tensor blk.51.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+  98\n",
      "[468/543] Writing tensor blk.51.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+  99\n",
      "[469/543] Writing tensor blk.51.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 100\n",
      "[470/543] Writing tensor blk.51.attn_norm.weight                | size   6656           | type F32  | T+ 100\n",
      "[471/543] Writing tensor blk.51.ffn_norm.weight                 | size   6656           | type F32  | T+ 100\n",
      "[472/543] Writing tensor blk.52.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 100\n",
      "[473/543] Writing tensor blk.52.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 100\n",
      "[474/543] Writing tensor blk.52.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 100\n",
      "[475/543] Writing tensor blk.52.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 100\n",
      "[476/543] Writing tensor blk.52.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 100\n",
      "[477/543] Writing tensor blk.52.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 101\n",
      "[478/543] Writing tensor blk.52.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 102\n",
      "[479/543] Writing tensor blk.52.attn_norm.weight                | size   6656           | type F32  | T+ 102\n",
      "[480/543] Writing tensor blk.52.ffn_norm.weight                 | size   6656           | type F32  | T+ 102\n",
      "[481/543] Writing tensor blk.53.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 102\n",
      "[482/543] Writing tensor blk.53.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 102\n",
      "[483/543] Writing tensor blk.53.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 102\n",
      "[484/543] Writing tensor blk.53.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 102\n",
      "[485/543] Writing tensor blk.53.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 103\n",
      "[486/543] Writing tensor blk.53.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 103\n",
      "[487/543] Writing tensor blk.53.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 104\n",
      "[488/543] Writing tensor blk.53.attn_norm.weight                | size   6656           | type F32  | T+ 104\n",
      "[489/543] Writing tensor blk.53.ffn_norm.weight                 | size   6656           | type F32  | T+ 104\n",
      "[490/543] Writing tensor blk.54.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 104\n",
      "[491/543] Writing tensor blk.54.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 104\n",
      "[492/543] Writing tensor blk.54.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 104\n",
      "[493/543] Writing tensor blk.54.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 104\n",
      "[494/543] Writing tensor blk.54.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 104\n",
      "[495/543] Writing tensor blk.54.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 105\n",
      "[496/543] Writing tensor blk.54.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 105\n",
      "[497/543] Writing tensor blk.54.attn_norm.weight                | size   6656           | type F32  | T+ 106\n",
      "[498/543] Writing tensor blk.54.ffn_norm.weight                 | size   6656           | type F32  | T+ 106\n",
      "[499/543] Writing tensor blk.55.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 106\n",
      "[500/543] Writing tensor blk.55.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 106\n",
      "[501/543] Writing tensor blk.55.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 106\n",
      "[502/543] Writing tensor blk.55.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 106\n",
      "[503/543] Writing tensor blk.55.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 106\n",
      "[504/543] Writing tensor blk.55.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 107\n",
      "[505/543] Writing tensor blk.55.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 107\n",
      "[506/543] Writing tensor blk.55.attn_norm.weight                | size   6656           | type F32  | T+ 107\n",
      "[507/543] Writing tensor blk.55.ffn_norm.weight                 | size   6656           | type F32  | T+ 108\n",
      "[508/543] Writing tensor blk.56.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 108\n",
      "[509/543] Writing tensor blk.56.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 108\n",
      "[510/543] Writing tensor blk.56.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 108\n",
      "[511/543] Writing tensor blk.56.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 108\n",
      "[512/543] Writing tensor blk.56.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 108\n",
      "[513/543] Writing tensor blk.56.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 109\n",
      "[514/543] Writing tensor blk.56.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 109\n",
      "[515/543] Writing tensor blk.56.attn_norm.weight                | size   6656           | type F32  | T+ 109\n",
      "[516/543] Writing tensor blk.56.ffn_norm.weight                 | size   6656           | type F32  | T+ 109\n",
      "[517/543] Writing tensor blk.57.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 109\n",
      "[518/543] Writing tensor blk.57.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 109\n",
      "[519/543] Writing tensor blk.57.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 109\n",
      "[520/543] Writing tensor blk.57.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 109\n",
      "[521/543] Writing tensor blk.57.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 111\n",
      "[522/543] Writing tensor blk.57.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 112\n",
      "[523/543] Writing tensor blk.57.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 112\n",
      "[524/543] Writing tensor blk.57.attn_norm.weight                | size   6656           | type F32  | T+ 112\n",
      "[525/543] Writing tensor blk.57.ffn_norm.weight                 | size   6656           | type F32  | T+ 113\n",
      "[526/543] Writing tensor blk.58.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 113\n",
      "[527/543] Writing tensor blk.58.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 113\n",
      "[528/543] Writing tensor blk.58.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 113\n",
      "[529/543] Writing tensor blk.58.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 113\n",
      "[530/543] Writing tensor blk.58.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 113\n",
      "[531/543] Writing tensor blk.58.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 114\n",
      "[532/543] Writing tensor blk.58.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 114\n",
      "[533/543] Writing tensor blk.58.attn_norm.weight                | size   6656           | type F32  | T+ 114\n",
      "[534/543] Writing tensor blk.58.ffn_norm.weight                 | size   6656           | type F32  | T+ 114\n",
      "[535/543] Writing tensor blk.59.attn_q.weight                   | size   6656 x   6656  | type F16  | T+ 114\n",
      "[536/543] Writing tensor blk.59.attn_k.weight                   | size   6656 x   6656  | type F16  | T+ 114\n",
      "[537/543] Writing tensor blk.59.attn_v.weight                   | size   6656 x   6656  | type F16  | T+ 114\n",
      "[538/543] Writing tensor blk.59.attn_output.weight              | size   6656 x   6656  | type F16  | T+ 114\n",
      "[539/543] Writing tensor blk.59.ffn_gate.weight                 | size  17920 x   6656  | type F16  | T+ 115\n",
      "[540/543] Writing tensor blk.59.ffn_down.weight                 | size   6656 x  17920  | type F16  | T+ 115\n",
      "[541/543] Writing tensor blk.59.ffn_up.weight                   | size  17920 x   6656  | type F16  | T+ 116\n",
      "[542/543] Writing tensor blk.59.attn_norm.weight                | size   6656           | type F32  | T+ 116\n",
      "[543/543] Writing tensor blk.59.ffn_norm.weight                 | size   6656           | type F32  | T+ 116\n",
      "Wrote models/30B/ggml-model-f16.gguf\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "params = Params(n_vocab=32000, n_embd=8192, n_layer=80, n_ctx=4096, n_ff=22016, n_head=64, n_head_kv=64, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/65B'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F16    | [32000, 8192]\n",
      "norm.weight                                      -> output_norm.weight                       | F16    | [8192]\n",
      "output.weight                                    -> output.weight                            | F16    | [32000, 8192]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F16    | [8192]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F16    | [8192]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F16    | [8192]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F16    | [8192]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F16    | [8192]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F16    | [8192]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.6.attention.wq.weight                     -> blk.6.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wk.weight                     -> blk.6.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wv.weight                     -> blk.6.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.6.attention.wo.weight                     -> blk.6.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.6.feed_forward.w1.weight                  -> blk.6.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.6.feed_forward.w2.weight                  -> blk.6.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.6.feed_forward.w3.weight                  -> blk.6.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.6.attention_norm.weight                   -> blk.6.attn_norm.weight                   | F16    | [8192]\n",
      "layers.6.ffn_norm.weight                         -> blk.6.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.7.attention.wq.weight                     -> blk.7.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wk.weight                     -> blk.7.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wv.weight                     -> blk.7.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.7.attention.wo.weight                     -> blk.7.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.7.feed_forward.w1.weight                  -> blk.7.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.7.feed_forward.w2.weight                  -> blk.7.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.7.feed_forward.w3.weight                  -> blk.7.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.7.attention_norm.weight                   -> blk.7.attn_norm.weight                   | F16    | [8192]\n",
      "layers.7.ffn_norm.weight                         -> blk.7.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.8.attention.wq.weight                     -> blk.8.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wk.weight                     -> blk.8.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wv.weight                     -> blk.8.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.8.attention.wo.weight                     -> blk.8.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.8.feed_forward.w1.weight                  -> blk.8.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.8.feed_forward.w2.weight                  -> blk.8.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.8.feed_forward.w3.weight                  -> blk.8.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.8.attention_norm.weight                   -> blk.8.attn_norm.weight                   | F16    | [8192]\n",
      "layers.8.ffn_norm.weight                         -> blk.8.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.9.attention.wq.weight                     -> blk.9.attn_q.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wk.weight                     -> blk.9.attn_k.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wv.weight                     -> blk.9.attn_v.weight                      | F16    | [8192, 8192]\n",
      "layers.9.attention.wo.weight                     -> blk.9.attn_output.weight                 | F16    | [8192, 8192]\n",
      "layers.9.feed_forward.w1.weight                  -> blk.9.ffn_gate.weight                    | F16    | [22016, 8192]\n",
      "layers.9.feed_forward.w2.weight                  -> blk.9.ffn_down.weight                    | F16    | [8192, 22016]\n",
      "layers.9.feed_forward.w3.weight                  -> blk.9.ffn_up.weight                      | F16    | [22016, 8192]\n",
      "layers.9.attention_norm.weight                   -> blk.9.attn_norm.weight                   | F16    | [8192]\n",
      "layers.9.ffn_norm.weight                         -> blk.9.ffn_norm.weight                    | F16    | [8192]\n",
      "layers.10.attention.wq.weight                    -> blk.10.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wk.weight                    -> blk.10.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wv.weight                    -> blk.10.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.10.attention.wo.weight                    -> blk.10.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.10.feed_forward.w1.weight                 -> blk.10.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.10.feed_forward.w2.weight                 -> blk.10.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.10.feed_forward.w3.weight                 -> blk.10.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.10.attention_norm.weight                  -> blk.10.attn_norm.weight                  | F16    | [8192]\n",
      "layers.10.ffn_norm.weight                        -> blk.10.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.11.attention.wq.weight                    -> blk.11.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wk.weight                    -> blk.11.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wv.weight                    -> blk.11.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.11.attention.wo.weight                    -> blk.11.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.11.feed_forward.w1.weight                 -> blk.11.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.11.feed_forward.w2.weight                 -> blk.11.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.11.feed_forward.w3.weight                 -> blk.11.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.11.attention_norm.weight                  -> blk.11.attn_norm.weight                  | F16    | [8192]\n",
      "layers.11.ffn_norm.weight                        -> blk.11.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.12.attention.wq.weight                    -> blk.12.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wk.weight                    -> blk.12.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wv.weight                    -> blk.12.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.12.attention.wo.weight                    -> blk.12.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.12.feed_forward.w1.weight                 -> blk.12.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.12.feed_forward.w2.weight                 -> blk.12.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.12.feed_forward.w3.weight                 -> blk.12.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.12.attention_norm.weight                  -> blk.12.attn_norm.weight                  | F16    | [8192]\n",
      "layers.12.ffn_norm.weight                        -> blk.12.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.13.attention.wq.weight                    -> blk.13.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wk.weight                    -> blk.13.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wv.weight                    -> blk.13.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.13.attention.wo.weight                    -> blk.13.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.13.feed_forward.w1.weight                 -> blk.13.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.13.feed_forward.w2.weight                 -> blk.13.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.13.feed_forward.w3.weight                 -> blk.13.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.13.attention_norm.weight                  -> blk.13.attn_norm.weight                  | F16    | [8192]\n",
      "layers.13.ffn_norm.weight                        -> blk.13.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.14.attention.wq.weight                    -> blk.14.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wk.weight                    -> blk.14.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wv.weight                    -> blk.14.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.14.attention.wo.weight                    -> blk.14.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.14.feed_forward.w1.weight                 -> blk.14.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.14.feed_forward.w2.weight                 -> blk.14.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.14.feed_forward.w3.weight                 -> blk.14.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.14.attention_norm.weight                  -> blk.14.attn_norm.weight                  | F16    | [8192]\n",
      "layers.14.ffn_norm.weight                        -> blk.14.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.15.attention.wq.weight                    -> blk.15.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wk.weight                    -> blk.15.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wv.weight                    -> blk.15.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.15.attention.wo.weight                    -> blk.15.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.15.feed_forward.w1.weight                 -> blk.15.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.15.feed_forward.w2.weight                 -> blk.15.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.15.feed_forward.w3.weight                 -> blk.15.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.15.attention_norm.weight                  -> blk.15.attn_norm.weight                  | F16    | [8192]\n",
      "layers.15.ffn_norm.weight                        -> blk.15.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.16.attention.wq.weight                    -> blk.16.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wk.weight                    -> blk.16.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wv.weight                    -> blk.16.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.16.attention.wo.weight                    -> blk.16.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.16.feed_forward.w1.weight                 -> blk.16.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.16.feed_forward.w2.weight                 -> blk.16.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.16.feed_forward.w3.weight                 -> blk.16.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.16.attention_norm.weight                  -> blk.16.attn_norm.weight                  | F16    | [8192]\n",
      "layers.16.ffn_norm.weight                        -> blk.16.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.17.attention.wq.weight                    -> blk.17.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wk.weight                    -> blk.17.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wv.weight                    -> blk.17.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.17.attention.wo.weight                    -> blk.17.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.17.feed_forward.w1.weight                 -> blk.17.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.17.feed_forward.w2.weight                 -> blk.17.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.17.feed_forward.w3.weight                 -> blk.17.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.17.attention_norm.weight                  -> blk.17.attn_norm.weight                  | F16    | [8192]\n",
      "layers.17.ffn_norm.weight                        -> blk.17.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.18.attention.wq.weight                    -> blk.18.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wk.weight                    -> blk.18.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wv.weight                    -> blk.18.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.18.attention.wo.weight                    -> blk.18.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.18.feed_forward.w1.weight                 -> blk.18.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.18.feed_forward.w2.weight                 -> blk.18.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.18.feed_forward.w3.weight                 -> blk.18.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.18.attention_norm.weight                  -> blk.18.attn_norm.weight                  | F16    | [8192]\n",
      "layers.18.ffn_norm.weight                        -> blk.18.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.19.attention.wq.weight                    -> blk.19.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wk.weight                    -> blk.19.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wv.weight                    -> blk.19.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.19.attention.wo.weight                    -> blk.19.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.19.feed_forward.w1.weight                 -> blk.19.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.19.feed_forward.w2.weight                 -> blk.19.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.19.feed_forward.w3.weight                 -> blk.19.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.19.attention_norm.weight                  -> blk.19.attn_norm.weight                  | F16    | [8192]\n",
      "layers.19.ffn_norm.weight                        -> blk.19.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.20.attention.wq.weight                    -> blk.20.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wk.weight                    -> blk.20.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wv.weight                    -> blk.20.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.20.attention.wo.weight                    -> blk.20.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.20.feed_forward.w1.weight                 -> blk.20.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.20.feed_forward.w2.weight                 -> blk.20.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.20.feed_forward.w3.weight                 -> blk.20.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.20.attention_norm.weight                  -> blk.20.attn_norm.weight                  | F16    | [8192]\n",
      "layers.20.ffn_norm.weight                        -> blk.20.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.21.attention.wq.weight                    -> blk.21.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wk.weight                    -> blk.21.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wv.weight                    -> blk.21.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.21.attention.wo.weight                    -> blk.21.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.21.feed_forward.w1.weight                 -> blk.21.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.21.feed_forward.w2.weight                 -> blk.21.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.21.feed_forward.w3.weight                 -> blk.21.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.21.attention_norm.weight                  -> blk.21.attn_norm.weight                  | F16    | [8192]\n",
      "layers.21.ffn_norm.weight                        -> blk.21.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.22.attention.wq.weight                    -> blk.22.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wk.weight                    -> blk.22.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wv.weight                    -> blk.22.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.22.attention.wo.weight                    -> blk.22.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.22.feed_forward.w1.weight                 -> blk.22.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.22.feed_forward.w2.weight                 -> blk.22.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.22.feed_forward.w3.weight                 -> blk.22.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.22.attention_norm.weight                  -> blk.22.attn_norm.weight                  | F16    | [8192]\n",
      "layers.22.ffn_norm.weight                        -> blk.22.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.23.attention.wq.weight                    -> blk.23.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wk.weight                    -> blk.23.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wv.weight                    -> blk.23.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.23.attention.wo.weight                    -> blk.23.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.23.feed_forward.w1.weight                 -> blk.23.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.23.feed_forward.w2.weight                 -> blk.23.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.23.feed_forward.w3.weight                 -> blk.23.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.23.attention_norm.weight                  -> blk.23.attn_norm.weight                  | F16    | [8192]\n",
      "layers.23.ffn_norm.weight                        -> blk.23.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.24.attention.wq.weight                    -> blk.24.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wk.weight                    -> blk.24.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wv.weight                    -> blk.24.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.24.attention.wo.weight                    -> blk.24.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.24.feed_forward.w1.weight                 -> blk.24.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.24.feed_forward.w2.weight                 -> blk.24.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.24.feed_forward.w3.weight                 -> blk.24.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.24.attention_norm.weight                  -> blk.24.attn_norm.weight                  | F16    | [8192]\n",
      "layers.24.ffn_norm.weight                        -> blk.24.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.25.attention.wq.weight                    -> blk.25.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wk.weight                    -> blk.25.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wv.weight                    -> blk.25.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.25.attention.wo.weight                    -> blk.25.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.25.feed_forward.w1.weight                 -> blk.25.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.25.feed_forward.w2.weight                 -> blk.25.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.25.feed_forward.w3.weight                 -> blk.25.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.25.attention_norm.weight                  -> blk.25.attn_norm.weight                  | F16    | [8192]\n",
      "layers.25.ffn_norm.weight                        -> blk.25.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.26.attention.wq.weight                    -> blk.26.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wk.weight                    -> blk.26.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wv.weight                    -> blk.26.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.26.attention.wo.weight                    -> blk.26.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.26.feed_forward.w1.weight                 -> blk.26.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.26.feed_forward.w2.weight                 -> blk.26.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.26.feed_forward.w3.weight                 -> blk.26.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.26.attention_norm.weight                  -> blk.26.attn_norm.weight                  | F16    | [8192]\n",
      "layers.26.ffn_norm.weight                        -> blk.26.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.27.attention.wq.weight                    -> blk.27.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wk.weight                    -> blk.27.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wv.weight                    -> blk.27.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.27.attention.wo.weight                    -> blk.27.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.27.feed_forward.w1.weight                 -> blk.27.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.27.feed_forward.w2.weight                 -> blk.27.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.27.feed_forward.w3.weight                 -> blk.27.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.27.attention_norm.weight                  -> blk.27.attn_norm.weight                  | F16    | [8192]\n",
      "layers.27.ffn_norm.weight                        -> blk.27.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.28.attention.wq.weight                    -> blk.28.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wk.weight                    -> blk.28.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wv.weight                    -> blk.28.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.28.attention.wo.weight                    -> blk.28.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.28.feed_forward.w1.weight                 -> blk.28.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.28.feed_forward.w2.weight                 -> blk.28.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.28.feed_forward.w3.weight                 -> blk.28.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.28.attention_norm.weight                  -> blk.28.attn_norm.weight                  | F16    | [8192]\n",
      "layers.28.ffn_norm.weight                        -> blk.28.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.29.attention.wq.weight                    -> blk.29.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wk.weight                    -> blk.29.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wv.weight                    -> blk.29.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.29.attention.wo.weight                    -> blk.29.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.29.feed_forward.w1.weight                 -> blk.29.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.29.feed_forward.w2.weight                 -> blk.29.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.29.feed_forward.w3.weight                 -> blk.29.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.29.attention_norm.weight                  -> blk.29.attn_norm.weight                  | F16    | [8192]\n",
      "layers.29.ffn_norm.weight                        -> blk.29.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.30.attention.wq.weight                    -> blk.30.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wk.weight                    -> blk.30.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wv.weight                    -> blk.30.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.30.attention.wo.weight                    -> blk.30.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.30.feed_forward.w1.weight                 -> blk.30.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.30.feed_forward.w2.weight                 -> blk.30.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.30.feed_forward.w3.weight                 -> blk.30.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.30.attention_norm.weight                  -> blk.30.attn_norm.weight                  | F16    | [8192]\n",
      "layers.30.ffn_norm.weight                        -> blk.30.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.31.attention.wq.weight                    -> blk.31.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wk.weight                    -> blk.31.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wv.weight                    -> blk.31.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.31.attention.wo.weight                    -> blk.31.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.31.feed_forward.w1.weight                 -> blk.31.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.31.feed_forward.w2.weight                 -> blk.31.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.31.feed_forward.w3.weight                 -> blk.31.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.31.attention_norm.weight                  -> blk.31.attn_norm.weight                  | F16    | [8192]\n",
      "layers.31.ffn_norm.weight                        -> blk.31.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.32.attention.wq.weight                    -> blk.32.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wk.weight                    -> blk.32.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wv.weight                    -> blk.32.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.32.attention.wo.weight                    -> blk.32.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.32.feed_forward.w1.weight                 -> blk.32.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.32.feed_forward.w2.weight                 -> blk.32.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.32.feed_forward.w3.weight                 -> blk.32.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.32.attention_norm.weight                  -> blk.32.attn_norm.weight                  | F16    | [8192]\n",
      "layers.32.ffn_norm.weight                        -> blk.32.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.33.attention.wq.weight                    -> blk.33.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wk.weight                    -> blk.33.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wv.weight                    -> blk.33.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.33.attention.wo.weight                    -> blk.33.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.33.feed_forward.w1.weight                 -> blk.33.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.33.feed_forward.w2.weight                 -> blk.33.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.33.feed_forward.w3.weight                 -> blk.33.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.33.attention_norm.weight                  -> blk.33.attn_norm.weight                  | F16    | [8192]\n",
      "layers.33.ffn_norm.weight                        -> blk.33.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.34.attention.wq.weight                    -> blk.34.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wk.weight                    -> blk.34.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wv.weight                    -> blk.34.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.34.attention.wo.weight                    -> blk.34.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.34.feed_forward.w1.weight                 -> blk.34.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.34.feed_forward.w2.weight                 -> blk.34.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.34.feed_forward.w3.weight                 -> blk.34.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.34.attention_norm.weight                  -> blk.34.attn_norm.weight                  | F16    | [8192]\n",
      "layers.34.ffn_norm.weight                        -> blk.34.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.35.attention.wq.weight                    -> blk.35.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wk.weight                    -> blk.35.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wv.weight                    -> blk.35.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.35.attention.wo.weight                    -> blk.35.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.35.feed_forward.w1.weight                 -> blk.35.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.35.feed_forward.w2.weight                 -> blk.35.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.35.feed_forward.w3.weight                 -> blk.35.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.35.attention_norm.weight                  -> blk.35.attn_norm.weight                  | F16    | [8192]\n",
      "layers.35.ffn_norm.weight                        -> blk.35.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.36.attention.wq.weight                    -> blk.36.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wk.weight                    -> blk.36.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wv.weight                    -> blk.36.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.36.attention.wo.weight                    -> blk.36.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.36.feed_forward.w1.weight                 -> blk.36.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.36.feed_forward.w2.weight                 -> blk.36.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.36.feed_forward.w3.weight                 -> blk.36.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.36.attention_norm.weight                  -> blk.36.attn_norm.weight                  | F16    | [8192]\n",
      "layers.36.ffn_norm.weight                        -> blk.36.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.37.attention.wq.weight                    -> blk.37.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wk.weight                    -> blk.37.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wv.weight                    -> blk.37.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.37.attention.wo.weight                    -> blk.37.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.37.feed_forward.w1.weight                 -> blk.37.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.37.feed_forward.w2.weight                 -> blk.37.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.37.feed_forward.w3.weight                 -> blk.37.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.37.attention_norm.weight                  -> blk.37.attn_norm.weight                  | F16    | [8192]\n",
      "layers.37.ffn_norm.weight                        -> blk.37.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.38.attention.wq.weight                    -> blk.38.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wk.weight                    -> blk.38.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wv.weight                    -> blk.38.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.38.attention.wo.weight                    -> blk.38.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.38.feed_forward.w1.weight                 -> blk.38.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.38.feed_forward.w2.weight                 -> blk.38.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.38.feed_forward.w3.weight                 -> blk.38.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.38.attention_norm.weight                  -> blk.38.attn_norm.weight                  | F16    | [8192]\n",
      "layers.38.ffn_norm.weight                        -> blk.38.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.39.attention.wq.weight                    -> blk.39.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wk.weight                    -> blk.39.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wv.weight                    -> blk.39.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.39.attention.wo.weight                    -> blk.39.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.39.feed_forward.w1.weight                 -> blk.39.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.39.feed_forward.w2.weight                 -> blk.39.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.39.feed_forward.w3.weight                 -> blk.39.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.39.attention_norm.weight                  -> blk.39.attn_norm.weight                  | F16    | [8192]\n",
      "layers.39.ffn_norm.weight                        -> blk.39.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.40.attention.wq.weight                    -> blk.40.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wk.weight                    -> blk.40.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wv.weight                    -> blk.40.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.40.attention.wo.weight                    -> blk.40.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.40.feed_forward.w1.weight                 -> blk.40.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.40.feed_forward.w2.weight                 -> blk.40.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.40.feed_forward.w3.weight                 -> blk.40.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.40.attention_norm.weight                  -> blk.40.attn_norm.weight                  | F16    | [8192]\n",
      "layers.40.ffn_norm.weight                        -> blk.40.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.41.attention.wq.weight                    -> blk.41.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wk.weight                    -> blk.41.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wv.weight                    -> blk.41.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.41.attention.wo.weight                    -> blk.41.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.41.feed_forward.w1.weight                 -> blk.41.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.41.feed_forward.w2.weight                 -> blk.41.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.41.feed_forward.w3.weight                 -> blk.41.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.41.attention_norm.weight                  -> blk.41.attn_norm.weight                  | F16    | [8192]\n",
      "layers.41.ffn_norm.weight                        -> blk.41.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.42.attention.wq.weight                    -> blk.42.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wk.weight                    -> blk.42.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wv.weight                    -> blk.42.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.42.attention.wo.weight                    -> blk.42.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.42.feed_forward.w1.weight                 -> blk.42.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.42.feed_forward.w2.weight                 -> blk.42.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.42.feed_forward.w3.weight                 -> blk.42.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.42.attention_norm.weight                  -> blk.42.attn_norm.weight                  | F16    | [8192]\n",
      "layers.42.ffn_norm.weight                        -> blk.42.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.43.attention.wq.weight                    -> blk.43.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wk.weight                    -> blk.43.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wv.weight                    -> blk.43.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.43.attention.wo.weight                    -> blk.43.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.43.feed_forward.w1.weight                 -> blk.43.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.43.feed_forward.w2.weight                 -> blk.43.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.43.feed_forward.w3.weight                 -> blk.43.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.43.attention_norm.weight                  -> blk.43.attn_norm.weight                  | F16    | [8192]\n",
      "layers.43.ffn_norm.weight                        -> blk.43.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.44.attention.wq.weight                    -> blk.44.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wk.weight                    -> blk.44.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wv.weight                    -> blk.44.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.44.attention.wo.weight                    -> blk.44.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.44.feed_forward.w1.weight                 -> blk.44.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.44.feed_forward.w2.weight                 -> blk.44.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.44.feed_forward.w3.weight                 -> blk.44.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.44.attention_norm.weight                  -> blk.44.attn_norm.weight                  | F16    | [8192]\n",
      "layers.44.ffn_norm.weight                        -> blk.44.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.45.attention.wq.weight                    -> blk.45.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wk.weight                    -> blk.45.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wv.weight                    -> blk.45.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.45.attention.wo.weight                    -> blk.45.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.45.feed_forward.w1.weight                 -> blk.45.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.45.feed_forward.w2.weight                 -> blk.45.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.45.feed_forward.w3.weight                 -> blk.45.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.45.attention_norm.weight                  -> blk.45.attn_norm.weight                  | F16    | [8192]\n",
      "layers.45.ffn_norm.weight                        -> blk.45.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.46.attention.wq.weight                    -> blk.46.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wk.weight                    -> blk.46.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wv.weight                    -> blk.46.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.46.attention.wo.weight                    -> blk.46.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.46.feed_forward.w1.weight                 -> blk.46.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.46.feed_forward.w2.weight                 -> blk.46.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.46.feed_forward.w3.weight                 -> blk.46.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.46.attention_norm.weight                  -> blk.46.attn_norm.weight                  | F16    | [8192]\n",
      "layers.46.ffn_norm.weight                        -> blk.46.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.47.attention.wq.weight                    -> blk.47.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wk.weight                    -> blk.47.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wv.weight                    -> blk.47.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.47.attention.wo.weight                    -> blk.47.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.47.feed_forward.w1.weight                 -> blk.47.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.47.feed_forward.w2.weight                 -> blk.47.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.47.feed_forward.w3.weight                 -> blk.47.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.47.attention_norm.weight                  -> blk.47.attn_norm.weight                  | F16    | [8192]\n",
      "layers.47.ffn_norm.weight                        -> blk.47.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.48.attention.wq.weight                    -> blk.48.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wk.weight                    -> blk.48.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wv.weight                    -> blk.48.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.48.attention.wo.weight                    -> blk.48.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.48.feed_forward.w1.weight                 -> blk.48.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.48.feed_forward.w2.weight                 -> blk.48.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.48.feed_forward.w3.weight                 -> blk.48.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.48.attention_norm.weight                  -> blk.48.attn_norm.weight                  | F16    | [8192]\n",
      "layers.48.ffn_norm.weight                        -> blk.48.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.49.attention.wq.weight                    -> blk.49.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wk.weight                    -> blk.49.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wv.weight                    -> blk.49.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.49.attention.wo.weight                    -> blk.49.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.49.feed_forward.w1.weight                 -> blk.49.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.49.feed_forward.w2.weight                 -> blk.49.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.49.feed_forward.w3.weight                 -> blk.49.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.49.attention_norm.weight                  -> blk.49.attn_norm.weight                  | F16    | [8192]\n",
      "layers.49.ffn_norm.weight                        -> blk.49.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.50.attention.wq.weight                    -> blk.50.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wk.weight                    -> blk.50.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wv.weight                    -> blk.50.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.50.attention.wo.weight                    -> blk.50.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.50.feed_forward.w1.weight                 -> blk.50.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.50.feed_forward.w2.weight                 -> blk.50.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.50.feed_forward.w3.weight                 -> blk.50.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.50.attention_norm.weight                  -> blk.50.attn_norm.weight                  | F16    | [8192]\n",
      "layers.50.ffn_norm.weight                        -> blk.50.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.51.attention.wq.weight                    -> blk.51.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wk.weight                    -> blk.51.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wv.weight                    -> blk.51.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.51.attention.wo.weight                    -> blk.51.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.51.feed_forward.w1.weight                 -> blk.51.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.51.feed_forward.w2.weight                 -> blk.51.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.51.feed_forward.w3.weight                 -> blk.51.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.51.attention_norm.weight                  -> blk.51.attn_norm.weight                  | F16    | [8192]\n",
      "layers.51.ffn_norm.weight                        -> blk.51.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.52.attention.wq.weight                    -> blk.52.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wk.weight                    -> blk.52.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wv.weight                    -> blk.52.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.52.attention.wo.weight                    -> blk.52.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.52.feed_forward.w1.weight                 -> blk.52.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.52.feed_forward.w2.weight                 -> blk.52.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.52.feed_forward.w3.weight                 -> blk.52.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.52.attention_norm.weight                  -> blk.52.attn_norm.weight                  | F16    | [8192]\n",
      "layers.52.ffn_norm.weight                        -> blk.52.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.53.attention.wq.weight                    -> blk.53.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wk.weight                    -> blk.53.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wv.weight                    -> blk.53.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.53.attention.wo.weight                    -> blk.53.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.53.feed_forward.w1.weight                 -> blk.53.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.53.feed_forward.w2.weight                 -> blk.53.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.53.feed_forward.w3.weight                 -> blk.53.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.53.attention_norm.weight                  -> blk.53.attn_norm.weight                  | F16    | [8192]\n",
      "layers.53.ffn_norm.weight                        -> blk.53.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.54.attention.wq.weight                    -> blk.54.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wk.weight                    -> blk.54.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wv.weight                    -> blk.54.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.54.attention.wo.weight                    -> blk.54.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.54.feed_forward.w1.weight                 -> blk.54.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.54.feed_forward.w2.weight                 -> blk.54.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.54.feed_forward.w3.weight                 -> blk.54.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.54.attention_norm.weight                  -> blk.54.attn_norm.weight                  | F16    | [8192]\n",
      "layers.54.ffn_norm.weight                        -> blk.54.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.55.attention.wq.weight                    -> blk.55.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wk.weight                    -> blk.55.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wv.weight                    -> blk.55.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.55.attention.wo.weight                    -> blk.55.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.55.feed_forward.w1.weight                 -> blk.55.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.55.feed_forward.w2.weight                 -> blk.55.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.55.feed_forward.w3.weight                 -> blk.55.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.55.attention_norm.weight                  -> blk.55.attn_norm.weight                  | F16    | [8192]\n",
      "layers.55.ffn_norm.weight                        -> blk.55.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.56.attention.wq.weight                    -> blk.56.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wk.weight                    -> blk.56.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wv.weight                    -> blk.56.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.56.attention.wo.weight                    -> blk.56.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.56.feed_forward.w1.weight                 -> blk.56.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.56.feed_forward.w2.weight                 -> blk.56.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.56.feed_forward.w3.weight                 -> blk.56.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.56.attention_norm.weight                  -> blk.56.attn_norm.weight                  | F16    | [8192]\n",
      "layers.56.ffn_norm.weight                        -> blk.56.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.57.attention.wq.weight                    -> blk.57.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wk.weight                    -> blk.57.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wv.weight                    -> blk.57.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.57.attention.wo.weight                    -> blk.57.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.57.feed_forward.w1.weight                 -> blk.57.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.57.feed_forward.w2.weight                 -> blk.57.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.57.feed_forward.w3.weight                 -> blk.57.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.57.attention_norm.weight                  -> blk.57.attn_norm.weight                  | F16    | [8192]\n",
      "layers.57.ffn_norm.weight                        -> blk.57.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.58.attention.wq.weight                    -> blk.58.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wk.weight                    -> blk.58.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wv.weight                    -> blk.58.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.58.attention.wo.weight                    -> blk.58.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.58.feed_forward.w1.weight                 -> blk.58.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.58.feed_forward.w2.weight                 -> blk.58.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.58.feed_forward.w3.weight                 -> blk.58.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.58.attention_norm.weight                  -> blk.58.attn_norm.weight                  | F16    | [8192]\n",
      "layers.58.ffn_norm.weight                        -> blk.58.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.59.attention.wq.weight                    -> blk.59.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wk.weight                    -> blk.59.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wv.weight                    -> blk.59.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.59.attention.wo.weight                    -> blk.59.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.59.feed_forward.w1.weight                 -> blk.59.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.59.feed_forward.w2.weight                 -> blk.59.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.59.feed_forward.w3.weight                 -> blk.59.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.59.attention_norm.weight                  -> blk.59.attn_norm.weight                  | F16    | [8192]\n",
      "layers.59.ffn_norm.weight                        -> blk.59.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.60.attention.wq.weight                    -> blk.60.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wk.weight                    -> blk.60.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wv.weight                    -> blk.60.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.60.attention.wo.weight                    -> blk.60.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.60.feed_forward.w1.weight                 -> blk.60.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.60.feed_forward.w2.weight                 -> blk.60.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.60.feed_forward.w3.weight                 -> blk.60.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.60.attention_norm.weight                  -> blk.60.attn_norm.weight                  | F16    | [8192]\n",
      "layers.60.ffn_norm.weight                        -> blk.60.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.61.attention.wq.weight                    -> blk.61.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wk.weight                    -> blk.61.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wv.weight                    -> blk.61.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.61.attention.wo.weight                    -> blk.61.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.61.feed_forward.w1.weight                 -> blk.61.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.61.feed_forward.w2.weight                 -> blk.61.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.61.feed_forward.w3.weight                 -> blk.61.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.61.attention_norm.weight                  -> blk.61.attn_norm.weight                  | F16    | [8192]\n",
      "layers.61.ffn_norm.weight                        -> blk.61.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.62.attention.wq.weight                    -> blk.62.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wk.weight                    -> blk.62.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wv.weight                    -> blk.62.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.62.attention.wo.weight                    -> blk.62.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.62.feed_forward.w1.weight                 -> blk.62.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.62.feed_forward.w2.weight                 -> blk.62.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.62.feed_forward.w3.weight                 -> blk.62.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.62.attention_norm.weight                  -> blk.62.attn_norm.weight                  | F16    | [8192]\n",
      "layers.62.ffn_norm.weight                        -> blk.62.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.63.attention.wq.weight                    -> blk.63.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wk.weight                    -> blk.63.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wv.weight                    -> blk.63.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.63.attention.wo.weight                    -> blk.63.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.63.feed_forward.w1.weight                 -> blk.63.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.63.feed_forward.w2.weight                 -> blk.63.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.63.feed_forward.w3.weight                 -> blk.63.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.63.attention_norm.weight                  -> blk.63.attn_norm.weight                  | F16    | [8192]\n",
      "layers.63.ffn_norm.weight                        -> blk.63.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.64.attention.wq.weight                    -> blk.64.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wk.weight                    -> blk.64.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wv.weight                    -> blk.64.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.64.attention.wo.weight                    -> blk.64.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.64.feed_forward.w1.weight                 -> blk.64.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.64.feed_forward.w2.weight                 -> blk.64.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.64.feed_forward.w3.weight                 -> blk.64.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.64.attention_norm.weight                  -> blk.64.attn_norm.weight                  | F16    | [8192]\n",
      "layers.64.ffn_norm.weight                        -> blk.64.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.65.attention.wq.weight                    -> blk.65.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wk.weight                    -> blk.65.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wv.weight                    -> blk.65.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.65.attention.wo.weight                    -> blk.65.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.65.feed_forward.w1.weight                 -> blk.65.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.65.feed_forward.w2.weight                 -> blk.65.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.65.feed_forward.w3.weight                 -> blk.65.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.65.attention_norm.weight                  -> blk.65.attn_norm.weight                  | F16    | [8192]\n",
      "layers.65.ffn_norm.weight                        -> blk.65.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.66.attention.wq.weight                    -> blk.66.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wk.weight                    -> blk.66.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wv.weight                    -> blk.66.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.66.attention.wo.weight                    -> blk.66.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.66.feed_forward.w1.weight                 -> blk.66.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.66.feed_forward.w2.weight                 -> blk.66.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.66.feed_forward.w3.weight                 -> blk.66.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.66.attention_norm.weight                  -> blk.66.attn_norm.weight                  | F16    | [8192]\n",
      "layers.66.ffn_norm.weight                        -> blk.66.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.67.attention.wq.weight                    -> blk.67.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wk.weight                    -> blk.67.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wv.weight                    -> blk.67.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.67.attention.wo.weight                    -> blk.67.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.67.feed_forward.w1.weight                 -> blk.67.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.67.feed_forward.w2.weight                 -> blk.67.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.67.feed_forward.w3.weight                 -> blk.67.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.67.attention_norm.weight                  -> blk.67.attn_norm.weight                  | F16    | [8192]\n",
      "layers.67.ffn_norm.weight                        -> blk.67.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.68.attention.wq.weight                    -> blk.68.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wk.weight                    -> blk.68.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wv.weight                    -> blk.68.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.68.attention.wo.weight                    -> blk.68.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.68.feed_forward.w1.weight                 -> blk.68.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.68.feed_forward.w2.weight                 -> blk.68.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.68.feed_forward.w3.weight                 -> blk.68.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.68.attention_norm.weight                  -> blk.68.attn_norm.weight                  | F16    | [8192]\n",
      "layers.68.ffn_norm.weight                        -> blk.68.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.69.attention.wq.weight                    -> blk.69.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wk.weight                    -> blk.69.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wv.weight                    -> blk.69.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.69.attention.wo.weight                    -> blk.69.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.69.feed_forward.w1.weight                 -> blk.69.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.69.feed_forward.w2.weight                 -> blk.69.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.69.feed_forward.w3.weight                 -> blk.69.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.69.attention_norm.weight                  -> blk.69.attn_norm.weight                  | F16    | [8192]\n",
      "layers.69.ffn_norm.weight                        -> blk.69.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.70.attention.wq.weight                    -> blk.70.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wk.weight                    -> blk.70.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wv.weight                    -> blk.70.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.70.attention.wo.weight                    -> blk.70.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.70.feed_forward.w1.weight                 -> blk.70.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.70.feed_forward.w2.weight                 -> blk.70.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.70.feed_forward.w3.weight                 -> blk.70.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.70.attention_norm.weight                  -> blk.70.attn_norm.weight                  | F16    | [8192]\n",
      "layers.70.ffn_norm.weight                        -> blk.70.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.71.attention.wq.weight                    -> blk.71.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wk.weight                    -> blk.71.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wv.weight                    -> blk.71.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.71.attention.wo.weight                    -> blk.71.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.71.feed_forward.w1.weight                 -> blk.71.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.71.feed_forward.w2.weight                 -> blk.71.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.71.feed_forward.w3.weight                 -> blk.71.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.71.attention_norm.weight                  -> blk.71.attn_norm.weight                  | F16    | [8192]\n",
      "layers.71.ffn_norm.weight                        -> blk.71.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.72.attention.wq.weight                    -> blk.72.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wk.weight                    -> blk.72.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wv.weight                    -> blk.72.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.72.attention.wo.weight                    -> blk.72.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.72.feed_forward.w1.weight                 -> blk.72.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.72.feed_forward.w2.weight                 -> blk.72.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.72.feed_forward.w3.weight                 -> blk.72.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.72.attention_norm.weight                  -> blk.72.attn_norm.weight                  | F16    | [8192]\n",
      "layers.72.ffn_norm.weight                        -> blk.72.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.73.attention.wq.weight                    -> blk.73.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wk.weight                    -> blk.73.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wv.weight                    -> blk.73.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.73.attention.wo.weight                    -> blk.73.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.73.feed_forward.w1.weight                 -> blk.73.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.73.feed_forward.w2.weight                 -> blk.73.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.73.feed_forward.w3.weight                 -> blk.73.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.73.attention_norm.weight                  -> blk.73.attn_norm.weight                  | F16    | [8192]\n",
      "layers.73.ffn_norm.weight                        -> blk.73.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.74.attention.wq.weight                    -> blk.74.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wk.weight                    -> blk.74.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wv.weight                    -> blk.74.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.74.attention.wo.weight                    -> blk.74.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.74.feed_forward.w1.weight                 -> blk.74.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.74.feed_forward.w2.weight                 -> blk.74.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.74.feed_forward.w3.weight                 -> blk.74.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.74.attention_norm.weight                  -> blk.74.attn_norm.weight                  | F16    | [8192]\n",
      "layers.74.ffn_norm.weight                        -> blk.74.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.75.attention.wq.weight                    -> blk.75.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wk.weight                    -> blk.75.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wv.weight                    -> blk.75.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.75.attention.wo.weight                    -> blk.75.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.75.feed_forward.w1.weight                 -> blk.75.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.75.feed_forward.w2.weight                 -> blk.75.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.75.feed_forward.w3.weight                 -> blk.75.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.75.attention_norm.weight                  -> blk.75.attn_norm.weight                  | F16    | [8192]\n",
      "layers.75.ffn_norm.weight                        -> blk.75.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.76.attention.wq.weight                    -> blk.76.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wk.weight                    -> blk.76.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wv.weight                    -> blk.76.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.76.attention.wo.weight                    -> blk.76.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.76.feed_forward.w1.weight                 -> blk.76.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.76.feed_forward.w2.weight                 -> blk.76.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.76.feed_forward.w3.weight                 -> blk.76.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.76.attention_norm.weight                  -> blk.76.attn_norm.weight                  | F16    | [8192]\n",
      "layers.76.ffn_norm.weight                        -> blk.76.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.77.attention.wq.weight                    -> blk.77.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wk.weight                    -> blk.77.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wv.weight                    -> blk.77.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.77.attention.wo.weight                    -> blk.77.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.77.feed_forward.w1.weight                 -> blk.77.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.77.feed_forward.w2.weight                 -> blk.77.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.77.feed_forward.w3.weight                 -> blk.77.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.77.attention_norm.weight                  -> blk.77.attn_norm.weight                  | F16    | [8192]\n",
      "layers.77.ffn_norm.weight                        -> blk.77.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.78.attention.wq.weight                    -> blk.78.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wk.weight                    -> blk.78.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wv.weight                    -> blk.78.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.78.attention.wo.weight                    -> blk.78.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.78.feed_forward.w1.weight                 -> blk.78.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.78.feed_forward.w2.weight                 -> blk.78.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.78.feed_forward.w3.weight                 -> blk.78.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.78.attention_norm.weight                  -> blk.78.attn_norm.weight                  | F16    | [8192]\n",
      "layers.78.ffn_norm.weight                        -> blk.78.ffn_norm.weight                   | F16    | [8192]\n",
      "layers.79.attention.wq.weight                    -> blk.79.attn_q.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wk.weight                    -> blk.79.attn_k.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wv.weight                    -> blk.79.attn_v.weight                     | F16    | [8192, 8192]\n",
      "layers.79.attention.wo.weight                    -> blk.79.attn_output.weight                | F16    | [8192, 8192]\n",
      "layers.79.feed_forward.w1.weight                 -> blk.79.ffn_gate.weight                   | F16    | [22016, 8192]\n",
      "layers.79.feed_forward.w2.weight                 -> blk.79.ffn_down.weight                   | F16    | [8192, 22016]\n",
      "layers.79.feed_forward.w3.weight                 -> blk.79.ffn_up.weight                     | F16    | [22016, 8192]\n",
      "layers.79.attention_norm.weight                  -> blk.79.attn_norm.weight                  | F16    | [8192]\n",
      "layers.79.ffn_norm.weight                        -> blk.79.ffn_norm.weight                   | F16    | [8192]\n",
      "skipping tensor rope_freqs\n",
      "Writing models/65B/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/723] Writing tensor token_embd.weight                      | size  32000 x   8192  | type F16  | T+   1\n",
      "[  2/723] Writing tensor output_norm.weight                     | size   8192           | type F32  | T+   1\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type F16  | T+   1\n",
      "[  4/723] Writing tensor blk.0.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  5/723] Writing tensor blk.0.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  6/723] Writing tensor blk.0.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   1\n",
      "[  7/723] Writing tensor blk.0.attn_output.weight               | size   8192 x   8192  | type F16  | T+   1\n",
      "[  8/723] Writing tensor blk.0.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   1\n",
      "[  9/723] Writing tensor blk.0.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   2\n",
      "[ 10/723] Writing tensor blk.0.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   2\n",
      "[ 11/723] Writing tensor blk.0.attn_norm.weight                 | size   8192           | type F32  | T+   2\n",
      "[ 12/723] Writing tensor blk.0.ffn_norm.weight                  | size   8192           | type F32  | T+   2\n",
      "[ 13/723] Writing tensor blk.1.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 14/723] Writing tensor blk.1.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 15/723] Writing tensor blk.1.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 16/723] Writing tensor blk.1.attn_output.weight               | size   8192 x   8192  | type F16  | T+   2\n",
      "[ 17/723] Writing tensor blk.1.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   3\n",
      "[ 18/723] Writing tensor blk.1.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   4\n",
      "[ 19/723] Writing tensor blk.1.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   4\n",
      "[ 20/723] Writing tensor blk.1.attn_norm.weight                 | size   8192           | type F32  | T+   5\n",
      "[ 21/723] Writing tensor blk.1.ffn_norm.weight                  | size   8192           | type F32  | T+   5\n",
      "[ 22/723] Writing tensor blk.2.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 23/723] Writing tensor blk.2.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 24/723] Writing tensor blk.2.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 25/723] Writing tensor blk.2.attn_output.weight               | size   8192 x   8192  | type F16  | T+   5\n",
      "[ 26/723] Writing tensor blk.2.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   6\n",
      "[ 27/723] Writing tensor blk.2.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   6\n",
      "[ 28/723] Writing tensor blk.2.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   6\n",
      "[ 29/723] Writing tensor blk.2.attn_norm.weight                 | size   8192           | type F32  | T+   6\n",
      "[ 30/723] Writing tensor blk.2.ffn_norm.weight                  | size   8192           | type F32  | T+   6\n",
      "[ 31/723] Writing tensor blk.3.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 32/723] Writing tensor blk.3.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 33/723] Writing tensor blk.3.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 34/723] Writing tensor blk.3.attn_output.weight               | size   8192 x   8192  | type F16  | T+   6\n",
      "[ 35/723] Writing tensor blk.3.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   7\n",
      "[ 36/723] Writing tensor blk.3.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   7\n",
      "[ 37/723] Writing tensor blk.3.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   7\n",
      "[ 38/723] Writing tensor blk.3.attn_norm.weight                 | size   8192           | type F32  | T+   7\n",
      "[ 39/723] Writing tensor blk.3.ffn_norm.weight                  | size   8192           | type F32  | T+   7\n",
      "[ 40/723] Writing tensor blk.4.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 41/723] Writing tensor blk.4.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 42/723] Writing tensor blk.4.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 43/723] Writing tensor blk.4.attn_output.weight               | size   8192 x   8192  | type F16  | T+   7\n",
      "[ 44/723] Writing tensor blk.4.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+   8\n",
      "[ 45/723] Writing tensor blk.4.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+   8\n",
      "[ 46/723] Writing tensor blk.4.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+   8\n",
      "[ 47/723] Writing tensor blk.4.attn_norm.weight                 | size   8192           | type F32  | T+   9\n",
      "[ 48/723] Writing tensor blk.4.ffn_norm.weight                  | size   8192           | type F32  | T+   9\n",
      "[ 49/723] Writing tensor blk.5.attn_q.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 50/723] Writing tensor blk.5.attn_k.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 51/723] Writing tensor blk.5.attn_v.weight                    | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 52/723] Writing tensor blk.5.attn_output.weight               | size   8192 x   8192  | type F16  | T+   9\n",
      "[ 53/723] Writing tensor blk.5.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  10\n",
      "[ 54/723] Writing tensor blk.5.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  10\n",
      "[ 55/723] Writing tensor blk.5.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  11\n",
      "[ 56/723] Writing tensor blk.5.attn_norm.weight                 | size   8192           | type F32  | T+  11\n",
      "[ 57/723] Writing tensor blk.5.ffn_norm.weight                  | size   8192           | type F32  | T+  11\n",
      "[ 58/723] Writing tensor blk.6.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 59/723] Writing tensor blk.6.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 60/723] Writing tensor blk.6.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 61/723] Writing tensor blk.6.attn_output.weight               | size   8192 x   8192  | type F16  | T+  11\n",
      "[ 62/723] Writing tensor blk.6.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 63/723] Writing tensor blk.6.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  12\n",
      "[ 64/723] Writing tensor blk.6.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  12\n",
      "[ 65/723] Writing tensor blk.6.attn_norm.weight                 | size   8192           | type F32  | T+  12\n",
      "[ 66/723] Writing tensor blk.6.ffn_norm.weight                  | size   8192           | type F32  | T+  12\n",
      "[ 67/723] Writing tensor blk.7.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 68/723] Writing tensor blk.7.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 69/723] Writing tensor blk.7.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  12\n",
      "[ 70/723] Writing tensor blk.7.attn_output.weight               | size   8192 x   8192  | type F16  | T+  13\n",
      "[ 71/723] Writing tensor blk.7.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  13\n",
      "[ 72/723] Writing tensor blk.7.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  13\n",
      "[ 73/723] Writing tensor blk.7.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  13\n",
      "[ 74/723] Writing tensor blk.7.attn_norm.weight                 | size   8192           | type F32  | T+  14\n",
      "[ 75/723] Writing tensor blk.7.ffn_norm.weight                  | size   8192           | type F32  | T+  14\n",
      "[ 76/723] Writing tensor blk.8.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 77/723] Writing tensor blk.8.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 78/723] Writing tensor blk.8.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 79/723] Writing tensor blk.8.attn_output.weight               | size   8192 x   8192  | type F16  | T+  14\n",
      "[ 80/723] Writing tensor blk.8.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  14\n",
      "[ 81/723] Writing tensor blk.8.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  15\n",
      "[ 82/723] Writing tensor blk.8.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  15\n",
      "[ 83/723] Writing tensor blk.8.attn_norm.weight                 | size   8192           | type F32  | T+  15\n",
      "[ 84/723] Writing tensor blk.8.ffn_norm.weight                  | size   8192           | type F32  | T+  15\n",
      "[ 85/723] Writing tensor blk.9.attn_q.weight                    | size   8192 x   8192  | type F16  | T+  15\n",
      "[ 86/723] Writing tensor blk.9.attn_k.weight                    | size   8192 x   8192  | type F16  | T+  15\n",
      "[ 87/723] Writing tensor blk.9.attn_v.weight                    | size   8192 x   8192  | type F16  | T+  15\n",
      "[ 88/723] Writing tensor blk.9.attn_output.weight               | size   8192 x   8192  | type F16  | T+  15\n",
      "[ 89/723] Writing tensor blk.9.ffn_gate.weight                  | size  22016 x   8192  | type F16  | T+  16\n",
      "[ 90/723] Writing tensor blk.9.ffn_down.weight                  | size   8192 x  22016  | type F16  | T+  17\n",
      "[ 91/723] Writing tensor blk.9.ffn_up.weight                    | size  22016 x   8192  | type F16  | T+  17\n",
      "[ 92/723] Writing tensor blk.9.attn_norm.weight                 | size   8192           | type F32  | T+  17\n",
      "[ 93/723] Writing tensor blk.9.ffn_norm.weight                  | size   8192           | type F32  | T+  17\n",
      "[ 94/723] Writing tensor blk.10.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 95/723] Writing tensor blk.10.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  17\n",
      "[ 96/723] Writing tensor blk.10.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  18\n",
      "[ 97/723] Writing tensor blk.10.attn_output.weight              | size   8192 x   8192  | type F16  | T+  18\n",
      "[ 98/723] Writing tensor blk.10.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  18\n",
      "[ 99/723] Writing tensor blk.10.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  18\n",
      "[100/723] Writing tensor blk.10.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  19\n",
      "[101/723] Writing tensor blk.10.attn_norm.weight                | size   8192           | type F32  | T+  19\n",
      "[102/723] Writing tensor blk.10.ffn_norm.weight                 | size   8192           | type F32  | T+  19\n",
      "[103/723] Writing tensor blk.11.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[104/723] Writing tensor blk.11.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[105/723] Writing tensor blk.11.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  19\n",
      "[106/723] Writing tensor blk.11.attn_output.weight              | size   8192 x   8192  | type F16  | T+  19\n",
      "[107/723] Writing tensor blk.11.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  19\n",
      "[108/723] Writing tensor blk.11.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  20\n",
      "[109/723] Writing tensor blk.11.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  20\n",
      "[110/723] Writing tensor blk.11.attn_norm.weight                | size   8192           | type F32  | T+  20\n",
      "[111/723] Writing tensor blk.11.ffn_norm.weight                 | size   8192           | type F32  | T+  20\n",
      "[112/723] Writing tensor blk.12.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  20\n",
      "[113/723] Writing tensor blk.12.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  20\n",
      "[114/723] Writing tensor blk.12.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  20\n",
      "[115/723] Writing tensor blk.12.attn_output.weight              | size   8192 x   8192  | type F16  | T+  20\n",
      "[116/723] Writing tensor blk.12.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  21\n",
      "[117/723] Writing tensor blk.12.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  21\n",
      "[118/723] Writing tensor blk.12.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  21\n",
      "[119/723] Writing tensor blk.12.attn_norm.weight                | size   8192           | type F32  | T+  21\n",
      "[120/723] Writing tensor blk.12.ffn_norm.weight                 | size   8192           | type F32  | T+  21\n",
      "[121/723] Writing tensor blk.13.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[122/723] Writing tensor blk.13.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[123/723] Writing tensor blk.13.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  21\n",
      "[124/723] Writing tensor blk.13.attn_output.weight              | size   8192 x   8192  | type F16  | T+  22\n",
      "[125/723] Writing tensor blk.13.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  23\n",
      "[126/723] Writing tensor blk.13.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  23\n",
      "[127/723] Writing tensor blk.13.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  24\n",
      "[128/723] Writing tensor blk.13.attn_norm.weight                | size   8192           | type F32  | T+  24\n",
      "[129/723] Writing tensor blk.13.ffn_norm.weight                 | size   8192           | type F32  | T+  24\n",
      "[130/723] Writing tensor blk.14.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[131/723] Writing tensor blk.14.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[132/723] Writing tensor blk.14.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  24\n",
      "[133/723] Writing tensor blk.14.attn_output.weight              | size   8192 x   8192  | type F16  | T+  24\n",
      "[134/723] Writing tensor blk.14.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  24\n",
      "[135/723] Writing tensor blk.14.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  25\n",
      "[136/723] Writing tensor blk.14.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  25\n",
      "[137/723] Writing tensor blk.14.attn_norm.weight                | size   8192           | type F32  | T+  25\n",
      "[138/723] Writing tensor blk.14.ffn_norm.weight                 | size   8192           | type F32  | T+  25\n",
      "[139/723] Writing tensor blk.15.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[140/723] Writing tensor blk.15.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[141/723] Writing tensor blk.15.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  25\n",
      "[142/723] Writing tensor blk.15.attn_output.weight              | size   8192 x   8192  | type F16  | T+  25\n",
      "[143/723] Writing tensor blk.15.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  26\n",
      "[144/723] Writing tensor blk.15.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  26\n",
      "[145/723] Writing tensor blk.15.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  26\n",
      "[146/723] Writing tensor blk.15.attn_norm.weight                | size   8192           | type F32  | T+  26\n",
      "[147/723] Writing tensor blk.15.ffn_norm.weight                 | size   8192           | type F32  | T+  26\n",
      "[148/723] Writing tensor blk.16.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[149/723] Writing tensor blk.16.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[150/723] Writing tensor blk.16.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  26\n",
      "[151/723] Writing tensor blk.16.attn_output.weight              | size   8192 x   8192  | type F16  | T+  26\n",
      "[152/723] Writing tensor blk.16.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  27\n",
      "[153/723] Writing tensor blk.16.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  27\n",
      "[154/723] Writing tensor blk.16.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  28\n",
      "[155/723] Writing tensor blk.16.attn_norm.weight                | size   8192           | type F32  | T+  28\n",
      "[156/723] Writing tensor blk.16.ffn_norm.weight                 | size   8192           | type F32  | T+  28\n",
      "[157/723] Writing tensor blk.17.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[158/723] Writing tensor blk.17.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[159/723] Writing tensor blk.17.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  28\n",
      "[160/723] Writing tensor blk.17.attn_output.weight              | size   8192 x   8192  | type F16  | T+  29\n",
      "[161/723] Writing tensor blk.17.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  30\n",
      "[162/723] Writing tensor blk.17.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  30\n",
      "[163/723] Writing tensor blk.17.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  30\n",
      "[164/723] Writing tensor blk.17.attn_norm.weight                | size   8192           | type F32  | T+  30\n",
      "[165/723] Writing tensor blk.17.ffn_norm.weight                 | size   8192           | type F32  | T+  30\n",
      "[166/723] Writing tensor blk.18.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[167/723] Writing tensor blk.18.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[168/723] Writing tensor blk.18.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  30\n",
      "[169/723] Writing tensor blk.18.attn_output.weight              | size   8192 x   8192  | type F16  | T+  30\n",
      "[170/723] Writing tensor blk.18.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  31\n",
      "[171/723] Writing tensor blk.18.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  31\n",
      "[172/723] Writing tensor blk.18.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  31\n",
      "[173/723] Writing tensor blk.18.attn_norm.weight                | size   8192           | type F32  | T+  31\n",
      "[174/723] Writing tensor blk.18.ffn_norm.weight                 | size   8192           | type F32  | T+  31\n",
      "[175/723] Writing tensor blk.19.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[176/723] Writing tensor blk.19.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  31\n",
      "[177/723] Writing tensor blk.19.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  32\n",
      "[178/723] Writing tensor blk.19.attn_output.weight              | size   8192 x   8192  | type F16  | T+  32\n",
      "[179/723] Writing tensor blk.19.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  32\n",
      "[180/723] Writing tensor blk.19.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  32\n",
      "[181/723] Writing tensor blk.19.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  32\n",
      "[182/723] Writing tensor blk.19.attn_norm.weight                | size   8192           | type F32  | T+  33\n",
      "[183/723] Writing tensor blk.19.ffn_norm.weight                 | size   8192           | type F32  | T+  33\n",
      "[184/723] Writing tensor blk.20.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  33\n",
      "[185/723] Writing tensor blk.20.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  33\n",
      "[186/723] Writing tensor blk.20.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  33\n",
      "[187/723] Writing tensor blk.20.attn_output.weight              | size   8192 x   8192  | type F16  | T+  33\n",
      "[188/723] Writing tensor blk.20.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  33\n",
      "[189/723] Writing tensor blk.20.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  34\n",
      "[190/723] Writing tensor blk.20.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  34\n",
      "[191/723] Writing tensor blk.20.attn_norm.weight                | size   8192           | type F32  | T+  34\n",
      "[192/723] Writing tensor blk.20.ffn_norm.weight                 | size   8192           | type F32  | T+  34\n",
      "[193/723] Writing tensor blk.21.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[194/723] Writing tensor blk.21.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[195/723] Writing tensor blk.21.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  34\n",
      "[196/723] Writing tensor blk.21.attn_output.weight              | size   8192 x   8192  | type F16  | T+  35\n",
      "[197/723] Writing tensor blk.21.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  36\n",
      "[198/723] Writing tensor blk.21.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  36\n",
      "[199/723] Writing tensor blk.21.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  36\n",
      "[200/723] Writing tensor blk.21.attn_norm.weight                | size   8192           | type F32  | T+  36\n",
      "[201/723] Writing tensor blk.21.ffn_norm.weight                 | size   8192           | type F32  | T+  36\n",
      "[202/723] Writing tensor blk.22.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  36\n",
      "[203/723] Writing tensor blk.22.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[204/723] Writing tensor blk.22.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  37\n",
      "[205/723] Writing tensor blk.22.attn_output.weight              | size   8192 x   8192  | type F16  | T+  37\n",
      "[206/723] Writing tensor blk.22.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  37\n",
      "[207/723] Writing tensor blk.22.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  37\n",
      "[208/723] Writing tensor blk.22.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  38\n",
      "[209/723] Writing tensor blk.22.attn_norm.weight                | size   8192           | type F32  | T+  38\n",
      "[210/723] Writing tensor blk.22.ffn_norm.weight                 | size   8192           | type F32  | T+  38\n",
      "[211/723] Writing tensor blk.23.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  38\n",
      "[212/723] Writing tensor blk.23.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  38\n",
      "[213/723] Writing tensor blk.23.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  38\n",
      "[214/723] Writing tensor blk.23.attn_output.weight              | size   8192 x   8192  | type F16  | T+  38\n",
      "[215/723] Writing tensor blk.23.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  38\n",
      "[216/723] Writing tensor blk.23.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  39\n",
      "[217/723] Writing tensor blk.23.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  39\n",
      "[218/723] Writing tensor blk.23.attn_norm.weight                | size   8192           | type F32  | T+  39\n",
      "[219/723] Writing tensor blk.23.ffn_norm.weight                 | size   8192           | type F32  | T+  39\n",
      "[220/723] Writing tensor blk.24.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[221/723] Writing tensor blk.24.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[222/723] Writing tensor blk.24.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  39\n",
      "[223/723] Writing tensor blk.24.attn_output.weight              | size   8192 x   8192  | type F16  | T+  39\n",
      "[224/723] Writing tensor blk.24.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  40\n",
      "[225/723] Writing tensor blk.24.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  40\n",
      "[226/723] Writing tensor blk.24.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  40\n",
      "[227/723] Writing tensor blk.24.attn_norm.weight                | size   8192           | type F32  | T+  41\n",
      "[228/723] Writing tensor blk.24.ffn_norm.weight                 | size   8192           | type F32  | T+  41\n",
      "[229/723] Writing tensor blk.25.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  41\n",
      "[230/723] Writing tensor blk.25.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  41\n",
      "[231/723] Writing tensor blk.25.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  41\n",
      "[232/723] Writing tensor blk.25.attn_output.weight              | size   8192 x   8192  | type F16  | T+  41\n",
      "[233/723] Writing tensor blk.25.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  42\n",
      "[234/723] Writing tensor blk.25.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  43\n",
      "[235/723] Writing tensor blk.25.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  43\n",
      "[236/723] Writing tensor blk.25.attn_norm.weight                | size   8192           | type F32  | T+  43\n",
      "[237/723] Writing tensor blk.25.ffn_norm.weight                 | size   8192           | type F32  | T+  43\n",
      "[238/723] Writing tensor blk.26.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[239/723] Writing tensor blk.26.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[240/723] Writing tensor blk.26.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  43\n",
      "[241/723] Writing tensor blk.26.attn_output.weight              | size   8192 x   8192  | type F16  | T+  43\n",
      "[242/723] Writing tensor blk.26.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  44\n",
      "[243/723] Writing tensor blk.26.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  44\n",
      "[244/723] Writing tensor blk.26.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  44\n",
      "[245/723] Writing tensor blk.26.attn_norm.weight                | size   8192           | type F32  | T+  44\n",
      "[246/723] Writing tensor blk.26.ffn_norm.weight                 | size   8192           | type F32  | T+  44\n",
      "[247/723] Writing tensor blk.27.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[248/723] Writing tensor blk.27.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[249/723] Writing tensor blk.27.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  44\n",
      "[250/723] Writing tensor blk.27.attn_output.weight              | size   8192 x   8192  | type F16  | T+  45\n",
      "[251/723] Writing tensor blk.27.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  45\n",
      "[252/723] Writing tensor blk.27.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  45\n",
      "[253/723] Writing tensor blk.27.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  46\n",
      "[254/723] Writing tensor blk.27.attn_norm.weight                | size   8192           | type F32  | T+  46\n",
      "[255/723] Writing tensor blk.27.ffn_norm.weight                 | size   8192           | type F32  | T+  46\n",
      "[256/723] Writing tensor blk.28.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[257/723] Writing tensor blk.28.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[258/723] Writing tensor blk.28.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  46\n",
      "[259/723] Writing tensor blk.28.attn_output.weight              | size   8192 x   8192  | type F16  | T+  46\n",
      "[260/723] Writing tensor blk.28.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  46\n",
      "[261/723] Writing tensor blk.28.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  47\n",
      "[262/723] Writing tensor blk.28.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  47\n",
      "[263/723] Writing tensor blk.28.attn_norm.weight                | size   8192           | type F32  | T+  48\n",
      "[264/723] Writing tensor blk.28.ffn_norm.weight                 | size   8192           | type F32  | T+  48\n",
      "[265/723] Writing tensor blk.29.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[266/723] Writing tensor blk.29.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[267/723] Writing tensor blk.29.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  48\n",
      "[268/723] Writing tensor blk.29.attn_output.weight              | size   8192 x   8192  | type F16  | T+  48\n",
      "[269/723] Writing tensor blk.29.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  49\n",
      "[270/723] Writing tensor blk.29.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  49\n",
      "[271/723] Writing tensor blk.29.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  49\n",
      "[272/723] Writing tensor blk.29.attn_norm.weight                | size   8192           | type F32  | T+  49\n",
      "[273/723] Writing tensor blk.29.ffn_norm.weight                 | size   8192           | type F32  | T+  49\n",
      "[274/723] Writing tensor blk.30.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[275/723] Writing tensor blk.30.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  49\n",
      "[276/723] Writing tensor blk.30.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  50\n",
      "[277/723] Writing tensor blk.30.attn_output.weight              | size   8192 x   8192  | type F16  | T+  50\n",
      "[278/723] Writing tensor blk.30.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  50\n",
      "[279/723] Writing tensor blk.30.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  50\n",
      "[280/723] Writing tensor blk.30.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  51\n",
      "[281/723] Writing tensor blk.30.attn_norm.weight                | size   8192           | type F32  | T+  51\n",
      "[282/723] Writing tensor blk.30.ffn_norm.weight                 | size   8192           | type F32  | T+  51\n",
      "[283/723] Writing tensor blk.31.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[284/723] Writing tensor blk.31.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[285/723] Writing tensor blk.31.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  51\n",
      "[286/723] Writing tensor blk.31.attn_output.weight              | size   8192 x   8192  | type F16  | T+  51\n",
      "[287/723] Writing tensor blk.31.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  51\n",
      "[288/723] Writing tensor blk.31.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  52\n",
      "[289/723] Writing tensor blk.31.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  52\n",
      "[290/723] Writing tensor blk.31.attn_norm.weight                | size   8192           | type F32  | T+  52\n",
      "[291/723] Writing tensor blk.31.ffn_norm.weight                 | size   8192           | type F32  | T+  52\n",
      "[292/723] Writing tensor blk.32.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[293/723] Writing tensor blk.32.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[294/723] Writing tensor blk.32.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  52\n",
      "[295/723] Writing tensor blk.32.attn_output.weight              | size   8192 x   8192  | type F16  | T+  52\n",
      "[296/723] Writing tensor blk.32.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  53\n",
      "[297/723] Writing tensor blk.32.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  53\n",
      "[298/723] Writing tensor blk.32.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  54\n",
      "[299/723] Writing tensor blk.32.attn_norm.weight                | size   8192           | type F32  | T+  54\n",
      "[300/723] Writing tensor blk.32.ffn_norm.weight                 | size   8192           | type F32  | T+  54\n",
      "[301/723] Writing tensor blk.33.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  54\n",
      "[302/723] Writing tensor blk.33.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  54\n",
      "[303/723] Writing tensor blk.33.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  54\n",
      "[304/723] Writing tensor blk.33.attn_output.weight              | size   8192 x   8192  | type F16  | T+  54\n",
      "[305/723] Writing tensor blk.33.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  55\n",
      "[306/723] Writing tensor blk.33.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  55\n",
      "[307/723] Writing tensor blk.33.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  56\n",
      "[308/723] Writing tensor blk.33.attn_norm.weight                | size   8192           | type F32  | T+  56\n",
      "[309/723] Writing tensor blk.33.ffn_norm.weight                 | size   8192           | type F32  | T+  56\n",
      "[310/723] Writing tensor blk.34.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  56\n",
      "[311/723] Writing tensor blk.34.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  56\n",
      "[312/723] Writing tensor blk.34.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  56\n",
      "[313/723] Writing tensor blk.34.attn_output.weight              | size   8192 x   8192  | type F16  | T+  56\n",
      "[314/723] Writing tensor blk.34.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  56\n",
      "[315/723] Writing tensor blk.34.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  57\n",
      "[316/723] Writing tensor blk.34.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  57\n",
      "[317/723] Writing tensor blk.34.attn_norm.weight                | size   8192           | type F32  | T+  57\n",
      "[318/723] Writing tensor blk.34.ffn_norm.weight                 | size   8192           | type F32  | T+  57\n",
      "[319/723] Writing tensor blk.35.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  57\n",
      "[320/723] Writing tensor blk.35.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  57\n",
      "[321/723] Writing tensor blk.35.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  57\n",
      "[322/723] Writing tensor blk.35.attn_output.weight              | size   8192 x   8192  | type F16  | T+  57\n",
      "[323/723] Writing tensor blk.35.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  58\n",
      "[324/723] Writing tensor blk.35.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  58\n",
      "[325/723] Writing tensor blk.35.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  58\n",
      "[326/723] Writing tensor blk.35.attn_norm.weight                | size   8192           | type F32  | T+  58\n",
      "[327/723] Writing tensor blk.35.ffn_norm.weight                 | size   8192           | type F32  | T+  58\n",
      "[328/723] Writing tensor blk.36.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[329/723] Writing tensor blk.36.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[330/723] Writing tensor blk.36.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  58\n",
      "[331/723] Writing tensor blk.36.attn_output.weight              | size   8192 x   8192  | type F16  | T+  58\n",
      "[332/723] Writing tensor blk.36.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  59\n",
      "[333/723] Writing tensor blk.36.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  59\n",
      "[334/723] Writing tensor blk.36.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  60\n",
      "[335/723] Writing tensor blk.36.attn_norm.weight                | size   8192           | type F32  | T+  60\n",
      "[336/723] Writing tensor blk.36.ffn_norm.weight                 | size   8192           | type F32  | T+  60\n",
      "[337/723] Writing tensor blk.37.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[338/723] Writing tensor blk.37.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[339/723] Writing tensor blk.37.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  60\n",
      "[340/723] Writing tensor blk.37.attn_output.weight              | size   8192 x   8192  | type F16  | T+  60\n",
      "[341/723] Writing tensor blk.37.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  62\n",
      "[342/723] Writing tensor blk.37.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  62\n",
      "[343/723] Writing tensor blk.37.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  62\n",
      "[344/723] Writing tensor blk.37.attn_norm.weight                | size   8192           | type F32  | T+  62\n",
      "[345/723] Writing tensor blk.37.ffn_norm.weight                 | size   8192           | type F32  | T+  62\n",
      "[346/723] Writing tensor blk.38.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[347/723] Writing tensor blk.38.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[348/723] Writing tensor blk.38.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  62\n",
      "[349/723] Writing tensor blk.38.attn_output.weight              | size   8192 x   8192  | type F16  | T+  62\n",
      "[350/723] Writing tensor blk.38.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  63\n",
      "[351/723] Writing tensor blk.38.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  63\n",
      "[352/723] Writing tensor blk.38.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  63\n",
      "[353/723] Writing tensor blk.38.attn_norm.weight                | size   8192           | type F32  | T+  63\n",
      "[354/723] Writing tensor blk.38.ffn_norm.weight                 | size   8192           | type F32  | T+  63\n",
      "[355/723] Writing tensor blk.39.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  63\n",
      "[356/723] Writing tensor blk.39.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  63\n",
      "[357/723] Writing tensor blk.39.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  63\n",
      "[358/723] Writing tensor blk.39.attn_output.weight              | size   8192 x   8192  | type F16  | T+  63\n",
      "[359/723] Writing tensor blk.39.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  64\n",
      "[360/723] Writing tensor blk.39.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  64\n",
      "[361/723] Writing tensor blk.39.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  64\n",
      "[362/723] Writing tensor blk.39.attn_norm.weight                | size   8192           | type F32  | T+  65\n",
      "[363/723] Writing tensor blk.39.ffn_norm.weight                 | size   8192           | type F32  | T+  65\n",
      "[364/723] Writing tensor blk.40.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[365/723] Writing tensor blk.40.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[366/723] Writing tensor blk.40.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  65\n",
      "[367/723] Writing tensor blk.40.attn_output.weight              | size   8192 x   8192  | type F16  | T+  65\n",
      "[368/723] Writing tensor blk.40.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  65\n",
      "[369/723] Writing tensor blk.40.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  66\n",
      "[370/723] Writing tensor blk.40.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  66\n",
      "[371/723] Writing tensor blk.40.attn_norm.weight                | size   8192           | type F32  | T+  66\n",
      "[372/723] Writing tensor blk.40.ffn_norm.weight                 | size   8192           | type F32  | T+  66\n",
      "[373/723] Writing tensor blk.41.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[374/723] Writing tensor blk.41.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[375/723] Writing tensor blk.41.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  66\n",
      "[376/723] Writing tensor blk.41.attn_output.weight              | size   8192 x   8192  | type F16  | T+  66\n",
      "[377/723] Writing tensor blk.41.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  68\n",
      "[378/723] Writing tensor blk.41.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  68\n",
      "[379/723] Writing tensor blk.41.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  68\n",
      "[380/723] Writing tensor blk.41.attn_norm.weight                | size   8192           | type F32  | T+  68\n",
      "[381/723] Writing tensor blk.41.ffn_norm.weight                 | size   8192           | type F32  | T+  68\n",
      "[382/723] Writing tensor blk.42.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[383/723] Writing tensor blk.42.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[384/723] Writing tensor blk.42.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  68\n",
      "[385/723] Writing tensor blk.42.attn_output.weight              | size   8192 x   8192  | type F16  | T+  68\n",
      "[386/723] Writing tensor blk.42.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  69\n",
      "[387/723] Writing tensor blk.42.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  69\n",
      "[388/723] Writing tensor blk.42.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  69\n",
      "[389/723] Writing tensor blk.42.attn_norm.weight                | size   8192           | type F32  | T+  70\n",
      "[390/723] Writing tensor blk.42.ffn_norm.weight                 | size   8192           | type F32  | T+  70\n",
      "[391/723] Writing tensor blk.43.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[392/723] Writing tensor blk.43.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[393/723] Writing tensor blk.43.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  70\n",
      "[394/723] Writing tensor blk.43.attn_output.weight              | size   8192 x   8192  | type F16  | T+  70\n",
      "[395/723] Writing tensor blk.43.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  70\n",
      "[396/723] Writing tensor blk.43.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  70\n",
      "[397/723] Writing tensor blk.43.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  71\n",
      "[398/723] Writing tensor blk.43.attn_norm.weight                | size   8192           | type F32  | T+  71\n",
      "[399/723] Writing tensor blk.43.ffn_norm.weight                 | size   8192           | type F32  | T+  71\n",
      "[400/723] Writing tensor blk.44.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[401/723] Writing tensor blk.44.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[402/723] Writing tensor blk.44.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  71\n",
      "[403/723] Writing tensor blk.44.attn_output.weight              | size   8192 x   8192  | type F16  | T+  71\n",
      "[404/723] Writing tensor blk.44.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  71\n",
      "[405/723] Writing tensor blk.44.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  72\n",
      "[406/723] Writing tensor blk.44.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  73\n",
      "[407/723] Writing tensor blk.44.attn_norm.weight                | size   8192           | type F32  | T+  73\n",
      "[408/723] Writing tensor blk.44.ffn_norm.weight                 | size   8192           | type F32  | T+  73\n",
      "[409/723] Writing tensor blk.45.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  73\n",
      "[410/723] Writing tensor blk.45.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  73\n",
      "[411/723] Writing tensor blk.45.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  73\n",
      "[412/723] Writing tensor blk.45.attn_output.weight              | size   8192 x   8192  | type F16  | T+  73\n",
      "[413/723] Writing tensor blk.45.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  74\n",
      "[414/723] Writing tensor blk.45.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  74\n",
      "[415/723] Writing tensor blk.45.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  75\n",
      "[416/723] Writing tensor blk.45.attn_norm.weight                | size   8192           | type F32  | T+  75\n",
      "[417/723] Writing tensor blk.45.ffn_norm.weight                 | size   8192           | type F32  | T+  75\n",
      "[418/723] Writing tensor blk.46.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[419/723] Writing tensor blk.46.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[420/723] Writing tensor blk.46.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  75\n",
      "[421/723] Writing tensor blk.46.attn_output.weight              | size   8192 x   8192  | type F16  | T+  75\n",
      "[422/723] Writing tensor blk.46.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  75\n",
      "[423/723] Writing tensor blk.46.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  76\n",
      "[424/723] Writing tensor blk.46.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  76\n",
      "[425/723] Writing tensor blk.46.attn_norm.weight                | size   8192           | type F32  | T+  76\n",
      "[426/723] Writing tensor blk.46.ffn_norm.weight                 | size   8192           | type F32  | T+  76\n",
      "[427/723] Writing tensor blk.47.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  76\n",
      "[428/723] Writing tensor blk.47.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  76\n",
      "[429/723] Writing tensor blk.47.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  76\n",
      "[430/723] Writing tensor blk.47.attn_output.weight              | size   8192 x   8192  | type F16  | T+  76\n",
      "[431/723] Writing tensor blk.47.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  77\n",
      "[432/723] Writing tensor blk.47.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  77\n",
      "[433/723] Writing tensor blk.47.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  77\n",
      "[434/723] Writing tensor blk.47.attn_norm.weight                | size   8192           | type F32  | T+  77\n",
      "[435/723] Writing tensor blk.47.ffn_norm.weight                 | size   8192           | type F32  | T+  77\n",
      "[436/723] Writing tensor blk.48.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[437/723] Writing tensor blk.48.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[438/723] Writing tensor blk.48.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  77\n",
      "[439/723] Writing tensor blk.48.attn_output.weight              | size   8192 x   8192  | type F16  | T+  77\n",
      "[440/723] Writing tensor blk.48.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  78\n",
      "[441/723] Writing tensor blk.48.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  78\n",
      "[442/723] Writing tensor blk.48.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  79\n",
      "[443/723] Writing tensor blk.48.attn_norm.weight                | size   8192           | type F32  | T+  79\n",
      "[444/723] Writing tensor blk.48.ffn_norm.weight                 | size   8192           | type F32  | T+  79\n",
      "[445/723] Writing tensor blk.49.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  79\n",
      "[446/723] Writing tensor blk.49.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  79\n",
      "[447/723] Writing tensor blk.49.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  79\n",
      "[448/723] Writing tensor blk.49.attn_output.weight              | size   8192 x   8192  | type F16  | T+  79\n",
      "[449/723] Writing tensor blk.49.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  80\n",
      "[450/723] Writing tensor blk.49.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  81\n",
      "[451/723] Writing tensor blk.49.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  81\n",
      "[452/723] Writing tensor blk.49.attn_norm.weight                | size   8192           | type F32  | T+  81\n",
      "[453/723] Writing tensor blk.49.ffn_norm.weight                 | size   8192           | type F32  | T+  81\n",
      "[454/723] Writing tensor blk.50.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  81\n",
      "[455/723] Writing tensor blk.50.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  81\n",
      "[456/723] Writing tensor blk.50.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  81\n",
      "[457/723] Writing tensor blk.50.attn_output.weight              | size   8192 x   8192  | type F16  | T+  81\n",
      "[458/723] Writing tensor blk.50.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  82\n",
      "[459/723] Writing tensor blk.50.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  82\n",
      "[460/723] Writing tensor blk.50.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  82\n",
      "[461/723] Writing tensor blk.50.attn_norm.weight                | size   8192           | type F32  | T+  82\n",
      "[462/723] Writing tensor blk.50.ffn_norm.weight                 | size   8192           | type F32  | T+  82\n",
      "[463/723] Writing tensor blk.51.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[464/723] Writing tensor blk.51.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[465/723] Writing tensor blk.51.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  82\n",
      "[466/723] Writing tensor blk.51.attn_output.weight              | size   8192 x   8192  | type F16  | T+  82\n",
      "[467/723] Writing tensor blk.51.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  83\n",
      "[468/723] Writing tensor blk.51.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  83\n",
      "[469/723] Writing tensor blk.51.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  83\n",
      "[470/723] Writing tensor blk.51.attn_norm.weight                | size   8192           | type F32  | T+  83\n",
      "[471/723] Writing tensor blk.51.ffn_norm.weight                 | size   8192           | type F32  | T+  83\n",
      "[472/723] Writing tensor blk.52.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  83\n",
      "[473/723] Writing tensor blk.52.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  84\n",
      "[474/723] Writing tensor blk.52.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  84\n",
      "[475/723] Writing tensor blk.52.attn_output.weight              | size   8192 x   8192  | type F16  | T+  84\n",
      "[476/723] Writing tensor blk.52.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  84\n",
      "[477/723] Writing tensor blk.52.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  85\n",
      "[478/723] Writing tensor blk.52.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  85\n",
      "[479/723] Writing tensor blk.52.attn_norm.weight                | size   8192           | type F32  | T+  85\n",
      "[480/723] Writing tensor blk.52.ffn_norm.weight                 | size   8192           | type F32  | T+  85\n",
      "[481/723] Writing tensor blk.53.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[482/723] Writing tensor blk.53.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[483/723] Writing tensor blk.53.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  85\n",
      "[484/723] Writing tensor blk.53.attn_output.weight              | size   8192 x   8192  | type F16  | T+  86\n",
      "[485/723] Writing tensor blk.53.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  87\n",
      "[486/723] Writing tensor blk.53.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  87\n",
      "[487/723] Writing tensor blk.53.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  87\n",
      "[488/723] Writing tensor blk.53.attn_norm.weight                | size   8192           | type F32  | T+  87\n",
      "[489/723] Writing tensor blk.53.ffn_norm.weight                 | size   8192           | type F32  | T+  87\n",
      "[490/723] Writing tensor blk.54.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[491/723] Writing tensor blk.54.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[492/723] Writing tensor blk.54.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  87\n",
      "[493/723] Writing tensor blk.54.attn_output.weight              | size   8192 x   8192  | type F16  | T+  87\n",
      "[494/723] Writing tensor blk.54.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  88\n",
      "[495/723] Writing tensor blk.54.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  88\n",
      "[496/723] Writing tensor blk.54.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  88\n",
      "[497/723] Writing tensor blk.54.attn_norm.weight                | size   8192           | type F32  | T+  88\n",
      "[498/723] Writing tensor blk.54.ffn_norm.weight                 | size   8192           | type F32  | T+  88\n",
      "[499/723] Writing tensor blk.55.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[500/723] Writing tensor blk.55.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[501/723] Writing tensor blk.55.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  88\n",
      "[502/723] Writing tensor blk.55.attn_output.weight              | size   8192 x   8192  | type F16  | T+  89\n",
      "[503/723] Writing tensor blk.55.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  89\n",
      "[504/723] Writing tensor blk.55.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  89\n",
      "[505/723] Writing tensor blk.55.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  89\n",
      "[506/723] Writing tensor blk.55.attn_norm.weight                | size   8192           | type F32  | T+  90\n",
      "[507/723] Writing tensor blk.55.ffn_norm.weight                 | size   8192           | type F32  | T+  90\n",
      "[508/723] Writing tensor blk.56.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  90\n",
      "[509/723] Writing tensor blk.56.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  90\n",
      "[510/723] Writing tensor blk.56.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  90\n",
      "[511/723] Writing tensor blk.56.attn_output.weight              | size   8192 x   8192  | type F16  | T+  90\n",
      "[512/723] Writing tensor blk.56.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  90\n",
      "[513/723] Writing tensor blk.56.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  91\n",
      "[514/723] Writing tensor blk.56.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  91\n",
      "[515/723] Writing tensor blk.56.attn_norm.weight                | size   8192           | type F32  | T+  92\n",
      "[516/723] Writing tensor blk.56.ffn_norm.weight                 | size   8192           | type F32  | T+  92\n",
      "[517/723] Writing tensor blk.57.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  92\n",
      "[518/723] Writing tensor blk.57.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  92\n",
      "[519/723] Writing tensor blk.57.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  92\n",
      "[520/723] Writing tensor blk.57.attn_output.weight              | size   8192 x   8192  | type F16  | T+  92\n",
      "[521/723] Writing tensor blk.57.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  93\n",
      "[522/723] Writing tensor blk.57.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  93\n",
      "[523/723] Writing tensor blk.57.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  93\n",
      "[524/723] Writing tensor blk.57.attn_norm.weight                | size   8192           | type F32  | T+  93\n",
      "[525/723] Writing tensor blk.57.ffn_norm.weight                 | size   8192           | type F32  | T+  93\n",
      "[526/723] Writing tensor blk.58.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  93\n",
      "[527/723] Writing tensor blk.58.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[528/723] Writing tensor blk.58.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  94\n",
      "[529/723] Writing tensor blk.58.attn_output.weight              | size   8192 x   8192  | type F16  | T+  94\n",
      "[530/723] Writing tensor blk.58.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  94\n",
      "[531/723] Writing tensor blk.58.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  94\n",
      "[532/723] Writing tensor blk.58.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  95\n",
      "[533/723] Writing tensor blk.58.attn_norm.weight                | size   8192           | type F32  | T+  95\n",
      "[534/723] Writing tensor blk.58.ffn_norm.weight                 | size   8192           | type F32  | T+  95\n",
      "[535/723] Writing tensor blk.59.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  95\n",
      "[536/723] Writing tensor blk.59.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  95\n",
      "[537/723] Writing tensor blk.59.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  95\n",
      "[538/723] Writing tensor blk.59.attn_output.weight              | size   8192 x   8192  | type F16  | T+  95\n",
      "[539/723] Writing tensor blk.59.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  95\n",
      "[540/723] Writing tensor blk.59.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  96\n",
      "[541/723] Writing tensor blk.59.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  96\n",
      "[542/723] Writing tensor blk.59.attn_norm.weight                | size   8192           | type F32  | T+  96\n",
      "[543/723] Writing tensor blk.59.ffn_norm.weight                 | size   8192           | type F32  | T+  96\n",
      "[544/723] Writing tensor blk.60.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[545/723] Writing tensor blk.60.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[546/723] Writing tensor blk.60.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  96\n",
      "[547/723] Writing tensor blk.60.attn_output.weight              | size   8192 x   8192  | type F16  | T+  96\n",
      "[548/723] Writing tensor blk.60.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  97\n",
      "[549/723] Writing tensor blk.60.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  97\n",
      "[550/723] Writing tensor blk.60.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+  97\n",
      "[551/723] Writing tensor blk.60.attn_norm.weight                | size   8192           | type F32  | T+  98\n",
      "[552/723] Writing tensor blk.60.ffn_norm.weight                 | size   8192           | type F32  | T+  98\n",
      "[553/723] Writing tensor blk.61.attn_q.weight                   | size   8192 x   8192  | type F16  | T+  98\n",
      "[554/723] Writing tensor blk.61.attn_k.weight                   | size   8192 x   8192  | type F16  | T+  98\n",
      "[555/723] Writing tensor blk.61.attn_v.weight                   | size   8192 x   8192  | type F16  | T+  98\n",
      "[556/723] Writing tensor blk.61.attn_output.weight              | size   8192 x   8192  | type F16  | T+  98\n",
      "[557/723] Writing tensor blk.61.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+  99\n",
      "[558/723] Writing tensor blk.61.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+  99\n",
      "[559/723] Writing tensor blk.61.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 100\n",
      "[560/723] Writing tensor blk.61.attn_norm.weight                | size   8192           | type F32  | T+ 100\n",
      "[561/723] Writing tensor blk.61.ffn_norm.weight                 | size   8192           | type F32  | T+ 100\n",
      "[562/723] Writing tensor blk.62.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[563/723] Writing tensor blk.62.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[564/723] Writing tensor blk.62.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 100\n",
      "[565/723] Writing tensor blk.62.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 100\n",
      "[566/723] Writing tensor blk.62.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 100\n",
      "[567/723] Writing tensor blk.62.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 101\n",
      "[568/723] Writing tensor blk.62.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 101\n",
      "[569/723] Writing tensor blk.62.attn_norm.weight                | size   8192           | type F32  | T+ 101\n",
      "[570/723] Writing tensor blk.62.ffn_norm.weight                 | size   8192           | type F32  | T+ 101\n",
      "[571/723] Writing tensor blk.63.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[572/723] Writing tensor blk.63.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[573/723] Writing tensor blk.63.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 101\n",
      "[574/723] Writing tensor blk.63.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 101\n",
      "[575/723] Writing tensor blk.63.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 102\n",
      "[576/723] Writing tensor blk.63.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 102\n",
      "[577/723] Writing tensor blk.63.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 102\n",
      "[578/723] Writing tensor blk.63.attn_norm.weight                | size   8192           | type F32  | T+ 102\n",
      "[579/723] Writing tensor blk.63.ffn_norm.weight                 | size   8192           | type F32  | T+ 102\n",
      "[580/723] Writing tensor blk.64.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[581/723] Writing tensor blk.64.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[582/723] Writing tensor blk.64.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 102\n",
      "[583/723] Writing tensor blk.64.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 102\n",
      "[584/723] Writing tensor blk.64.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 103\n",
      "[585/723] Writing tensor blk.64.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 104\n",
      "[586/723] Writing tensor blk.64.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 104\n",
      "[587/723] Writing tensor blk.64.attn_norm.weight                | size   8192           | type F32  | T+ 104\n",
      "[588/723] Writing tensor blk.64.ffn_norm.weight                 | size   8192           | type F32  | T+ 104\n",
      "[589/723] Writing tensor blk.65.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[590/723] Writing tensor blk.65.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[591/723] Writing tensor blk.65.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 104\n",
      "[592/723] Writing tensor blk.65.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 105\n",
      "[593/723] Writing tensor blk.65.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 106\n",
      "[594/723] Writing tensor blk.65.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 106\n",
      "[595/723] Writing tensor blk.65.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 106\n",
      "[596/723] Writing tensor blk.65.attn_norm.weight                | size   8192           | type F32  | T+ 106\n",
      "[597/723] Writing tensor blk.65.ffn_norm.weight                 | size   8192           | type F32  | T+ 106\n",
      "[598/723] Writing tensor blk.66.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[599/723] Writing tensor blk.66.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[600/723] Writing tensor blk.66.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 106\n",
      "[601/723] Writing tensor blk.66.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 106\n",
      "[602/723] Writing tensor blk.66.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 107\n",
      "[603/723] Writing tensor blk.66.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 107\n",
      "[604/723] Writing tensor blk.66.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 107\n",
      "[605/723] Writing tensor blk.66.attn_norm.weight                | size   8192           | type F32  | T+ 107\n",
      "[606/723] Writing tensor blk.66.ffn_norm.weight                 | size   8192           | type F32  | T+ 107\n",
      "[607/723] Writing tensor blk.67.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 107\n",
      "[608/723] Writing tensor blk.67.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 107\n",
      "[609/723] Writing tensor blk.67.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 108\n",
      "[610/723] Writing tensor blk.67.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 108\n",
      "[611/723] Writing tensor blk.67.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 108\n",
      "[612/723] Writing tensor blk.67.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 108\n",
      "[613/723] Writing tensor blk.67.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 108\n",
      "[614/723] Writing tensor blk.67.attn_norm.weight                | size   8192           | type F32  | T+ 109\n",
      "[615/723] Writing tensor blk.67.ffn_norm.weight                 | size   8192           | type F32  | T+ 109\n",
      "[616/723] Writing tensor blk.68.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[617/723] Writing tensor blk.68.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[618/723] Writing tensor blk.68.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 109\n",
      "[619/723] Writing tensor blk.68.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 109\n",
      "[620/723] Writing tensor blk.68.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 109\n",
      "[621/723] Writing tensor blk.68.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 110\n",
      "[622/723] Writing tensor blk.68.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 110\n",
      "[623/723] Writing tensor blk.68.attn_norm.weight                | size   8192           | type F32  | T+ 111\n",
      "[624/723] Writing tensor blk.68.ffn_norm.weight                 | size   8192           | type F32  | T+ 111\n",
      "[625/723] Writing tensor blk.69.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 111\n",
      "[626/723] Writing tensor blk.69.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 111\n",
      "[627/723] Writing tensor blk.69.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 111\n",
      "[628/723] Writing tensor blk.69.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 111\n",
      "[629/723] Writing tensor blk.69.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 112\n",
      "[630/723] Writing tensor blk.69.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 112\n",
      "[631/723] Writing tensor blk.69.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 112\n",
      "[632/723] Writing tensor blk.69.attn_norm.weight                | size   8192           | type F32  | T+ 112\n",
      "[633/723] Writing tensor blk.69.ffn_norm.weight                 | size   8192           | type F32  | T+ 112\n",
      "[634/723] Writing tensor blk.70.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[635/723] Writing tensor blk.70.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[636/723] Writing tensor blk.70.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 112\n",
      "[637/723] Writing tensor blk.70.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 113\n",
      "[638/723] Writing tensor blk.70.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 113\n",
      "[639/723] Writing tensor blk.70.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 113\n",
      "[640/723] Writing tensor blk.70.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 114\n",
      "[641/723] Writing tensor blk.70.attn_norm.weight                | size   8192           | type F32  | T+ 114\n",
      "[642/723] Writing tensor blk.70.ffn_norm.weight                 | size   8192           | type F32  | T+ 114\n",
      "[643/723] Writing tensor blk.71.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[644/723] Writing tensor blk.71.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[645/723] Writing tensor blk.71.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 114\n",
      "[646/723] Writing tensor blk.71.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 114\n",
      "[647/723] Writing tensor blk.71.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 114\n",
      "[648/723] Writing tensor blk.71.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 115\n",
      "[649/723] Writing tensor blk.71.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 115\n",
      "[650/723] Writing tensor blk.71.attn_norm.weight                | size   8192           | type F32  | T+ 115\n",
      "[651/723] Writing tensor blk.71.ffn_norm.weight                 | size   8192           | type F32  | T+ 115\n",
      "[652/723] Writing tensor blk.72.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[653/723] Writing tensor blk.72.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[654/723] Writing tensor blk.72.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 115\n",
      "[655/723] Writing tensor blk.72.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 115\n",
      "[656/723] Writing tensor blk.72.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 116\n",
      "[657/723] Writing tensor blk.72.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 116\n",
      "[658/723] Writing tensor blk.72.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 117\n",
      "[659/723] Writing tensor blk.72.attn_norm.weight                | size   8192           | type F32  | T+ 117\n",
      "[660/723] Writing tensor blk.72.ffn_norm.weight                 | size   8192           | type F32  | T+ 117\n",
      "[661/723] Writing tensor blk.73.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 117\n",
      "[662/723] Writing tensor blk.73.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 117\n",
      "[663/723] Writing tensor blk.73.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 117\n",
      "[664/723] Writing tensor blk.73.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 117\n",
      "[665/723] Writing tensor blk.73.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 118\n",
      "[666/723] Writing tensor blk.73.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 118\n",
      "[667/723] Writing tensor blk.73.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 119\n",
      "[668/723] Writing tensor blk.73.attn_norm.weight                | size   8192           | type F32  | T+ 119\n",
      "[669/723] Writing tensor blk.73.ffn_norm.weight                 | size   8192           | type F32  | T+ 119\n",
      "[670/723] Writing tensor blk.74.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[671/723] Writing tensor blk.74.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[672/723] Writing tensor blk.74.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 119\n",
      "[673/723] Writing tensor blk.74.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 119\n",
      "[674/723] Writing tensor blk.74.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 119\n",
      "[675/723] Writing tensor blk.74.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 120\n",
      "[676/723] Writing tensor blk.74.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 120\n",
      "[677/723] Writing tensor blk.74.attn_norm.weight                | size   8192           | type F32  | T+ 120\n",
      "[678/723] Writing tensor blk.74.ffn_norm.weight                 | size   8192           | type F32  | T+ 120\n",
      "[679/723] Writing tensor blk.75.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[680/723] Writing tensor blk.75.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[681/723] Writing tensor blk.75.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 120\n",
      "[682/723] Writing tensor blk.75.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 120\n",
      "[683/723] Writing tensor blk.75.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 121\n",
      "[684/723] Writing tensor blk.75.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 121\n",
      "[685/723] Writing tensor blk.75.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 121\n",
      "[686/723] Writing tensor blk.75.attn_norm.weight                | size   8192           | type F32  | T+ 121\n",
      "[687/723] Writing tensor blk.75.ffn_norm.weight                 | size   8192           | type F32  | T+ 121\n",
      "[688/723] Writing tensor blk.76.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 121\n",
      "[689/723] Writing tensor blk.76.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 121\n",
      "[690/723] Writing tensor blk.76.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 121\n",
      "[691/723] Writing tensor blk.76.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 121\n",
      "[692/723] Writing tensor blk.76.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 122\n",
      "[693/723] Writing tensor blk.76.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 122\n",
      "[694/723] Writing tensor blk.76.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 123\n",
      "[695/723] Writing tensor blk.76.attn_norm.weight                | size   8192           | type F32  | T+ 123\n",
      "[696/723] Writing tensor blk.76.ffn_norm.weight                 | size   8192           | type F32  | T+ 123\n",
      "[697/723] Writing tensor blk.77.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 123\n",
      "[698/723] Writing tensor blk.77.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 123\n",
      "[699/723] Writing tensor blk.77.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 123\n",
      "[700/723] Writing tensor blk.77.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 123\n",
      "[701/723] Writing tensor blk.77.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 125\n",
      "[702/723] Writing tensor blk.77.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 125\n",
      "[703/723] Writing tensor blk.77.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 125\n",
      "[704/723] Writing tensor blk.77.attn_norm.weight                | size   8192           | type F32  | T+ 125\n",
      "[705/723] Writing tensor blk.77.ffn_norm.weight                 | size   8192           | type F32  | T+ 125\n",
      "[706/723] Writing tensor blk.78.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[707/723] Writing tensor blk.78.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[708/723] Writing tensor blk.78.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 125\n",
      "[709/723] Writing tensor blk.78.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 125\n",
      "[710/723] Writing tensor blk.78.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 126\n",
      "[711/723] Writing tensor blk.78.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 126\n",
      "[712/723] Writing tensor blk.78.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 126\n",
      "[713/723] Writing tensor blk.78.attn_norm.weight                | size   8192           | type F32  | T+ 126\n",
      "[714/723] Writing tensor blk.78.ffn_norm.weight                 | size   8192           | type F32  | T+ 126\n",
      "[715/723] Writing tensor blk.79.attn_q.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[716/723] Writing tensor blk.79.attn_k.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[717/723] Writing tensor blk.79.attn_v.weight                   | size   8192 x   8192  | type F16  | T+ 126\n",
      "[718/723] Writing tensor blk.79.attn_output.weight              | size   8192 x   8192  | type F16  | T+ 127\n",
      "[719/723] Writing tensor blk.79.ffn_gate.weight                 | size  22016 x   8192  | type F16  | T+ 127\n",
      "[720/723] Writing tensor blk.79.ffn_down.weight                 | size   8192 x  22016  | type F16  | T+ 127\n",
      "[721/723] Writing tensor blk.79.ffn_up.weight                   | size  22016 x   8192  | type F16  | T+ 127\n",
      "[722/723] Writing tensor blk.79.attn_norm.weight                | size   8192           | type F32  | T+ 127\n",
      "[723/723] Writing tensor blk.79.ffn_norm.weight                 | size   8192           | type F32  | T+ 127\n",
      "Wrote models/65B/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/7B/ggml-model-f16.gguf' to './models/7B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 1714336 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
      "[   4/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   9/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  22/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  82/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 109/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 262/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 271/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 280/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 282/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 290/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 14578.84 ms\n",
      "main:    total time = 14578.84 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/13B/ggml-model-f16.gguf' to './models/13B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llama_model_quantize_internal: meta size = 1718656 bytes\n",
      "[   1/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   312.50 MiB ->    87.89 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   312.50 MiB ->   128.17 MiB | hist: \n",
      "[   4/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  12/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  13/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  21/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  22/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  30/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  31/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  39/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  40/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  48/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  49/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  57/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  58/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  66/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  67/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  75/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  76/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  84/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  85/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  93/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  94/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 102/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 109/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 111/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 118/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 120/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 129/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 138/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 147/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 156/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 165/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 174/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 183/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 192/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 201/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 210/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 219/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 228/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 237/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 246/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 255/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 264/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 273/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 282/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 291/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 300/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 309/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 318/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 327/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 334/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 336/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 343/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 345/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 352/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 354/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =    50.00 MiB ->    14.06 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 361/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, quantizing to q4_0 .. size =   135.00 MiB ->    37.97 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 362/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 363/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 34168.25 ms\n",
      "main:    total time = 34168.25 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/30B/ggml-model-f16.gguf' to './models/30B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llama_model_quantize_internal: meta size = 1729408 bytes\n",
      "[   1/ 543]                    token_embd.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   406.25 MiB ->   114.26 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                   output_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight - [ 6656, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   406.25 MiB ->   166.63 MiB | hist: \n",
      "[   4/ 543]                  blk.0.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]                  blk.0.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]                  blk.0.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]             blk.0.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]                blk.0.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[   9/ 543]                blk.0.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  10/ 543]                  blk.0.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  11/ 543]               blk.0.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  12/ 543]                blk.0.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  13/ 543]                  blk.1.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]                  blk.1.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]                  blk.1.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]             blk.1.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]                blk.1.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 543]                blk.1.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]                  blk.1.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]               blk.1.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  21/ 543]                blk.1.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  22/ 543]                  blk.2.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]                  blk.2.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]                  blk.2.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]             blk.2.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]                blk.2.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 543]                blk.2.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]                  blk.2.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]               blk.2.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  30/ 543]                blk.2.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  31/ 543]                  blk.3.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]                  blk.3.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]                  blk.3.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]             blk.3.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]                blk.3.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 543]                blk.3.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  37/ 543]                  blk.3.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 543]               blk.3.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  39/ 543]                blk.3.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  40/ 543]                  blk.4.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]                  blk.4.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]                  blk.4.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]             blk.4.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]                blk.4.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 543]                blk.4.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]                  blk.4.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]               blk.4.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  48/ 543]                blk.4.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  49/ 543]                  blk.5.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]                  blk.5.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]                  blk.5.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]             blk.5.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]                blk.5.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 543]                blk.5.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]                  blk.5.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]               blk.5.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  57/ 543]                blk.5.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  58/ 543]                  blk.6.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]                  blk.6.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]                  blk.6.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]             blk.6.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]                blk.6.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 543]                blk.6.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]                  blk.6.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]               blk.6.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  66/ 543]                blk.6.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  67/ 543]                  blk.7.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]                  blk.7.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]                  blk.7.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]             blk.7.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]                blk.7.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 543]                blk.7.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]                  blk.7.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]               blk.7.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  75/ 543]                blk.7.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  76/ 543]                  blk.8.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]                  blk.8.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]                  blk.8.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]             blk.8.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]                blk.8.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 543]                blk.8.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]                  blk.8.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]               blk.8.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  84/ 543]                blk.8.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  85/ 543]                  blk.9.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]                  blk.9.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]                  blk.9.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]             blk.9.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]                blk.9.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 543]                blk.9.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]                  blk.9.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]               blk.9.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  93/ 543]                blk.9.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[  94/ 543]                 blk.10.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]                 blk.10.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]                 blk.10.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]            blk.10.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]               blk.10.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 543]               blk.10.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]                 blk.10.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]              blk.10.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 102/ 543]               blk.10.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]                 blk.11.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]                 blk.11.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]                 blk.11.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]            blk.11.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]               blk.11.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 543]               blk.11.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]                 blk.11.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]              blk.11.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 111/ 543]               blk.11.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]                 blk.12.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]                 blk.12.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]                 blk.12.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]            blk.12.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]               blk.12.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 543]               blk.12.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]                 blk.12.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]              blk.12.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 120/ 543]               blk.12.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]                 blk.13.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]                 blk.13.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]                 blk.13.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]            blk.13.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]               blk.13.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 543]               blk.13.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]                 blk.13.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]              blk.13.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 129/ 543]               blk.13.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]                 blk.14.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]                 blk.14.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]                 blk.14.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]            blk.14.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]               blk.14.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 543]               blk.14.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 543]                 blk.14.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 543]              blk.14.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 138/ 543]               blk.14.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]                 blk.15.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]                 blk.15.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]                 blk.15.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]            blk.15.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]               blk.15.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 543]               blk.15.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]                 blk.15.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]              blk.15.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 147/ 543]               blk.15.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]                 blk.16.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]                 blk.16.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]                 blk.16.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]            blk.16.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]               blk.16.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 543]               blk.16.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]                 blk.16.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]              blk.16.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 156/ 543]               blk.16.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]                 blk.17.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]                 blk.17.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]                 blk.17.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]            blk.17.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]               blk.17.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 543]               blk.17.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]                 blk.17.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]              blk.17.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 165/ 543]               blk.17.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]                 blk.18.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]                 blk.18.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]                 blk.18.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]            blk.18.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]               blk.18.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 543]               blk.18.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]                 blk.18.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]              blk.18.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 174/ 543]               blk.18.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]                 blk.19.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]                 blk.19.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]                 blk.19.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]            blk.19.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]               blk.19.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 543]               blk.19.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 181/ 543]                 blk.19.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 543]              blk.19.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 183/ 543]               blk.19.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]                 blk.20.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]                 blk.20.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]                 blk.20.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]            blk.20.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]               blk.20.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 543]               blk.20.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 190/ 543]                 blk.20.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 543]              blk.20.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 192/ 543]               blk.20.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]                 blk.21.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]                 blk.21.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]                 blk.21.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]            blk.21.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]               blk.21.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 543]               blk.21.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]                 blk.21.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]              blk.21.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 201/ 543]               blk.21.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]                 blk.22.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]                 blk.22.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]                 blk.22.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]            blk.22.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]               blk.22.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 543]               blk.22.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]                 blk.22.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]              blk.22.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 210/ 543]               blk.22.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]                 blk.23.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]                 blk.23.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]                 blk.23.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]            blk.23.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]               blk.23.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 543]               blk.23.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]                 blk.23.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]              blk.23.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 219/ 543]               blk.23.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]                 blk.24.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]                 blk.24.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]                 blk.24.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]            blk.24.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]               blk.24.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 543]               blk.24.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]                 blk.24.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]              blk.24.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 228/ 543]               blk.24.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]                 blk.25.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]                 blk.25.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]                 blk.25.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]            blk.25.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]               blk.25.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 543]               blk.25.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]                 blk.25.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]              blk.25.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 237/ 543]               blk.25.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]                 blk.26.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]                 blk.26.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]                 blk.26.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]            blk.26.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]               blk.26.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 543]               blk.26.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]                 blk.26.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]              blk.26.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 246/ 543]               blk.26.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]                 blk.27.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]                 blk.27.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]                 blk.27.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]            blk.27.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]               blk.27.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 543]               blk.27.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]                 blk.27.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]              blk.27.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 255/ 543]               blk.27.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]                 blk.28.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]                 blk.28.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]                 blk.28.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]            blk.28.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]               blk.28.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 543]               blk.28.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]                 blk.28.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]              blk.28.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 264/ 543]               blk.28.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]                 blk.29.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]                 blk.29.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]                 blk.29.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]            blk.29.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]               blk.29.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 543]               blk.29.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]                 blk.29.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]              blk.29.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 273/ 543]               blk.29.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]                 blk.30.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]                 blk.30.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]                 blk.30.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]            blk.30.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]               blk.30.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 543]               blk.30.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]                 blk.30.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]              blk.30.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 282/ 543]               blk.30.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]                 blk.31.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]                 blk.31.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]                 blk.31.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]            blk.31.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]               blk.31.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 543]               blk.31.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]                 blk.31.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]              blk.31.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 291/ 543]               blk.31.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]                 blk.32.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]                 blk.32.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]                 blk.32.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]            blk.32.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]               blk.32.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 543]               blk.32.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]                 blk.32.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]              blk.32.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 300/ 543]               blk.32.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]                 blk.33.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]                 blk.33.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]                 blk.33.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]            blk.33.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]               blk.33.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 543]               blk.33.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]                 blk.33.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]              blk.33.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 309/ 543]               blk.33.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]                 blk.34.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]                 blk.34.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]                 blk.34.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]            blk.34.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]               blk.34.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 543]               blk.34.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]                 blk.34.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]              blk.34.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 318/ 543]               blk.34.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]                 blk.35.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]                 blk.35.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]                 blk.35.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]            blk.35.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]               blk.35.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 543]               blk.35.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]                 blk.35.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]              blk.35.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 327/ 543]               blk.35.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]                 blk.36.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]                 blk.36.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]                 blk.36.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]            blk.36.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]               blk.36.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 543]               blk.36.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]                 blk.36.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]              blk.36.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 336/ 543]               blk.36.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]                 blk.37.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]                 blk.37.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]                 blk.37.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]            blk.37.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]               blk.37.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 543]               blk.37.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]                 blk.37.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]              blk.37.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 345/ 543]               blk.37.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]                 blk.38.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]                 blk.38.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]                 blk.38.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]            blk.38.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]               blk.38.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 543]               blk.38.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]                 blk.38.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]              blk.38.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 354/ 543]               blk.38.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]                 blk.39.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]                 blk.39.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]                 blk.39.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]            blk.39.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]               blk.39.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 543]               blk.39.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]                 blk.39.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]              blk.39.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 363/ 543]               blk.39.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]                 blk.40.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]                 blk.40.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]                 blk.40.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]            blk.40.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]               blk.40.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 543]               blk.40.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]                 blk.40.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]              blk.40.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 372/ 543]               blk.40.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]                 blk.41.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]                 blk.41.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]                 blk.41.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]            blk.41.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]               blk.41.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 543]               blk.41.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]                 blk.41.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]              blk.41.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 381/ 543]               blk.41.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]                 blk.42.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]                 blk.42.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]                 blk.42.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]            blk.42.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]               blk.42.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 543]               blk.42.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]                 blk.42.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]              blk.42.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 390/ 543]               blk.42.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]                 blk.43.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]                 blk.43.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]                 blk.43.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]            blk.43.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]               blk.43.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 543]               blk.43.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]                 blk.43.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]              blk.43.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 399/ 543]               blk.43.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]                 blk.44.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]                 blk.44.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]                 blk.44.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]            blk.44.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]               blk.44.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 543]               blk.44.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]                 blk.44.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]              blk.44.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 408/ 543]               blk.44.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]                 blk.45.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]                 blk.45.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]                 blk.45.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]            blk.45.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]               blk.45.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 543]               blk.45.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]                 blk.45.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]              blk.45.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 417/ 543]               blk.45.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]                 blk.46.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]                 blk.46.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]                 blk.46.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]            blk.46.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]               blk.46.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 543]               blk.46.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]                 blk.46.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]              blk.46.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 426/ 543]               blk.46.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]                 blk.47.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]                 blk.47.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]                 blk.47.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]            blk.47.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]               blk.47.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 543]               blk.47.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]                 blk.47.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]              blk.47.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 435/ 543]               blk.47.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]                 blk.48.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]                 blk.48.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]                 blk.48.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]            blk.48.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]               blk.48.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 543]               blk.48.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]                 blk.48.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]              blk.48.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 444/ 543]               blk.48.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]                 blk.49.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]                 blk.49.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]                 blk.49.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]            blk.49.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]               blk.49.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 543]               blk.49.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]                 blk.49.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]              blk.49.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 453/ 543]               blk.49.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]                 blk.50.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]                 blk.50.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]                 blk.50.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]            blk.50.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]               blk.50.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 543]               blk.50.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]                 blk.50.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]              blk.50.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 462/ 543]               blk.50.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]                 blk.51.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]                 blk.51.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]                 blk.51.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]            blk.51.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]               blk.51.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 543]               blk.51.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]                 blk.51.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]              blk.51.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 471/ 543]               blk.51.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]                 blk.52.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]                 blk.52.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]                 blk.52.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]            blk.52.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]               blk.52.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 543]               blk.52.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]                 blk.52.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]              blk.52.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 480/ 543]               blk.52.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]                 blk.53.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]                 blk.53.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]                 blk.53.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]            blk.53.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]               blk.53.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 543]               blk.53.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]                 blk.53.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]              blk.53.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 489/ 543]               blk.53.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]                 blk.54.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]                 blk.54.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]                 blk.54.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]            blk.54.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]               blk.54.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 543]               blk.54.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]                 blk.54.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]              blk.54.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 498/ 543]               blk.54.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]                 blk.55.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]                 blk.55.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]                 blk.55.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]            blk.55.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]               blk.55.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 543]               blk.55.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]                 blk.55.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]              blk.55.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 507/ 543]               blk.55.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]                 blk.56.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]                 blk.56.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]                 blk.56.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]            blk.56.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]               blk.56.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 543]               blk.56.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 514/ 543]                 blk.56.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 543]              blk.56.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 516/ 543]               blk.56.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]                 blk.57.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]                 blk.57.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]                 blk.57.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]            blk.57.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]               blk.57.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 543]               blk.57.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 523/ 543]                 blk.57.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 543]              blk.57.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 525/ 543]               blk.57.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]                 blk.58.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]                 blk.58.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]                 blk.58.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]            blk.58.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]               blk.58.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 543]               blk.58.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 532/ 543]                 blk.58.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 543]              blk.58.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 534/ 543]               blk.58.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]                 blk.59.attn_q.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]                 blk.59.attn_k.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]                 blk.59.attn_v.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]            blk.59.attn_output.weight - [ 6656,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =    84.50 MiB ->    23.77 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]               blk.59.ffn_gate.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 540/ 543]               blk.59.ffn_down.weight - [17920,  6656,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 541/ 543]                 blk.59.ffn_up.weight - [ 6656, 17920,     1,     1], type =    f16, quantizing to q4_0 .. size =   227.50 MiB ->    63.98 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 542/ 543]              blk.59.attn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "[ 543/ 543]               blk.59.ffn_norm.weight - [ 6656,     1,     1,     1], type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 99177.49 ms\n",
      "main:    total time = 99177.49 ms\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './models/65B/ggml-model-f16.gguf' to './models/65B/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llama_model_quantize_internal: meta size = 1740160 bytes\n",
      "[   1/ 723]                    token_embd.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   500.00 MiB ->   140.62 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                   output_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight - [ 8192, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   500.00 MiB ->   205.08 MiB | hist: \n",
      "[   4/ 723]                  blk.0.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]                  blk.0.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]                  blk.0.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]             blk.0.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]                blk.0.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[   9/ 723]                blk.0.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 723]                  blk.0.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  11/ 723]               blk.0.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  12/ 723]                blk.0.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  13/ 723]                  blk.1.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]                  blk.1.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]                  blk.1.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]             blk.1.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]                blk.1.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  18/ 723]                blk.1.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]                  blk.1.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]               blk.1.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  21/ 723]                blk.1.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  22/ 723]                  blk.2.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]                  blk.2.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]                  blk.2.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]             blk.2.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]                blk.2.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 723]                blk.2.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]                  blk.2.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]               blk.2.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  30/ 723]                blk.2.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  31/ 723]                  blk.3.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]                  blk.3.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]                  blk.3.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]             blk.3.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]                blk.3.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 723]                blk.3.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]                  blk.3.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]               blk.3.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  39/ 723]                blk.3.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  40/ 723]                  blk.4.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]                  blk.4.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]                  blk.4.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]             blk.4.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]                blk.4.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 723]                blk.4.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]                  blk.4.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]               blk.4.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  48/ 723]                blk.4.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  49/ 723]                  blk.5.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]                  blk.5.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]                  blk.5.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]             blk.5.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]                blk.5.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 723]                blk.5.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]                  blk.5.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]               blk.5.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  57/ 723]                blk.5.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  58/ 723]                  blk.6.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]                  blk.6.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]                  blk.6.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]             blk.6.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]                blk.6.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 723]                blk.6.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]                  blk.6.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]               blk.6.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  66/ 723]                blk.6.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  67/ 723]                  blk.7.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]                  blk.7.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]                  blk.7.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]             blk.7.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]                blk.7.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 723]                blk.7.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]                  blk.7.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]               blk.7.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  75/ 723]                blk.7.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  76/ 723]                  blk.8.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]                  blk.8.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]                  blk.8.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]             blk.8.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]                blk.8.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 723]                blk.8.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]                  blk.8.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]               blk.8.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  84/ 723]                blk.8.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  85/ 723]                  blk.9.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]                  blk.9.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]                  blk.9.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]             blk.9.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]                blk.9.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 723]                blk.9.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 723]                  blk.9.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 723]               blk.9.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  93/ 723]                blk.9.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[  94/ 723]                 blk.10.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]                 blk.10.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]                 blk.10.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]            blk.10.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]               blk.10.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 723]               blk.10.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 723]                 blk.10.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 723]              blk.10.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 102/ 723]               blk.10.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]                 blk.11.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]                 blk.11.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]                 blk.11.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]            blk.11.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]               blk.11.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 723]               blk.11.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]                 blk.11.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]              blk.11.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 111/ 723]               blk.11.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]                 blk.12.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]                 blk.12.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]                 blk.12.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]            blk.12.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]               blk.12.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 117/ 723]               blk.12.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]                 blk.12.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]              blk.12.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 120/ 723]               blk.12.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]                 blk.13.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]                 blk.13.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]                 blk.13.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]            blk.13.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]               blk.13.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 723]               blk.13.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]                 blk.13.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]              blk.13.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 129/ 723]               blk.13.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]                 blk.14.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]                 blk.14.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]                 blk.14.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]            blk.14.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]               blk.14.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 723]               blk.14.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]                 blk.14.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]              blk.14.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 138/ 723]               blk.14.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]                 blk.15.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]                 blk.15.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]                 blk.15.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]            blk.15.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]               blk.15.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 723]               blk.15.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 723]                 blk.15.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 723]              blk.15.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 147/ 723]               blk.15.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]                 blk.16.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]                 blk.16.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]                 blk.16.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]            blk.16.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]               blk.16.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 723]               blk.16.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]                 blk.16.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]              blk.16.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 156/ 723]               blk.16.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]                 blk.17.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]                 blk.17.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]                 blk.17.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]            blk.17.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]               blk.17.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 162/ 723]               blk.17.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]                 blk.17.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]              blk.17.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 165/ 723]               blk.17.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]                 blk.18.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]                 blk.18.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]                 blk.18.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]            blk.18.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]               blk.18.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 723]               blk.18.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]                 blk.18.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]              blk.18.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 174/ 723]               blk.18.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]                 blk.19.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]                 blk.19.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]                 blk.19.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]            blk.19.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]               blk.19.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 180/ 723]               blk.19.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]                 blk.19.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]              blk.19.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 183/ 723]               blk.19.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]                 blk.20.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]                 blk.20.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]                 blk.20.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]            blk.20.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]               blk.20.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 723]               blk.20.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]                 blk.20.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]              blk.20.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 192/ 723]               blk.20.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]                 blk.21.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]                 blk.21.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]                 blk.21.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]            blk.21.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]               blk.21.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 198/ 723]               blk.21.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]                 blk.21.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]              blk.21.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 201/ 723]               blk.21.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]                 blk.22.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]                 blk.22.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]                 blk.22.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]            blk.22.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]               blk.22.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 723]               blk.22.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]                 blk.22.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]              blk.22.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 210/ 723]               blk.22.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]                 blk.23.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]                 blk.23.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]                 blk.23.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]            blk.23.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]               blk.23.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 723]               blk.23.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]                 blk.23.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]              blk.23.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 219/ 723]               blk.23.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]                 blk.24.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]                 blk.24.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]                 blk.24.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]            blk.24.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]               blk.24.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 225/ 723]               blk.24.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]                 blk.24.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]              blk.24.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 228/ 723]               blk.24.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]                 blk.25.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]                 blk.25.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]                 blk.25.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]            blk.25.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]               blk.25.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 234/ 723]               blk.25.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]                 blk.25.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]              blk.25.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 237/ 723]               blk.25.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]                 blk.26.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]                 blk.26.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]                 blk.26.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]            blk.26.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]               blk.26.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 243/ 723]               blk.26.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]                 blk.26.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]              blk.26.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 246/ 723]               blk.26.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]                 blk.27.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]                 blk.27.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]                 blk.27.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]            blk.27.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]               blk.27.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 252/ 723]               blk.27.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]                 blk.27.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]              blk.27.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 255/ 723]               blk.27.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]                 blk.28.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]                 blk.28.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]                 blk.28.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]            blk.28.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]               blk.28.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 261/ 723]               blk.28.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]                 blk.28.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]              blk.28.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 264/ 723]               blk.28.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]                 blk.29.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]                 blk.29.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]                 blk.29.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]            blk.29.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]               blk.29.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 270/ 723]               blk.29.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]                 blk.29.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]              blk.29.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 273/ 723]               blk.29.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]                 blk.30.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]                 blk.30.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]                 blk.30.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]            blk.30.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]               blk.30.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 279/ 723]               blk.30.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]                 blk.30.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]              blk.30.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 282/ 723]               blk.30.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]                 blk.31.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]                 blk.31.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]                 blk.31.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]            blk.31.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]               blk.31.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 288/ 723]               blk.31.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]                 blk.31.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]              blk.31.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 291/ 723]               blk.31.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]                 blk.32.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]                 blk.32.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]                 blk.32.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]            blk.32.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]               blk.32.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 297/ 723]               blk.32.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]                 blk.32.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]              blk.32.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 300/ 723]               blk.32.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]                 blk.33.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]                 blk.33.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]                 blk.33.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]            blk.33.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]               blk.33.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 306/ 723]               blk.33.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]                 blk.33.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]              blk.33.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 309/ 723]               blk.33.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]                 blk.34.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]                 blk.34.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]                 blk.34.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]            blk.34.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]               blk.34.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 315/ 723]               blk.34.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]                 blk.34.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]              blk.34.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 318/ 723]               blk.34.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]                 blk.35.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]                 blk.35.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]                 blk.35.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]            blk.35.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]               blk.35.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 324/ 723]               blk.35.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]                 blk.35.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]              blk.35.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 327/ 723]               blk.35.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]                 blk.36.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]                 blk.36.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]                 blk.36.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]            blk.36.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]               blk.36.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 333/ 723]               blk.36.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]                 blk.36.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]              blk.36.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 336/ 723]               blk.36.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]                 blk.37.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]                 blk.37.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]                 blk.37.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]            blk.37.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]               blk.37.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 342/ 723]               blk.37.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]                 blk.37.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]              blk.37.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 345/ 723]               blk.37.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]                 blk.38.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]                 blk.38.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]                 blk.38.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]            blk.38.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]               blk.38.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 351/ 723]               blk.38.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]                 blk.38.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]              blk.38.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 354/ 723]               blk.38.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]                 blk.39.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]                 blk.39.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]                 blk.39.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]            blk.39.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]               blk.39.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 360/ 723]               blk.39.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]                 blk.39.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]              blk.39.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 363/ 723]               blk.39.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]                 blk.40.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]                 blk.40.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]                 blk.40.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]            blk.40.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]               blk.40.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 369/ 723]               blk.40.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]                 blk.40.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]              blk.40.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 372/ 723]               blk.40.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]                 blk.41.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]                 blk.41.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]                 blk.41.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]            blk.41.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]               blk.41.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 378/ 723]               blk.41.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]                 blk.41.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]              blk.41.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 381/ 723]               blk.41.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]                 blk.42.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]                 blk.42.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]                 blk.42.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]            blk.42.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]               blk.42.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 387/ 723]               blk.42.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]                 blk.42.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]              blk.42.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 390/ 723]               blk.42.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]                 blk.43.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]                 blk.43.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]                 blk.43.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]            blk.43.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]               blk.43.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 396/ 723]               blk.43.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]                 blk.43.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]              blk.43.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 399/ 723]               blk.43.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]                 blk.44.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]                 blk.44.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]                 blk.44.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]            blk.44.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]               blk.44.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 405/ 723]               blk.44.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]                 blk.44.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]              blk.44.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 408/ 723]               blk.44.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]                 blk.45.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]                 blk.45.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]                 blk.45.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]            blk.45.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]               blk.45.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 414/ 723]               blk.45.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]                 blk.45.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]              blk.45.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 417/ 723]               blk.45.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]                 blk.46.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]                 blk.46.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]                 blk.46.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]            blk.46.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]               blk.46.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 423/ 723]               blk.46.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]                 blk.46.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]              blk.46.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 426/ 723]               blk.46.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]                 blk.47.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]                 blk.47.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]                 blk.47.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]            blk.47.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]               blk.47.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 432/ 723]               blk.47.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]                 blk.47.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]              blk.47.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 435/ 723]               blk.47.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]                 blk.48.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]                 blk.48.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]                 blk.48.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]            blk.48.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]               blk.48.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 441/ 723]               blk.48.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]                 blk.48.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]              blk.48.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 444/ 723]               blk.48.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]                 blk.49.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]                 blk.49.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]                 blk.49.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]            blk.49.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]               blk.49.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 450/ 723]               blk.49.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]                 blk.49.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]              blk.49.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 453/ 723]               blk.49.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]                 blk.50.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]                 blk.50.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]                 blk.50.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]            blk.50.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]               blk.50.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 459/ 723]               blk.50.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]                 blk.50.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]              blk.50.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 462/ 723]               blk.50.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]                 blk.51.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]                 blk.51.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]                 blk.51.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]            blk.51.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]               blk.51.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 468/ 723]               blk.51.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]                 blk.51.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]              blk.51.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 471/ 723]               blk.51.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]                 blk.52.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]                 blk.52.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]                 blk.52.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]            blk.52.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]               blk.52.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 477/ 723]               blk.52.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]                 blk.52.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]              blk.52.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 480/ 723]               blk.52.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]                 blk.53.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]                 blk.53.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]                 blk.53.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]            blk.53.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]               blk.53.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 486/ 723]               blk.53.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]                 blk.53.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]              blk.53.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 489/ 723]               blk.53.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]                 blk.54.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]                 blk.54.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]                 blk.54.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]            blk.54.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]               blk.54.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 495/ 723]               blk.54.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]                 blk.54.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]              blk.54.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 498/ 723]               blk.54.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]                 blk.55.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]                 blk.55.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]                 blk.55.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]            blk.55.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]               blk.55.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 504/ 723]               blk.55.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]                 blk.55.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]              blk.55.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 507/ 723]               blk.55.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]                 blk.56.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]                 blk.56.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]                 blk.56.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]            blk.56.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]               blk.56.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 513/ 723]               blk.56.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]                 blk.56.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]              blk.56.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 516/ 723]               blk.56.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]                 blk.57.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]                 blk.57.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]                 blk.57.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]            blk.57.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]               blk.57.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 522/ 723]               blk.57.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]                 blk.57.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]              blk.57.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 525/ 723]               blk.57.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]                 blk.58.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]                 blk.58.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]                 blk.58.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]            blk.58.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]               blk.58.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 531/ 723]               blk.58.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]                 blk.58.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]              blk.58.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 534/ 723]               blk.58.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]                 blk.59.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]                 blk.59.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]                 blk.59.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]            blk.59.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]               blk.59.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 540/ 723]               blk.59.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]                 blk.59.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]              blk.59.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 543/ 723]               blk.59.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]                 blk.60.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]                 blk.60.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]                 blk.60.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]            blk.60.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]               blk.60.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 549/ 723]               blk.60.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]                 blk.60.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]              blk.60.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 552/ 723]               blk.60.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]                 blk.61.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]                 blk.61.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]                 blk.61.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]            blk.61.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]               blk.61.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 558/ 723]               blk.61.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]                 blk.61.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]              blk.61.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 561/ 723]               blk.61.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]                 blk.62.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]                 blk.62.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]                 blk.62.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]            blk.62.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]               blk.62.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 567/ 723]               blk.62.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]                 blk.62.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]              blk.62.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 570/ 723]               blk.62.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]                 blk.63.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]                 blk.63.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]                 blk.63.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]            blk.63.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]               blk.63.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 576/ 723]               blk.63.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]                 blk.63.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]              blk.63.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 579/ 723]               blk.63.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]                 blk.64.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]                 blk.64.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]                 blk.64.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]            blk.64.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]               blk.64.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 585/ 723]               blk.64.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]                 blk.64.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]              blk.64.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 588/ 723]               blk.64.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]                 blk.65.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]                 blk.65.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]                 blk.65.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]            blk.65.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]               blk.65.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 594/ 723]               blk.65.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]                 blk.65.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]              blk.65.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 597/ 723]               blk.65.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]                 blk.66.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]                 blk.66.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]                 blk.66.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]            blk.66.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]               blk.66.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 603/ 723]               blk.66.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]                 blk.66.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]              blk.66.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 606/ 723]               blk.66.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]                 blk.67.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]                 blk.67.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]                 blk.67.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]            blk.67.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]               blk.67.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 612/ 723]               blk.67.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]                 blk.67.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]              blk.67.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 615/ 723]               blk.67.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]                 blk.68.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]                 blk.68.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]                 blk.68.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]            blk.68.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]               blk.68.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 621/ 723]               blk.68.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]                 blk.68.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]              blk.68.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 624/ 723]               blk.68.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]                 blk.69.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]                 blk.69.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]                 blk.69.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]            blk.69.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]               blk.69.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 630/ 723]               blk.69.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]                 blk.69.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]              blk.69.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 633/ 723]               blk.69.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]                 blk.70.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]                 blk.70.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]                 blk.70.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]            blk.70.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]               blk.70.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 639/ 723]               blk.70.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]                 blk.70.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]              blk.70.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 642/ 723]               blk.70.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]                 blk.71.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]                 blk.71.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]                 blk.71.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]            blk.71.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]               blk.71.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 648/ 723]               blk.71.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]                 blk.71.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]              blk.71.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 651/ 723]               blk.71.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]                 blk.72.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]                 blk.72.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]                 blk.72.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]            blk.72.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]               blk.72.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 657/ 723]               blk.72.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]                 blk.72.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]              blk.72.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 660/ 723]               blk.72.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]                 blk.73.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]                 blk.73.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]                 blk.73.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]            blk.73.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]               blk.73.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 666/ 723]               blk.73.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]                 blk.73.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]              blk.73.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 669/ 723]               blk.73.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]                 blk.74.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]                 blk.74.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]                 blk.74.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]            blk.74.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]               blk.74.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 675/ 723]               blk.74.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]                 blk.74.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]              blk.74.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 678/ 723]               blk.74.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]                 blk.75.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]                 blk.75.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]                 blk.75.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]            blk.75.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]               blk.75.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 684/ 723]               blk.75.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]                 blk.75.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]              blk.75.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 687/ 723]               blk.75.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]                 blk.76.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]                 blk.76.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]                 blk.76.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]            blk.76.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]               blk.76.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 693/ 723]               blk.76.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 694/ 723]                 blk.76.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 695/ 723]              blk.76.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 696/ 723]               blk.76.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]                 blk.77.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]                 blk.77.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]                 blk.77.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]            blk.77.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]               blk.77.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 702/ 723]               blk.77.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 703/ 723]                 blk.77.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 704/ 723]              blk.77.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 705/ 723]               blk.77.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]                 blk.78.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]                 blk.78.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]                 blk.78.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]            blk.78.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]               blk.78.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 711/ 723]               blk.78.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 712/ 723]                 blk.78.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 713/ 723]              blk.78.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 714/ 723]               blk.78.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]                 blk.79.attn_q.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]                 blk.79.attn_k.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]                 blk.79.attn_v.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]            blk.79.attn_output.weight - [ 8192,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   128.00 MiB ->    36.00 MiB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]               blk.79.ffn_gate.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 720/ 723]               blk.79.ffn_down.weight - [22016,  8192,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 721/ 723]                 blk.79.ffn_up.weight - [ 8192, 22016,     1,     1], type =    f16, quantizing to q4_0 .. size =   344.00 MiB ->    96.75 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 722/ 723]              blk.79.attn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "[ 723/ 723]               blk.79.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 214576.95 ms\n",
      "main:    total time = 214576.95 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.gguf ./models/13B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.gguf ./models/30B/ggml-model-q4_0.gguf q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.gguf ./models/65B/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS:  \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
      "removed 'build-info.o'\n",
      "removed 'common.o'\n",
      "removed 'console.o'\n",
      "removed 'ggml-alloc.o'\n",
      "removed 'ggml-backend.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml-quants.o'\n",
      "removed 'ggml.o'\n",
      "removed 'grammar-parser.o'\n",
      "removed 'llama.o'\n",
      "removed 'sampling.o'\n",
      "removed 'train.o'\n",
      "removed 'tests/test-c.o'\n",
      "removed 'benchmark-matmult'\n",
      "removed 'common/build-info.cpp'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'vdot'\n",
      "removed 'q8dot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'convert-llama2c-to-ggml'\n",
      "removed 'simple'\n",
      "removed 'batched'\n",
      "removed 'batched-bench'\n",
      "removed 'save-load-state'\n",
      "removed 'server'\n",
      "removed 'gguf'\n",
      "removed 'llama-bench'\n",
      "removed 'libllava.a'\n",
      "removed 'llava-cli'\n",
      "removed 'baby-llama'\n",
      "removed 'beam-search'\n",
      "removed 'speculative'\n",
      "removed 'infill'\n",
      "removed 'tokenize'\n",
      "removed 'parallel'\n",
      "removed 'finetune'\n",
      "removed 'export-lora'\n",
      "removed 'lookahead'\n",
      "removed 'lookup'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS: -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
      "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
      "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib   -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib  -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88ef08-68f5-4655-ac49-d0368c11b004",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33724f57-0378-4652-86b1-2b4858d56528",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db5a686-a0d6-4af2-9753-fd4e6b42a706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703318879\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 3577.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, in spite of frequent bulls from Rome, did pretty much the same thing: only that, when she had nothing else to do, she took a pleasure in expelling members from her House of Commons for libels on the Crown, and in quartering soldiers on suspected gentlemen in their own houses. \n",
      " But it is my business here to speak of neither England nor France. My business lies elsewhere: such as it is I shall try to do it. I was born in the year 1775; a generation now almost gone into its rest, but which, by reason of that indisputable claim to originality which comes from being the first batch of any article whatsoever, is commonly considered entitled to be heard on all public questions before the next generation (the “new broom,” as it was then called) shall have had time to get itself squeaky and worn at the wrong end. In our day men were governed by authority: but the Newgate Calendar was their only instructor in morals. The manners of gentlemen were described in a book which no gentleman would read, and which every other man could not read. \n",
      " My father’s circumstances being such as to render his son’s early introduction into the world unnecessary (for he left me nothing but himself), I was allowed to stay at home until I should be ready for it. This promise I faithfully kept: so that my abode there was rather longer than most young people’s, and a little out of the common way. \n",
      " When I came into life, the state of France was such as to render her position unattractive; so that not many young Frenchmen chose to stay at home, but preferred going abroad for the sake of amusement and information. The result was, that my country became overrun with foreigners. \n",
      " It is now full sixty years ago: there was a young fellow named Gargantua (though his real name was Ragotin) living in the neighbourhood of Paris. His father had been an apothecary: but he left off practising, and turned farmer on account of a violent passion for beer. \n",
      " It is well known that he took care of two children when he married; which was all his family consisted of. But before these arrived at the proper age to make their appearance in the world, a great many more than were intended came into it: and among these Gargantua and Pantagruel were born on the 15th day of August in the year 1532. It was in this neighbourhood that my ancestor first took notice of me; though he had some difficulty in finding out my name, which was given to me on account of a strong desire I felt of getting as much as possible to eat. For though it be commonly believed that my name is derived from the word Gargle (which signifies eating a great deal) and not from Gargantua (the old man with a beard who is the father of Pantagruel) this opinion has not been confirmed by any good authority, and may therefore be received as false. \n",
      " At the same time that I was born, another child also came into the world; but in order to save his mother’s life, he was not named till a month afterwards: for at the age of eight months he had so nearly finished her that she died. He was accordingly called Sganarelle. \n",
      " At this time our country began to be filled with Germans and Italians; for which reason there were very few young people of French blood. There was only one or two in the whole neighbourhood: but they had been educated at home by a certain eccentric old fellow, who called himself The Pantagruelist. This is the man from whom my father derived his name, and who taught him all that he knew of science and literature. \n",
      " I am not yet quite sure whether or not I am descended from an honest family; but there can be no doubt that both my parents were persons of quality: for my mother had a great-grandfather (who was her grandmother’s husband) who held the title of baron. So far as regards birth, however, I cannot claim to belong to any particular rank; for it is not yet known how long ago our family was created.\n",
      "The Pantagruelist was an extremely curious man: in addition to a thorough knowledge of all the branches of learning, he knew all the arts and trades practiced in his own day, as well as the customs and usages of every people under heaven; and having spent the greatest part of his life travelling over various countries, he had observed everything with the utmost exactness. He also made it his business to keep himself informed concerning all the marvellous stories which have ever been told by any nation: such as\n",
      "llama_print_timings:        load time =    7875.82 ms\n",
      "llama_print_timings:      sample time =     349.05 ms /  1024 runs   (    0.34 ms per token,  2933.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.42 ms /   494 tokens (    0.27 ms per token,  3730.53 tokens per second)\n",
      "llama_print_timings:        eval time =    8650.60 ms /  1023 runs   (    8.46 ms per token,   118.26 tokens per second)\n",
      "llama_print_timings:       total time =    9416.22 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9324415-e69d-4ed2-8b56-2094995fbe8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703318898\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 3577.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m An exchequer, founded on such a basis, was bound to come to grief in due time: which it did; being unable to meet its engagements to its creditors, among whom were the shareholders in the Mississippi scheme, who found themselves cheated with the rest. A National Debt ensued; and with it an income-tax. \n",
      " This document presents the most abstruse problems of mathematics. If you cannot cope with it, consult your maths tutor or the author of a suitable book. I have myself solved this equation to my satisfaction; but not sufficiently well for the purpose intended (viz. to give a solution which will work), and which requires that the coefficient 67425091 should be between -370 and +370, so that it may be possible to set up a cubic equation with roots all three of which are positive. I shall content myself with proving this document correct for its own purpose: viz. as far as it goes towards giving a solution which will work; leaving it to the reader (with my blessings) to find the rest. \n",
      " Above the words 'Tax' and 'Nominal Value' appear the letters C R B N; with which some of you may be acquainted already, being intended as an abbreviation for Christian Rothbury Blackburn North; though whether this is really his name I cannot say, having never known him except by his initials. \n",
      " The words in bold print are what Mr T's solution requires; and the letter x is required to take the place of any one or more of them. In these particular cases, the letters C R B N are not suitable, for they must be distinct: but they serve as an example of how the x might be replaced by 67425091 if necessary. \n",
      " It will be seen that, in Mr T's solution, it is impossible to insert more than one letter between the words 'Tax' and 'Nominal Value.' If you wish to make an extension of his solution, and set up a cubic equation with three roots all positive, this limitation must be observed. \n",
      " Mr T's solution shows that there are two possibilities, in which x = 1 or 67425091; the former being the correct answer for the purpose in view (i.e. for the use of people like himself who have to pay an income-tax). But it would be a great mistake to infer from this that either of these solutions is less likely than the other, since the x = 67425091 solution is much more general; as will appear when we come to consider it in detail. I can only point out at present that for this particular problem, where x = 1 has been found to be true, you have no guarantee that it would also prove true if the words were arranged differently and the letters in another position. \n",
      " By way of further illustration, the two solutions might be written thus: -\n",
      "(x) + (x) = 2987634710592065;\n",
      "(x) - (x) = 4540819387978062.\n",
      "Both of these solutions are correct, since the words may be arranged in various ways without affecting the result; and this is a feature that distinguishes this equation from any other which can be formed by using only letters (in this case a, b, c). \n",
      " We will now consider Mr T's solution in detail. \n",
      " His first step was to add together all the letters in bold print and obtain the total sum = 1346. This is equivalent to taking a sum of cubes and finding that it yields 2987634710592065. This shows that Mr T's solution has found a way out of the puzzle, in which it was assumed that each letter had an equal value. It will be recalled that all the letters have been given equal weight; but now we can see that they really weigh something more, and that it is only the number of them that is equal. \n",
      " To consider a little further this last point, suppose you were asked to add up 1346 apples or peaches; then it would be quite another matter to add up 1346 letters in which each letter is represented by a number corresponding with its place in the alphabet (these numbers being given in Table A); and finally there would have to be added up all these sums, which we shall call C, B, A respectively. Thus the total of C = 1218, B = 970, A = 368. But this is not quite what has happened\n",
      "llama_print_timings:        load time =    1131.92 ms\n",
      "llama_print_timings:      sample time =     353.78 ms /  1024 runs   (    0.35 ms per token,  2894.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.18 ms /   494 tokens (    0.27 ms per token,  3765.79 tokens per second)\n",
      "llama_print_timings:        eval time =    8675.77 ms /  1023 runs   (    8.48 ms per token,   117.91 tokens per second)\n",
      "llama_print_timings:       total time =    9443.88 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0cf40d6-f822-4ba8-a315-c3819dd2af9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703318910\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 3577.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was in a ferment, because she had made neither, nor spent any. The nation had become insensible to the merits of either; and Mr. Windham, a man of sense, publicly asserted that there were not more than five men in the country who could talk about nothing else. It being now necessary to find out whether or no those five would consent to take upon themselves the national debt, which had been accumulating during the last four years, at an average charge of one hundred and fifty millions per ann., they were set to work; and, in a short time, informed the nation that it could not be done. \n",
      " The King then took the matter into his own hands, as was his royal prerogative. He resolved that, since there seemed no possibility of getting through with less than half the sum asked for (which amounted to about eight hundred millions), he would pay the whole: whereupon there arose a national hue and cry after him: in which he fled to Weymouth. \n",
      " Mr. Burke now stepped forward, in his own person; and, by an original effort of eloquence, persuaded the Commons that their Majesties’ debts were to be considered as public benefits: that they were honourable, not private; national, not particular; patriotic, not sectional. They had nothing to do with individual or sectional interests: these only could make them feel and understand how great was the debt; but their duty lay in making it less by every means. \n",
      " This speech is said to have been received throughout the kingdom with transports of gratitude. The nation, by the blessings of Providence, had, in Mr. Burke’s words, a Minister who could speak out what it felt: who had so great a command over language as to say the things which were in their minds. \n",
      " The only difficulty was that, being a member for the county of Bristol, and not having received an appointment as Secretary of State; he could neither be present at the council board nor in the Cabinet; so that he was obliged to sit up half the night writing out his speeches upon a slate-frame (as he afterwards told me), which he sent over by the post. Whenever a debate was coming on, or a vote was about to be taken, he would appear at Mr. Pitt’s elbow: and then, in half an hour, the House of Commons might hear the great speaker, not as a Member for Bristol, but as Minister to all parts of Great Britain and Ireland. \n",
      " This speech was followed by several others; and the public mind began to be influenced in the right way. It was at first a little difficult to reconcile Mr. Burke with those whom he afterwards opposed so vehemently: not that he had ever been one of the Whig party, but from his principles being in some respects opposed to theirs; yet as both parties were opposed to Mr. Fox’s system, and the war was the great question which the whole kingdom considered, it is very probable that he found much more union of opinion in his own than in either of those two others: for, at that time, all who were against Mr. Fox’s policy seemed to be united. The Duke of Portland (who had been Mr. Fox’s chief supporter) became the minister; and Lord North was placed at the head of the government.\n",
      "Mr. Burke was not only a most acute and eloquent debater, but he was an excellent speaker. I do not mean that he had the least appearance of affectation; on the contrary, he spoke in a plain, unaffected manner: yet the force with which his arguments were delivered, made him extremely effective. In order to exhibit this as far as possible, we must be permitted to draw a brief comparison between Lord North and Mr. Burke.\n",
      "Lord North’s speeches are, I believe, not recorded; but in all he ever spoke or wrote upon political subjects, he appeared to speak from memoranda. He always had his notes ready before him; so that they seemed rather like a collection of lawyers’ arguments, than the flowing eloquence and impassioned declamation which was peculiar to Mr. Burke: for though Lord North delivered himself well in parliamentary discussion, yet it wanted that rapid succession of ideas, which struck the sensibility of the hearer by a combination of force and elegance; whereas Mr. Burke never lost sight of any important point in the subject under consideration, and was ever ready to make a powerful impression on his hearers.\n",
      "There were two speeches delivered at the opening of the session in 1782, which are particularly worthy of mention: one of them was that by Mr. Burke against Mr. Fox’s motion respecting American Affairs;\n",
      "llama_print_timings:        load time =    1026.95 ms\n",
      "llama_print_timings:      sample time =     356.20 ms /  1024 runs   (    0.35 ms per token,  2874.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.63 ms /   494 tokens (    0.26 ms per token,  3781.64 tokens per second)\n",
      "llama_print_timings:        eval time =    8671.25 ms /  1023 runs   (    8.48 ms per token,   117.98 tokens per second)\n",
      "llama_print_timings:       total time =    9442.30 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a124de-0e22-49fc-8ea3-d324d831a961",
   "metadata": {},
   "source": [
    "### 7B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518024e7-d24d-4701-a788-6445d04b3ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703318922\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  250.11 MiB\n",
      "llm_load_tensors: VRAM used           = 12603.02 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, after long labour and many struggles, was endeavouring to get over some of the same obstacles which had hindered her illustrious neighbour for so many years, though the road lay uphill for the poor Island as well as for the rich Republic. In fact, things were much as usual on both sides the water; but at least Nature seemed to have made up her mind, by a judicious shake of the earth, or an adroit adjustment of tides and currents, that the two nations should be kept apart; England having had some such shock as a thunder-storm is said to give to the system when it wants air: while France was in all respects like a pear, full grown but unripe, or at most a peach, hardly yet fully coloured. \n",
      " At the very time of these events—and no period could have been more striking, or might have been supposed better adapted to the display of supernatural illuminations, if such had been thought expedient by those concerned in them—it came to pass that a man who was neither priest nor monk, and whose name is not yet forgotten though he be dead and gone these eighteen years, passed through France from one extreme corner to another; carrying on his travels what were called letters of naturalisation, or rather naturalization itself, in the name of King George the Third. It would seem as if some occult powers had been set at work, which in this case had their full effect upon the whole kingdom. \n",
      " In passing from the first point to that where his travels ended, and leaving Paris behind him for good and all, he might have thought that he was doing a most rational thing: seeing that his family was of ancient French extraction, having long lived in Paris itself, while there were some who had even been called nobles; so that the last step taken by this wandering stranger, whom people stared at wherever he went, was very natural, and would have been done without a thought but for certain unlucky circumstances. \n",
      " Those circumstances were the wars which at the time were carrying desolation through France, as well as through many other countries; and though it is not the purpose of this narrative to describe them, there are some things which cannot be passed over. Amongst these was the sequestration of French property, which made it impossible for any man who had a hundred thousand livres, or even ten times that amount in money or bills upon his friends, to obtain an exit from France. \n",
      " It is not necessary to say more; and indeed the reader knows as much already, because he has heard some part of what has happened since: but if he knew all he would have understood better what followed; and so it was natural that this man of property, who had made a long journey on foot, should have been greatly annoyed, and that at times he should have spoken in a manner which did not become one who is going to seek a home amongst strangers. \n",
      " However, as he knew nothing of his own country but its name, and the little it contained of his private history, there was no need for him to talk much: he had only to show that he came from London; and though a man may be thought too poor to give anyone much trouble, still he is not disgraced by having come from an island so populous as England. \n",
      " In such cases the name of London is enough to bring back the smile which goes with the salutation, 'Good day!' and in a few minutes a good many people have been questioned on the subject: some say it is better for business; others that it is better for society; while those who are too old-fashioned to say either, but can only reply in their turn, say it must be the largest city in Christendom. \n",
      " In this way they talk and talk again of the city, until the newcomer hears nothing else: 'How large London is!' 'You cannot imagine how large! you must see it yourself.' This was all he ever heard said about the place; and at last one day he said, 'What is large?' \n",
      "'Why,' they replied, 'as large as London is large. It is such a distance from one end to the other that it would take two days to walk across it.' 'If there were two days for every step, and if each foot took the same time in making its journey, I think the distance might be very great indeed: but perhaps it was not so with you?' \n",
      "'Oh, no! it was not like that. Each of us had our own particular pace; but there are many steps between London and Edinburgh, which we could walk on two days.' 'But how long is that distance?' he said, in a tone which seemed to imply that they should tell him the truth if they would be believed. \n",
      "llama_print_timings:        load time =   12374.19 ms\n",
      "llama_print_timings:      sample time =     357.60 ms /  1024 runs   (    0.35 ms per token,  2863.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =      96.34 ms /   494 tokens (    0.20 ms per token,  5127.89 tokens per second)\n",
      "llama_print_timings:        eval time =   23134.66 ms /  1023 runs   (   22.61 ms per token,    44.22 tokens per second)\n",
      "llama_print_timings:       total time =   23872.57 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec4b98bb-a046-4be5-9f98-cc5f64724a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703318959\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  250.11 MiB\n",
      "llm_load_tensors: VRAM used           = 12603.02 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, after a brief but desperate struggle, in which its shopkeepers especially distinguished themselves by patriotic firmness, had taken to doing likewise; and, as if in atonement, went about chopping off heads right and left. In both countries, the people were democratic – not through choice, but through necessity: they had no choice. \n",
      " Nor was the King of France (Louis the Sixteenth) one whit less democratical than his subjects; for he was a king by accident, like Oliver Cromwell, and a Frenchman merely to avoid starvation. He had been elected with extreme difficulty, after a terrible civil war in which thirty millions of pounds sterling, or their equivalent in paper money, were expended: it being clearly understood that if he failed, somebody else must be substituted who should be able to pay; otherwise there would have been no substitute. \n",
      " For the people in France had come at last to know what an estate tax is; and a few of them even had ventured to express themselves upon the subject with some asperity. They were tired, too, of seeing the King’s son-in-law (Monsieur), whom they hated and feared, marching about the capital with a long white beard and wig; and calling himself – from a habit he had acquired during his captivity in Prussia – Citizen De Broglie. They were weary too of seeing that strange man in the scarlet robes whom they called Le Coadjuteur, parading about Paris at all hours of the day and night with a large white cross on his breast; crying out piteously for the conversion of heretics to his own religion – which is the religion of salvation through penance. \n",
      " In short, it seemed as if France were sick to death of monarchy: the Bourbons having given them nothing but misery and insolence, after long ages of oppression; and all the other monarchies in Europe having proved no better. \n",
      " This then was the situation when our story opens. The French people had got to be tired of their government as the Americans had done before them – except that it was a little worse with them: they were not so lucky as we were, or at least one-half of us: for they had no Declaration of Independence in which to ventilate their complaints; and consequently went about swearing fearfully, and hacking each other’s brains out – a proceeding which is generally attended with inconvenience. \n",
      " The monarchy itself was not better than the nation; for if they had been contented with robbing us of our property only – and confining themselves to such petty acts of oppression as were absolutely necessary in order to prevent the people from cutting each other’s throats every Sunday morning at church, we should not have cared. But a king is worse than a republican government; because a man cannot hit him over the head without hitting the whole nation. \n",
      " The result was that France was going through such revolutions as are likely to make one’s head ache if he thinks about them too much – unless he happens to be an obstinate Jacobin like myself; in which case they do not hurt him at all. \n",
      " I have said then, that our story opens when the monarchy and aristocracy were so far on their trial (it would be more correct to say on their murderers) as to have been obliged to put it out of their own hands by retiring from France altogether. In this situation we find our friend D’Artagnan, who had now become an old man: although he was still as well able to take care of himself and his friends, as in the time when he cut off a man’s head with one blow, or ran at full gallop into an enemy’s ranks. \n",
      " This worthy personage lived alone – I believe – for we do not hear that any of the other members of the former royal household were ever seen again in France: a fact which showed what estimation they must have held themselves in thereafter. He had, however, a nephew living; but he was in no way connected with the service – at least it seemed so to D’Artagnan, who thought it proper to inform his relations that he intended to pass for an old bachelor and a man of private fortune. \n",
      " This dissimulation succeeded admirably; for, although the young gentleman might be occasionally seen walking about Paris with the old man (D’Artagnan having only one horse), no one dreamed of imagining that the latter was any relation to the former – neither indeed would they have thought it probable. \n",
      " D’Artagnan and his nephew had no connection in fact; but, as I said above, he was obliged to adopt him for\n",
      "llama_print_timings:        load time =    3697.20 ms\n",
      "llama_print_timings:      sample time =     353.57 ms /  1024 runs   (    0.35 ms per token,  2896.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =      93.21 ms /   494 tokens (    0.19 ms per token,  5299.86 tokens per second)\n",
      "llama_print_timings:        eval time =   23133.59 ms /  1023 runs   (   22.61 ms per token,    44.22 tokens per second)\n",
      "llama_print_timings:       total time =   23865.42 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c07219-6f1a-4e38-9590-bb0fe7cea15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703318989\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  250.11 MiB\n",
      "llm_load_tensors: VRAM used           = 12603.02 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 256.00 MB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB\n",
      "llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was a stagnant pool; France was an impetuous torrent. England had conquered half the world: France began to dream of conquering the other half.\n",
      "In this year 1789, the shadow of war lay heavy on the nations. All eyes were turned towards America; for there, on that remote continent, the clashing of arms was heard already: a faint rumour of battle rolled from far West Florida, or it might be far West India Isles, as if in mockery of that more august field which now held Europe's eyes. On both sides, at every point where colonies existed, war was talked of and menaced; and it would seem to have been thought a matter of course by the strongest minds, that war must come to pass: the only difference of opinion being as to which side should be the aggressor.\n",
      "France had sent over her minister with instructions to make an effort for peace: but she could not persuade herself that a man so weak and wily in his measures could succeed. France was determined to try it, if needful; and England was no less determined to frustrate it. There is never wanting at such times some little circumstance or other, which renders the determination of one party or the other more urgent: as if Nature herself would aid in the strife by making both parties feel that their ruin depends upon success. Such a circumstance was now about to be offered; and it was not a circumstance at all to which either party could lay claim.\n",
      "The year 1789 had been one of great suffering, especially among the poorer classes in England: and now the harvest came on to add to their distress by the highest possible degree, namely, famine; so that some of the most ignorant men began to ask themselves why they should pay for such a government as could permit them to be thus oppressed. At one time a scarcity was followed by plenty, and vice versâ: but both scarcity and plenty seemed now to have left off all attempts at changeableness, so as not to give even this much relief; and the whole country was suffering alike from excessive heat and inundation of water.\n",
      "At such a time it would naturally be expected that something unusual might happen: and surely there never was a more extraordinary occurrence than the one which now took place at a very small village in Devonshire. It seems to have been regarded by many as an omen, and even now, though no doubt some will say so, I should hardly dare to mention it in these days of improved science and knowledge. It is said that an unusual number of people were present; for they were all gathered there at a fair held on the occasion of one of their annual religious meetings, which are still held at various places throughout this country: as in some districts they have been kept up through many generations of men, while it would be difficult to point out any place where such an anniversary has ever been observed with more regularity than in the parish of Nettlecombe-Ferrers. It is also said that most people present had no suspicion whatever as to what was about to take place, but that they were all perfectly well and comfortable. I have heard many persons speak of it with astonishment; some with awe and some with pleasure: yet there are many other persons who remember the occurrence so well that they may be expected to speak of it without any feeling whatever.\n",
      "There was no very remarkable incident connected with this fair as such: though some one or two persons recollect an argument between two men, who came from different sides of a moor which separated their farms, and ended by their coming together in a state of profound friendship; others still say that they had never known such fine weather for the season; and then there are those who know nothing but this one circumstance, which is, I fear, my only proof. It was about five o'clock on Wednesday afternoon: they had been feasting as usual from an hour before sunrise until noon or two, when a little breeze came up at the end of the day, and people began to go away and get ready for their return home; but some were still there in groups under the trees, or sitting together on the turf, as I have known them do many times.\n",
      "Then they all heard a voice—I cannot describe it, unless you should hear that other voice which I suppose might be called a human voice, though very different from ours; but if you ever hear it you will not forget it; and this voice spoke for about half an hour. At first it seemed as if two or three men were speaking: then one alone—as if some of the others had got up and moved away, which was a little difficult to be understood in that strange dialect, as no doubt we\n",
      "llama_print_timings:        load time =    3544.66 ms\n",
      "llama_print_timings:      sample time =     352.17 ms /  1024 runs   (    0.34 ms per token,  2907.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =      94.70 ms /   494 tokens (    0.19 ms per token,  5216.58 tokens per second)\n",
      "llama_print_timings:        eval time =   23139.82 ms /  1023 runs   (   22.62 ms per token,    44.21 tokens per second)\n",
      "llama_print_timings:       total time =   23872.10 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b6f4e-6235-47a1-8942-f8d04e6f1b29",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8088f7bc-2a09-43d7-8940-5721b1af6bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703319018\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   88.03 MiB\n",
      "llm_load_tensors: VRAM used           = 6936.01 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never killed any one; and cutting off the head of a popular general who had never drawn his sword against the republic, or even said any thing against it.\n",
      "A land of mercy indeed! whose lawmaker has so much mercy in him, that he will not allow a power higher than himself to interfere with its execution. \n",
      "#   EARLY MEMORIES OF WILLIAM GODWIN AND HIS HOUSEHOLD\n",
      "Born April 4th, 1794.\n",
      "\"The history of the growth of my mind begins here.\n",
      "\"My father's house was a place of resort to men who were famous in various professions or pursuits; for it is certain that if my father had no other merit he possessed that of being an excellent entertainer, and a converse with him was never unamusing. I well remember Mr. Charles James Fox, Lord Melbourne, Colonel Paget (afterwards Earl Camden) and Dr. Parr were amongst the frequenters of his house; and also Mr. Wilberforce, who in this my earliest recollection is seated upon a high stool in an arm-chair by the side of the fire in our little parlour upstairs, while my father was on one knee before him. But my very first remembrance of men or women whom I saw, and whose persons were not familiar to me from infancy, is of two men who frequented at this time our house – Mr. William Godwin the novelist, and his son, an infant a few months old.\n",
      "\"This child was born after my father had married for the third time. My mother was Miss Harriet Hume, a very worthy woman, but of no very high connexion: she possessed considerable beauty, as her features were beautifully regular, and I well remember that her eyes were large and dark, with black lashes. But she had not been very long in the house before she became my mother – so soon indeed did I become acquainted with a father's name, which till then, when I looked about me for his countenance and protection, I never found – that I was the daughter of such parents; or rather, perhaps, it is better to say, that I felt no sense of disappointment in finding they were not so.\n",
      "\"My earliest recollections of any kind are connected with Mr William Godwin. He was a large, strong man; his hair was thick, and curled naturally: he had an interesting countenance; but his forehead was very high and rather narrow. I never saw him but in black clothes, and though I am not sure that the memory of his dress is distinct from the recollection of his person, I have no doubt about the former. I remember him always as a well-dressed man – for at this time I had scarcely ever seen anyone who did not look as if he wore shabby clothes: and my father was one of those persons whose wardrobe consisted mainly of grey cloth frocks.\n",
      "\"I recollect Mr Godwin well, but whether he was tall or short I cannot say; he is said to have been five feet eleven inches in height, and he certainly did not appear to me much smaller than my father. My mother spoke very highly of him as a friend, and she loved him dearly for his kindness to me. When she was confined with her second child, which proved to be myself – for I had three sisters, of whom two died in infancy, but one survived to womanhood – Mr Godwin called every day. He would then have been a man about thirty years old: he is described by those who knew him later as a very handsome man, and in person he was the handsomest of any I have ever seen.\"\n",
      "Charles Lloyd's recollections were written in 1859, when his own memory of things had faded but that of Mary Godwin was still fresh. He did not remember what Mrs Shelley had told him about her father, nor whether he had met William Godwin himself at Warnham; he had probably only seen the latter when visiting Warnham Park in 1813, with Shelley and his wife Harriet. The old house where Charles Lloyd lived is gone, but the field on which the hut stood still exists: a broad pasture with tall grass and wild flowers, with an enormous oak tree at one end of it and, across the field, a low wooded hillside. It was there that I made my way by footpaths in search of Shelley's Hut; and as I approached along\n",
      "llama_print_timings:        load time =    7670.32 ms\n",
      "llama_print_timings:      sample time =     354.12 ms /  1024 runs   (    0.35 ms per token,  2891.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     221.49 ms /   494 tokens (    0.45 ms per token,  2230.36 tokens per second)\n",
      "llama_print_timings:        eval time =   15077.94 ms /  1023 runs   (   14.74 ms per token,    67.85 tokens per second)\n",
      "llama_print_timings:       total time =   15938.13 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2c6185c-75d7-4a75-914e-e198684b3c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703319043\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   88.03 MiB\n",
      "llm_load_tensors: VRAM used           = 6936.01 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honor to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that these fine exemplary Christians of France, who read Livy, would in the twelfth century have burned the Pagans alive, if they had found them; and it is certain they did so, in the fifteenth and sixteenth, to many thousands of unhappy women, who had the misfortune of being accused by that class of informers who were encouraged by the Inquisition. It is likely enough that some intelligent men fancied there was a little inconsistency between burning people alive, for not believing in witchcraft, and believing in it themselves; and that they were not more inconsistent than any other person in believing anything because they are taught it by accidental authority. These men therefore got together whatever evidence they could collect, out of the old records concerning witches and sorcerers; which consisted principally of the declarations of persons who had been hanged on account of their superstition. Such evidences these were not very likely to have made up a strong case; but as a certain quantity of what these men called diabolical agency was necessary to constitute such offenses, they collected enough to produce conviction in the minds of twelve good Catholic jurymen. Among these witnesses, it is to be hoped that some were not quite so stupid and superstitious as the rest; but it should seem from what they deposed, that their judgments could not have been very much improved by the experience of twelve years’ confinement in an Inquisition prison. One thing seems certain, at all events: namely, that there were more people hanged for these imaginary offenses at one period than ever there was any necessity to hang; and if some of them may be supposed to have been guilty of crimes that would have been punished by the civil magistrate in any age or country, it is certain they could not all have been.\n",
      "In the year 1738, about fourteen years after Defoe wrote his “History of Witchcraft,” there were several people hanged for witchcraft at Edinburgh: one of them was Janet Horne, who had formerly been servant in a family. Her sister was an old woman, of very great natural parts. This old woman had frequently heard her say that if she (the old woman) died before her, she should die within these fourteen years. In process of time she did die: and as this circumstance could not be made to appear otherwise than by a supernatural cause, it was immediately concluded the young woman must have killed her; for in Scotland they had then no other notion of a witch but as one who kills people secretly, and in such a manner that their death cannot be accounted for naturally. When Janet Horne heard she was accused by some of being guilty of this murder, it seems she grew extremely uneasy: however, her sister had left her some part of her effects; among which were some trinkets: upon the accession of these she had made herself some clothes, and as a matter of course, when her sister was dead, they began to say that Janet Horne used to wear such and such things at certain times and seasons. A good deal more of this nature happened; but there is no use in repeating it here. All I have to remark upon the whole story is, that it has all the appearance of being true. The accused was tried by a jury of fifteen: she had not one friend to stand for her life on that trial, and the jury brought her in guilty.\n",
      "\n",
      "At the time this Janet Horne was condemned there were some others charged with the same crime; and among them a man called Patrick Sellar who is well known here by his infamous character. This man was acquitted of all imputations; but the jury recommended him to be brought before another, which was done accordingly. The story against this man was that he had caused several people in the country near Durness to die, by throwing them into a well which was near his house. As the proof here rested entirely upon the evidence of some women who were charged with being witches and convicted for so saying; it must have appeared very strange that this Patrick Sellar should be acquitted whilst Janet Horne was condemned to suffer. But when it came to the consideration whether these people had been thrown into the well by witchcraft or not, there could\n",
      "llama_print_timings:        load time =    1967.25 ms\n",
      "llama_print_timings:      sample time =     352.32 ms /  1024 runs   (    0.34 ms per token,  2906.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     217.52 ms /   494 tokens (    0.44 ms per token,  2271.01 tokens per second)\n",
      "llama_print_timings:        eval time =   15063.16 ms /  1023 runs   (   14.72 ms per token,    67.91 tokens per second)\n",
      "llama_print_timings:       total time =   15916.67 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0b39cd7-6046-4f03-ab13-50efe29377c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703319062\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   88.03 MiB\n",
      "llm_load_tensors: VRAM used           = 6936.01 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init: VRAM kv self = 400.00 MB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 78.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 75.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 7411.01 MiB (model: 6936.01 MiB, context: 475.00 MiB)\n",
      "\n",
      "system_info: n_threads = 32 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that these fine old ecclesiastical doctors of divinity, and their monastic pupils with the water-wills and water-bellies of Rome in their heads, had heard of this poor creature for some days before; had known who he was, and where he lived. It is likely enough that the family were already much alarmed, and had made what preparations they could to soften or turn aside the blow which they apprehended. But what did it avail? They must have been terribly shaken and astonished by the ferocious message from the church, which gave them instant warning that their son was doomed; that no entreaties should or could avail; and that he had only to choose how he would die. Had this young man offended in any way against the church? Did she think a dreadful example must be made of him for some act of this kind? Or were they so empty, vain, and foolish as to fancy, by butchering one miserable youth after another in this barbarous manner, that they should terrify the rest of mankind into submission to their wills and their ways?\n",
      "\n",
      "“These are the things which the church of Rome did; these are the things for the sake of which she so vehemently persecuted the Lollards. Yet, with all her insolence and violence, it is not probable that she should have been able to carry her point but for a much more powerful enemy--an enemy from within, who had for centuries been corrupting both body and soul; an enemy which is now busy undermining the constitution of society and preparing its final fall. This was the unnatural passion of avarice; the rapacious desire of having money without work, in a word, usury or interest-taking; that vile species of robbery which first turned England into a nursery for beggars and impostors, and then set them loose upon society to plunder it with unceasing mischief.\n",
      "\n",
      "“I have been writing this little book,” says Sir Francis, “while the war was raging; and I would here warn all the people who think themselves too wise to be taught anything by those they despise--and I include the priests among them--to look at things a little more closely before they go any further in their reckless zeal against religion. Look round you, I beseech you, and see what is happening. The Lollards are not only being tolerated as if by consent, but they are actually becoming more numerous, thanks to the new teaching of John Wycliffe, which is now spreading with astonishing rapidity; while we see that our enemies the French, instead of following their own preachers and going on crusade against England as they threatened, have been quietly devoting themselves to commerce, making fortunes out of it, and amusing themselves in a thousand other ways. Meanwhile, how is it with us? Have we done anything except sit still while the Lollards were being fed and clothed by their followers all over the country? Is this what King Henry VI., our sovereign lord, gave England the crown for when he had gotten possession of it--this idle life of eating and drinking in taverns at home and abroad? or to be more precise, on the high seas?”\n",
      "\n",
      "It seems that all through these years Henry VI. had been spending most of his time with his merry followers at home in England or abroad; while he had also taken a fancy for sea-voyages, and was frequently absent from court on such expeditions, which lasted several weeks at a stretch. So that when the war broke out between France and England, he could not be found either to fight with his army or sail to sea against his enemy’s; and the people were glad of it.\n",
      "\n",
      "[Illustration: THE CROWN OF ENGLAND.--KING HENRY VI.]\n",
      "\n",
      "This was a sad time for the realm in which the Crown Prince had grown up, for it was found that he could neither govern nor fight--and this is something far worse than having nothing to eat. So after Henry V.’s death and his widow Queen Catherine’s short reign, followed by that of Regent Mortimer and all the fighting and trouble between parties which went on from 1380-5,\n",
      "llama_print_timings:        load time =    1866.50 ms\n",
      "llama_print_timings:      sample time =     357.54 ms /  1024 runs   (    0.35 ms per token,  2863.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.87 ms /   494 tokens (    0.45 ms per token,  2236.63 tokens per second)\n",
      "llama_print_timings:        eval time =   15046.11 ms /  1023 runs   (   14.71 ms per token,    67.99 tokens per second)\n",
      "llama_print_timings:       total time =   15907.82 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb27f0-9bb9-4ee5-b7d6-617348a5e1d5",
   "metadata": {},
   "source": [
    "### 13B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9de77b-77e4-4b90-b88e-a4d54857a2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703319081\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  312.64 MiB\n",
      "llm_load_tensors: VRAM used           = 24514.08 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "..............................................................\n",
      "CUDA error 2 at ggml-cuda.cu:9081: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9081: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab51db-d528-440d-ad00-e9f76d2b4b5d",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c306d621-040a-44e7-bb85-f7e64fcd3785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703319098\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.21 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  114.46 MiB\n",
      "llm_load_tensors: VRAM used           = 17390.64 MiB\n",
      "llm_load_tensors: offloading 60 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 61/61 layers to GPU\n",
      "........................................................................................\n",
      "CUDA error 2 at ggml-cuda.cu:9081: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9081: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b24fab-f886-498f-a9f5-d91bf3d5a189",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1691 (7082d24)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703319799\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080, compute capability 8.9\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.28 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  140.90 MiB\n",
      "llm_load_tensors: VRAM used           = 34950.11 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "............................................\n",
      "CUDA error 2 at ggml-cuda.cu:9081: out of memory\n",
      "current device: 0\n",
      "GGML_ASSERT: ggml-cuda.cu:9081: !\"CUDA error\"\n"
     ]
    }
   ],
   "source": [
    "# Out of memory\n",
    "!./main --color --no-mmap -ngl 10000 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
